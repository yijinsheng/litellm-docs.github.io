"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[38347],{28453(e,n,s){s.d(n,{R:()=>i,x:()=>a});var t=s(96540);const o={},r=t.createContext(o);function i(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),t.createElement(r.Provider,{value:n},e.children)}},47966(e,n,s){s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>i,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"type":"mdx","permalink":"/stream","source":"@site/src/pages/stream.md","title":"Streaming Responses & Async Completion","description":"- Streaming Responses","frontMatter":{},"unlisted":false}');var o=s(74848),r=s(28453);const i={},a="Streaming Responses & Async Completion",l={},c=[{value:"Streaming Responses",id:"streaming-responses",level:2},{value:"Usage",id:"usage",level:3},{value:"Async Completion",id:"async-completion",level:2},{value:"Usage",id:"usage-1",level:3},{value:"Streaming Token Usage",id:"streaming-token-usage",level:2},{value:"SDK",id:"sdk",level:3},{value:"PROXY",id:"proxy",level:3}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"streaming-responses--async-completion",children:"Streaming Responses & Async Completion"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"#streaming-responses",children:"Streaming Responses"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"#async-completion",children:"Async Completion"})}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"streaming-responses",children:"Streaming Responses"}),"\n",(0,o.jsxs)(n.p,{children:["LiteLLM supports streaming the model response back by passing ",(0,o.jsx)(n.code,{children:"stream=True"})," as an argument to the completion function"]}),"\n",(0,o.jsx)(n.h3,{id:"usage",children:"Usage"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"response = completion(model=\"gpt-3.5-turbo\", messages=messages, stream=True)\nfor chunk in response:\n    print(chunk['choices'][0]['delta'])\n\n"})}),"\n",(0,o.jsx)(n.h2,{id:"async-completion",children:"Async Completion"}),"\n",(0,o.jsxs)(n.p,{children:["Asynchronous Completion with LiteLLM\nLiteLLM provides an asynchronous version of the completion function called ",(0,o.jsx)(n.code,{children:"acompletion"})]}),"\n",(0,o.jsx)(n.h3,{id:"usage-1",children:"Usage"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'from litellm import acompletion\nimport asyncio\n\nasync def test_get_response():\n    user_message = "Hello, how are you?"\n    messages = [{"content": user_message, "role": "user"}]\n    response = await acompletion(model="gpt-3.5-turbo", messages=messages)\n    return response\n\nresponse = asyncio.run(test_get_response())\nprint(response)\n\n'})}),"\n",(0,o.jsx)(n.h2,{id:"streaming-token-usage",children:"Streaming Token Usage"}),"\n",(0,o.jsx)(n.p,{children:"Supported across all providers. Works the same as openai."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.code,{children:'stream_options={"include_usage": True}'})}),"\n",(0,o.jsx)(n.p,{children:"If set, an additional chunk will be streamed before the data: [DONE] message. The usage field on this chunk shows the token usage statistics for the entire request, and the choices field will always be an empty array. All other chunks will also include a usage field, but with a null value."}),"\n",(0,o.jsx)(n.h3,{id:"sdk",children:"SDK"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from litellm import completion \nimport os\n\nos.environ["OPENAI_API_KEY"] = "" \n\nresponse = completion(model="gpt-3.5-turbo", messages=messages, stream=True, stream_options={"include_usage": True})\nfor chunk in response:\n    print(chunk[\'choices\'][0][\'delta\'])\n'})}),"\n",(0,o.jsx)(n.h3,{id:"proxy",children:"PROXY"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'curl https://0.0.0.0:4000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $OPENAI_API_KEY" \\\n  -d \'{\n    "model": "gpt-4o",\n    "messages": [\n      {\n        "role": "system",\n        "content": "You are a helpful assistant."\n      },\n      {\n        "role": "user",\n        "content": "Hello!"\n      }\n    ],\n    "stream": true,\n    "stream_options": {"include_usage": true}\n  }\'\n\n'})})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}}}]);