(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[46820],{7227(e,n,s){"use strict";s.d(n,{A:()=>l});s(96540);var i=s(18215);const t="tabItem_Ymn6";var o=s(74848);function l({children:e,hidden:n,className:s}){return(0,o.jsx)("div",{role:"tabpanel",className:(0,i.A)(t,s),hidden:n,children:e})}},49489(e,n,s){"use strict";s.d(n,{A:()=>A});var i=s(96540),t=s(18215),o=s(24245),l=s(56347),r=s(36494),a=s(62814),c=s(45167),d=s(69900);function h(e){return i.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,i.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m(e){const{values:n,children:s}=e;return(0,i.useMemo)(()=>{const e=n??function(e){return h(e).map(({props:{value:e,label:n,attributes:s,default:i}})=>({value:e,label:n,attributes:s,default:i}))}(s);return function(e){const n=(0,c.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,s])}function p({value:e,tabValues:n}){return n.some(n=>n.value===e)}function g({queryString:e=!1,groupId:n}){const s=(0,l.W6)(),t=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,a.aZ)(t),(0,i.useCallback)(e=>{if(!t)return;const n=new URLSearchParams(s.location.search);n.set(t,e),s.replace({...s.location,search:n.toString()})},[t,s])]}function u(e){const{defaultValue:n,queryString:s=!1,groupId:t}=e,o=m(e),[l,a]=(0,i.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const s=n.find(e=>e.default)??n[0];if(!s)throw new Error("Unexpected error: 0 tabValues");return s.value}({defaultValue:n,tabValues:o})),[c,h]=g({queryString:s,groupId:t}),[u,x]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[s,t]=(0,d.Dv)(n);return[s,(0,i.useCallback)(e=>{n&&t.set(e)},[n,t])]}({groupId:t}),j=(()=>{const e=c??u;return p({value:e,tabValues:o})?e:null})();(0,r.A)(()=>{j&&a(j)},[j]);return{selectedValue:l,selectValue:(0,i.useCallback)(e=>{if(!p({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);a(e),h(e),x(e)},[h,x,o]),tabValues:o}}var x=s(11062);const j="tabList__CuJ",f="tabItem_LNqP";var _=s(74848);function y({className:e,block:n,selectedValue:s,selectValue:i,tabValues:l}){const r=[],{blockElementScrollPositionUntilNextRender:a}=(0,o.a_)(),c=e=>{const n=e.currentTarget,t=r.indexOf(n),o=l[t].value;o!==s&&(a(n),i(o))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const s=r.indexOf(e.currentTarget)+1;n=r[s]??r[0];break}case"ArrowLeft":{const s=r.indexOf(e.currentTarget)-1;n=r[s]??r[r.length-1];break}}n?.focus()};return(0,_.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,t.A)("tabs",{"tabs--block":n},e),children:l.map(({value:e,label:n,attributes:i})=>(0,_.jsx)("li",{role:"tab",tabIndex:s===e?0:-1,"aria-selected":s===e,ref:e=>{r.push(e)},onKeyDown:d,onClick:c,...i,className:(0,t.A)("tabs__item",f,i?.className,{"tabs__item--active":s===e}),children:n??e},e))})}function v({lazy:e,children:n,selectedValue:s}){const o=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=o.find(e=>e.props.value===s);return e?(0,i.cloneElement)(e,{className:(0,t.A)("margin-top--md",e.props.className)}):null}return(0,_.jsx)("div",{className:"margin-top--md",children:o.map((e,n)=>(0,i.cloneElement)(e,{key:n,hidden:e.props.value!==s}))})}function b(e){const n=u(e);return(0,_.jsxs)("div",{className:(0,t.A)("tabs-container",j),children:[(0,_.jsx)(y,{...n,...e}),(0,_.jsx)(v,{...n,...e})]})}function A(e){const n=(0,x.A)();return(0,_.jsx)(b,{...e,children:h(e.children)},String(n))}},58998(e,n,s){"use strict";s.r(n),s.d(n,{assets:()=>h,contentTitle:()=>d,default:()=>g,frontMatter:()=>c,metadata:()=>i,toc:()=>m});const i=JSON.parse('{"id":"providers/gemini","title":"Gemini - Google AI Studio","description":"| Property | Details |","source":"@site/docs/providers/gemini.md","sourceDirName":"providers","slug":"/providers/gemini","permalink":"/docs/providers/gemini","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Vertex AI \u4ee3\u7406\u5f15\u64ce","permalink":"/docs/providers/vertex_ai_agent_engine"},"next":{"title":"Gemini \u89c6\u9891\u751f\u6210 (Veo)","permalink":"/docs/providers/gemini/videos"}}');var t=s(74848),o=s(28453),l=s(90547),r=s(49489),a=s(7227);const c={},d="Gemini - Google AI Studio",h={},m=[{value:"API Keys",id:"api-keys",level:2},{value:"Sample Usage",id:"sample-usage",level:2},{value:"Supported OpenAI Params",id:"supported-openai-params",level:2},{value:"Usage - Thinking / <code>reasoning_content</code>",id:"usage---thinking--reasoning_content",level:2},{value:"Gemini 3+ Models - <code>thinking_level</code> Parameter",id:"gemini-3-models---thinking_level-parameter",level:3},{value:"Pass <code>thinking</code> to Gemini models",id:"pass-thinking-to-gemini-models",level:3},{value:"Text-to-Speech (TTS) Audio Output",id:"text-to-speech-tts-audio-output",level:2},{value:"Supported Models",id:"supported-models",level:3},{value:"Limitations",id:"limitations",level:3},{value:"Quick Start",id:"quick-start",level:3},{value:"Advanced Usage",id:"advanced-usage",level:3},{value:"Passing Gemini Specific Params",id:"passing-gemini-specific-params",level:2},{value:"Response schema",id:"response-schema",level:3},{value:"GenerationConfig Params",id:"generationconfig-params",level:3},{value:"Specifying Safety Settings",id:"specifying-safety-settings",level:2},{value:"Tool Calling",id:"tool-calling",level:2},{value:"Google Search Tool",id:"google-search-tool",level:3},{value:"URL Context",id:"url-context",level:3},{value:"Google Search Retrieval",id:"google-search-retrieval",level:3},{value:"Code Execution Tool",id:"code-execution-tool",level:3},{value:"Computer Use Tool",id:"computer-use-tool",level:3},{value:"Environment Mapping",id:"environment-mapping",level:3},{value:"Thought Signatures",id:"thought-signatures",level:2},{value:"How Thought Signatures Work",id:"how-thought-signatures-work",level:3},{value:"Enabling Thought Signatures",id:"enabling-thought-signatures",level:3},{value:"Multi-Turn Function Calling with Thought Signatures",id:"multi-turn-function-calling-with-thought-signatures",level:3},{value:"Important Notes",id:"important-notes",level:3},{value:"JSON Mode",id:"json-mode",level:2},{value:"Image Resolution Control (Gemini 3+)",id:"image-resolution-control-gemini-3",level:2},{value:"Sample Usage",id:"sample-usage-1",level:2},{value:"Usage - PDF / Videos / etc. Files",id:"usage---pdf--videos--etc-files",level:2},{value:"Inline Data (e.g. audio stream)",id:"inline-data-eg-audio-stream",level:3},{value:"https:// file",id:"https-file",level:3},{value:"gs:// file",id:"gs-file",level:3},{value:"Chat Models",id:"chat-models",level:2},{value:"Context Caching",id:"context-caching",level:2},{value:"Custom TTL Support",id:"custom-ttl-support",level:3},{value:"Architecture Diagram",id:"architecture-diagram",level:3},{value:"Example Usage",id:"example-usage",level:3},{value:"Image Generation",id:"image-generation",level:2},{value:"Image Generation Pricing",id:"image-generation-pricing",level:3}];function p(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"gemini---google-ai-studio",children:"Gemini - Google AI Studio"})}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Property"}),(0,t.jsx)(n.th,{children:"Details"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Description"}),(0,t.jsx)(n.td,{children:"Google AI Studio is a fully-managed AI development platform for building and using generative AI."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Provider Route on LiteLLM"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"gemini/"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Provider Doc"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://aistudio.google.com/",children:"Google AI Studio \u2197"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"API Endpoint for Provider"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://generativelanguage.googleapis.com",children:"https://generativelanguage.googleapis.com"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Supported OpenAI Endpoints"}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"/chat/completions"}),", ",(0,t.jsx)(n.a,{href:"../embedding/supported_embedding#gemini-ai-embedding-models",children:(0,t.jsx)(n.code,{children:"/embeddings"})}),", ",(0,t.jsx)(n.code,{children:"/completions"}),", ",(0,t.jsx)(n.a,{href:"/docs/providers/gemini/videos",children:(0,t.jsx)(n.code,{children:"/videos"})}),", ",(0,t.jsx)(n.a,{href:"/docs/image_edits",children:(0,t.jsx)(n.code,{children:"/images/edits"})})]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Pass-through Endpoint"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"/docs/pass_through/google_ai_studio",children:"Supported"})})]})]})]}),"\n",(0,t.jsx)("br",{}),"\n",(0,t.jsx)(n.h2,{id:"api-keys",children:"API Keys"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import os\nos.environ["GEMINI_API_KEY"] = "your-api-key"\n'})}),"\n",(0,t.jsx)(n.h2,{id:"sample-usage",children:"Sample Usage"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\nos.environ[\'GEMINI_API_KEY\'] = ""\nresponse = completion(\n    model="gemini/gemini-pro", \n    messages=[{"role": "user", "content": "write code for saying hi from LiteLLM"}]\n)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"supported-openai-params",children:"Supported OpenAI Params"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"temperature"}),"\n",(0,t.jsx)(n.li,{children:"top_p"}),"\n",(0,t.jsx)(n.li,{children:"max_tokens"}),"\n",(0,t.jsx)(n.li,{children:"max_completion_tokens"}),"\n",(0,t.jsx)(n.li,{children:"stream"}),"\n",(0,t.jsx)(n.li,{children:"tools"}),"\n",(0,t.jsx)(n.li,{children:"tool_choice"}),"\n",(0,t.jsx)(n.li,{children:"functions"}),"\n",(0,t.jsx)(n.li,{children:"response_format"}),"\n",(0,t.jsx)(n.li,{children:"n"}),"\n",(0,t.jsx)(n.li,{children:"stop"}),"\n",(0,t.jsx)(n.li,{children:"logprobs"}),"\n",(0,t.jsx)(n.li,{children:"frequency_penalty"}),"\n",(0,t.jsx)(n.li,{children:"modalities"}),"\n",(0,t.jsx)(n.li,{children:"reasoning_content"}),"\n",(0,t.jsx)(n.li,{children:"audio (for TTS models only)"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Anthropic Params"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"thinking (used to set max budget tokens across anthropic/gemini models)"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://github.com/BerriAI/litellm/blob/main/litellm/llms/gemini/chat/transformation.py#L70",children:(0,t.jsx)(n.strong,{children:"See Updated List"})})}),"\n",(0,t.jsxs)(n.h2,{id:"usage---thinking--reasoning_content",children:["Usage - Thinking / ",(0,t.jsx)(n.code,{children:"reasoning_content"})]}),"\n",(0,t.jsxs)(n.p,{children:["LiteLLM translates OpenAI's ",(0,t.jsx)(n.code,{children:"reasoning_effort"})," to Gemini's ",(0,t.jsx)(n.code,{children:"thinking"})," parameter. ",(0,t.jsx)(n.a,{href:"https://github.com/BerriAI/litellm/blob/620664921902d7a9bfb29897a7b27c1a7ef4ddfb/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py#L362",children:"Code"})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Cost Optimization:"})," Use ",(0,t.jsx)(n.code,{children:'reasoning_effort="none"'})," (OpenAI standard) for significant cost savings - up to 96% cheaper. ",(0,t.jsx)(n.a,{href:"https://ai.google.dev/gemini-api/docs/openai",children:"Google's docs"})]}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.p,{children:"Note: Reasoning cannot be turned off on Gemini 2.5 Pro models."})}),"\n",(0,t.jsx)(n.admonition,{title:"Gemini 3 Models",type:"tip",children:(0,t.jsxs)(n.p,{children:["For ",(0,t.jsx)(n.strong,{children:"Gemini 3+ models"})," (e.g., ",(0,t.jsx)(n.code,{children:"gemini-3-pro-preview"}),"), LiteLLM automatically maps ",(0,t.jsx)(n.code,{children:"reasoning_effort"})," to the new ",(0,t.jsx)(n.code,{children:"thinking_level"})," parameter instead of ",(0,t.jsx)(n.code,{children:"thinking_budget"}),". The ",(0,t.jsx)(n.code,{children:"thinking_level"})," parameter uses ",(0,t.jsx)(n.code,{children:'"low"'})," or ",(0,t.jsx)(n.code,{children:'"high"'})," values for better control over reasoning depth."]})}),"\n",(0,t.jsx)(n.admonition,{title:"Image Models",type:"warning",children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Gemini image models"})," (e.g., ",(0,t.jsx)(n.code,{children:"gemini-3-pro-image-preview"}),", ",(0,t.jsx)(n.code,{children:"gemini-2.0-flash-exp-image-generation"}),") do ",(0,t.jsx)(n.strong,{children:"not"})," support the ",(0,t.jsx)(n.code,{children:"thinking_level"})," parameter. LiteLLM automatically excludes image models from receiving thinking configuration to prevent API errors."]})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mapping for Gemini 2.5 and earlier models"})}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"reasoning_effort"}),(0,t.jsx)(n.th,{children:"thinking"}),(0,t.jsx)(n.th,{children:"Notes"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"none"'}),(0,t.jsx)(n.td,{children:'"budget_tokens": 0, "includeThoughts": false'}),(0,t.jsxs)(n.td,{children:["\ud83d\udcb0 ",(0,t.jsx)(n.strong,{children:"Recommended for cost optimization"})," - OpenAI-compatible, always 0"]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"disable"'}),(0,t.jsx)(n.td,{children:'"budget_tokens": DEFAULT (0), "includeThoughts": false'}),(0,t.jsx)(n.td,{children:"LiteLLM-specific, configurable via env var"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"low"'}),(0,t.jsx)(n.td,{children:'"budget_tokens": 1024'}),(0,t.jsx)(n.td,{})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"medium"'}),(0,t.jsx)(n.td,{children:'"budget_tokens": 2048'}),(0,t.jsx)(n.td,{})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"high"'}),(0,t.jsx)(n.td,{children:'"budget_tokens": 4096'}),(0,t.jsx)(n.td,{})]})]})]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mapping for Gemini 3+ models"})}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"reasoning_effort"}),(0,t.jsx)(n.th,{children:"thinking_level"}),(0,t.jsx)(n.th,{children:"Notes"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"minimal"'}),(0,t.jsx)(n.td,{children:'"low"'}),(0,t.jsx)(n.td,{children:"Minimizes latency and cost"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"low"'}),(0,t.jsx)(n.td,{children:'"low"'}),(0,t.jsx)(n.td,{children:"Best for simple instruction following or chat"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"medium"'}),(0,t.jsx)(n.td,{children:'"high"'}),(0,t.jsx)(n.td,{children:"Maps to high (medium not yet available)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"high"'}),(0,t.jsx)(n.td,{children:'"high"'}),(0,t.jsx)(n.td,{children:"Maximizes reasoning depth"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"disable"'}),(0,t.jsx)(n.td,{children:'"low"'}),(0,t.jsx)(n.td,{children:"Cannot fully disable thinking in Gemini 3"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"none"'}),(0,t.jsx)(n.td,{children:'"low"'}),(0,t.jsx)(n.td,{children:"Cannot fully disable thinking in Gemini 3"})]})]})]}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(a.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion\n\n# Cost-optimized: Use reasoning_effort="none" for best pricing\nresp = completion(\n    model="gemini/gemini-2.0-flash-thinking-exp-01-21",\n    messages=[{"role": "user", "content": "What is the capital of France?"}],\n    reasoning_effort="none",  # Up to 96% cheaper!\n)\n\n# Or use other levels: "low", "medium", "high"\nresp = completion(\n    model="gemini/gemini-2.5-flash-preview-04-17",\n    messages=[{"role": "user", "content": "What is the capital of France?"}],\n    reasoning_effort="low",\n)\n\n'})})}),(0,t.jsxs)(a.A,{value:"proxy",label:"PROXY",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Setup config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"- model_name: gemini-2.5-flash\n  litellm_params:\n    model: gemini/gemini-2.5-flash-preview-04-17\n    api_key: os.environ/GEMINI_API_KEY\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Test it!"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl http://0.0.0.0:4000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer <YOUR-LITELLM-KEY>" \\\n  -d \'{\n    "model": "gemini-2.5-flash",\n    "messages": [{"role": "user", "content": "What is the capital of France?"}],\n    "reasoning_effort": "low"\n  }\'\n'})})]})]}),"\n",(0,t.jsxs)(n.h3,{id:"gemini-3-models---thinking_level-parameter",children:["Gemini 3+ Models - ",(0,t.jsx)(n.code,{children:"thinking_level"})," Parameter"]}),"\n",(0,t.jsxs)(n.p,{children:["For Gemini 3+ models (e.g., ",(0,t.jsx)(n.code,{children:"gemini-3-pro-preview"}),"), you can use the new ",(0,t.jsx)(n.code,{children:"thinking_level"})," parameter directly:"]}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(a.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion\n\n# Use thinking_level for Gemini 3 models\nresp = completion(\n    model="gemini/gemini-3-pro-preview",\n    messages=[{"role": "user", "content": "Solve this complex math problem step by step."}],\n    reasoning_effort="high",  # Options: "low" or "high"\n)\n\n# Low thinking level for faster, simpler tasks\nresp = completion(\n    model="gemini/gemini-3-pro-preview",\n    messages=[{"role": "user", "content": "What is the weather today?"}],\n    reasoning_effort="low",  # Minimizes latency and cost\n)\n'})})}),(0,t.jsx)(a.A,{value:"proxy",label:"PROXY",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl http://0.0.0.0:4000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer <YOUR-LITELLM-KEY>" \\\n  -d \'{\n    "model": "gemini-3-pro-preview",\n    "messages": [{"role": "user", "content": "Solve this complex problem."}],\n    "reasoning_effort": "high"\n  }\'\n'})})})]}),"\n",(0,t.jsxs)(n.admonition,{type:"warning",children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Temperature Recommendation for Gemini 3 Models"})}),(0,t.jsxs)(n.p,{children:["For Gemini 3 models, LiteLLM defaults ",(0,t.jsx)(n.code,{children:"temperature"})," to ",(0,t.jsx)(n.code,{children:"1.0"})," and strongly recommends keeping it at this default. Setting ",(0,t.jsx)(n.code,{children:"temperature < 1.0"})," can cause:"]}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Infinite loops"}),"\n",(0,t.jsx)(n.li,{children:"Degraded reasoning performance"}),"\n",(0,t.jsx)(n.li,{children:"Failure on complex tasks"}),"\n"]}),(0,t.jsxs)(n.p,{children:["LiteLLM will automatically set ",(0,t.jsx)(n.code,{children:"temperature=1.0"})," if not specified for Gemini 3+ models."]})]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Expected Response"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"ModelResponse(\n    id='chatcmpl-c542d76d-f675-4e87-8e5f-05855f5d0f5e',\n    created=1740470510,\n    model='claude-3-7-sonnet-20250219',\n    object='chat.completion',\n    system_fingerprint=None,\n    choices=[\n        Choices(\n            finish_reason='stop',\n            index=0,\n            message=Message(\n                content=\"The capital of France is Paris.\",\n                role='assistant',\n                tool_calls=None,\n                function_call=None,\n                reasoning_content='The capital of France is Paris. This is a very straightforward factual question.'\n            ),\n        )\n    ],\n    usage=Usage(\n        completion_tokens=68,\n        prompt_tokens=42,\n        total_tokens=110,\n        completion_tokens_details=None,\n        prompt_tokens_details=PromptTokensDetailsWrapper(\n            audio_tokens=None,\n            cached_tokens=0,\n            text_tokens=None,\n            image_tokens=None\n        ),\n        cache_creation_input_tokens=0,\n        cache_read_input_tokens=0\n    )\n)\n"})}),"\n",(0,t.jsxs)(n.h3,{id:"pass-thinking-to-gemini-models",children:["Pass ",(0,t.jsx)(n.code,{children:"thinking"})," to Gemini models"]}),"\n",(0,t.jsxs)(n.p,{children:["You can also pass the ",(0,t.jsx)(n.code,{children:"thinking"})," parameter to Gemini models."]}),"\n",(0,t.jsxs)(n.p,{children:["This is translated to Gemini's ",(0,t.jsxs)(n.a,{href:"https://ai.google.dev/gemini-api/docs/thinking#set-budget",children:[(0,t.jsx)(n.code,{children:"thinkingConfig"})," parameter"]}),"."]}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(a.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'response = litellm.completion(\n  model="gemini/gemini-2.5-flash-preview-04-17",\n  messages=[{"role": "user", "content": "What is the capital of France?"}],\n  thinking={"type": "enabled", "budget_tokens": 1024},\n)\n'})})}),(0,t.jsx)(a.A,{value:"proxy",label:"PROXY",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl http://0.0.0.0:4000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer $LITELLM_KEY" \\\n  -d \'{\n    "model": "gemini/gemini-2.5-flash-preview-04-17",\n    "messages": [{"role": "user", "content": "What is the capital of France?"}],\n    "thinking": {"type": "enabled", "budget_tokens": 1024}\n  }\'\n'})})})]}),"\n",(0,t.jsx)(n.h2,{id:"text-to-speech-tts-audio-output",children:"Text-to-Speech (TTS) Audio Output"}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["LiteLLM supports Gemini TTS models that can generate audio responses using the OpenAI-compatible ",(0,t.jsx)(n.code,{children:"audio"})," parameter format."]})}),"\n",(0,t.jsx)(n.h3,{id:"supported-models",children:"Supported Models"}),"\n",(0,t.jsxs)(n.p,{children:["LiteLLM supports Gemini TTS models with audio capabilities (e.g. ",(0,t.jsx)(n.code,{children:"gemini-2.5-flash-preview-tts"})," and ",(0,t.jsx)(n.code,{children:"gemini-2.5-pro-preview-tts"}),"). For the complete list of available TTS models and voices, see the ",(0,t.jsx)(n.a,{href:"https://ai.google.dev/gemini-api/docs/speech-generation",children:"official Gemini TTS documentation"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"limitations",children:"Limitations"}),"\n",(0,t.jsxs)(n.admonition,{type:"warning",children:[(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Important Limitations"}),":"]}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Gemini TTS models only support the ",(0,t.jsx)(n.code,{children:"pcm16"})," audio format"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Streaming support has not been added"})," to TTS models yet"]}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"modalities"})," parameter must be set to ",(0,t.jsx)(n.code,{children:"['audio']"})," for TTS requests"]}),"\n"]})]}),"\n",(0,t.jsx)(n.h3,{id:"quick-start",children:"Quick Start"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(a.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\nos.environ[\'GEMINI_API_KEY\'] = "your-api-key"\n\nresponse = completion(\n    model="gemini/gemini-2.5-flash-preview-tts",\n    messages=[{"role": "user", "content": "Say hello in a friendly voice"}],\n    modalities=["audio"],  # Required for TTS models\n    audio={\n        "voice": "Kore",\n        "format": "pcm16"  # Required: must be "pcm16"\n    }\n)\n\nprint(response)\n'})})}),(0,t.jsxs)(a.A,{value:"proxy",label:"PROXY",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Setup config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: gemini-tts-flash\n    litellm_params:\n      model: gemini/gemini-2.5-flash-preview-tts\n      api_key: os.environ/GEMINI_API_KEY\n  - model_name: gemini-tts-pro\n    litellm_params:\n      model: gemini/gemini-2.5-pro-preview-tts\n      api_key: os.environ/GEMINI_API_KEY\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Make TTS request"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl http://0.0.0.0:4000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer <YOUR-LITELLM-KEY>" \\\n  -d \'{\n    "model": "gemini-tts-flash",\n    "messages": [{"role": "user", "content": "Say hello in a friendly voice"}],\n    "modalities": ["audio"],\n    "audio": {\n      "voice": "Kore",\n      "format": "pcm16"\n    }\n  }\'\n'})})]})]}),"\n",(0,t.jsx)(n.h3,{id:"advanced-usage",children:"Advanced Usage"}),"\n",(0,t.jsx)(n.p,{children:"You can combine TTS with other Gemini features:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'response = completion(\n    model="gemini/gemini-2.5-pro-preview-tts",\n    messages=[\n        {"role": "system", "content": "You are a helpful assistant that speaks clearly."},\n        {"role": "user", "content": "Explain quantum computing in simple terms"}\n    ],\n    modalities=["audio"],\n    audio={\n        "voice": "Charon",\n        "format": "pcm16"\n    },\n    temperature=0.7,\n    max_tokens=150\n)\n'})}),"\n",(0,t.jsxs)(n.p,{children:["For more information about Gemini's TTS capabilities and available voices, see the ",(0,t.jsx)(n.a,{href:"https://ai.google.dev/gemini-api/docs/speech-generation",children:"official Gemini TTS documentation"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"passing-gemini-specific-params",children:"Passing Gemini Specific Params"}),"\n",(0,t.jsx)(n.h3,{id:"response-schema",children:"Response schema"}),"\n",(0,t.jsxs)(n.p,{children:["LiteLLM supports sending ",(0,t.jsx)(n.code,{children:"response_schema"})," as a param for Gemini-1.5-Pro on Google AI Studio."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Response Schema"})}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(a.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion \nimport json \nimport os \n\nos.environ[\'GEMINI_API_KEY\'] = ""\n\nmessages = [\n    {\n        "role": "user",\n        "content": "List 5 popular cookie recipes."\n    }\n]\n\nresponse_schema = {\n        "type": "array",\n        "items": {\n            "type": "object",\n            "properties": {\n                "recipe_name": {\n                    "type": "string",\n                },\n            },\n            "required": ["recipe_name"],\n        },\n    }\n\n\ncompletion(\n    model="gemini/gemini-1.5-pro", \n    messages=messages, \n    response_format={"type": "json_object", "response_schema": response_schema} # \ud83d\udc48 KEY CHANGE\n    )\n\nprint(json.loads(completion.choices[0].message.content))\n'})})}),(0,t.jsxs)(a.A,{value:"proxy",label:"PROXY",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Add model to config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: gemini-pro\n    litellm_params:\n      model: gemini/gemini-1.5-pro\n      api_key: os.environ/GEMINI_API_KEY\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start Proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"$ litellm --config /path/to/config.yaml\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Make Request!"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -X POST \'http://0.0.0.0:4000/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n  "model": "gemini-pro",\n  "messages": [\n        {"role": "user", "content": "List 5 popular cookie recipes."}\n    ],\n  "response_format": {"type": "json_object", "response_schema": { \n        "type": "array",\n        "items": {\n            "type": "object",\n            "properties": {\n                "recipe_name": {\n                    "type": "string",\n                },\n            },\n            "required": ["recipe_name"],\n        },\n    }}\n}\n\'\n'})})]})]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Validate Schema"})}),"\n",(0,t.jsxs)(n.p,{children:["To validate the response_schema, set ",(0,t.jsx)(n.code,{children:"enforce_validation: true"}),"."]}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(a.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion, JSONSchemaValidationError\ntry: \n\tcompletion(\n    model="gemini/gemini-1.5-pro", \n    messages=messages, \n    response_format={\n        "type": "json_object", \n        "response_schema": response_schema,\n        "enforce_validation": true # \ud83d\udc48 KEY CHANGE\n    }\n\t)\nexcept JSONSchemaValidationError as e: \n\tprint("Raw Response: {}".format(e.raw_response))\n\traise e\n'})})}),(0,t.jsxs)(a.A,{value:"proxy",label:"PROXY",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Add model to config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: gemini-pro\n    litellm_params:\n      model: gemini/gemini-1.5-pro\n      api_key: os.environ/GEMINI_API_KEY\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start Proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"$ litellm --config /path/to/config.yaml\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Make Request!"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -X POST \'http://0.0.0.0:4000/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n  "model": "gemini-pro",\n  "messages": [\n        {"role": "user", "content": "List 5 popular cookie recipes."}\n    ],\n  "response_format": {"type": "json_object", "response_schema": { \n        "type": "array",\n        "items": {\n            "type": "object",\n            "properties": {\n                "recipe_name": {\n                    "type": "string",\n                },\n            },\n            "required": ["recipe_name"],\n        },\n    }, \n    "enforce_validation": true\n    }\n}\n\'\n'})})]})]}),"\n",(0,t.jsxs)(n.p,{children:["LiteLLM will validate the response against the schema, and raise a ",(0,t.jsx)(n.code,{children:"JSONSchemaValidationError"})," if the response does not match the schema."]}),"\n",(0,t.jsxs)(n.p,{children:["JSONSchemaValidationError inherits from ",(0,t.jsx)(n.code,{children:"openai.APIError"})]}),"\n",(0,t.jsxs)(n.p,{children:["Access the raw response with ",(0,t.jsx)(n.code,{children:"e.raw_response"})]}),"\n",(0,t.jsx)(n.h3,{id:"generationconfig-params",children:"GenerationConfig Params"}),"\n",(0,t.jsxs)(n.p,{children:["To pass additional GenerationConfig params - e.g. ",(0,t.jsx)(n.code,{children:"topK"}),", just pass it in the request body of the call, and LiteLLM will pass it straight through as a key-value pair in the request body."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://ai.google.dev/api/generate-content#v1beta.GenerationConfig",children:(0,t.jsx)(n.strong,{children:"See Gemini GenerationConfigParams"})})}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(a.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion \nimport json \nimport os \n\nos.environ[\'GEMINI_API_KEY\'] = ""\n\nmessages = [\n    {\n        "role": "user",\n        "content": "List 5 popular cookie recipes."\n    }\n]\n\ncompletion(\n    model="gemini/gemini-1.5-pro", \n    messages=messages, \n    topK=1 # \ud83d\udc48 KEY CHANGE\n)\n\nprint(json.loads(completion.choices[0].message.content))\n'})})}),(0,t.jsxs)(a.A,{value:"proxy",label:"PROXY",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Add model to config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: gemini-pro\n    litellm_params:\n      model: gemini/gemini-1.5-pro\n      api_key: os.environ/GEMINI_API_KEY\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start Proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"$ litellm --config /path/to/config.yaml\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Make Request!"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -X POST \'http://0.0.0.0:4000/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n  "model": "gemini-pro",\n  "messages": [\n        {"role": "user", "content": "List 5 popular cookie recipes."}\n    ],\n  "topK": 1 # \ud83d\udc48 KEY CHANGE\n}\n\'\n'})})]})]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Validate Schema"})}),"\n",(0,t.jsxs)(n.p,{children:["To validate the response_schema, set ",(0,t.jsx)(n.code,{children:"enforce_validation: true"}),"."]}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(a.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion, JSONSchemaValidationError\ntry: \n\tcompletion(\n    model="gemini/gemini-1.5-pro", \n    messages=messages, \n    response_format={\n        "type": "json_object", \n        "response_schema": response_schema,\n        "enforce_validation": true # \ud83d\udc48 KEY CHANGE\n    }\n\t)\nexcept JSONSchemaValidationError as e: \n\tprint("Raw Response: {}".format(e.raw_response))\n\traise e\n'})})}),(0,t.jsxs)(a.A,{value:"proxy",label:"PROXY",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Add model to config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: gemini-pro\n    litellm_params:\n      model: gemini/gemini-1.5-pro\n      api_key: os.environ/GEMINI_API_KEY\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start Proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"$ litellm --config /path/to/config.yaml\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Make Request!"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -X POST \'http://0.0.0.0:4000/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n  "model": "gemini-pro",\n  "messages": [\n        {"role": "user", "content": "List 5 popular cookie recipes."}\n    ],\n  "response_format": {"type": "json_object", "response_schema": { \n        "type": "array",\n        "items": {\n            "type": "object",\n            "properties": {\n                "recipe_name": {\n                    "type": "string",\n                },\n            },\n            "required": ["recipe_name"],\n        },\n    }, \n    "enforce_validation": true\n    }\n}\n\'\n'})})]})]}),"\n",(0,t.jsx)(n.h2,{id:"specifying-safety-settings",children:"Specifying Safety Settings"}),"\n",(0,t.jsxs)(n.p,{children:["In certain use-cases you may need to make calls to the models and pass ",(0,t.jsx)(n.a,{href:"https://ai.google.dev/docs/safety_setting_gemini",children:"safety settings"})," different from the defaults. To do so, simple pass the ",(0,t.jsx)(n.code,{children:"safety_settings"})," argument to ",(0,t.jsx)(n.code,{children:"completion"})," or ",(0,t.jsx)(n.code,{children:"acompletion"}),". For example:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'response = completion(\n    model="gemini/gemini-pro", \n    messages=[{"role": "user", "content": "write code for saying hi from LiteLLM"}],\n    safety_settings=[\n        {\n            "category": "HARM_CATEGORY_HARASSMENT",\n            "threshold": "BLOCK_NONE",\n        },\n        {\n            "category": "HARM_CATEGORY_HATE_SPEECH",\n            "threshold": "BLOCK_NONE",\n        },\n        {\n            "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",\n            "threshold": "BLOCK_NONE",\n        },\n        {\n            "category": "HARM_CATEGORY_DANGEROUS_CONTENT",\n            "threshold": "BLOCK_NONE",\n        },\n    ]\n)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"tool-calling",children:"Tool Calling"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n# set env\nos.environ["GEMINI_API_KEY"] = ".."\n\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "get_current_weather",\n            "description": "Get the current weather in a given location",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "location": {\n                        "type": "string",\n                        "description": "The city and state, e.g. San Francisco, CA",\n                    },\n                    "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},\n                },\n                "required": ["location"],\n            },\n        },\n    }\n]\nmessages = [{"role": "user", "content": "What\'s the weather like in Boston today?"}]\n\nresponse = completion(\n    model="gemini/gemini-1.5-flash",\n    messages=messages,\n    tools=tools,\n)\n# Add any assertions, here to check response args\nprint(response)\nassert isinstance(response.choices[0].message.tool_calls[0].function.name, str)\nassert isinstance(\n    response.choices[0].message.tool_calls[0].function.arguments, str\n)\n\n\n'})}),"\n",(0,t.jsx)(n.h3,{id:"google-search-tool",children:"Google Search Tool"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(a.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\nos.environ["GEMINI_API_KEY"] = ".."\n\ntools = [{"googleSearch": {}}] # \ud83d\udc48 ADD GOOGLE SEARCH\n\nresponse = completion(\n    model="gemini/gemini-2.0-flash",\n    messages=[{"role": "user", "content": "What is the weather in San Francisco?"}],\n    tools=tools,\n)\n\nprint(response)\n'})})}),(0,t.jsxs)(a.A,{value:"proxy",label:"PROXY",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Setup config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: gemini-2.0-flash\n    litellm_params:\n      model: gemini/gemini-2.0-flash\n      api_key: os.environ/GEMINI_API_KEY\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start Proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"$ litellm --config /path/to/config.yaml\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Make Request!"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -X POST \'http://0.0.0.0:4000/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n  "model": "gemini-2.0-flash",\n  "messages": [{"role": "user", "content": "What is the weather in San Francisco?"}],\n  "tools": [{"googleSearch": {}}]\n}\n\'\n'})})]})]}),"\n",(0,t.jsx)(n.h3,{id:"url-context",children:"URL Context"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(a.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\nos.environ["GEMINI_API_KEY"] = ".."\n\n# \ud83d\udc47 ADD URL CONTEXT\ntools = [{"urlContext": {}}]\n\nresponse = completion(\n    model="gemini/gemini-2.0-flash",\n    messages=[{"role": "user", "content": "Summarize this document: https://ai.google.dev/gemini-api/docs/models"}],\n    tools=tools,\n)\n\nprint(response)\n\n# Access URL context metadata\nurl_context_metadata = response.model_extra[\'vertex_ai_url_context_metadata\']\nurlMetadata = url_context_metadata[0][\'urlMetadata\'][0]\nprint(f"Retrieved URL: {urlMetadata[\'retrievedUrl\']}")\nprint(f"Retrieval Status: {urlMetadata[\'urlRetrievalStatus\']}")\n'})})}),(0,t.jsxs)(a.A,{value:"proxy",label:"PROXY",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Setup config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: gemini-2.0-flash\n    litellm_params:\n      model: gemini/gemini-2.0-flash\n      api_key: os.environ/GEMINI_API_KEY\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start Proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"$ litellm --config /path/to/config.yaml\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Make Request!"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -X POST \'http://0.0.0.0:4000/chat/completions\' \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer <YOUR-LITELLM-KEY>" \\\n  -d \'{\n    "model": "gemini-2.0-flash",\n    "messages": [{"role": "user", "content": "Summarize this document: https://ai.google.dev/gemini-api/docs/models"}],\n    "tools": [{"urlContext": {}}]\n  }\'\n'})})]})]}),"\n",(0,t.jsx)(n.h3,{id:"google-search-retrieval",children:"Google Search Retrieval"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(a.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\nos.environ["GEMINI_API_KEY"] = ".."\n\ntools = [{"googleSearch": {}}] # \ud83d\udc48 ADD GOOGLE SEARCH\n\nresponse = completion(\n    model="gemini/gemini-2.0-flash",\n    messages=[{"role": "user", "content": "What is the weather in San Francisco?"}],\n    tools=tools,\n)\n\nprint(response)\n'})})}),(0,t.jsxs)(a.A,{value:"proxy",label:"PROXY",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Setup config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: gemini-2.0-flash\n    litellm_params:\n      model: gemini/gemini-2.0-flash\n      api_key: os.environ/GEMINI_API_KEY\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start Proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"$ litellm --config /path/to/config.yaml\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Make Request!"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -X POST \'http://0.0.0.0:4000/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n  "model": "gemini-2.0-flash",\n  "messages": [{"role": "user", "content": "What is the weather in San Francisco?"}],\n  "tools": [{"googleSearch": {}}]\n}\n\'\n'})})]})]}),"\n",(0,t.jsx)(n.h3,{id:"code-execution-tool",children:"Code Execution Tool"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(a.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\nos.environ["GEMINI_API_KEY"] = ".."\n\ntools = [{"codeExecution": {}}] # \ud83d\udc48 ADD GOOGLE SEARCH\n\nresponse = completion(\n    model="gemini/gemini-2.0-flash",\n    messages=[{"role": "user", "content": "What is the weather in San Francisco?"}],\n    tools=tools,\n)\n\nprint(response)\n'})})}),(0,t.jsxs)(a.A,{value:"proxy",label:"PROXY",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Setup config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: gemini-2.0-flash\n    litellm_params:\n      model: gemini/gemini-2.0-flash\n      api_key: os.environ/GEMINI_API_KEY\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start Proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"$ litellm --config /path/to/config.yaml\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Make Request!"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -X POST \'http://0.0.0.0:4000/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n  "model": "gemini-2.0-flash",\n  "messages": [{"role": "user", "content": "What is the weather in San Francisco?"}],\n  "tools": [{"codeExecution": {}}]\n}\n\'\n'})})]})]}),"\n",(0,t.jsx)(n.h3,{id:"computer-use-tool",children:"Computer Use Tool"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(a.A,{value:"sdk",label:"LiteLLM Python SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\nos.environ["GEMINI_API_KEY"] = "your-api-key"\n\n# Computer Use tool with browser environment\ntools = [\n    {\n        "type": "computer_use",\n        "environment": "browser",  # optional: "browser" or "unspecified"\n        "excluded_predefined_functions": ["drag_and_drop"]  # optional\n    }\n]\n\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "text",\n                "text": "Navigate to google.com and search for \'LiteLLM\'"\n            },\n            {\n                "type": "image_url",\n                "image_url": {\n                    "url": "data:image/png;base64,..."  # screenshot of current browser state\n                }\n            }\n        ]\n    }\n]\n\nresponse = completion(\n    model="gemini/gemini-2.5-computer-use-preview-10-2025",\n    messages=messages,\n    tools=tools,\n)\n\nprint(response)\n\n# Handling tool responses with screenshots\n# When the model makes a tool call, send the response back with a screenshot:\nif response.choices[0].message.tool_calls:\n    tool_call = response.choices[0].message.tool_calls[0]\n    \n    # Add assistant message with tool call\n    messages.append(response.choices[0].message.model_dump())\n    \n    # Add tool response with screenshot\n    messages.append({\n        "role": "tool",\n        "tool_call_id": tool_call.id,\n        "content": [\n            {\n                "type": "text",\n                "text": \'{"url": "https://example.com", "status": "completed"}\'\n            },\n            {\n                "type": "input_image",\n                "image_url": "data:image/png;base64,..."  # New screenshot after action (Can send an image url as well, litellm handles the conversion)\n            }\n        ]\n    })\n    \n    # Continue conversation with updated screenshot\n    response = completion(\n        model="gemini/gemini-2.5-computer-use-preview-10-2025",\n        messages=messages,\n        tools=tools,\n    )\n'})})}),(0,t.jsxs)(a.A,{value:"proxy",label:"LiteLLM Proxy Server",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Add model to config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: gemini-computer-use\n    litellm_params:\n      model: gemini/gemini-2.5-computer-use-preview-10-2025\n      api_key: os.environ/GEMINI_API_KEY\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Make request"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl http://0.0.0.0:4000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer sk-1234" \\\n  -d \'{\n    "model": "gemini-computer-use",\n    "messages": [\n      {\n        "role": "user",\n        "content": [\n          {\n            "type": "text",\n            "text": "Click on the search button"\n          },\n          {\n            "type": "image_url",\n            "image_url": {\n              "url": "data:image/png;base64,..."\n            }\n          }\n        ]\n      }\n    ],\n    "tools": [\n      {\n        "type": "computer_use",\n        "environment": "browser"\n      }\n    ]\n  }\'\n'})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Tool Response Format:"})}),(0,t.jsx)(n.p,{children:"When responding to Computer Use tool calls, include the URL and screenshot:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n  "role": "tool",\n  "tool_call_id": "call_abc123",\n  "content": [\n    {\n      "type": "text",\n      "text": "{\\"url\\": \\"https://example.com\\", \\"status\\": \\"completed\\"}"\n    },\n    {\n      "type": "input_image",\n      "image_url": "data:image/png;base64,..."\n    }\n  ]\n}\n'})})]})]}),"\n",(0,t.jsx)(n.h3,{id:"environment-mapping",children:"Environment Mapping"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"LiteLLM Input"}),(0,t.jsx)(n.th,{children:"Gemini API Value"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'"browser"'})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"ENVIRONMENT_BROWSER"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'"unspecified"'})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"ENVIRONMENT_UNSPECIFIED"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"ENVIRONMENT_BROWSER"})}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"ENVIRONMENT_BROWSER"})," (passed through)"]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"ENVIRONMENT_UNSPECIFIED"})}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"ENVIRONMENT_UNSPECIFIED"})," (passed through)"]})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"thought-signatures",children:"Thought Signatures"}),"\n",(0,t.jsx)(n.p,{children:"Thought signatures are encrypted representations of the model's internal reasoning process for a given turn in a conversation. By passing thought signatures back to the model in subsequent requests, you provide it with the context of its previous thoughts, allowing it to build upon its reasoning and maintain a coherent line of inquiry."}),"\n",(0,t.jsx)(n.p,{children:"Thought signatures are particularly important for multi-turn function calling scenarios where the model needs to maintain context across multiple tool invocations."}),"\n",(0,t.jsx)(n.h3,{id:"how-thought-signatures-work",children:"How Thought Signatures Work"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Function calls with signatures"}),": When Gemini returns a function call, it includes a ",(0,t.jsx)(n.code,{children:"thought_signature"})," in the response"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Preservation"}),": LiteLLM automatically extracts and stores thought signatures in ",(0,t.jsx)(n.code,{children:"provider_specific_fields"})," of tool calls"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Return in conversation history"}),": When you include the assistant's message with tool calls in subsequent requests, LiteLLM automatically preserves and returns the thought signatures to Gemini"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Parallel function calls"}),": Only the first function call in a parallel set has a thought signature"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sequential function calls"}),": Each function call in a multi-step sequence has its own signature"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"enabling-thought-signatures",children:"Enabling Thought Signatures"}),"\n",(0,t.jsx)(n.p,{children:"To enable thought signatures, you need to enable thinking/reasoning:"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(a.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion\n\nresponse = completion(\n    model="gemini/gemini-2.5-flash",\n    messages=[{"role": "user", "content": "What\'s the weather in Tokyo?"}],\n    tools=[...],\n    reasoning_effort="low",  # Enable thinking to get thought signatures\n)\n'})})}),(0,t.jsx)(a.A,{value:"proxy",label:"PROXY",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl http://localhost:4000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer sk-1234" \\\n  -d \'{\n    "model": "gemini-2.5-flash",\n    "messages": [{"role": "user", "content": "What\'\\\'\'s the weather in Tokyo?"}],\n    "tools": [...],\n    "reasoning_effort": "low"\n  }\'\n'})})})]}),"\n",(0,t.jsx)(n.h3,{id:"multi-turn-function-calling-with-thought-signatures",children:"Multi-Turn Function Calling with Thought Signatures"}),"\n",(0,t.jsx)(n.p,{children:"When building conversation history for multi-turn function calling, you must include the thought signatures from previous responses. LiteLLM handles this automatically when you append the full assistant message to your conversation history."}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(a.A,{value:"sdk",label:"OpenAI Client",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\nimport json\n\nclient = OpenAI(api_key="sk-1234", base_url="http://localhost:4000")\n\ndef get_current_temperature(location: str) -> dict:\n    """Gets the current weather temperature for a given location."""\n    return {"temperature": 30, "unit": "celsius"}\n\ndef set_thermostat_temperature(temperature: int) -> dict:\n    """Sets the thermostat to a desired temperature."""\n    return {"status": "success"}\n\nget_weather_declaration = {\n    "name": "get_current_temperature",\n    "description": "Gets the current weather temperature for a given location.",\n    "parameters": {\n        "type": "object",\n        "properties": {"location": {"type": "string"}},\n        "required": ["location"],\n    },\n}\n\nset_thermostat_declaration = {\n    "name": "set_thermostat_temperature",\n    "description": "Sets the thermostat to a desired temperature.",\n    "parameters": {\n        "type": "object",\n        "properties": {"temperature": {"type": "integer"}},\n        "required": ["temperature"],\n    },\n}\n\n# Initial request\nmessages = [\n    {"role": "user", "content": "If it\'s too hot or too cold in London, set the thermostat to a comfortable level."}\n]\n\nresponse = client.chat.completions.create(\n    model="gemini-2.5-flash",\n    messages=messages,\n    tools=[get_weather_declaration, set_thermostat_declaration],\n    reasoning_effort="low"\n)\n\n# Append the assistant\'s message (includes thought signatures automatically)\nmessages.append(response.choices[0].message)\n\n# Execute tool calls and append results\nfor tool_call in response.choices[0].message.tool_calls:\n    if tool_call.function.name == "get_current_temperature":\n        result = get_current_temperature(**json.loads(tool_call.function.arguments))\n        messages.append({\n            "role": "tool",\n            "content": json.dumps(result),\n            "tool_call_id": tool_call.id\n        })\n\n# Second request - thought signatures are automatically preserved\nresponse2 = client.chat.completions.create(\n    model="gemini-2.5-flash",\n    messages=messages,\n    tools=[get_weather_declaration, set_thermostat_declaration],\n    reasoning_effort="low"\n)\n\nprint(response2.choices[0].message.content)\n'})})}),(0,t.jsxs)(a.A,{value:"curl",label:"cURL",children:[(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Step 1: Initial request\ncurl --location \'http://localhost:4000/v1/chat/completions\' \\\n  --header \'Content-Type: application/json\' \\\n  --header \'Authorization: Bearer sk-1234\' \\\n  --data \'{\n    "model": "gemini-2.5-flash",\n    "messages": [\n      {\n        "role": "user",\n        "content": "If it\'\\\'\'s too hot or too cold in London, set the thermostat to a comfortable level."\n      }\n    ],\n    "tools": [\n      {\n        "type": "function",\n        "function": {\n          "name": "get_current_temperature",\n          "description": "Gets the current weather temperature for a given location.",\n          "parameters": {\n            "type": "object",\n            "properties": {\n              "location": {"type": "string"}\n            },\n            "required": ["location"]\n          }\n        }\n      },\n      {\n        "type": "function",\n        "function": {\n          "name": "set_thermostat_temperature",\n          "description": "Sets the thermostat to a desired temperature.",\n          "parameters": {\n            "type": "object",\n            "properties": {\n              "temperature": {"type": "integer"}\n            },\n            "required": ["temperature"]\n          }\n        }\n      }\n    ],\n    "tool_choice": "auto",\n    "reasoning_effort": "low"\n  }\'\n'})}),(0,t.jsxs)(n.p,{children:["The response will include tool calls with thought signatures in ",(0,t.jsx)(n.code,{children:"provider_specific_fields"}),":"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n  "choices": [{\n    "message": {\n      "role": "assistant",\n      "tool_calls": [{\n        "id": "call_abc123",\n        "type": "function",\n        "function": {\n          "name": "get_current_temperature",\n          "arguments": "{\\"location\\": \\"London\\"}"\n        },\n        "index": 0,\n        "provider_specific_fields": {\n          "thought_signature": "CpcHAdHtim9+q4rstcbvQC0ic4x1/vqQlCJWgE+UZ6dTLYGHMMBkF/AxqL5UmP6SY46uYC8t4BTFiXG5zkw6EMJ...=="\n        }\n      }]\n    }\n  }]\n}\n'})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Step 2: Follow-up request with tool response\n# Include the assistant message from Step 1 (with thought signatures in provider_specific_fields)\ncurl --location \'http://localhost:4000/v1/chat/completions\' \\\n  --header \'Content-Type: application/json\' \\\n  --header \'Authorization: Bearer sk-1234\' \\\n  --data \'{\n    "model": "gemini-2.5-flash",\n    "messages": [\n      {\n        "role": "user",\n        "content": "If it\'\\\'\'s too hot or too cold in London, set the thermostat to a comfortable level."\n      },\n      {\n        "role": "assistant",\n        "content": null,\n        "tool_calls": [\n          {\n            "id": "call_c130b9f8c2c042e9b65e39a88245",\n            "type": "function",\n            "function": {\n              "name": "get_current_temperature",\n              "arguments": "{\\"location\\": \\"London\\"}"\n            },\n            "index": 0,\n            "provider_specific_fields": {\n              "thought_signature": "CpcHAdHtim9+q4rstcbvQC0ic4x1/vqQlCJWgE+UZ6dTLYGHMMBkF/AxqL5UmP6SY46uYC8t4BTFiXG5zkw6EMJ...=="\n            }\n          }\n        ]\n      },\n      {\n        "role": "tool",\n        "content": "{\\"temperature\\": 30, \\"unit\\": \\"celsius\\"}",\n        "tool_call_id": "call_c130b9f8c2c042e9b65e39a88245"\n      }\n    ],\n    "tools": [\n      {\n        "type": "function",\n        "function": {\n          "name": "get_current_temperature",\n          "description": "Gets the current weather temperature for a given location.",\n          "parameters": {\n            "type": "object",\n            "properties": {\n              "location": {"type": "string"}\n            },\n            "required": ["location"]\n          }\n        }\n      },\n      {\n        "type": "function",\n        "function": {\n          "name": "set_thermostat_temperature",\n          "description": "Sets the thermostat to a desired temperature.",\n          "parameters": {\n            "type": "object",\n            "properties": {\n              "temperature": {"type": "integer"}\n            },\n            "required": ["temperature"]\n          }\n        }\n      }\n    ],\n    "tool_choice": "auto",\n    "reasoning_effort": "low"\n  }\'\n'})})]})]}),"\n",(0,t.jsx)(n.h3,{id:"important-notes",children:"Important Notes"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Automatic Handling"}),": LiteLLM automatically extracts thought signatures from Gemini responses and preserves them when you include assistant messages in conversation history. You don't need to manually extract or manage them."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Parallel Function Calls"}),": When the model makes parallel function calls, only the first function call will have a thought signature. Subsequent parallel calls won't have signatures."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sequential Function Calls"}),": In multi-step function calling scenarios, each step's first function call will have its own thought signature that must be preserved."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Required for Context"}),": Thought signatures are essential for maintaining reasoning context across multi-turn conversations with function calling. Without them, the model may lose context of its previous reasoning."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Format"}),": Thought signatures are stored in ",(0,t.jsx)(n.code,{children:"provider_specific_fields.thought_signature"})," of tool calls in the response, and are automatically included when you append the assistant message to your conversation history."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Chat Completions Clients"}),": With chat completions clients where you cannot control whether or not the previous assistant message is included as-is (ex langchain's ChatOpenAI), LiteLLM also preserves the thought signature by appending it to the tool call id (",(0,t.jsx)(n.code,{children:"call_123__thought__<thought-signature>"}),") and extracting it back out before sending the outbound request to Gemini."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"json-mode",children:"JSON Mode"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(a.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion \nimport json \nimport os \n\nos.environ[\'GEMINI_API_KEY\'] = ""\n\nmessages = [\n    {\n        "role": "user",\n        "content": "List 5 popular cookie recipes."\n    }\n]\n\n\n\ncompletion(\n    model="gemini/gemini-1.5-pro", \n    messages=messages, \n    response_format={"type": "json_object"} # \ud83d\udc48 KEY CHANGE\n)\n\nprint(json.loads(completion.choices[0].message.content))\n'})})}),(0,t.jsxs)(a.A,{value:"proxy",label:"PROXY",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Add model to config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: gemini-pro\n    litellm_params:\n      model: gemini/gemini-1.5-pro\n      api_key: os.environ/GEMINI_API_KEY\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start Proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"$ litellm --config /path/to/config.yaml\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Make Request!"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -X POST \'http://0.0.0.0:4000/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n  "model": "gemini-pro",\n  "messages": [\n        {"role": "user", "content": "List 5 popular cookie recipes."}\n    ],\n  "response_format": {"type": "json_object"}\n}\n\'\n'})})]})]}),"\n",(0,t.jsx)(n.h1,{id:"gemini-pro-vision",children:"Gemini-Pro-Vision"}),"\n",(0,t.jsxs)(n.p,{children:["LiteLLM Supports the following image types passed in ",(0,t.jsx)(n.code,{children:"url"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Images with direct links - ",(0,t.jsx)(n.a,{href:"https://storage.googleapis.com/github-repo/img/gemini/intro/landmark3.jpg",children:"https://storage.googleapis.com/github-repo/img/gemini/intro/landmark3.jpg"})]}),"\n",(0,t.jsx)(n.li,{children:"Image in local storage - ./localimage.jpeg"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"image-resolution-control-gemini-3",children:"Image Resolution Control (Gemini 3+)"}),"\n",(0,t.jsxs)(n.p,{children:["For Gemini 3+ models, LiteLLM supports per-part media resolution control using OpenAI's ",(0,t.jsx)(n.code,{children:"detail"})," parameter. This allows you to specify different resolution levels for individual images in your request."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:["Supported ",(0,t.jsx)(n.code,{children:"detail"})," values:"]})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:'"low"'})," - Maps to ",(0,t.jsx)(n.code,{children:'media_resolution: "low"'})," (280 tokens for images, 70 tokens per frame for videos)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:'"high"'})," - Maps to ",(0,t.jsx)(n.code,{children:'media_resolution: "high"'})," (1120 tokens for images)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:'"auto"'})," or ",(0,t.jsx)(n.code,{children:"None"})," - Model decides optimal resolution (no ",(0,t.jsx)(n.code,{children:"media_resolution"})," set)"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Usage Example:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion\n\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "image_url",\n                "image_url": {\n                    "url": "https://example.com/chart.png",\n                    "detail": "high"  # High resolution for detailed chart analysis\n                }\n            },\n            {\n                "type": "text",\n                "text": "Analyze this chart"\n            },\n            {\n                "type": "image_url",\n                "image_url": {\n                    "url": "https://example.com/icon.png",\n                    "detail": "low"  # Low resolution for simple icon\n                }\n            }\n        ]\n    }\n]\n\nresponse = completion(\n    model="gemini/gemini-3-pro-preview",\n    messages=messages,\n)\n'})}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Per-Part Resolution:"})," Each image in your request can have its own ",(0,t.jsx)(n.code,{children:"detail"})," setting, allowing mixed-resolution requests (e.g., a high-res chart alongside a low-res icon). This feature is only available for Gemini 3+ models."]})}),"\n",(0,t.jsx)(n.h2,{id:"sample-usage-1",children:"Sample Usage"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import os\nimport litellm\nfrom dotenv import load_dotenv\n\n# Load the environment variables from .env file\nload_dotenv()\nos.environ["GEMINI_API_KEY"] = os.getenv(\'GEMINI_API_KEY\')\n\nprompt = \'Describe the image in a few sentences.\'\n# Note: You can pass here the URL or Path of image directly.\nimage_url = \'https://storage.googleapis.com/github-repo/img/gemini/intro/landmark3.jpg\'\n\n# Create the messages payload according to the documentation\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "text",\n                "text": prompt\n            },\n            {\n                "type": "image_url",\n                "image_url": {"url": image_url}\n            }\n        ]\n    }\n]\n\n# Make the API call to Gemini model\nresponse = litellm.completion(\n    model="gemini/gemini-pro-vision",\n    messages=messages,\n)\n\n# Extract the response content\ncontent = response.get(\'choices\', [{}])[0].get(\'message\', {}).get(\'content\')\n\n# Print the result\nprint(content)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"usage---pdf--videos--etc-files",children:"Usage - PDF / Videos / etc. Files"}),"\n",(0,t.jsx)(n.h3,{id:"inline-data-eg-audio-stream",children:"Inline Data (e.g. audio stream)"}),"\n",(0,t.jsx)(n.p,{children:"LiteLLM follows the OpenAI format and accepts sending inline data as an encoded base64 string."}),"\n",(0,t.jsx)(n.p,{children:"The format to follow is"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"data:<mime_type>;base64,<encoded_data>\n"})}),"\n",(0,t.jsx)(n.p,{children:"** LITELLM CALL **"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import litellm\nfrom pathlib import Path\nimport base64\nimport os\n\nos.environ["GEMINI_API_KEY"] = "" \n\nlitellm.set_verbose = True # \ud83d\udc48 See Raw call \n\naudio_bytes = Path("speech_vertex.mp3").read_bytes()\nencoded_data = base64.b64encode(audio_bytes).decode("utf-8")\nprint("Audio Bytes = {}".format(audio_bytes))\nmodel = "gemini/gemini-1.5-flash"\nresponse = litellm.completion(\n    model=model,\n    messages=[\n        {\n            "role": "user",\n            "content": [\n                {"type": "text", "text": "Please summarize the audio."},\n                {\n                    "type": "file",\n                    "file": {\n                        "file_data": "data:audio/mp3;base64,{}".format(encoded_data), # \ud83d\udc48 SET MIME_TYPE + DATA\n                    }\n                },\n            ],\n        }\n    ],\n)\n'})}),"\n",(0,t.jsx)(n.p,{children:"** Equivalent GOOGLE API CALL **"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Initialize a Gemini model appropriate for your use case.\nmodel = genai.GenerativeModel(\'models/gemini-1.5-flash\')\n\n# Create the prompt.\nprompt = "Please summarize the audio."\n\n# Load the samplesmall.mp3 file into a Python Blob object containing the audio\n# file\'s bytes and then pass the prompt and the audio to Gemini.\nresponse = model.generate_content([\n    prompt,\n    {\n        "mime_type": "audio/mp3",\n        "data": pathlib.Path(\'samplesmall.mp3\').read_bytes()\n    }\n])\n\n# Output Gemini\'s response to the prompt and the inline audio.\nprint(response.text)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"https-file",children:"https:// file"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import litellm\nimport os\n\nos.environ["GEMINI_API_KEY"] = "" \n\nlitellm.set_verbose = True # \ud83d\udc48 See Raw call \n\nmodel = "gemini/gemini-1.5-flash"\nresponse = litellm.completion(\n    model=model,\n    messages=[\n        {\n            "role": "user",\n            "content": [\n                {"type": "text", "text": "Please summarize the file."},\n                {\n                    "type": "file",\n                    "file": {\n                        "file_id": "https://storage...", # \ud83d\udc48 SET THE IMG URL\n                        "format": "application/pdf" # OPTIONAL\n                    }\n                },\n            ],\n        }\n    ],\n)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"gs-file",children:"gs:// file"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import litellm\nimport os\n\nos.environ["GEMINI_API_KEY"] = "" \n\nlitellm.set_verbose = True # \ud83d\udc48 See Raw call \n\nmodel = "gemini/gemini-1.5-flash"\nresponse = litellm.completion(\n    model=model,\n    messages=[\n        {\n            "role": "user",\n            "content": [\n                {"type": "text", "text": "Please summarize the file."},\n                {\n                    "type": "file",\n                    "file": {\n                        "file_id": "gs://storage...", # \ud83d\udc48 SET THE IMG URL\n                        "format": "application/pdf" # OPTIONAL\n                    }\n                },\n            ],\n        }\n    ],\n)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"chat-models",children:"Chat Models"}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:["We support ALL Gemini models, just set ",(0,t.jsx)(n.code,{children:"model=gemini/<any-model-on-gemini>"})," as a prefix when sending litellm requests"]})})}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Model Name"}),(0,t.jsx)(n.th,{children:"Function Call"}),(0,t.jsx)(n.th,{children:"Required OS Variables"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"gemini-pro"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='gemini/gemini-pro', messages)"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"os.environ['GEMINI_API_KEY']"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"gemini-1.5-pro-latest"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='gemini/gemini-1.5-pro-latest', messages)"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"os.environ['GEMINI_API_KEY']"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"gemini-2.0-flash"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='gemini/gemini-2.0-flash', messages)"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"os.environ['GEMINI_API_KEY']"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"gemini-2.0-flash-exp"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='gemini/gemini-2.0-flash-exp', messages)"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"os.environ['GEMINI_API_KEY']"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"gemini-2.0-flash-lite-preview-02-05"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='gemini/gemini-2.0-flash-lite-preview-02-05', messages)"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"os.environ['GEMINI_API_KEY']"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"gemini-2.5-flash-preview-09-2025"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='gemini/gemini-2.5-flash-preview-09-2025', messages)"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"os.environ['GEMINI_API_KEY']"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"gemini-2.5-flash-lite-preview-09-2025"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='gemini/gemini-2.5-flash-lite-preview-09-2025', messages)"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"os.environ['GEMINI_API_KEY']"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"gemini-flash-latest"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='gemini/gemini-flash-latest', messages)"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"os.environ['GEMINI_API_KEY']"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"gemini-flash-lite-latest"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='gemini/gemini-flash-lite-latest', messages)"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"os.environ['GEMINI_API_KEY']"})})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"context-caching",children:"Context Caching"}),"\n",(0,t.jsx)(n.p,{children:"Use Google AI Studio context caching is supported by"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'{\n    {\n        "role": "system",\n        "content": ...,\n        "cache_control": {"type": "ephemeral"} # \ud83d\udc48 KEY CHANGE\n    },\n    ...\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:"in your message content block."}),"\n",(0,t.jsx)(n.h3,{id:"custom-ttl-support",children:"Custom TTL Support"}),"\n",(0,t.jsxs)(n.p,{children:["You can now specify a custom Time-To-Live (TTL) for your cached content using the ",(0,t.jsx)(n.code,{children:"ttl"})," parameter:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'{\n    {\n        "role": "system",\n        "content": ...,\n        "cache_control": {\n            "type": "ephemeral",\n            "ttl": "3600s"  # \ud83d\udc48 Cache for 1 hour\n        }\n    },\n    ...\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"TTL Format Requirements:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Must be a string ending with 's' for seconds"}),"\n",(0,t.jsx)(n.li,{children:"Must contain a positive number (can be decimal)"}),"\n",(0,t.jsxs)(n.li,{children:["Examples: ",(0,t.jsx)(n.code,{children:'"3600s"'})," (1 hour), ",(0,t.jsx)(n.code,{children:'"7200s"'})," (2 hours), ",(0,t.jsx)(n.code,{children:'"1800s"'})," (30 minutes), ",(0,t.jsx)(n.code,{children:'"1.5s"'})," (1.5 seconds)"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"TTL Behavior:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"If multiple cached messages have different TTLs, the first valid TTL encountered will be used"}),"\n",(0,t.jsx)(n.li,{children:"Invalid TTL formats are ignored and the cache will use Google's default expiration time"}),"\n",(0,t.jsx)(n.li,{children:"If no TTL is specified, Google's default cache expiration (approximately 1 hour) applies"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"architecture-diagram",children:"Architecture Diagram"}),"\n",(0,t.jsx)(l.A,{img:s(92737)}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Notes:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://github.com/BerriAI/litellm/blob/main/litellm/llms/vertex_ai/context_caching/vertex_ai_context_caching.py#L255",children:"Relevant code"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Gemini Context Caching only allows 1 block of continuous messages to be cached."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["If multiple non-continuous blocks contain ",(0,t.jsx)(n.code,{children:"cache_control"})," - the first continuous block will be used. (sent to ",(0,t.jsx)(n.code,{children:"/cachedContent"})," in the ",(0,t.jsx)(n.a,{href:"https://ai.google.dev/api/caching#cache_create-SHELL",children:"Gemini format"}),")"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["The raw request to Gemini's ",(0,t.jsx)(n.code,{children:"/generateContent"})," endpoint looks like this:"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -X POST "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-001:generateContent?key=$GOOGLE_API_KEY" \\\n-H \'Content-Type: application/json\' \\\n-d \'{\n      "contents": [\n        {\n          "parts":[{\n            "text": "Please summarize this transcript"\n          }],\n          "role": "user"\n        },\n      ],\n      "cachedContent": "\'$CACHE_NAME\'"\n    }\'\n\n'})}),"\n",(0,t.jsx)(n.h3,{id:"example-usage",children:"Example Usage"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(a.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion \n\nfor _ in range(2): \n    resp = completion(\n        model="gemini/gemini-1.5-pro",\n        messages=[\n        # System Message\n            {\n                "role": "system",\n                "content": [\n                    {\n                        "type": "text",\n                        "text": "Here is the full text of a complex legal agreement" * 4000,\n                        "cache_control": {"type": "ephemeral"}, # \ud83d\udc48 KEY CHANGE\n                    }\n                ],\n            },\n            # marked for caching with the cache_control parameter, so that this checkpoint can read from the previous cache.\n            {\n                "role": "user",\n                "content": [\n                    {\n                        "type": "text",\n                        "text": "What are the key terms and conditions in this agreement?",\n                        "cache_control": {"type": "ephemeral"},\n                    }\n                ],\n            }]\n    )\n\n    print(resp.usage) # \ud83d\udc48 2nd usage block will be less, since cached tokens used\n'})})}),(0,t.jsx)(a.A,{value:"sdk-ttl",label:"SDK with Custom TTL",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion \n\n# Cache for 2 hours (7200 seconds)\nresp = completion(\n    model="gemini/gemini-1.5-pro",\n    messages=[\n        {\n            "role": "system",\n            "content": [\n                {\n                    "type": "text",\n                    "text": "Here is the full text of a complex legal agreement" * 4000,\n                    "cache_control": {\n                        "type": "ephemeral", \n                        "ttl": "7200s"  # \ud83d\udc48 Cache for 2 hours\n                    },\n                }\n            ],\n        },\n        {\n            "role": "user",\n            "content": [\n                {\n                    "type": "text",\n                    "text": "What are the key terms and conditions in this agreement?",\n                    "cache_control": {\n                        "type": "ephemeral",\n                        "ttl": "3600s"  # \ud83d\udc48 This TTL will be ignored (first one is used)\n                    },\n                }\n            ],\n        }\n    ]\n)\n\nprint(resp.usage)\n'})})}),(0,t.jsxs)(a.A,{value:"proxy",label:"PROXY",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Setup config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n    - model_name: gemini-1.5-pro\n      litellm_params:\n        model: gemini/gemini-1.5-pro\n        api_key: os.environ/GEMINI_API_KEY\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Test it!"}),"\n"]}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"/docs/proxy/user_keys#request-format",children:(0,t.jsx)(n.strong,{children:"See Langchain, OpenAI JS, Llamaindex, etc. examples"})})}),(0,t.jsxs)(r.A,{children:[(0,t.jsx)(a.A,{value:"curl",label:"Curl",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl --location \'http://0.0.0.0:4000/chat/completions\' \\\n    --header \'Content-Type: application/json\' \\\n    --data \'{\n    "model": "gemini-1.5-pro",\n    "messages": [\n        # System Message\n            {\n                "role": "system",\n                "content": [\n                    {\n                        "type": "text",\n                        "text": "Here is the full text of a complex legal agreement" * 4000,\n                        "cache_control": {"type": "ephemeral"}, # \ud83d\udc48 KEY CHANGE\n                    }\n                ],\n            },\n            # marked for caching with the cache_control parameter, so that this checkpoint can read from the previous cache.\n            {\n                "role": "user",\n                "content": [\n                    {\n                        "type": "text",\n                        "text": "What are the key terms and conditions in this agreement?",\n                        "cache_control": {"type": "ephemeral"},\n                    }\n                ],\n            }],\n}\'\n'})})}),(0,t.jsx)(a.A,{value:"curl-ttl",label:"Curl with Custom TTL",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl --location \'http://0.0.0.0:4000/chat/completions\' \\\n    --header \'Content-Type: application/json\' \\\n    --data \'{\n    "model": "gemini-1.5-pro",\n    "messages": [\n        {\n            "role": "system",\n            "content": [\n                {\n                    "type": "text",\n                    "text": "Here is the full text of a complex legal agreement" * 4000,\n                    "cache_control": {\n                        "type": "ephemeral",\n                        "ttl": "7200s"\n                    }\n                }\n            ]\n        },\n        {\n            "role": "user",\n            "content": [\n                {\n                    "type": "text",\n                    "text": "What are the key terms and conditions in this agreement?",\n                    "cache_control": {\n                        "type": "ephemeral",\n                        "ttl": "3600s"\n                    }\n                }\n            ]\n        }\n    ]\n}\'\n'})})}),(0,t.jsx)(a.A,{value:"openai-python",label:"OpenAI Python SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import openai\nclient = openai.AsyncOpenAI(\n    api_key="anything",            # litellm proxy api key\n    base_url="http://0.0.0.0:4000" # litellm proxy base url\n)\n\n\nresponse = await client.chat.completions.create(\n    model="gemini-1.5-pro",\n    messages=[\n        {\n            "role": "system",\n            "content": [\n                    {\n                        "type": "text",\n                        "text": "Here is the full text of a complex legal agreement" * 4000,\n                        "cache_control": {"type": "ephemeral"}, # \ud83d\udc48 KEY CHANGE\n                    }\n            ],\n        },\n        {\n            "role": "user",\n            "content": "what are the key terms and conditions in this agreement?",\n        },\n    ]\n)\n\n'})})}),(0,t.jsx)(a.A,{value:"openai-python-ttl",label:"OpenAI Python SDK with TTL",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import openai\nclient = openai.AsyncOpenAI(\n    api_key="anything",            # litellm proxy api key\n    base_url="http://0.0.0.0:4000" # litellm proxy base url\n)\n\nresponse = await client.chat.completions.create(\n    model="gemini-1.5-pro",\n    messages=[\n        {\n            "role": "system",\n            "content": [\n                {\n                    "type": "text",\n                    "text": "Here is the full text of a complex legal agreement" * 4000,\n                    "cache_control": {\n                        "type": "ephemeral",\n                        "ttl": "7200s"  # Cache for 2 hours\n                    }\n                }\n            ],\n        },\n        {\n            "role": "user",\n            "content": "what are the key terms and conditions in this agreement?",\n        },\n    ]\n)\n'})})})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"image-generation",children:"Image Generation"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(a.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion \n\nresponse = completion(\n    model="gemini/gemini-2.0-flash-exp-image-generation",\n    messages=[{"role": "user", "content": "Generate an image of a cat"}],\n    modalities=["image", "text"],\n)\nassert response.choices[0].message.content is not None # "data:image/png;base64,e4rr.."\n'})})}),(0,t.jsxs)(a.A,{value:"proxy",label:"PROXY",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Setup config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: gemini-2.0-flash-exp-image-generation\n    litellm_params:\n      model: gemini/gemini-2.0-flash-exp-image-generation\n      api_key: os.environ/GEMINI_API_KEY\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Test it!"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -L -X POST \'http://localhost:4000/v1/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n    "model": "gemini-2.0-flash-exp-image-generation",\n    "messages": [{"role": "user", "content": "Generate an image of a cat"}],\n    "modalities": ["image", "text"]\n}\'\n'})})]})]}),"\n",(0,t.jsx)(n.h3,{id:"image-generation-pricing",children:"Image Generation Pricing"}),"\n",(0,t.jsxs)(n.p,{children:["Gemini image generation models (like ",(0,t.jsx)(n.code,{children:"gemini-3-pro-image-preview"}),") return ",(0,t.jsx)(n.code,{children:"image_tokens"})," in the response usage. These tokens are priced differently from text tokens:"]}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Token Type"}),(0,t.jsx)(n.th,{children:"Price per 1M tokens"}),(0,t.jsx)(n.th,{children:"Price per token"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Text output"}),(0,t.jsx)(n.td,{children:"$12"}),(0,t.jsx)(n.td,{children:"$0.000012"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Image output"}),(0,t.jsx)(n.td,{children:"$120"}),(0,t.jsx)(n.td,{children:"$0.00012"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"The number of image tokens depends on the output resolution:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Resolution"}),(0,t.jsx)(n.th,{children:"Tokens per image"}),(0,t.jsx)(n.th,{children:"Cost per image"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"1K-2K (1024x1024 to 2048x2048)"}),(0,t.jsx)(n.td,{children:"1,120"}),(0,t.jsx)(n.td,{children:"$0.134"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"4K (4096x4096)"}),(0,t.jsx)(n.td,{children:"2,000"}),(0,t.jsx)(n.td,{children:"$0.24"})]})]})]}),"\n",(0,t.jsxs)(n.p,{children:["LiteLLM automatically calculates costs using ",(0,t.jsx)(n.code,{children:"output_cost_per_image_token"})," from the model pricing configuration."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Example response usage:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n    "completion_tokens_details": {\n        "reasoning_tokens": 225,\n        "text_tokens": 0,\n        "image_tokens": 1120\n    }\n}\n'})}),"\n",(0,t.jsxs)(n.p,{children:["For more details, see ",(0,t.jsx)(n.a,{href:"https://ai.google.dev/gemini-api/docs/pricing",children:"Google's Gemini pricing documentation"}),"."]})]})}function g(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},92737(e,n,s){e.exports={src:Object.assign(Object.create({toString(){return this.src}}),{srcSet:s.p+"assets/ideal-img/gemini_context_caching.3da8f2b.640.png 640w,"+s.p+"assets/ideal-img/gemini_context_caching.1d6e4bf.1884.png 1884w",images:[{path:s.p+"assets/ideal-img/gemini_context_caching.3da8f2b.640.png",width:640,height:477},{path:s.p+"assets/ideal-img/gemini_context_caching.1d6e4bf.1884.png",width:1884,height:1404}],src:s.p+"assets/ideal-img/gemini_context_caching.3da8f2b.640.png",placeholder:void 0,width:640,height:477}),preSrc:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAHCAYAAAAxrNxjAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAf0lEQVR4nFWPVwoEMQxDc4aUSa8QyP0v6EWCzDIfQgY/y7Z6nkd675JSEu+91FrprTV6KYWuAOScCey96Vprcc7RrbWUugWSMXDOeTeEECTG+AcxDdAYwybWIXnOyfoDXnitxRQIGz6J0L0VIIQ0nIANLwhojEGhiS9RA7rf/wBMa1qjmXvzBAAAAABJRU5ErkJggg=="}}}]);