"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[33754],{28453(e,n,o){o.d(n,{R:()=>i,x:()=>a});var t=o(96540);const r={},s=t.createContext(r);function i(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),t.createElement(s.Provider,{value:n},e.children)}},77664(e,n,o){o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>i,metadata:()=>t,toc:()=>p});const t=JSON.parse('{"id":"tutorials/provider_specific_params","title":"provider_specific_params","description":"Setting provider-specific Params","source":"@site/docs/tutorials/provider_specific_params.md","sourceDirName":"tutorials","slug":"/tutorials/provider_specific_params","permalink":"/docs/tutorials/provider_specific_params","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{}}');var r=o(74848),s=o(28453);const i={},a=void 0,c={},p=[{value:"Setting provider-specific Params",id:"setting-provider-specific-params",level:3}];function l(e){const n={code:"code",h3:"h3",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h3,{id:"setting-provider-specific-params",children:"Setting provider-specific Params"}),"\n",(0,r.jsx)(n.p,{children:"Goal: Set max tokens across OpenAI + Cohere"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"1. via completion"})}),"\n",(0,r.jsx)(n.p,{children:"LiteLLM will automatically translate max_tokens to the naming convention followed by that specific model provider."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\n## set ENV variables \nos.environ["OPENAI_API_KEY"] = "your-openai-key" \nos.environ["COHERE_API_KEY"] = "your-cohere-key" \n\nmessages = [{ "content": "Hello, how are you?","role": "user"}]\n\n# openai call\nresponse = completion(model="gpt-3.5-turbo", messages=messages, max_tokens=100)\n\n# cohere call\nresponse = completion(model="command-nightly", messages=messages, max_tokens=100)\nprint(response)\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"2. via provider-specific config"})}),"\n",(0,r.jsxs)(n.p,{children:["For every provider on LiteLLM, we've gotten their specific params (following their naming conventions, etc.). You can just set it for that provider by pulling up that provider via ",(0,r.jsx)(n.code,{children:"litellm.<provider_name>Config"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"All provider configs are typed and have docstrings, so you should see them autocompleted for you in VSCode with an explanation of what it means."}),"\n",(0,r.jsx)(n.p,{children:"Here's an example of setting max tokens through provider configs."})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}}}]);