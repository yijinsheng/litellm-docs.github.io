(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[95194],{7227(e,n,t){"use strict";t.d(n,{A:()=>o});t(96540);var r=t(18215);const s="tabItem_Ymn6";var i=t(74848);function o({children:e,hidden:n,className:t}){return(0,i.jsx)("div",{role:"tabpanel",className:(0,r.A)(s,t),hidden:n,children:e})}},9409(e,n,t){e.exports={src:Object.assign(Object.create({toString(){return this.src}}),{srcSet:t.p+"assets/ideal-img/langfuse_prompt_management_model_config.8ecebc3.640.png 640w,"+t.p+"assets/ideal-img/langfuse_prompt_management_model_config.935405f.1920.png 1920w",images:[{path:t.p+"assets/ideal-img/langfuse_prompt_management_model_config.8ecebc3.640.png",width:640,height:289},{path:t.p+"assets/ideal-img/langfuse_prompt_management_model_config.935405f.1920.png",width:1920,height:868}],src:t.p+"assets/ideal-img/langfuse_prompt_management_model_config.8ecebc3.640.png",placeholder:void 0,width:640,height:289}),preSrc:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAbUlEQVR4nHXNvQ6CMABF4QKmtChQ2vJXFDDi4OL7v90x6QgyfNM9yRXj8sWHD5nqSVV3kOQt8jYhCvtCVgvKPFH1+ldezYjGvzFuo3TbaRzDu50pdIfRnku8P8r0gJjcSlM/sGVAXgOpHuKw9wOuujto/7cApwAAAABJRU5ErkJggg=="}},10563(e,n,t){e.exports={src:Object.assign(Object.create({toString(){return this.src}}),{srcSet:t.p+"assets/ideal-img/prompt_management_architecture_doc.903cef6.640.png 640w,"+t.p+"assets/ideal-img/prompt_management_architecture_doc.95ced58.1920.png 1920w",images:[{path:t.p+"assets/ideal-img/prompt_management_architecture_doc.903cef6.640.png",width:640,height:294},{path:t.p+"assets/ideal-img/prompt_management_architecture_doc.95ced58.1920.png",width:1920,height:883}],src:t.p+"assets/ideal-img/prompt_management_architecture_doc.903cef6.640.png",placeholder:void 0,width:640,height:294}),preSrc:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAn0lEQVR4nC3MMQ7CMAwF0Nwgje2odpqkt2LgGFCGDhwGZYOdpWLpoUCRWhlF6mC9r/8lGwCYAeCJiA8AKCJSxnEsKaXSOkR8IeLNIOJKREpEe7Pve8056zAMioib9765GBH5pJQ0hPALIVRmrjnnGmOsIvJtm4i8DTOvzNw+7YfaPPImIuq9X4xz7tR13WytvVprJ+fc1DzuQkT3GOP5DxqnM4thuDWKAAAAAElFTkSuQmCC"}},15886(e,n,t){"use strict";t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>u,frontMatter:()=>p,metadata:()=>r,toc:()=>m});const r=JSON.parse('{"id":"proxy/prompt_management","title":"Prompt Management","description":"Run experiments or change the specific model (e.g. from gpt-4o to gpt4o-mini finetune) from your prompt management tool (e.g. Langfuse) instead of making changes in the application.","source":"@site/docs/proxy/prompt_management.md","sourceDirName":"proxy","slug":"/proxy/prompt_management","permalink":"/docs/proxy/prompt_management","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"integrationsSidebar","previous":{"title":"LiteLLM \u63d0\u793a\u7ba1\u7406 (GitOps)","permalink":"/docs/proxy/native_litellm_prompt"},"next":{"title":"Arize Phoenix \u63d0\u793a\u7ba1\u7406","permalink":"/docs/proxy/arize_phoenix_prompts"}}');var s=t(74848),i=t(28453),o=t(90547),l=t(49489),a=t(7227);const p={},c="Prompt Management",d={},m=[{value:"Onboarding Prompts via config.yaml",id:"onboarding-prompts-via-configyaml",level:2},{value:"Basic Structure",id:"basic-structure",level:3},{value:"Understanding <code>prompt_integration</code>",id:"understanding-prompt_integration",level:3},{value:"Supported Integrations",id:"supported-integrations",level:3},{value:"Complete Example",id:"complete-example",level:3},{value:"How It Works",id:"how-it-works",level:3},{value:"Using Config-Loaded Prompts",id:"using-config-loaded-prompts",level:3},{value:"Prompt Schema Reference",id:"prompt-schema-reference",level:3},{value:"Notes",id:"notes",level:3},{value:"Quick Start",id:"quick-start",level:2},{value:"How to set model",id:"how-to-set-model",level:2},{value:"Set the model on LiteLLM",id:"set-the-model-on-litellm",level:3},{value:"Set the model in Langfuse",id:"set-the-model-in-langfuse",level:3},{value:"What is &#39;prompt_variables&#39;?",id:"what-is-prompt_variables",level:2},{value:"What is &#39;prompt_id&#39;?",id:"what-is-prompt_id",level:2},{value:"What will the formatted prompt look like?",id:"what-will-the-formatted-prompt-look-like",level:2},{value:"<code>/chat/completions</code> messages",id:"chatcompletions-messages",level:3},{value:"Architectural Overview",id:"architectural-overview",level:2},{value:"API Reference",id:"api-reference",level:2}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"prompt-management",children:"Prompt Management"})}),"\n",(0,s.jsx)(n.p,{children:"Run experiments or change the specific model (e.g. from gpt-4o to gpt4o-mini finetune) from your prompt management tool (e.g. Langfuse) instead of making changes in the application."}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Supported Integrations"}),(0,s.jsx)(n.th,{children:"Link"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Native LiteLLM GitOps (.prompt files)"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"native_litellm_prompt",children:"Get Started"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Langfuse"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"https://langfuse.com/docs/prompts/get-started",children:"Get Started"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Humanloop"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"../observability/humanloop",children:"Get Started"})})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"onboarding-prompts-via-configyaml",children:"Onboarding Prompts via config.yaml"}),"\n",(0,s.jsxs)(n.p,{children:["You can onboard and initialize prompts directly in your ",(0,s.jsx)(n.code,{children:"config.yaml"})," file. This allows you to:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Load prompts at proxy startup"}),"\n",(0,s.jsx)(n.li,{children:"Manage prompts as code alongside your proxy configuration"}),"\n",(0,s.jsx)(n.li,{children:"Use any supported prompt integration (dotprompt, Langfuse, BitBucket, GitLab, custom)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"basic-structure",children:"Basic Structure"}),"\n",(0,s.jsxs)(n.p,{children:["Add a ",(0,s.jsx)(n.code,{children:"prompts"})," field to your config.yaml:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'model_list:\n  - model_name: gpt-4\n    litellm_params:\n      model: openai/gpt-4\n      api_key: os.environ/OPENAI_API_KEY\n\nprompts:\n  - prompt_id: "my_prompt_id"\n    litellm_params:\n      prompt_id: "my_prompt_id"\n      prompt_integration: "dotprompt"  # or langfuse, bitbucket, gitlab, custom\n      # integration-specific parameters below\n'})}),"\n",(0,s.jsxs)(n.h3,{id:"understanding-prompt_integration",children:["Understanding ",(0,s.jsx)(n.code,{children:"prompt_integration"})]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"prompt_integration"})," field determines where and how prompts are loaded:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"dotprompt"})}),": Load from local ",(0,s.jsx)(n.code,{children:".prompt"})," files or inline content"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"langfuse"})}),": Fetch prompts from Langfuse prompt management"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"bitbucket"})}),": Load from BitBucket repository ",(0,s.jsx)(n.code,{children:".prompt"})," files (team-based access control)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"gitlab"})}),": Load from GitLab repository ",(0,s.jsx)(n.code,{children:".prompt"})," files (team-based access control)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"custom"})}),": Use your own custom prompt management implementation"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Each integration has its own configuration parameters and access control mechanisms."}),"\n",(0,s.jsx)(n.h3,{id:"supported-integrations",children:"Supported Integrations"}),"\n",(0,s.jsxs)(l.A,{children:[(0,s.jsxs)(a.A,{value:"dotprompt",label:"DotPrompt (File-based)",children:[(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Option 1: Using a prompt directory"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'prompts:\n  - prompt_id: "hello"\n    litellm_params:\n      prompt_id: "hello"\n      prompt_integration: "dotprompt"\n      prompt_directory: "./prompts"  # Directory containing .prompt files\n\nlitellm_settings:\n  global_prompt_directory: "./prompts"  # Global setting for all dotprompt integrations\n'})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Option 2: Using inline prompt data"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'prompts:\n  - prompt_id: "my_inline_prompt"\n    litellm_params:\n      prompt_id: "my_inline_prompt"\n      prompt_integration: "dotprompt"\n      prompt_data:\n        my_inline_prompt:\n          content: "Hello {{name}}! How can I help you with {{topic}}?"\n          metadata:\n            model: "gpt-4"\n            temperature: 0.7\n            max_tokens: 150\n'})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Option 3: Using dotprompt_content for single prompts"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'prompts:\n  - prompt_id: "simple_prompt"\n    litellm_params:\n      prompt_id: "simple_prompt"\n      prompt_integration: "dotprompt"\n      dotprompt_content: |\n        ---\n        model: gpt-4\n        temperature: 0.7\n        ---\n        System: You are a helpful assistant.\n        \n        User: {{user_message}}\n'})}),(0,s.jsxs)(n.p,{children:["Create ",(0,s.jsx)(n.code,{children:".prompt"})," files in your prompt directory:"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# prompts/hello.prompt\n---\nmodel: gpt-4\ntemperature: 0.7\n---\nSystem: You are a helpful assistant.\n\nUser: {{user_message}}\n"})})]}),(0,s.jsx)(a.A,{value:"langfuse",label:"Langfuse",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'prompts:\n  - prompt_id: "my_langfuse_prompt"\n    litellm_params:\n      prompt_id: "my_langfuse_prompt"\n      prompt_integration: "langfuse"\n      langfuse_public_key: "os.environ/LANGFUSE_PUBLIC_KEY"\n      langfuse_secret_key: "os.environ/LANGFUSE_SECRET_KEY"\n      langfuse_host: "https://cloud.langfuse.com"  # optional\n\nlitellm_settings:\n  langfuse_public_key: "os.environ/LANGFUSE_PUBLIC_KEY"  # Global setting\n  langfuse_secret_key: "os.environ/LANGFUSE_SECRET_KEY"  # Global setting\n'})})}),(0,s.jsxs)(a.A,{value:"bitbucket",label:"BitBucket",children:[(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'prompts:\n  - prompt_id: "my_bitbucket_prompt"\n    litellm_params:\n      prompt_id: "my_bitbucket_prompt"\n      prompt_integration: "bitbucket"\n      bitbucket_workspace: "your-workspace"\n      bitbucket_repository: "your-repo"\n      bitbucket_access_token: "os.environ/BITBUCKET_ACCESS_TOKEN"\n      bitbucket_branch: "main"  # optional, defaults to main\n\nlitellm_settings:\n  global_bitbucket_config:\n    workspace: "your-workspace"\n    repository: "your-repo"\n    access_token: "os.environ/BITBUCKET_ACCESS_TOKEN"\n    branch: "main"\n'})}),(0,s.jsxs)(n.p,{children:["Your BitBucket repository should contain ",(0,s.jsx)(n.code,{children:".prompt"})," files:"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# prompts/my_bitbucket_prompt.prompt\n---\nmodel: gpt-4\ntemperature: 0.7\n---\nSystem: You are a helpful assistant.\n\nUser: {{user_message}}\n"})})]}),(0,s.jsxs)(a.A,{value:"gitlab",label:"GitLab",children:[(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'prompts:\n  - prompt_id: "my_gitlab_prompt"\n    litellm_params:\n      prompt_id: "my_gitlab_prompt"\n      prompt_integration: "gitlab"\n      gitlab_project: "group/sub/repo"\n      gitlab_access_token: "os.environ/GITLAB_ACCESS_TOKEN"\n      gitlab_branch: "main"  # optional\n      gitlab_prompts_path: "prompts"  # optional, defaults to root\n\nlitellm_settings:\n  global_gitlab_config:\n    project: "group/sub/repo"\n    access_token: "os.environ/GITLAB_ACCESS_TOKEN"\n    branch: "main"\n'})}),(0,s.jsxs)(n.p,{children:["Your GitLab repository should contain ",(0,s.jsx)(n.code,{children:".prompt"})," files:"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# prompts/my_gitlab_prompt.prompt\n---\nmodel: gpt-4\ntemperature: 0.7\n---\nSystem: You are a helpful assistant.\n\nUser: {{user_message}}\n"})})]})]}),"\n",(0,s.jsx)(n.h3,{id:"complete-example",children:"Complete Example"}),"\n",(0,s.jsx)(n.p,{children:"Here's a complete example showing multiple prompts with different integrations:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'model_list:\n  - model_name: gpt-4\n    litellm_params:\n      model: openai/gpt-4\n      api_key: os.environ/OPENAI_API_KEY\n\nprompts:\n  # File-based dotprompt\n  - prompt_id: "coding_assistant"\n    litellm_params:\n      prompt_id: "coding_assistant"\n      prompt_integration: "dotprompt"\n      prompt_directory: "./prompts"\n  \n  # Inline dotprompt\n  - prompt_id: "simple_chat"\n    litellm_params:\n      prompt_id: "simple_chat"\n      prompt_integration: "dotprompt"\n      prompt_data:\n        simple_chat:\n          content: "You are a {{personality}} assistant. User: {{message}}"\n          metadata:\n            model: "gpt-4"\n            temperature: 0.8\n  \n  # Langfuse prompt\n  - prompt_id: "langfuse_chat"\n    litellm_params:\n      prompt_id: "langfuse_chat"\n      prompt_integration: "langfuse"\n      langfuse_public_key: "os.environ/LANGFUSE_PUBLIC_KEY"\n      langfuse_secret_key: "os.environ/LANGFUSE_SECRET_KEY"\n\nlitellm_settings:\n  global_prompt_directory: "./prompts"\n'})}),"\n",(0,s.jsx)(n.h3,{id:"how-it-works",children:"How It Works"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"At Startup"}),": When the proxy starts, it reads the ",(0,s.jsx)(n.code,{children:"prompts"})," field from ",(0,s.jsx)(n.code,{children:"config.yaml"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Initialization"}),": Each prompt is initialized based on its ",(0,s.jsx)(n.code,{children:"prompt_integration"})," type"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"In-Memory Storage"}),": Prompts are stored in the ",(0,s.jsx)(n.code,{children:"IN_MEMORY_PROMPT_REGISTRY"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Access"}),": Use these prompts via the ",(0,s.jsx)(n.code,{children:"/v1/chat/completions"})," endpoint with ",(0,s.jsx)(n.code,{children:"prompt_id"})," in the request"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"using-config-loaded-prompts",children:"Using Config-Loaded Prompts"}),"\n",(0,s.jsx)(n.p,{children:"After loading prompts via config.yaml, use them in your API requests:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -L -X POST \'http://0.0.0.0:4000/v1/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n    "model": "gpt-4",\n    "prompt_id": "coding_assistant",\n    "prompt_variables": {\n        "language": "python",\n        "task": "create a web scraper"\n    }\n}\'\n'})}),"\n",(0,s.jsx)(n.h3,{id:"prompt-schema-reference",children:"Prompt Schema Reference"}),"\n",(0,s.jsxs)(n.p,{children:["Each prompt in the ",(0,s.jsx)(n.code,{children:"prompts"})," list requires:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"prompt_id"})})," (string, required): Unique identifier for the prompt"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"litellm_params"})})," (object, required): Configuration for the prompt","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"prompt_id"})})," (string, required): Must match the top-level prompt_id"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"prompt_integration"})})," (string, required): One of: ",(0,s.jsx)(n.code,{children:"dotprompt"}),", ",(0,s.jsx)(n.code,{children:"langfuse"}),", ",(0,s.jsx)(n.code,{children:"bitbucket"}),", ",(0,s.jsx)(n.code,{children:"gitlab"}),", ",(0,s.jsx)(n.code,{children:"custom"})]}),"\n",(0,s.jsx)(n.li,{children:"Additional integration-specific parameters (see tabs above)"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"prompt_info"})})," (object, optional): Metadata about the prompt","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"prompt_type"})})," (string): Defaults to ",(0,s.jsx)(n.code,{children:'"config"'})," for config-loaded prompts"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"notes",children:"Notes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Config-loaded prompts have ",(0,s.jsx)(n.code,{children:'prompt_type: "config"'})," and ",(0,s.jsx)(n.strong,{children:"cannot be updated"})," via the API"]}),"\n",(0,s.jsxs)(n.li,{children:["To update config prompts, modify your ",(0,s.jsx)(n.code,{children:"config.yaml"})," and restart the proxy"]}),"\n",(0,s.jsxs)(n.li,{children:["For dynamic prompts that can be updated via API, use the ",(0,s.jsx)(n.code,{children:"/prompts"})," endpoints instead"]}),"\n",(0,s.jsx)(n.li,{children:"All supported integrations work with config-loaded prompts"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,s.jsxs)(l.A,{children:[(0,s.jsx)(a.A,{value:"sdk",label:"SDK",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import os \nimport litellm\n\nos.environ["LANGFUSE_PUBLIC_KEY"] = "public_key" # [OPTIONAL] set here or in `.completion`\nos.environ["LANGFUSE_SECRET_KEY"] = "secret_key" # [OPTIONAL] set here or in `.completion`\n\nlitellm.set_verbose = True # see raw request to provider\n\nresp = litellm.completion(\n    model="langfuse/gpt-3.5-turbo",\n    prompt_id="test-chat-prompt",\n    prompt_variables={"user_message": "this is used"}, # [OPTIONAL]\n    messages=[{"role": "user", "content": "<IGNORED>"}],\n)\n'})})}),(0,s.jsxs)(a.A,{value:"proxy",label:"PROXY",children:[(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Setup config.yaml"}),"\n"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'model_list:\n  - model_name: my-langfuse-model\n    litellm_params:\n      model: langfuse/openai-model\n      prompt_id: "<langfuse_prompt_id>"\n      api_key: os.environ/OPENAI_API_KEY\n  - model_name: openai-model\n    litellm_params:\n      model: openai/gpt-3.5-turbo\n      api_key: os.environ/OPENAI_API_KEY\n'})}),(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:"Start the proxy"}),"\n"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"litellm --config config.yaml --detailed_debug\n"})}),(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsx)(n.li,{children:"Test it!"}),"\n"]}),(0,s.jsxs)(l.A,{children:[(0,s.jsx)(a.A,{value:"curl",label:"CURL",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -L -X POST \'http://0.0.0.0:4000/v1/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n    "model": "my-langfuse-model",\n    "messages": [\n        {\n            "role": "user",\n            "content": "THIS WILL BE IGNORED"\n        }\n    ],\n    "prompt_variables": {\n        "key": "this is used"\n    }\n}\'\n'})})}),(0,s.jsx)(a.A,{value:"OpenAI Python SDK",label:"OpenAI Python SDK",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\nclient = openai.OpenAI(\n    api_key="anything",\n    base_url="http://0.0.0.0:4000"\n)\n\n# request sent to model set on litellm proxy, `litellm --model`\nresponse = client.chat.completions.create(\n    model="gpt-3.5-turbo",\n    messages = [\n        {\n            "role": "user",\n            "content": "this is a test request, write a short poem"\n        }\n    ],\n    extra_body={\n        "prompt_variables": { # [OPTIONAL]\n            "key": "this is used"\n        }\n    }\n)\n\nprint(response)\n'})})})]})]})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Expected Logs:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"POST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.openai.com/v1/ \\\n-d '{'model': 'gpt-3.5-turbo', 'messages': <YOUR LANGFUSE PROMPT TEMPLATE>}'\n"})}),"\n",(0,s.jsx)(n.h2,{id:"how-to-set-model",children:"How to set model"}),"\n",(0,s.jsx)(n.h3,{id:"set-the-model-on-litellm",children:"Set the model on LiteLLM"}),"\n",(0,s.jsxs)(n.p,{children:["You can do ",(0,s.jsx)(n.code,{children:"langfuse/<litellm_model_name>"})]}),"\n",(0,s.jsxs)(l.A,{children:[(0,s.jsx)(a.A,{value:"sdk",label:"SDK",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'litellm.completion(\n    model="langfuse/gpt-3.5-turbo", # or `langfuse/anthropic/claude-3-5-sonnet`\n    ...\n)\n'})})}),(0,s.jsx)(a.A,{value:"proxy",label:"PROXY",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n      model: langfuse/gpt-3.5-turbo # OR langfuse/anthropic/claude-3-5-sonnet\n      prompt_id: <langfuse_prompt_id>\n      api_key: os.environ/OPENAI_API_KEY\n"})})})]}),"\n",(0,s.jsx)(n.h3,{id:"set-the-model-in-langfuse",children:"Set the model in Langfuse"}),"\n",(0,s.jsx)(n.p,{children:"If the model is specified in the Langfuse config, it will be used."}),"\n",(0,s.jsx)(o.A,{img:t(9409)}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n      model: azure/chatgpt-v-2\n      api_key: os.environ/AZURE_API_KEY\n      api_base: os.environ/AZURE_API_BASE\n"})}),"\n",(0,s.jsx)(n.h2,{id:"what-is-prompt_variables",children:"What is 'prompt_variables'?"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"prompt_variables"}),": A dictionary of variables that will be used to replace parts of the prompt."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"what-is-prompt_id",children:"What is 'prompt_id'?"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"prompt_id"}),": The ID of the prompt that will be used for the request."]}),"\n"]}),"\n",(0,s.jsx)(o.A,{img:t(42230)}),"\n",(0,s.jsx)(n.h2,{id:"what-will-the-formatted-prompt-look-like",children:"What will the formatted prompt look like?"}),"\n",(0,s.jsxs)(n.h3,{id:"chatcompletions-messages",children:[(0,s.jsx)(n.code,{children:"/chat/completions"})," messages"]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"messages"})," field sent in by the client is ignored."]}),"\n",(0,s.jsxs)(n.p,{children:["The Langfuse prompt will replace the ",(0,s.jsx)(n.code,{children:"messages"})," field."]}),"\n",(0,s.jsxs)(n.p,{children:["To replace parts of the prompt, use the ",(0,s.jsx)(n.code,{children:"prompt_variables"})," field. ",(0,s.jsx)(n.a,{href:"https://github.com/BerriAI/litellm/blob/017f83d038f85f93202a083cf334de3544a3af01/litellm/integrations/langfuse/langfuse_prompt_management.py#L127",children:"See how prompt variables are used"})]}),"\n",(0,s.jsx)(n.p,{children:"If the Langfuse prompt is a string, it will be sent as a user message (not all providers support system messages)."}),"\n",(0,s.jsx)(n.p,{children:"If the Langfuse prompt is a list, it will be sent as is (Langfuse chat prompts are OpenAI compatible)."}),"\n",(0,s.jsx)(n.h2,{id:"architectural-overview",children:"Architectural Overview"}),"\n",(0,s.jsx)(o.A,{img:t(10563)}),"\n",(0,s.jsx)(n.h2,{id:"api-reference",children:"API Reference"}),"\n",(0,s.jsxs)(n.p,{children:["These are the params you can pass to the ",(0,s.jsx)(n.code,{children:"litellm.completion"})," function in SDK and ",(0,s.jsx)(n.code,{children:"litellm_params"})," in config.yaml"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"prompt_id: str # required\nprompt_variables: Optional[dict] # optional\nprompt_version: Optional[int] # optional\nlangfuse_public_key: Optional[str] # optional\nlangfuse_secret: Optional[str] # optional\nlangfuse_secret_key: Optional[str] # optional\nlangfuse_host: Optional[str] # optional\n"})})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},42230(e,n,t){e.exports={src:Object.assign(Object.create({toString(){return this.src}}),{srcSet:t.p+"assets/ideal-img/langfuse_prompt_id.050ce42.640.png 640w,"+t.p+"assets/ideal-img/langfuse_prompt_id.103dcd4.1920.png 1920w",images:[{path:t.p+"assets/ideal-img/langfuse_prompt_id.050ce42.640.png",width:640,height:257},{path:t.p+"assets/ideal-img/langfuse_prompt_id.103dcd4.1920.png",width:1920,height:772}],src:t.p+"assets/ideal-img/langfuse_prompt_id.050ce42.640.png",placeholder:void 0,width:640,height:257}),preSrc:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAECAYAAAC3OK7NAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAbUlEQVR4nGXKywrCMBRF0dhWc5tHg1JDGmojhdqJ4P9/3XZgEMHB4sDmqNey85w3xGVaM3Gymc5MNDp+tX1C+ctKHwoSCnooyFC3tk9fUP6648YHbtww5xUJ9+r3eENpP9PZxNFmGkkcdPwnkTdCky+sFUdDGgAAAABJRU5ErkJggg=="}},49489(e,n,t){"use strict";t.d(n,{A:()=>y});var r=t(96540),s=t(18215),i=t(24245),o=t(56347),l=t(36494),a=t(62814),p=t(45167),c=t(69900);function d(e){return r.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m(e){const{values:n,children:t}=e;return(0,r.useMemo)(()=>{const e=n??function(e){return d(e).map(({props:{value:e,label:n,attributes:t,default:r}})=>({value:e,label:n,attributes:t,default:r}))}(t);return function(e){const n=(0,p.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function h({value:e,tabValues:n}){return n.some(n=>n.value===e)}function u({queryString:e=!1,groupId:n}){const t=(0,o.W6)(),s=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,a.aZ)(s),(0,r.useCallback)(e=>{if(!s)return;const n=new URLSearchParams(t.location.search);n.set(s,e),t.replace({...t.location,search:n.toString()})},[s,t])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:s}=e,i=m(e),[o,a]=(0,r.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!h({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:i})),[p,d]=u({queryString:t,groupId:s}),[g,_]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,s]=(0,c.Dv)(n);return[t,(0,r.useCallback)(e=>{n&&s.set(e)},[n,s])]}({groupId:s}),x=(()=>{const e=p??g;return h({value:e,tabValues:i})?e:null})();(0,l.A)(()=>{x&&a(x)},[x]);return{selectedValue:o,selectValue:(0,r.useCallback)(e=>{if(!h({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);a(e),d(e),_(e)},[d,_,i]),tabValues:i}}var _=t(11062);const x="tabList__CuJ",j="tabItem_LNqP";var f=t(74848);function b({className:e,block:n,selectedValue:t,selectValue:r,tabValues:o}){const l=[],{blockElementScrollPositionUntilNextRender:a}=(0,i.a_)(),p=e=>{const n=e.currentTarget,s=l.indexOf(n),i=o[s].value;i!==t&&(a(n),r(i))},c=e=>{let n=null;switch(e.key){case"Enter":p(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return(0,f.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":n},e),children:o.map(({value:e,label:n,attributes:r})=>(0,f.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{l.push(e)},onKeyDown:c,onClick:p,...r,className:(0,s.A)("tabs__item",j,r?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function A({lazy:e,children:n,selectedValue:t}){const i=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=i.find(e=>e.props.value===t);return e?(0,r.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,f.jsx)("div",{className:"margin-top--md",children:i.map((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function v(e){const n=g(e);return(0,f.jsxs)("div",{className:(0,s.A)("tabs-container",x),children:[(0,f.jsx)(b,{...n,...e}),(0,f.jsx)(A,{...n,...e})]})}function y(e){const n=(0,_.A)();return(0,f.jsx)(v,{...e,children:d(e.children)},String(n))}}}]);