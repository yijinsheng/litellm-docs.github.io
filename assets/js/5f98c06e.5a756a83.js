"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[43271],{28453(e,n,o){o.d(n,{R:()=>l,x:()=>i});var t=o(96540);const s={},r=t.createContext(s);function l(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),t.createElement(r.Provider,{value:n},e.children)}},35494(e,n,o){o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>d,frontMatter:()=>l,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"type":"mdx","permalink":"/token_usage","source":"@site/src/pages/token_usage.md","title":"Token Usage","description":"By default LiteLLM returns token usage in all completion requests (See here)","frontMatter":{},"unlisted":false}');var s=o(74848),r=o(28453);const l={},i="Token Usage",c={},a=[{value:"Example Usage",id:"example-usage",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"token-usage",children:"Token Usage"})}),"\n",(0,s.jsxs)(n.p,{children:["By default LiteLLM returns token usage in all completion requests (",(0,s.jsx)(n.a,{href:"https://litellm.readthedocs.io/en/latest/output/",children:"See here"}),")"]}),"\n",(0,s.jsx)(n.p,{children:"However, we also expose 3 public helper functions to calculate token usage across providers:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"token_counter"}),": This returns the number of tokens for a given input - it uses the tokenizer based on the model, and defaults to tiktoken if no model-specific tokenizer is available."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"cost_per_token"}),": This returns the cost (in USD) for prompt (input) and completion (output) tokens. It utilizes our model_cost map which can be found in ",(0,s.jsx)(n.code,{children:"__init__.py"})," and also as a ",(0,s.jsx)(n.a,{href:"https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json",children:"community resource"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"completion_cost"}),": This returns the overall cost (in USD) for a given LLM API Call. It combines ",(0,s.jsx)(n.code,{children:"token_counter"})," and ",(0,s.jsx)(n.code,{children:"cost_per_token"})," to return the cost for that query (counting both cost of input and output)."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"example-usage",children:"Example Usage"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"token_counter"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import token_counter\n\nmessages = [{"role": "user", "content": "Hey, how\'s it going"}]\nprint(token_counter(model="gpt-3.5-turbo", messages=messages))\n'})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"cost_per_token"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import cost_per_token\n\nprompt_tokens =  5\ncompletion_tokens = 10\nprompt_tokens_cost_usd_dollar, completion_tokens_cost_usd_dollar = cost_per_token(model="gpt-3.5-turbo", prompt_tokens=prompt_tokens, completion_tokens=completion_tokens))\n\nprint(prompt_tokens_cost_usd_dollar, completion_tokens_cost_usd_dollar)\n'})}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"completion_cost"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion_cost\n\nprompt = "Hey, how\'s it going"\ncompletion = "Hi, I\'m gpt - I am doing well"\ncost_of_query = completion_cost(model="gpt-3.5-turbo", prompt=prompt, completion=completion))\n\nprint(cost_of_query)\n'})})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}}}]);