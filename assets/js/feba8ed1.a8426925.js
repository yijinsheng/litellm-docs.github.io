"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[68331],{7227(e,n,l){l.d(n,{A:()=>r});l(96540);var a=l(18215);const s="tabItem_Ymn6";var t=l(74848);function r({children:e,hidden:n,className:l}){return(0,t.jsx)("div",{role:"tabpanel",className:(0,a.A)(s,l),hidden:n,children:e})}},49489(e,n,l){l.d(n,{A:()=>v});var a=l(96540),s=l(18215),t=l(24245),r=l(56347),o=l(36494),i=l(62814),c=l(45167),d=l(69900);function p(e){return a.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m(e){const{values:n,children:l}=e;return(0,a.useMemo)(()=>{const e=n??function(e){return p(e).map(({props:{value:e,label:n,attributes:l,default:a}})=>({value:e,label:n,attributes:l,default:a}))}(l);return function(e){const n=(0,c.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,l])}function h({value:e,tabValues:n}){return n.some(n=>n.value===e)}function u({queryString:e=!1,groupId:n}){const l=(0,r.W6)(),s=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,i.aZ)(s),(0,a.useCallback)(e=>{if(!s)return;const n=new URLSearchParams(l.location.search);n.set(s,e),l.replace({...l.location,search:n.toString()})},[s,l])]}function g(e){const{defaultValue:n,queryString:l=!1,groupId:s}=e,t=m(e),[r,i]=(0,a.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!h({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const l=n.find(e=>e.default)??n[0];if(!l)throw new Error("Unexpected error: 0 tabValues");return l.value}({defaultValue:n,tabValues:t})),[c,p]=u({queryString:l,groupId:s}),[g,b]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[l,s]=(0,d.Dv)(n);return[l,(0,a.useCallback)(e=>{n&&s.set(e)},[n,s])]}({groupId:s}),x=(()=>{const e=c??g;return h({value:e,tabValues:t})?e:null})();(0,o.A)(()=>{x&&i(x)},[x]);return{selectedValue:r,selectValue:(0,a.useCallback)(e=>{if(!h({value:e,tabValues:t}))throw new Error(`Can't select invalid tab value=${e}`);i(e),p(e),b(e)},[p,b,t]),tabValues:t}}var b=l(11062);const x="tabList__CuJ",f="tabItem_LNqP";var _=l(74848);function k({className:e,block:n,selectedValue:l,selectValue:a,tabValues:r}){const o=[],{blockElementScrollPositionUntilNextRender:i}=(0,t.a_)(),c=e=>{const n=e.currentTarget,s=o.indexOf(n),t=r[s].value;t!==l&&(i(n),a(t))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const l=o.indexOf(e.currentTarget)+1;n=o[l]??o[0];break}case"ArrowLeft":{const l=o.indexOf(e.currentTarget)-1;n=o[l]??o[o.length-1];break}}n?.focus()};return(0,_.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":n},e),children:r.map(({value:e,label:n,attributes:a})=>(0,_.jsx)("li",{role:"tab",tabIndex:l===e?0:-1,"aria-selected":l===e,ref:e=>{o.push(e)},onKeyDown:d,onClick:c,...a,className:(0,s.A)("tabs__item",f,a?.className,{"tabs__item--active":l===e}),children:n??e},e))})}function j({lazy:e,children:n,selectedValue:l}){const t=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=t.find(e=>e.props.value===l);return e?(0,a.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,_.jsx)("div",{className:"margin-top--md",children:t.map((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==l}))})}function y(e){const n=g(e);return(0,_.jsxs)("div",{className:(0,s.A)("tabs-container",x),children:[(0,_.jsx)(k,{...n,...e}),(0,_.jsx)(j,{...n,...e})]})}function v(e){const n=(0,b.A)();return(0,_.jsx)(y,{...e,children:p(e.children)},String(n))}},66903(e,n,l){l.r(n),l.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>h,frontMatter:()=>i,metadata:()=>a,toc:()=>p});const a=JSON.parse('{"id":"proxy/reliability","title":"Fallbacks","description":"If a call fails after num_retries, fallback to another model group.","source":"@site/docs/proxy/reliability.md","sourceDirName":"proxy","slug":"/proxy/reliability","permalink":"/docs/proxy/reliability","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Budget Routing","permalink":"/docs/proxy/provider_budget_routing"},"next":{"title":"\u6807\u7b7e\u8def\u7531","permalink":"/docs/proxy/tag_routing"}}');var s=l(74848),t=l(28453),r=(l(90547),l(49489)),o=l(7227);const i={},c="Fallbacks",d={},p=[{value:"Quick Start",id:"quick-start",level:2},{value:"1. Setup fallbacks",id:"1-setup-fallbacks",level:3},{value:"2. Start Proxy",id:"2-start-proxy",level:3},{value:"3. Test Fallbacks",id:"3-test-fallbacks",level:3},{value:"Explanation",id:"explanation",level:3},{value:"Client Side Fallbacks",id:"client-side-fallbacks",level:2},{value:"Control Fallback Prompts",id:"control-fallback-prompts",level:3},{value:"Content Policy Violation Fallback",id:"content-policy-violation-fallback",level:2},{value:"Context Window Exceeded Fallback",id:"context-window-exceeded-fallback",level:2},{value:"Advanced",id:"advanced",level:2},{value:"Fallbacks + Retries + Timeouts + Cooldowns",id:"fallbacks--retries--timeouts--cooldowns",level:3},{value:"Fallback to Specific Model ID",id:"fallback-to-specific-model-id",level:3},{value:"Test Fallbacks!",id:"test-fallbacks",level:3},{value:"<strong>Regular Fallbacks</strong>",id:"regular-fallbacks",level:4},{value:"<strong>Content Policy Fallbacks</strong>",id:"content-policy-fallbacks",level:4},{value:"<strong>Context Window Fallbacks</strong>",id:"context-window-fallbacks",level:4},{value:"Context Window Fallbacks (Pre-Call Checks + Fallbacks)",id:"context-window-fallbacks-pre-call-checks--fallbacks",level:3},{value:"Content Policy Fallbacks",id:"content-policy-fallbacks-1",level:3},{value:"Default Fallbacks",id:"default-fallbacks",level:3},{value:"EU-Region Filtering (Pre-Call Checks)",id:"eu-region-filtering-pre-call-checks",level:3},{value:"Setting Fallbacks for Wildcard Models",id:"setting-fallbacks-for-wildcard-models",level:3},{value:"Disable Fallbacks (Per Request/Key)",id:"disable-fallbacks-per-requestkey",level:3}];function m(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"fallbacks",children:"Fallbacks"})}),"\n",(0,s.jsx)(n.p,{children:"If a call fails after num_retries, fallback to another model group."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Quick Start ",(0,s.jsx)(n.a,{href:"/docs/proxy/load_balancing",children:"load balancing"})]}),"\n",(0,s.jsxs)(n.li,{children:["Quick Start ",(0,s.jsx)(n.a,{href:"#client-side-fallbacks",children:"client side fallbacks"})]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Fallbacks are typically done from one ",(0,s.jsx)(n.code,{children:"model_name"})," to another ",(0,s.jsx)(n.code,{children:"model_name"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,s.jsx)(n.h3,{id:"1-setup-fallbacks",children:"1. Setup fallbacks"}),"\n",(0,s.jsx)(n.p,{children:"Key change:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'fallbacks=[{"gpt-3.5-turbo": ["gpt-4"]}]\n'})}),"\n",(0,s.jsxs)(r.A,{children:[(0,s.jsx)(o.A,{value:"sdk",label:"SDK",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import Router \nrouter = Router(\n  model_list=[\n    {\n      "model_name": "gpt-3.5-turbo",\n      "litellm_params": {\n        "model": "azure/<your-deployment-name>",\n        "api_base": "<your-azure-endpoint>",\n        "api_key": "<your-azure-api-key>",\n        "rpm": 6\n      }\n    },\n    {\n      "model_name": "gpt-4",\n      "litellm_params": {\n        "model": "azure/gpt-4-ca",\n        "api_base": "https://my-endpoint-canada-berri992.openai.azure.com/",\n        "api_key": "<your-azure-api-key>",\n        "rpm": 6\n      }\n    }\n  ],\n  fallbacks=[{"gpt-3.5-turbo": ["gpt-4"]}] # \ud83d\udc48 KEY CHANGE\n)\n\n'})})}),(0,s.jsx)(o.A,{value:"proxy",label:"PROXY",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'model_list:\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n      model: azure/<your-deployment-name>\n      api_base: <your-azure-endpoint>\n      api_key: <your-azure-api-key>\n      rpm: 6      # Rate limit for this deployment: in requests per minute (rpm)\n  - model_name: gpt-4\n    litellm_params:\n      model: azure/gpt-4-ca\n      api_base: https://my-endpoint-canada-berri992.openai.azure.com/\n      api_key: <your-azure-api-key>\n      rpm: 6\n\nrouter_settings:\n  fallbacks: [{"gpt-3.5-turbo": ["gpt-4"]}]\n'})})})]}),"\n",(0,s.jsx)(n.h3,{id:"2-start-proxy",children:"2. Start Proxy"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-test-fallbacks",children:"3. Test Fallbacks"}),"\n",(0,s.jsxs)(n.p,{children:["Pass ",(0,s.jsx)(n.code,{children:"mock_testing_fallbacks=true"})," in request body, to trigger fallbacks."]}),"\n",(0,s.jsxs)(r.A,{children:[(0,s.jsx)(o.A,{value:"sdk",label:"SDK",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'\nfrom litellm import Router\n\nmodel_list = [{..}, {..}] # defined in Step 1.\n\nrouter = Router(model_list=model_list, fallbacks=[{"bad-model": ["my-good-model"]}])\n\nresponse = router.completion(\n  model="bad-model",\n  messages=[{"role": "user", "content": "Hey, how\'s it going?"}],\n  mock_testing_fallbacks=True,\n)\n'})})}),(0,s.jsx)(o.A,{value:"proxy",label:"PROXY",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -X POST \'http://0.0.0.0:4000/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n  "model": "my-bad-model",\n  "messages": [\n    {\n      "role": "user",\n      "content": "ping"\n    }\n  ],\n  "mock_testing_fallbacks": true # \ud83d\udc48 KEY CHANGE\n}\n\'\n'})})})]}),"\n",(0,s.jsx)(n.h3,{id:"explanation",children:"Explanation"}),"\n",(0,s.jsx)(n.p,{children:'Fallbacks are done in-order - ["gpt-3.5-turbo, "gpt-4", "gpt-4-32k"], will do \'gpt-3.5-turbo\' first, then \'gpt-4\', etc.'}),"\n",(0,s.jsxs)(n.p,{children:["You can also set ",(0,s.jsx)(n.a,{href:"#default-fallbacks",children:(0,s.jsx)(n.code,{children:"default_fallbacks"})}),", in case a specific model group is misconfigured / bad."]}),"\n",(0,s.jsx)(n.p,{children:"There are 3 types of fallbacks:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"content_policy_fallbacks"}),": For litellm.ContentPolicyViolationError - LiteLLM maps content policy violation errors across providers ",(0,s.jsx)(n.a,{href:"https://github.com/BerriAI/litellm/blob/89a43c872a1e3084519fb9de159bf52f5447c6c4/litellm/utils.py#L8495C27-L8495C54",children:(0,s.jsx)(n.strong,{children:"See Code"})})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"context_window_fallbacks"}),": For litellm.ContextWindowExceededErrors - LiteLLM maps context window error messages across providers ",(0,s.jsx)(n.a,{href:"https://github.com/BerriAI/litellm/blob/89a43c872a1e3084519fb9de159bf52f5447c6c4/litellm/utils.py#L8469",children:(0,s.jsx)(n.strong,{children:"See Code"})})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"fallbacks"}),": For all remaining errors - e.g. litellm.RateLimitError"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"client-side-fallbacks",children:"Client Side Fallbacks"}),"\n",(0,s.jsxs)(n.p,{children:["Set fallbacks in the ",(0,s.jsx)(n.code,{children:".completion()"})," call for SDK and client-side for proxy."]}),"\n",(0,s.jsx)(n.p,{children:"In this request the following will occur:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["The request to ",(0,s.jsx)(n.code,{children:'model="zephyr-beta"'})," will fail"]}),"\n",(0,s.jsxs)(n.li,{children:["litellm proxy will loop through all the model_groups specified in ",(0,s.jsx)(n.code,{children:'fallbacks=["gpt-3.5-turbo"]'})]}),"\n",(0,s.jsxs)(n.li,{children:["The request to ",(0,s.jsx)(n.code,{children:'model="gpt-3.5-turbo"'})," will succeed and the client making the request will get a response from gpt-3.5-turbo"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["\ud83d\udc49 Key Change: ",(0,s.jsx)(n.code,{children:'"fallbacks": ["gpt-3.5-turbo"]'})]}),"\n",(0,s.jsxs)(r.A,{children:[(0,s.jsx)(o.A,{value:"sdk",label:"SDK",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import Router\n\nrouter = Router(model_list=[..]) # defined in Step 1.\n\nresp = router.completion(\n    model="gpt-3.5-turbo",\n    messages=[{"role": "user", "content": "Hey, how\'s it going?"}],\n    mock_testing_fallbacks=True, # \ud83d\udc48 trigger fallbacks\n    fallbacks=[\n        {\n            "model": "claude-3-haiku",\n            "messages": [{"role": "user", "content": "What is LiteLLM?"}],\n        }\n    ],\n)\n\nprint(resp)\n'})})}),(0,s.jsx)(o.A,{value:"proxy",label:"PROXY",children:(0,s.jsxs)(r.A,{children:[(0,s.jsx)(o.A,{value:"openai",label:"OpenAI Python v1.0.0+",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\nclient = openai.OpenAI(\n    api_key="anything",\n    base_url="http://0.0.0.0:4000"\n)\n\nresponse = client.chat.completions.create(\n    model="zephyr-beta",\n    messages = [\n        {\n            "role": "user",\n            "content": "this is a test request, write a short poem"\n        }\n    ],\n    extra_body={\n        "fallbacks": ["gpt-3.5-turbo"]\n    }\n)\n\nprint(response)\n'})})}),(0,s.jsx)(o.A,{value:"Curl",label:"Curl Request",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'curl --location \'http://0.0.0.0:4000/chat/completions\' \\\n    --header \'Content-Type: application/json\' \\\n    --data \'{\n    "model": "zephyr-beta"",\n    "messages": [\n        {\n        "role": "user",\n        "content": "what llm are you"\n        }\n    ],\n    "fallbacks": ["gpt-3.5-turbo"]\n}\'\n'})})}),(0,s.jsx)(o.A,{value:"langchain",label:"Langchain",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate,\n)\nfrom langchain.schema import HumanMessage, SystemMessage\nimport os \n\nos.environ["OPENAI_API_KEY"] = "anything"\n\nchat = ChatOpenAI(\n    openai_api_base="http://0.0.0.0:4000",\n    model="zephyr-beta",\n    extra_body={\n        "fallbacks": ["gpt-3.5-turbo"]\n    }\n)\n\nmessages = [\n    SystemMessage(\n        content="You are a helpful assistant that im using to make a test request to."\n    ),\n    HumanMessage(\n        content="test from litellm. tell me why it\'s amazing in 1 sentence"\n    ),\n]\nresponse = chat(messages)\n\nprint(response)\n'})})})]})})]}),"\n",(0,s.jsx)(n.h3,{id:"control-fallback-prompts",children:"Control Fallback Prompts"}),"\n",(0,s.jsx)(n.p,{children:"Pass in messages/temperature/etc. per model in fallback (works for embedding/image generation/etc. as well)."}),"\n",(0,s.jsx)(n.p,{children:"Key Change:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'fallbacks = [\n  {\n    "model": <model_name>,\n    "messages": <model-specific-messages>\n    ... # any other model-specific parameters\n  }\n]\n'})}),"\n",(0,s.jsxs)(r.A,{children:[(0,s.jsx)(o.A,{value:"sdk",label:"SDK",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import Router\n\nrouter = Router(model_list=[..]) # defined in Step 1.\n\nresp = router.completion(\n    model="gpt-3.5-turbo",\n    messages=[{"role": "user", "content": "Hey, how\'s it going?"}],\n    mock_testing_fallbacks=True, # \ud83d\udc48 trigger fallbacks\n    fallbacks=[\n        {\n            "model": "claude-3-haiku",\n            "messages": [{"role": "user", "content": "What is LiteLLM?"}],\n        }\n    ],\n)\n\nprint(resp)\n'})})}),(0,s.jsx)(o.A,{value:"proxy",label:"PROXY",children:(0,s.jsxs)(r.A,{children:[(0,s.jsx)(o.A,{value:"openai",label:"OpenAI Python v1.0.0+",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\nclient = openai.OpenAI(\n    api_key="anything",\n    base_url="http://0.0.0.0:4000"\n)\n\nresponse = client.chat.completions.create(\n    model="zephyr-beta",\n    messages = [\n        {\n            "role": "user",\n            "content": "this is a test request, write a short poem"\n        }\n    ],\n    extra_body={\n      "fallbacks": [{\n          "model": "claude-3-haiku",\n          "messages": [{"role": "user", "content": "What is LiteLLM?"}]\n      }]\n    }\n)\n\nprint(response)\n'})})}),(0,s.jsx)(o.A,{value:"Curl",label:"Curl Request",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -L -X POST \'http://0.0.0.0:4000/v1/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n    "model": "gpt-3.5-turbo",\n    "messages": [\n      {\n        "role": "user",\n        "content": [\n          {\n            "type": "text",\n            "text": "Hi, how are you ?"\n          }\n        ]\n      }\n    ],\n    "fallbacks": [{\n        "model": "claude-3-haiku",\n        "messages": [{"role": "user", "content": "What is LiteLLM?"}]\n    }],\n    "mock_testing_fallbacks": true\n}\'\n'})})}),(0,s.jsx)(o.A,{value:"langchain",label:"Langchain",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate,\n)\nfrom langchain.schema import HumanMessage, SystemMessage\nimport os \n\nos.environ["OPENAI_API_KEY"] = "anything"\n\nchat = ChatOpenAI(\n    openai_api_base="http://0.0.0.0:4000",\n    model="zephyr-beta",\n    extra_body={\n      "fallbacks": [{\n          "model": "claude-3-haiku",\n          "messages": [{"role": "user", "content": "What is LiteLLM?"}]\n      }]\n    }\n)\n\nmessages = [\n    SystemMessage(\n        content="You are a helpful assistant that im using to make a test request to."\n    ),\n    HumanMessage(\n        content="test from litellm. tell me why it\'s amazing in 1 sentence"\n    ),\n]\nresponse = chat(messages)\n\nprint(response)\n'})})})]})})]}),"\n",(0,s.jsx)(n.h2,{id:"content-policy-violation-fallback",children:"Content Policy Violation Fallback"}),"\n",(0,s.jsx)(n.p,{children:"Key change:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'content_policy_fallbacks=[{"claude-2": ["my-fallback-model"]}]\n'})}),"\n",(0,s.jsxs)(r.A,{children:[(0,s.jsx)(o.A,{value:"sdk",label:"SDK",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import Router \n\nrouter = Router(\n  model_list=[\n    {\n      "model_name": "claude-2",\n      "litellm_params": {\n        "model": "claude-2",\n        "api_key": "",\n        "mock_response": Exception("content filtering policy"),\n      },\n    },\n    {\n      "model_name": "my-fallback-model",\n      "litellm_params": {\n        "model": "claude-2",\n        "api_key": "",\n        "mock_response": "This works!",\n      },\n    },\n  ],\n  content_policy_fallbacks=[{"claude-2": ["my-fallback-model"]}], # \ud83d\udc48 KEY CHANGE\n  # fallbacks=[..], # [OPTIONAL]\n  # context_window_fallbacks=[..], # [OPTIONAL]\n)\n\nresponse = router.completion(\n  model="claude-2",\n  messages=[{"role": "user", "content": "Hey, how\'s it going?"}],\n)\n'})})}),(0,s.jsxs)(o.A,{value:"proxy",label:"PROXY",children:[(0,s.jsx)(n.p,{children:"In your proxy config.yaml just add this line \ud83d\udc47"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'router_settings:\n  content_policy_fallbacks=[{"claude-2": ["my-fallback-model"]}]\n'})}),(0,s.jsx)(n.p,{children:"Start proxy"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n\n# RUNNING on http://0.0.0.0:4000\n"})})]})]}),"\n",(0,s.jsx)(n.h2,{id:"context-window-exceeded-fallback",children:"Context Window Exceeded Fallback"}),"\n",(0,s.jsx)(n.p,{children:"Key change:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'context_window_fallbacks=[{"claude-2": ["my-fallback-model"]}]\n'})}),"\n",(0,s.jsxs)(r.A,{children:[(0,s.jsx)(o.A,{value:"sdk",label:"SDK",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import Router \n\nrouter = Router(\n  model_list=[\n    {\n      "model_name": "claude-2",\n      "litellm_params": {\n        "model": "claude-2",\n        "api_key": "",\n        "mock_response": Exception("prompt is too long"),\n      },\n    },\n    {\n      "model_name": "my-fallback-model",\n      "litellm_params": {\n        "model": "claude-2",\n        "api_key": "",\n        "mock_response": "This works!",\n      },\n    },\n  ],\n  context_window_fallbacks=[{"claude-2": ["my-fallback-model"]}], # \ud83d\udc48 KEY CHANGE\n  # fallbacks=[..], # [OPTIONAL]\n  # content_policy_fallbacks=[..], # [OPTIONAL]\n)\n\nresponse = router.completion(\n  model="claude-2",\n  messages=[{"role": "user", "content": "Hey, how\'s it going?"}],\n)\n'})})}),(0,s.jsxs)(o.A,{value:"proxy",label:"PROXY",children:[(0,s.jsx)(n.p,{children:"In your proxy config.yaml just add this line \ud83d\udc47"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'router_settings:\n  context_window_fallbacks=[{"claude-2": ["my-fallback-model"]}]\n'})}),(0,s.jsx)(n.p,{children:"Start proxy"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n\n# RUNNING on http://0.0.0.0:4000\n"})})]})]}),"\n",(0,s.jsx)(n.h2,{id:"advanced",children:"Advanced"}),"\n",(0,s.jsx)(n.h3,{id:"fallbacks--retries--timeouts--cooldowns",children:"Fallbacks + Retries + Timeouts + Cooldowns"}),"\n",(0,s.jsx)(n.p,{children:"To set fallbacks, just do:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'litellm_settings:\n  fallbacks: [{"zephyr-beta": ["gpt-3.5-turbo"]}] \n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Covers all errors (429, 500, etc.)"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Set via config"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'model_list:\n  - model_name: zephyr-beta\n    litellm_params:\n        model: huggingface/HuggingFaceH4/zephyr-7b-beta\n        api_base: http://0.0.0.0:8001\n  - model_name: zephyr-beta\n    litellm_params:\n        model: huggingface/HuggingFaceH4/zephyr-7b-beta\n        api_base: http://0.0.0.0:8002\n  - model_name: zephyr-beta\n    litellm_params:\n        model: huggingface/HuggingFaceH4/zephyr-7b-beta\n        api_base: http://0.0.0.0:8003\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n        model: gpt-3.5-turbo\n        api_key: <my-openai-key>\n  - model_name: gpt-3.5-turbo-16k\n    litellm_params:\n        model: gpt-3.5-turbo-16k\n        api_key: <my-openai-key>\n\nlitellm_settings:\n  num_retries: 3 # retry call 3 times on each model_name (e.g. zephyr-beta)\n  request_timeout: 10 # raise Timeout error if call takes longer than 10s. Sets litellm.request_timeout \n  fallbacks: [{"zephyr-beta": ["gpt-3.5-turbo"]}] # fallback to gpt-3.5-turbo if call fails num_retries \n  allowed_fails: 3 # cooldown model if it fails > 1 call in a minute. \n  cooldown_time: 30 # how long to cooldown model if fails/min > allowed_fails\n'})}),"\n",(0,s.jsx)(n.h3,{id:"fallback-to-specific-model-id",children:"Fallback to Specific Model ID"}),"\n",(0,s.jsx)(n.p,{children:"If all models in a group are in cooldown (e.g. rate limited), LiteLLM will fallback to the model with the specific model ID."}),"\n",(0,s.jsx)(n.p,{children:"This skips any cooldown check for the fallback model."}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Specify the model ID in ",(0,s.jsx)(n.code,{children:"model_info"})]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: gpt-4\n    litellm_params:\n      model: openai/gpt-4\n    model_info:\n      id: my-specific-model-id # \ud83d\udc48 KEY CHANGE\n  - model_name: gpt-4\n    litellm_params:\n      model: azure/chatgpt-v-2\n      api_base: os.environ/AZURE_API_BASE\n      api_key: os.environ/AZURE_API_KEY\n  - model_name: anthropic-claude\n    litellm_params:\n      model: anthropic/claude-3-opus-20240229\n      api_key: os.environ/ANTHROPIC_API_KEY\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Note:"})," This will only fallback to the model with the specific model ID. If you want to fallback to another model group, you can set ",(0,s.jsx)(n.code,{children:'fallbacks=[{"gpt-4": ["anthropic-claude"]}]'})]}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:"Set fallbacks in config"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'litellm_settings:\n  fallbacks: [{"gpt-4": ["my-specific-model-id"]}]\n'})}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsx)(n.li,{children:"Test it!"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -X POST \'http://0.0.0.0:4000/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n  "model": "gpt-4",\n  "messages": [\n    {\n      "role": "user",\n      "content": "ping"\n    }\n  ],\n  "mock_testing_fallbacks": true\n}\'\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Validate it works, by checking the response header ",(0,s.jsx)(n.code,{children:"x-litellm-model-id"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"x-litellm-model-id: my-specific-model-id\n"})}),"\n",(0,s.jsx)(n.h3,{id:"test-fallbacks",children:"Test Fallbacks!"}),"\n",(0,s.jsx)(n.p,{children:"Check if your fallbacks are working as expected."}),"\n",(0,s.jsx)(n.h4,{id:"regular-fallbacks",children:(0,s.jsx)(n.strong,{children:"Regular Fallbacks"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -X POST \'http://0.0.0.0:4000/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n  "model": "my-bad-model",\n  "messages": [\n    {\n      "role": "user",\n      "content": "ping"\n    }\n  ],\n  "mock_testing_fallbacks": true # \ud83d\udc48 KEY CHANGE\n}\n\'\n'})}),"\n",(0,s.jsx)(n.h4,{id:"content-policy-fallbacks",children:(0,s.jsx)(n.strong,{children:"Content Policy Fallbacks"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -X POST \'http://0.0.0.0:4000/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n  "model": "my-bad-model",\n  "messages": [\n    {\n      "role": "user",\n      "content": "ping"\n    }\n  ],\n  "mock_testing_content_policy_fallbacks": true # \ud83d\udc48 KEY CHANGE\n}\n\'\n'})}),"\n",(0,s.jsx)(n.h4,{id:"context-window-fallbacks",children:(0,s.jsx)(n.strong,{children:"Context Window Fallbacks"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -X POST \'http://0.0.0.0:4000/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n  "model": "my-bad-model",\n  "messages": [\n    {\n      "role": "user",\n      "content": "ping"\n    }\n  ],\n  "mock_testing_context_window_fallbacks": true # \ud83d\udc48 KEY CHANGE\n}\n\'\n'})}),"\n",(0,s.jsx)(n.h3,{id:"context-window-fallbacks-pre-call-checks--fallbacks",children:"Context Window Fallbacks (Pre-Call Checks + Fallbacks)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Before call is made"})," check if a call is within model context window with  ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"enable_pre_call_checks: true"})}),"."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://github.com/BerriAI/litellm/blob/c9e6b05cfb20dfb17272218e2555d6b496c47f6f/litellm/router.py#L2163",children:(0,s.jsx)(n.strong,{children:"See Code"})})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"1. Setup config"})}),"\n",(0,s.jsxs)(n.p,{children:["For azure deployments, set the base model. Pick the base model from ",(0,s.jsx)(n.a,{href:"https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json",children:"this list"}),", all the azure models start with azure/."]}),"\n",(0,s.jsxs)(r.A,{children:[(0,s.jsxs)(o.A,{value:"same-group",label:"Same Group",children:[(0,s.jsx)(n.p,{children:"Filter older instances of a model (e.g. gpt-3.5-turbo) with smaller context windows"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'router_settings:\n  enable_pre_call_checks: true # 1. Enable pre-call checks\n\nmodel_list:\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n    model: azure/chatgpt-v-2\n    api_base: os.environ/AZURE_API_BASE\n    api_key: os.environ/AZURE_API_KEY\n    api_version: "2023-07-01-preview"\n    model_info:\n    base_model: azure/gpt-4-1106-preview # 2. \ud83d\udc48 (azure-only) SET BASE MODEL\n\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n    model: gpt-3.5-turbo-1106\n    api_key: os.environ/OPENAI_API_KEY\n'})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"2. Start proxy"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n\n# RUNNING on http://0.0.0.0:4000\n"})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"3. Test it!"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\nclient = openai.OpenAI(\n    api_key="anything",\n    base_url="http://0.0.0.0:4000"\n)\n\ntext = "What is the meaning of 42?" * 5000\n\n# request sent to model set on litellm proxy, `litellm --model`\nresponse = client.chat.completions.create(\n    model="gpt-3.5-turbo",\n    messages = [\n      {"role": "system", "content": text},\n      {"role": "user", "content": "Who was Alexander?"},\n    ],\n)\n\nprint(response)\n'})})]}),(0,s.jsxs)(o.A,{value:"different-group",label:"Context Window Fallbacks (Different Groups)",children:[(0,s.jsx)(n.p,{children:"Fallback to larger models if current model is too small."}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'router_settings:\n  enable_pre_call_checks: true # 1. Enable pre-call checks\n\nmodel_list:\n  - model_name: gpt-3.5-turbo-small\n    litellm_params:\n    model: azure/chatgpt-v-2\n      api_base: os.environ/AZURE_API_BASE\n      api_key: os.environ/AZURE_API_KEY\n      api_version: "2023-07-01-preview"\n      model_info:\n      base_model: azure/gpt-4-1106-preview # 2. \ud83d\udc48 (azure-only) SET BASE MODEL\n\n  - model_name: gpt-3.5-turbo-large\n    litellm_params:\n      model: gpt-3.5-turbo-1106\n      api_key: os.environ/OPENAI_API_KEY\n\n  - model_name: claude-opus\n    litellm_params:\n      model: claude-3-opus-20240229\n      api_key: os.environ/ANTHROPIC_API_KEY\n\nlitellm_settings:\n  context_window_fallbacks: [{"gpt-3.5-turbo-small": ["gpt-3.5-turbo-large", "claude-opus"]}]\n'})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"2. Start proxy"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n\n# RUNNING on http://0.0.0.0:4000\n"})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"3. Test it!"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\nclient = openai.OpenAI(\n    api_key="anything",\n    base_url="http://0.0.0.0:4000"\n)\n\ntext = "What is the meaning of 42?" * 5000\n\n# request sent to model set on litellm proxy, `litellm --model`\nresponse = client.chat.completions.create(\n    model="gpt-3.5-turbo",\n    messages = [\n      {"role": "system", "content": text},\n      {"role": "user", "content": "Who was Alexander?"},\n    ],\n)\n\nprint(response)\n'})})]})]}),"\n",(0,s.jsx)(n.h3,{id:"content-policy-fallbacks-1",children:"Content Policy Fallbacks"}),"\n",(0,s.jsx)(n.p,{children:"Fallback across providers (e.g. from Azure OpenAI to Anthropic) if you hit content policy violation errors."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'model_list:\n  - model_name: gpt-3.5-turbo-small\n    litellm_params:\n    model: azure/chatgpt-v-2\n        api_base: os.environ/AZURE_API_BASE\n        api_key: os.environ/AZURE_API_KEY\n        api_version: "2023-07-01-preview"\n\n    - model_name: claude-opus\n      litellm_params:\n        model: claude-3-opus-20240229\n        api_key: os.environ/ANTHROPIC_API_KEY\n\nlitellm_settings:\n  content_policy_fallbacks: [{"gpt-3.5-turbo-small": ["claude-opus"]}]\n'})}),"\n",(0,s.jsx)(n.h3,{id:"default-fallbacks",children:"Default Fallbacks"}),"\n",(0,s.jsx)(n.p,{children:"You can also set default_fallbacks, in case a specific model group is misconfigured / bad."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'model_list:\n  - model_name: gpt-3.5-turbo-small\n    litellm_params:\n    model: azure/chatgpt-v-2\n        api_base: os.environ/AZURE_API_BASE\n        api_key: os.environ/AZURE_API_KEY\n        api_version: "2023-07-01-preview"\n\n    - model_name: claude-opus\n      litellm_params:\n        model: claude-3-opus-20240229\n        api_key: os.environ/ANTHROPIC_API_KEY\n\nlitellm_settings:\n  default_fallbacks: ["claude-opus"]\n'})}),"\n",(0,s.jsx)(n.p,{children:"This will default to claude-opus in case any model fails."}),"\n",(0,s.jsxs)(n.p,{children:["A model-specific fallbacks (e.g. ",(0,s.jsx)(n.code,{children:'{"gpt-3.5-turbo-small": ["claude-opus"]}'}),") overrides default fallback."]}),"\n",(0,s.jsx)(n.h3,{id:"eu-region-filtering-pre-call-checks",children:"EU-Region Filtering (Pre-Call Checks)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Before call is made"})," check if a call is within model context window with  ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"enable_pre_call_checks: true"})}),"."]}),"\n",(0,s.jsx)(n.p,{children:"Set 'region_name' of deployment."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Note:"})," LiteLLM can automatically infer region_name for Vertex AI, Bedrock, and IBM WatsonxAI based on your litellm params. For Azure, set ",(0,s.jsx)(n.code,{children:"litellm.enable_preview = True"}),"."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"1. Set Config"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'router_settings:\n  enable_pre_call_checks: true # 1. Enable pre-call checks\n\nmodel_list:\n- model_name: gpt-3.5-turbo\n  litellm_params:\n    model: azure/chatgpt-v-2\n    api_base: os.environ/AZURE_API_BASE\n    api_key: os.environ/AZURE_API_KEY\n    api_version: "2023-07-01-preview"\n    region_name: "eu" # \ud83d\udc48 SET EU-REGION\n\n- model_name: gpt-3.5-turbo\n  litellm_params:\n    model: gpt-3.5-turbo-1106\n    api_key: os.environ/OPENAI_API_KEY\n\n- model_name: gemini-pro\n  litellm_params:\n    model: vertex_ai/gemini-pro-1.5\n    vertex_project: adroit-crow-1234\n    vertex_location: us-east1 # \ud83d\udc48 AUTOMATICALLY INFERS \'region_name\'\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"2. Start proxy"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n\n# RUNNING on http://0.0.0.0:4000\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"3. Test it!"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\nclient = openai.OpenAI(\n    api_key="anything",\n    base_url="http://0.0.0.0:4000"\n)\n\n# request sent to model set on litellm proxy, `litellm --model`\nresponse = client.chat.completions.with_raw_response.create(\n    model="gpt-3.5-turbo",\n    messages = [{"role": "user", "content": "Who was Alexander?"}]\n)\n\nprint(response)\n\nprint(f"response.headers.get(\'x-litellm-model-api-base\')")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"setting-fallbacks-for-wildcard-models",children:"Setting Fallbacks for Wildcard Models"}),"\n",(0,s.jsxs)(n.p,{children:["You can set fallbacks for wildcard models (e.g. ",(0,s.jsx)(n.code,{children:"azure/*"}),") in your config file."]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Setup config"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'model_list:\n  - model_name: "gpt-4o"\n    litellm_params:\n      model: "openai/gpt-4o"\n      api_key: os.environ/OPENAI_API_KEY\n  - model_name: "azure/*"\n    litellm_params:\n      model: "azure/*"\n      api_key: os.environ/AZURE_API_KEY\n      api_base: os.environ/AZURE_API_BASE\n\nlitellm_settings:\n  fallbacks: [{"gpt-4o": ["azure/gpt-4o"]}]\n'})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:"Start Proxy"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsx)(n.li,{children:"Test it!"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -L -X POST \'http://0.0.0.0:4000/v1/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n    "model": "gpt-4o",\n    "messages": [\n      {\n        "role": "user",\n        "content": [    \n          {\n            "type": "text",\n            "text": "what color is red"\n          }\n        ]\n      }\n    ],\n    "max_tokens": 300,\n    "mock_testing_fallbacks": true\n}\'\n'})}),"\n",(0,s.jsx)(n.h3,{id:"disable-fallbacks-per-requestkey",children:"Disable Fallbacks (Per Request/Key)"}),"\n",(0,s.jsxs)(r.A,{children:[(0,s.jsxs)(o.A,{value:"request",label:"Per Request",children:[(0,s.jsxs)(n.p,{children:["You can disable fallbacks per key by setting ",(0,s.jsx)(n.code,{children:"disable_fallbacks: true"})," in your request body."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -L -X POST \'http://0.0.0.0:4000/v1/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n    "messages": [\n        {\n            "role": "user",\n            "content": "List 5 important events in the XIX century"\n        }\n    ],\n    "model": "gpt-3.5-turbo",\n    "disable_fallbacks": true # \ud83d\udc48 DISABLE FALLBACKS\n}\'\n'})})]}),(0,s.jsxs)(o.A,{value:"key",label:"Per Key",children:[(0,s.jsxs)(n.p,{children:["You can disable fallbacks per key by setting ",(0,s.jsx)(n.code,{children:"disable_fallbacks: true"})," in your key metadata."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"curl -L -X POST 'http://0.0.0.0:4000/key/generate' \\\n-H 'Authorization: Bearer sk-1234' \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"metadata\": {\n        \"disable_fallbacks\": true\n    }\n}'\n"})})]})]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}}}]);