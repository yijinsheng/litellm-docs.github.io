"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[9336],{7227(e,n,l){l.d(n,{A:()=>r});l(96540);var a=l(18215);const t="tabItem_Ymn6";var s=l(74848);function r({children:e,hidden:n,className:l}){return(0,s.jsx)("div",{role:"tabpanel",className:(0,a.A)(t,l),hidden:n,children:e})}},28453(e,n,l){l.d(n,{R:()=>r,x:()=>i});var a=l(96540);const t={},s=a.createContext(t);function r(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),a.createElement(s.Provider,{value:n},e.children)}},49489(e,n,l){l.d(n,{A:()=>A});var a=l(96540),t=l(18215),s=l(24245),r=l(56347),i=l(36494),o=l(62814),c=l(45167),d=l(69900);function p(e){return a.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m(e){const{values:n,children:l}=e;return(0,a.useMemo)(()=>{const e=n??function(e){return p(e).map(({props:{value:e,label:n,attributes:l,default:a}})=>({value:e,label:n,attributes:l,default:a}))}(l);return function(e){const n=(0,c.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,l])}function u({value:e,tabValues:n}){return n.some(n=>n.value===e)}function h({queryString:e=!1,groupId:n}){const l=(0,r.W6)(),t=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,o.aZ)(t),(0,a.useCallback)(e=>{if(!t)return;const n=new URLSearchParams(l.location.search);n.set(t,e),l.replace({...l.location,search:n.toString()})},[t,l])]}function x(e){const{defaultValue:n,queryString:l=!1,groupId:t}=e,s=m(e),[r,o]=(0,a.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!u({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const l=n.find(e=>e.default)??n[0];if(!l)throw new Error("Unexpected error: 0 tabValues");return l.value}({defaultValue:n,tabValues:s})),[c,p]=h({queryString:l,groupId:t}),[x,j]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[l,t]=(0,d.Dv)(n);return[l,(0,a.useCallback)(e=>{n&&t.set(e)},[n,t])]}({groupId:t}),g=(()=>{const e=c??x;return u({value:e,tabValues:s})?e:null})();(0,i.A)(()=>{g&&o(g)},[g]);return{selectedValue:r,selectValue:(0,a.useCallback)(e=>{if(!u({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);o(e),p(e),j(e)},[p,j,s]),tabValues:s}}var j=l(11062);const g="tabList__CuJ",f="tabItem_LNqP";var b=l(74848);function _({className:e,block:n,selectedValue:l,selectValue:a,tabValues:r}){const i=[],{blockElementScrollPositionUntilNextRender:o}=(0,s.a_)(),c=e=>{const n=e.currentTarget,t=i.indexOf(n),s=r[t].value;s!==l&&(o(n),a(s))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const l=i.indexOf(e.currentTarget)+1;n=i[l]??i[0];break}case"ArrowLeft":{const l=i.indexOf(e.currentTarget)-1;n=i[l]??i[i.length-1];break}}n?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,t.A)("tabs",{"tabs--block":n},e),children:r.map(({value:e,label:n,attributes:a})=>(0,b.jsx)("li",{role:"tab",tabIndex:l===e?0:-1,"aria-selected":l===e,ref:e=>{i.push(e)},onKeyDown:d,onClick:c,...a,className:(0,t.A)("tabs__item",f,a?.className,{"tabs__item--active":l===e}),children:n??e},e))})}function v({lazy:e,children:n,selectedValue:l}){const s=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=s.find(e=>e.props.value===l);return e?(0,a.cloneElement)(e,{className:(0,t.A)("margin-top--md",e.props.className)}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:s.map((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==l}))})}function y(e){const n=x(e);return(0,b.jsxs)("div",{className:(0,t.A)("tabs-container",g),children:[(0,b.jsx)(_,{...n,...e}),(0,b.jsx)(v,{...n,...e})]})}function A(e){const n=(0,j.A)();return(0,b.jsx)(y,{...e,children:p(e.children)},String(n))}},57866(e,n,l){l.r(n),l.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>u,frontMatter:()=>o,metadata:()=>a,toc:()=>p});const a=JSON.parse('{"id":"providers/replicate","title":"Replicate","description":"LiteLLM supports all models on Replicate","source":"@site/docs/providers/replicate.md","sourceDirName":"providers","slug":"/providers/replicate","permalink":"/docs/providers/replicate","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Recraft","permalink":"/docs/providers/recraft"},"next":{"title":"RunwayML - \u56fe\u50cf\u751f\u6210","permalink":"/docs/providers/runwayml/images"}}');var t=l(74848),s=l(28453),r=l(49489),i=l(7227);const o={},c="Replicate",d={},p=[{value:"Usage",id:"usage",level:2},{value:"API KEYS",id:"api-keys",level:3},{value:"Example Call",id:"example-call",level:3},{value:"Expected Replicate Call",id:"expected-replicate-call",level:3},{value:"Advanced Usage - Prompt Formatting",id:"advanced-usage---prompt-formatting",level:2},{value:"Advanced Usage - Calling Replicate Deployments",id:"advanced-usage---calling-replicate-deployments",level:2},{value:"Replicate Models",id:"replicate-models",level:2},{value:"Passing additional params - max_tokens, temperature",id:"passing-additional-params---max_tokens-temperature",level:2},{value:"Passings Replicate specific params",id:"passings-replicate-specific-params",level:2}];function m(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"replicate",children:"Replicate"})}),"\n",(0,t.jsx)(n.p,{children:"LiteLLM supports all models on Replicate"}),"\n",(0,t.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsxs)(i.A,{value:"sdk",label:"SDK",children:[(0,t.jsx)(n.h3,{id:"api-keys",children:"API KEYS"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import os \nos.environ["REPLICATE_API_KEY"] = ""\n'})}),(0,t.jsx)(n.h3,{id:"example-call",children:"Example Call"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n## set ENV variables\nos.environ["REPLICATE_API_KEY"] = "replicate key"\n\n# replicate llama-3 call\nresponse = completion(\n    model="replicate/meta/meta-llama-3-8b-instruct", \n    messages = [{ "content": "Hello, how are you?","role": "user"}]\n)\n'})})]}),(0,t.jsxs)(i.A,{value:"proxy",label:"PROXY",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Add models to your config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: llama-3\n    litellm_params:\n      model: replicate/meta/meta-llama-3-8b-instruct\n      api_key: os.environ/REPLICATE_API_KEY\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start the proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"$ litellm --config /path/to/config.yaml --debug\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Send Request to LiteLLM Proxy Server"}),"\n"]}),(0,t.jsxs)(r.A,{children:[(0,t.jsx)(i.A,{value:"openai",label:"OpenAI Python v1.0.0+",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import openai\nclient = openai.OpenAI(\n    api_key="sk-1234",             # pass litellm proxy key, if you\'re using virtual keys\n    base_url="http://0.0.0.0:4000" # litellm-proxy-base url\n)\n\nresponse = client.chat.completions.create(\n    model="llama-3",\n    messages = [\n      {\n          "role": "system",\n          "content": "Be a good human!"\n      },\n      {\n          "role": "user",\n          "content": "What do you know about earth?"\n      }\n  ]\n)\n\nprint(response)\n'})})}),(0,t.jsx)(i.A,{value:"curl",label:"curl",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:'curl --location \'http://0.0.0.0:4000/chat/completions\' \\\n    --header \'Authorization: Bearer sk-1234\' \\\n    --header \'Content-Type: application/json\' \\\n    --data \'{\n    "model": "llama-3",\n    "messages": [\n      {\n          "role": "system",\n          "content": "Be a good human!"\n      },\n      {\n          "role": "user",\n          "content": "What do you know about earth?"\n      }\n      ],\n}\'\n'})})})]}),(0,t.jsx)(n.h3,{id:"expected-replicate-call",children:"Expected Replicate Call"}),(0,t.jsx)(n.p,{children:"This is the call litellm will make to replicate, from the above example:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.replicate.com/v1/models/meta/meta-llama-3-8b-instruct \\\n-H 'Authorization: Token your-api-key' -H 'Content-Type: application/json' \\\n-d '{'version': 'meta/meta-llama-3-8b-instruct', 'input': {'prompt': '<|start_header_id|>system<|end_header_id|>\\n\\nBe a good human!<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat do you know about earth?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'}}'\n"})})]})]}),"\n",(0,t.jsx)(n.h2,{id:"advanced-usage---prompt-formatting",children:"Advanced Usage - Prompt Formatting"}),"\n",(0,t.jsxs)(n.p,{children:["LiteLLM has prompt template mappings for all ",(0,t.jsx)(n.code,{children:"meta-llama"})," llama3 instruct models. ",(0,t.jsx)(n.a,{href:"https://github.com/BerriAI/litellm/blob/4f46b4c3975cd0f72b8c5acb2cb429d23580c18a/litellm/llms/prompt_templates/factory.py#L1360",children:(0,t.jsx)(n.strong,{children:"See Code"})})]}),"\n",(0,t.jsx)(n.p,{children:"To apply a custom prompt template:"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(i.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import litellm\n\nimport os \nos.environ["REPLICATE_API_KEY"] = ""\n\n# Create your own custom prompt template \nlitellm.register_prompt_template(\n\t    model="togethercomputer/LLaMA-2-7B-32K",\n        initial_prompt_value="You are a good assistant" # [OPTIONAL]\n\t    roles={\n            "system": {\n                "pre_message": "[INST] <<SYS>>\\n", # [OPTIONAL]\n                "post_message": "\\n<</SYS>>\\n [/INST]\\n" # [OPTIONAL]\n            },\n            "user": { \n                "pre_message": "[INST] ", # [OPTIONAL]\n                "post_message": " [/INST]" # [OPTIONAL]\n            }, \n            "assistant": {\n                "pre_message": "\\n" # [OPTIONAL]\n                "post_message": "\\n" # [OPTIONAL]\n            }\n        }\n        final_prompt_value="Now answer as best you can:" # [OPTIONAL]\n)\n\ndef test_replicate_custom_model():\n    model = "replicate/togethercomputer/LLaMA-2-7B-32K"\n    response = completion(model=model, messages=messages)\n    print(response[\'choices\'][0][\'message\'][\'content\'])\n    return response\n\ntest_replicate_custom_model()\n'})})}),(0,t.jsx)(i.A,{value:"proxy",label:"PROXY",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'# Model-specific parameters\nmodel_list:\n  - model_name: mistral-7b # model alias\n    litellm_params: # actual params for litellm.completion()\n      model: "replicate/mistralai/Mistral-7B-Instruct-v0.1" \n      api_key: os.environ/REPLICATE_API_KEY\n      initial_prompt_value: "\\n"\n      roles: {"system":{"pre_message":"<|im_start|>system\\n", "post_message":"<|im_end|>"}, "assistant":{"pre_message":"<|im_start|>assistant\\n","post_message":"<|im_end|>"}, "user":{"pre_message":"<|im_start|>user\\n","post_message":"<|im_end|>"}}\n      final_prompt_value: "\\n"\n      bos_token: "<s>"\n      eos_token: "</s>"\n      max_tokens: 4096\n'})})})]}),"\n",(0,t.jsx)(n.h2,{id:"advanced-usage---calling-replicate-deployments",children:"Advanced Usage - Calling Replicate Deployments"}),"\n",(0,t.jsxs)(n.p,{children:["Calling a ",(0,t.jsx)(n.a,{href:"https://replicate.com/deployments",children:"deployed replicate LLM"}),"\nAdd the ",(0,t.jsx)(n.code,{children:"replicate/deployments/"})," prefix to your model, so litellm will call the ",(0,t.jsx)(n.code,{children:"deployments"})," endpoint. This will call ",(0,t.jsx)(n.code,{children:"ishaan-jaff/ishaan-mistral"})," deployment on replicate"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'response = completion(\n    model="replicate/deployments/ishaan-jaff/ishaan-mistral", \n    messages= [{ "content": "Hello, how are you?","role": "user"}]\n)\n'})}),"\n",(0,t.jsx)(n.admonition,{title:"Replicate Cold Boots",type:"warning",children:(0,t.jsxs)(n.p,{children:["Replicate responses can take 3-5 mins due to replicate cold boots, if you're trying to debug try making the request with ",(0,t.jsx)(n.code,{children:"litellm.set_verbose=True"}),". ",(0,t.jsx)(n.a,{href:"https://replicate.com/docs/how-does-replicate-work#cold-boots",children:"More info on replicate cold boots"})]})}),"\n",(0,t.jsx)(n.h2,{id:"replicate-models",children:"Replicate Models"}),"\n",(0,t.jsx)(n.p,{children:"liteLLM supports all replicate LLMs"}),"\n",(0,t.jsxs)(n.p,{children:["For replicate models ensure to add a ",(0,t.jsx)(n.code,{children:"replicate/"})," prefix to the ",(0,t.jsx)(n.code,{children:"model"})," arg. liteLLM detects it using this arg."]}),"\n",(0,t.jsx)(n.p,{children:"Below are examples on how to call replicate LLMs using liteLLM"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Model Name"}),(0,t.jsx)(n.th,{children:"Function Call"}),(0,t.jsx)(n.th,{children:"Required OS Variables"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"replicate/llama-2-70b-chat"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf', messages)"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"os.environ['REPLICATE_API_KEY']"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"a16z-infra/llama-2-13b-chat"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='replicate/a16z-infra/llama-2-13b-chat:2a7f981751ec7fdf87b5b91ad4db53683a98082e9ff7bfd12c8cd5ea85980a52', messages)"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"os.environ['REPLICATE_API_KEY']"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"replicate/vicuna-13b"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='replicate/vicuna-13b:6282abe6a492de4145d7bb601023762212f9ddbbe78278bd6771c8b3b2f2a13b', messages)"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"os.environ['REPLICATE_API_KEY']"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"daanelson/flan-t5-large"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='replicate/daanelson/flan-t5-large:ce962b3f6792a57074a601d3979db5839697add2e4e02696b3ced4c022d4767f', messages)"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"os.environ['REPLICATE_API_KEY']"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"custom-llm"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='replicate/custom-llm-version-id', messages)"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"os.environ['REPLICATE_API_KEY']"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"replicate deployment"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='replicate/deployments/ishaan-jaff/ishaan-mistral', messages)"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"os.environ['REPLICATE_API_KEY']"})})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"passing-additional-params---max_tokens-temperature",children:"Passing additional params - max_tokens, temperature"}),"\n",(0,t.jsxs)(n.p,{children:["See all litellm.completion supported params ",(0,t.jsx)(n.a,{href:"https://docs.litellm.ai/docs/completion/input",children:"here"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# !pip install litellm\nfrom litellm import completion\nimport os\n## set ENV variables\nos.environ["REPLICATE_API_KEY"] = "replicate key"\n\n# replicate llama-2 call\nresponse = completion(\n    model="replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf", \n    messages = [{ "content": "Hello, how are you?","role": "user"}],\n    max_tokens=20,\n    temperature=0.5\n)\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"proxy"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"  model_list:\n    - model_name: llama-3\n      litellm_params:\n        model: replicate/meta/meta-llama-3-8b-instruct\n        api_key: os.environ/REPLICATE_API_KEY\n        max_tokens: 20\n        temperature: 0.5\n"})}),"\n",(0,t.jsx)(n.h2,{id:"passings-replicate-specific-params",children:"Passings Replicate specific params"}),"\n",(0,t.jsxs)(n.p,{children:["Send params ",(0,t.jsxs)(n.a,{href:"https://docs.litellm.ai/docs/completion/input",children:["not supported by ",(0,t.jsx)(n.code,{children:"litellm.completion()"})]})," but supported by Replicate by passing them to ",(0,t.jsx)(n.code,{children:"litellm.completion"})]}),"\n",(0,t.jsxs)(n.p,{children:["Example ",(0,t.jsx)(n.code,{children:"seed"}),", ",(0,t.jsx)(n.code,{children:"min_tokens"})," are Replicate specific param"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# !pip install litellm\nfrom litellm import completion\nimport os\n## set ENV variables\nos.environ["REPLICATE_API_KEY"] = "replicate key"\n\n# replicate llama-2 call\nresponse = completion(\n    model="replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf", \n    messages = [{ "content": "Hello, how are you?","role": "user"}],\n    seed=-1,\n    min_tokens=2,\n    top_k=20,\n)\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"proxy"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"  model_list:\n    - model_name: llama-3\n      litellm_params:\n        model: replicate/meta/meta-llama-3-8b-instruct\n        api_key: os.environ/REPLICATE_API_KEY\n        min_tokens: 2\n        top_k: 20\n"})})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(m,{...e})}):m(e)}}}]);