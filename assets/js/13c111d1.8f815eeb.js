"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[38820],{28453(e,n,t){t.d(n,{R:()=>r,x:()=>a});var s=t(96540);const o={},i=s.createContext(o);function r(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),s.createElement(i.Provider,{value:n},e.children)}},63724(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"load_test_sdk","title":"LiteLLM SDK vs OpenAI","description":"\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8d1f\u8f7d\u6d4b\u8bd5LiteLLM\u4e0eOpenAI\u7684\u811a\u672c\u3002","source":"@site/docs/load_test_sdk.md","sourceDirName":".","slug":"/load_test_sdk","permalink":"/docs/load_test_sdk","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"LiteLLM \u4ee3\u7406 - \u4f7f\u7528 locust \u8fbe\u5230 1K+ RPS \u8d1f\u8f7d\u6d4b\u8bd5","permalink":"/docs/load_test_advanced"},"next":{"title":"\u591a\u5b9e\u4f8b TPM/RPM (litellm.Router)","permalink":"/docs/load_test_rpm"}}');var o=t(74848),i=t(28453);const r={},a="LiteLLM SDK vs OpenAI",l={},c=[];function p(e){const n={code:"code",h1:"h1",header:"header",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"litellm-sdk-vs-openai",children:"LiteLLM SDK vs OpenAI"})}),"\n",(0,o.jsx)(n.p,{children:"\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8d1f\u8f7d\u6d4b\u8bd5LiteLLM\u4e0eOpenAI\u7684\u811a\u672c\u3002"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from openai import AsyncOpenAI, AsyncAzureOpenAI\nimport random, uuid\nimport time, asyncio, litellm\n# import logging\n# logging.basicConfig(level=logging.DEBUG)\n#### LITELLM PROXY #### \nlitellm_client = AsyncOpenAI(\n    api_key="sk-1234", # [\u8bf7\u66f4\u6539\u6b64\u90e8\u5206]\n    base_url="http://0.0.0.0:4000"\n)\n\n#### AZURE OPENAI CLIENT #### \nclient = AsyncAzureOpenAI(\n    api_key="my-api-key", # [\u8bf7\u66f4\u6539\u6b64\u90e8\u5206]\n    azure_endpoint="my-api-base", # [\u8bf7\u66f4\u6539\u6b64\u90e8\u5206]\n    api_version="2023-07-01-preview" \n)\n\n\n#### LITELLM ROUTER #### \nmodel_list = [\n  {\n    "model_name": "azure-canada",\n    "litellm_params": {\n      "model": "azure/my-azure-deployment-name", # [\u8bf7\u66f4\u6539\u6b64\u90e8\u5206]\n      "api_key": "my-api-key", # [\u8bf7\u66f4\u6539\u6b64\u90e8\u5206]\n      "api_base": "my-api-base", # [\u8bf7\u66f4\u6539\u6b64\u90e8\u5206]\n      "api_version": "2023-07-01-preview"\n    }\n  }\n]\n\nrouter = litellm.Router(model_list=model_list)\n\nasync def openai_completion():\n  try:\n    response = await client.chat.completions.create(\n              model="gpt-35-turbo",\n              messages=[{"role": "user", "content": f"This is a test: {uuid.uuid4()}"}],\n              stream=True\n          )\n    return response\n  except Exception as e:\n    print(e)\n    return None\n  \n\nasync def router_completion():\n  try:\n    response = await router.acompletion(\n              model="azure-canada", # [\u8bf7\u66f4\u6539\u6b64\u90e8\u5206]\n              messages=[{"role": "user", "content": f"This is a test: {uuid.uuid4()}"}],\n              stream=True\n          )\n    return response\n  except Exception as e:\n    print(e)\n    return None\n\nasync def proxy_completion_non_streaming():\n  try:\n    response = await litellm_client.chat.completions.create(\n              model="sagemaker-models", # [\u8bf7\u66f4\u6539\u6b64\u90e8\u5206]\uff08\u5982\u679c\u60a8\u5728\u4ee3\u7406\u4e2d\u8c03\u7528\u7684\u540d\u79f0\u4e0e\u6b64\u4e0d\u540c\uff09\n              messages=[{"role": "user", "content": f"This is a test: {uuid.uuid4()}"}],\n          )\n    return response\n  except Exception as e:\n    print(e)\n    return None\n\nasync def loadtest_fn():\n    start = time.time()\n    n = 500  # \u5e76\u53d1\u4efb\u52a1\u7684\u6570\u91cf\n    tasks = [proxy_completion_non_streaming() for _ in range(n)]\n    chat_completions = await asyncio.gather(*tasks)\n    successful_completions = [c for c in chat_completions if c is not None]\n    print(n, time.time() - start, len(successful_completions))\n\n# \u8fd0\u884c\u4e8b\u4ef6\u5faa\u73af\u4ee5\u6267\u884c\u5f02\u6b65\u51fd\u6570\nasyncio.run(loadtest_fn())\n\n'})})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}}}]);