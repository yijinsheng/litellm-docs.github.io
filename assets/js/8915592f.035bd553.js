"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[37882],{7227(e,n,s){s.d(n,{A:()=>r});s(96540);var l=s(18215);const t="tabItem_Ymn6";var o=s(74848);function r({children:e,hidden:n,className:s}){return(0,o.jsx)("div",{role:"tabpanel",className:(0,l.A)(t,s),hidden:n,children:e})}},49489(e,n,s){s.d(n,{A:()=>L});var l=s(96540),t=s(18215),o=s(24245),r=s(56347),i=s(36494),a=s(62814),m=s(45167),c=s(69900);function d(e){return l.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,l.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function p(e){const{values:n,children:s}=e;return(0,l.useMemo)(()=>{const e=n??function(e){return d(e).map(({props:{value:e,label:n,attributes:s,default:l}})=>({value:e,label:n,attributes:s,default:l}))}(s);return function(e){const n=(0,m.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,s])}function u({value:e,tabValues:n}){return n.some(n=>n.value===e)}function h({queryString:e=!1,groupId:n}){const s=(0,r.W6)(),t=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,a.aZ)(t),(0,l.useCallback)(e=>{if(!t)return;const n=new URLSearchParams(s.location.search);n.set(t,e),s.replace({...s.location,search:n.toString()})},[t,s])]}function g(e){const{defaultValue:n,queryString:s=!1,groupId:t}=e,o=p(e),[r,a]=(0,l.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!u({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const s=n.find(e=>e.default)??n[0];if(!s)throw new Error("Unexpected error: 0 tabValues");return s.value}({defaultValue:n,tabValues:o})),[m,d]=h({queryString:s,groupId:t}),[g,_]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[s,t]=(0,c.Dv)(n);return[s,(0,l.useCallback)(e=>{n&&t.set(e)},[n,t])]}({groupId:t}),x=(()=>{const e=m??g;return u({value:e,tabValues:o})?e:null})();(0,i.A)(()=>{x&&a(x)},[x]);return{selectedValue:r,selectValue:(0,l.useCallback)(e=>{if(!u({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);a(e),d(e),_(e)},[d,_,o]),tabValues:o}}var _=s(11062);const x="tabList__CuJ",j="tabItem_LNqP";var y=s(74848);function f({className:e,block:n,selectedValue:s,selectValue:l,tabValues:r}){const i=[],{blockElementScrollPositionUntilNextRender:a}=(0,o.a_)(),m=e=>{const n=e.currentTarget,t=i.indexOf(n),o=r[t].value;o!==s&&(a(n),l(o))},c=e=>{let n=null;switch(e.key){case"Enter":m(e);break;case"ArrowRight":{const s=i.indexOf(e.currentTarget)+1;n=i[s]??i[0];break}case"ArrowLeft":{const s=i.indexOf(e.currentTarget)-1;n=i[s]??i[i.length-1];break}}n?.focus()};return(0,y.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,t.A)("tabs",{"tabs--block":n},e),children:r.map(({value:e,label:n,attributes:l})=>(0,y.jsx)("li",{role:"tab",tabIndex:s===e?0:-1,"aria-selected":s===e,ref:e=>{i.push(e)},onKeyDown:c,onClick:m,...l,className:(0,t.A)("tabs__item",j,l?.className,{"tabs__item--active":s===e}),children:n??e},e))})}function b({lazy:e,children:n,selectedValue:s}){const o=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=o.find(e=>e.props.value===s);return e?(0,l.cloneElement)(e,{className:(0,t.A)("margin-top--md",e.props.className)}):null}return(0,y.jsx)("div",{className:"margin-top--md",children:o.map((e,n)=>(0,l.cloneElement)(e,{key:n,hidden:e.props.value!==s}))})}function v(e){const n=g(e);return(0,y.jsxs)("div",{className:(0,t.A)("tabs-container",x),children:[(0,y.jsx)(f,{...n,...e}),(0,y.jsx)(b,{...n,...e})]})}function L(e){const n=(0,_.A)();return(0,y.jsx)(v,{...e,children:d(e.children)},String(n))}},66222(e,n,s){s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>m,default:()=>u,frontMatter:()=>a,metadata:()=>l,toc:()=>d});const l=JSON.parse('{"id":"providers/custom_llm_server","title":"\u81ea\u5b9a\u4e49 API \u670d\u52a1\u5668\uff08\u81ea\u5b9a\u4e49\u683c\u5f0f\uff09","description":"\u901a\u8fc7 LiteLLM \u8c03\u7528\u60a8\u81ea\u5df1\u7684 torch-serve / \u5185\u90e8\u5927\u8bed\u8a00\u6a21\u578b API","source":"@site/docs/providers/custom_llm_server.md","sourceDirName":"providers","slug":"/providers/custom_llm_server","permalink":"/docs/providers/custom_llm_server","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"CompactifAI","permalink":"/docs/providers/compactifai"},"next":{"title":"Dashscope (Qwen API)","permalink":"/docs/providers/dashscope"}}');var t=s(74848),o=s(28453),r=s(49489),i=s(7227);s(90547);const a={},m="\u81ea\u5b9a\u4e49 API \u670d\u52a1\u5668\uff08\u81ea\u5b9a\u4e49\u683c\u5f0f\uff09",c={},d=[{value:"\u5feb\u901f\u5165\u95e8",id:"\u5feb\u901f\u5165\u95e8",level:2},{value:"\u6dfb\u52a0\u6d41\u5f0f\u4f20\u8f93\u652f\u6301",id:"\u6dfb\u52a0\u6d41\u5f0f\u4f20\u8f93\u652f\u6301",level:2},{value:"\u56fe\u50cf\u751f\u6210",id:"\u56fe\u50cf\u751f\u6210",level:2},{value:"\u56fe\u50cf\u7f16\u8f91",id:"\u56fe\u50cf\u7f16\u8f91",level:2},{value:"Anthropic <code>/v1/messages</code>",id:"anthropic-v1messages",level:2},{value:"\u989d\u5916\u53c2\u6570",id:"\u989d\u5916\u53c2\u6570",level:2},{value:"\u81ea\u5b9a\u4e49\u5904\u7406\u5668\u89c4\u8303",id:"\u81ea\u5b9a\u4e49\u5904\u7406\u5668\u89c4\u8303",level:2}];function p(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"\u81ea\u5b9a\u4e49-api-\u670d\u52a1\u5668\u81ea\u5b9a\u4e49\u683c\u5f0f",children:"\u81ea\u5b9a\u4e49 API \u670d\u52a1\u5668\uff08\u81ea\u5b9a\u4e49\u683c\u5f0f\uff09"})}),"\n",(0,t.jsx)(n.p,{children:"\u901a\u8fc7 LiteLLM \u8c03\u7528\u60a8\u81ea\u5df1\u7684 torch-serve / \u5185\u90e8\u5927\u8bed\u8a00\u6a21\u578b API"}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u5982\u9700\u8c03\u7528\u517c\u5bb9 OpenAI \u7684\u7aef\u70b9\uff0c\u8bf7\u53c2\u9605 ",(0,t.jsx)(n.a,{href:"/docs/providers/openai_compatible",children:"\u6b64\u5904"})]}),"\n",(0,t.jsxs)(n.li,{children:["\u5982\u9700\u4fee\u6539\u4ee3\u7406\u4e2d\u7684\u5165\u7ad9/\u51fa\u7ad9\u8c03\u7528\uff0c\u8bf7\u53c2\u9605 ",(0,t.jsx)(n.a,{href:"/docs/proxy/call_hooks",children:"\u6b64\u5904"})]}),"\n"]})}),"\n",(0,t.jsx)(n.p,{children:"\u652f\u6301\u7684\u8def\u7531\uff1a"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"/v1/chat/completions"})," -> ",(0,t.jsx)(n.code,{children:"litellm.acompletion"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"/v1/completions"})," -> ",(0,t.jsx)(n.code,{children:"litellm.atext_completion"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"/v1/embeddings"})," -> ",(0,t.jsx)(n.code,{children:"litellm.aembedding"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"/v1/images/generations"})," -> ",(0,t.jsx)(n.code,{children:"litellm.aimage_generation"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"/v1/images/edits"})," -> ",(0,t.jsx)(n.code,{children:"litellm.aimage_edit"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"/v1/messages"})," -> ",(0,t.jsx)(n.code,{children:"litellm.acompletion"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"\u5feb\u901f\u5165\u95e8",children:"\u5feb\u901f\u5165\u95e8"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:"showLineNumbers",children:'import litellm\nfrom litellm import CustomLLM, completion, get_llm_provider\n\n\nclass MyCustomLLM(CustomLLM):\n    def completion(self, *args, **kwargs) -> litellm.ModelResponse:\n        return litellm.completion(\n            model="gpt-3.5-turbo",\n            messages=[{"role": "user", "content": "Hello world"}],\n            mock_response="Hi!",\n        )  # type: ignore\n\nmy_custom_llm = MyCustomLLM()\n\nlitellm.custom_provider_map = [  # \ud83d\udc48 \u5173\u952e\u6b65\u9aa4 - \u6ce8\u518c\u5904\u7406\u5668\n    {"provider": "my-custom-llm", "custom_handler": my_custom_llm}\n]\n\nresp = completion(\n    model="my-custom-llm/my-fake-model",\n    messages=[{"role": "user", "content": "Hello world!"}],\n)\n\nassert resp.choices[0].message.content == "Hi!"\n\n\n## OpenAI \u4ee3\u7406\u4f7f\u7528\n\n1. \u8bbe\u7f6e\u60a8\u7684 `custom_handler.py` \u6587\u4ef6 \n\n```python\nimport litellm\nfrom litellm import CustomLLM, completion, get_llm_provider\n\n\nclass MyCustomLLM(CustomLLM):\n    def completion(self, *args, **kwargs) -> litellm.ModelResponse:\n        return litellm.completion(\n            model="gpt-3.5-turbo",\n            messages=[{"role": "user", "content": "Hello world"}],\n            mock_response="Hi!",\n        )  # type: ignore\n\n    async def acompletion(self, *args, **kwargs) -> litellm.ModelResponse:\n        return litellm.completion(\n            model="gpt-3.5-turbo",\n            messages=[{"role": "user", "content": "Hello world"}],\n            mock_response="Hi!",\n        )  # type: ignore\n\nmy_custom_llm = MyCustomLLM()\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsxs)(n.li,{children:["\u6dfb\u52a0\u5230 ",(0,t.jsx)(n.code,{children:"config.yaml"})]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"\u5728\u4ee5\u4e0b\u914d\u7f6e\u4e2d\uff0c\u6211\u4eec\u4f20\u9012"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["python_filename: ",(0,t.jsx)(n.code,{children:"custom_handler.py"})]}),"\n",(0,t.jsxs)(n.li,{children:["custom_handler_instance_name: ",(0,t.jsx)(n.code,{children:"my_custom_llm"}),"\u3002\u8fd9\u5b9a\u4e49\u5728\u6b65\u9aa41"]}),"\n",(0,t.jsxs)(n.li,{children:["custom_handler: ",(0,t.jsx)(n.code,{children:"custom_handler.my_custom_llm"})]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'model_list:\n  - model_name: "test-model"             \n    litellm_params:\n      model: "openai/text-embedding-ada-002"\n  - model_name: "my-custom-model"\n    litellm_params:\n      model: "my-custom-llm/my-model"\n\nlitellm_settings:\n  custom_provider_map:\n  - {"provider": "my-custom-llm", "custom_handler": custom_handler.my_custom_llm}\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"\u6d4b\u8bd5\u5b83\uff01"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -X POST \'http://0.0.0.0:4000/chat/completions\' \\\n-H \'Content-Type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n    "model": "my-custom-model",\n    "messages": [{"role": "user", "content": "Say \\"this is a test\\" in JSON!"}],\n}\'\n'})}),"\n",(0,t.jsx)(n.p,{children:"\u9884\u671f\u54cd\u5e94"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n    "id": "chatcmpl-06f1b9cd-08bc-43f7-9814-a69173921216",\n    "choices": [\n        {\n            "finish_reason": "stop",\n            "index": 0,\n            "message": {\n                "content": "Hi!",\n                "role": "assistant",\n                "tool_calls": null,\n                "function_call": null\n            }\n        }\n    ],\n    "created": 1721955063,\n    "model": "gpt-3.5-turbo",\n    "object": "chat.completion",\n    "system_fingerprint": null,\n    "usage": {\n        "prompt_tokens": 10,\n        "completion_tokens": 20,\n        "total_tokens": 30\n    }\n}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"\u6dfb\u52a0\u6d41\u5f0f\u4f20\u8f93\u652f\u6301",children:"\u6dfb\u52a0\u6d41\u5f0f\u4f20\u8f93\u652f\u6301"}),"\n",(0,t.jsx)(n.p,{children:"\u8fd9\u91cc\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u793a\u4f8b\uff0c\u8fd4\u56de Unix \u65f6\u95f4\u6233\u7528\u4e8e\u5b8c\u6210\u548c\u6d41\u5f0f\u4f20\u8f93\u7528\u4f8b\u3002"}),"\n",(0,t.jsxs)(n.p,{children:["\u611f\u8c22 ",(0,t.jsx)(n.a,{href:"https://github.com/stronk7",children:"@Eloy Lafuente"})," \u63d0\u4f9b\u6b64\u4ee3\u7801\u793a\u4f8b\u3002"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import time\nfrom typing import Iterator, AsyncIterator\nfrom litellm.types.utils import GenericStreamingChunk, ModelResponse\nfrom litellm import CustomLLM, completion, acompletion\n\nclass UnixTimeLLM(CustomLLM):\n    def completion(self, *args, **kwargs) -> ModelResponse:\n        return completion(\n            model="test/unixtime",\n            mock_response=str(int(time.time())),\n        )  # type: ignore\n\n    async def acompletion(self, *args, **kwargs) -> ModelResponse:\n        return await acompletion(\n            model="test/unixtime",\n            mock_response=str(int(time.time())),\n        )  # type: ignore\n\n    def streaming(self, *args, **kwargs) -> Iterator[GenericStreamingChunk]:\n        generic_streaming_chunk: GenericStreamingChunk = {\n            "finish_reason": "stop",\n            "index": 0,\n            "is_finished": True,\n            "text": str(int(time.time())),\n            "tool_use": None,\n            "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},\n        }\n        return generic_streaming_chunk # type: ignore\n\n    async def astreaming(self, *args, **kwargs) -> AsyncIterator[GenericStreamingChunk]:\n        generic_streaming_chunk: GenericStreamingChunk = {\n            "finish_reason": "stop",\n            "index": 0,\n            "is_finished": True,\n            "text": str(int(time.time())),\n            "tool_use": None,\n            "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},\n        }\n        yield generic_streaming_chunk # type: ignore\n\nunixtime = UnixTimeLLM()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"\u56fe\u50cf\u751f\u6210",children:"\u56fe\u50cf\u751f\u6210"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\u8bbe\u7f6e\u60a8\u7684 ",(0,t.jsx)(n.code,{children:"custom_handler.py"})," \u6587\u4ef6"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import litellm\nfrom litellm import CustomLLM\nfrom litellm.types.utils import ImageResponse, ImageObject\n\n\nclass MyCustomLLM(CustomLLM):\n    async def aimage_generation(self, model: str, prompt: str, model_response: ImageResponse, optional_params: dict, logging_obj: Any, timeout: Optional[Union[float, httpx.Timeout]] = None, client: Optional[AsyncHTTPHandler] = None,) -> ImageResponse:\n        return ImageResponse(\n            created=int(time.time()),\n            data=[ImageObject(url="https://example.com/image.png")],\n        )\n\nmy_custom_llm = MyCustomLLM()\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsxs)(n.li,{children:["\u6dfb\u52a0\u5230 ",(0,t.jsx)(n.code,{children:"config.yaml"})]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"\u5728\u4ee5\u4e0b\u914d\u7f6e\u4e2d\uff0c\u6211\u4eec\u4f20\u9012"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["python_filename: ",(0,t.jsx)(n.code,{children:"custom_handler.py"})]}),"\n",(0,t.jsxs)(n.li,{children:["custom_handler_instance_name: ",(0,t.jsx)(n.code,{children:"my_custom_llm"}),"\u3002\u8fd9\u5b9a\u4e49\u5728\u6b65\u9aa41"]}),"\n",(0,t.jsxs)(n.li,{children:["custom_handler: ",(0,t.jsx)(n.code,{children:"custom_handler.my_custom_llm"})]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'model_list:\n  - model_name: "test-model"             \n    litellm_params:\n      model: "openai/text-embedding-ada-002"\n  - model_name: "my-custom-model"\n    litellm_params:\n      model: "my-custom-llm/my-model"\n\nlitellm_settings:\n  custom_provider_map:\n  - {"provider": "my-custom-llm", "custom_handler": custom_handler.my_custom_llm}\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"\u6d4b\u8bd5\u5b83\uff01"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"curl -X POST 'http://0.0.0.0:4000/v1/images/generations' \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer sk-1234' \\\n-d '{\n    \"model\": \"my-custom-model\",\n    \"prompt\": \"A cute baby sea otter\",\n}'\n"})}),"\n",(0,t.jsx)(n.p,{children:"\u9884\u671f\u54cd\u5e94"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n    "created": 1721955063,\n    "data": [{"url": "https://example.com/image.png"}],\n}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"\u56fe\u50cf\u7f16\u8f91",children:"\u56fe\u50cf\u7f16\u8f91"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\u8bbe\u7f6e\u60a8\u7684 ",(0,t.jsx)(n.code,{children:"custom_handler.py"})," \u6587\u4ef6"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import litellm\nfrom litellm import CustomLLM\nfrom litellm.types.utils import ImageResponse, ImageObject\nimport time\n\nclass MyCustomLLM(CustomLLM):\n    async def aimage_edit(\n        self,\n        model: str,\n        image: Any,\n        prompt: str,\n        model_response: ImageResponse,\n        api_key: Optional[str],\n        api_base: Optional[str],\n        optional_params: dict,\n        logging_obj: Any,\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        client: Optional[AsyncHTTPHandler] = None,\n    ) -> ImageResponse:\n        # \u60a8\u7684\u81ea\u5b9a\u4e49\u56fe\u50cf\u7f16\u8f91\u903b\u8f91\u5728\u6b64\u5904\n        # \u4f8b\u5982\uff0c\u8c03\u7528 Stability AI\u3001Black Forest Labs \u7b49\u3002\n        return ImageResponse(\n            created=int(time.time()),\n            data=[ImageObject(url="https://example.com/edited-image.png")],\n        )\n\nmy_custom_llm = MyCustomLLM()\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsxs)(n.li,{children:["\u6dfb\u52a0\u5230 ",(0,t.jsx)(n.code,{children:"config.yaml"})]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"\u5728\u4ee5\u4e0b\u914d\u7f6e\u4e2d\uff0c\u6211\u4eec\u4f20\u9012"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["python_filename: ",(0,t.jsx)(n.code,{children:"custom_handler.py"})]}),"\n",(0,t.jsxs)(n.li,{children:["custom_handler_instance_name: ",(0,t.jsx)(n.code,{children:"my_custom_llm"}),"\u3002\u8fd9\u5b9a\u4e49\u5728\u6b65\u9aa41"]}),"\n",(0,t.jsxs)(n.li,{children:["custom_handler: ",(0,t.jsx)(n.code,{children:"custom_handler.my_custom_llm"})]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'model_list:\n  - model_name: "my-custom-image-edit-model"\n    litellm_params:\n      model: "my-custom-llm/my-model"\n\nlitellm_settings:\n  custom_provider_map:\n  - {"provider": "my-custom-llm", "custom_handler": custom_handler.my_custom_llm}\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"\u6d4b\u8bd5\u5b83\uff01"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"curl -X POST 'http://0.0.0.0:4000/v1/images/edits' \\\n-H 'Authorization: Bearer sk-1234' \\\n-F 'model=my-custom-image-edit-model' \\\n-F 'image=@/path/to/image.png' \\\n-F 'prompt=Make the sky blue'\n"})}),"\n",(0,t.jsx)(n.p,{children:"\u9884\u671f\u54cd\u5e94"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n    "created": 1721955063,\n    "data": [{"url": "https://example.com/edited-image.png"}],\n}\n'})}),"\n",(0,t.jsxs)(n.h2,{id:"anthropic-v1messages",children:["Anthropic ",(0,t.jsx)(n.code,{children:"/v1/messages"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u96c6\u6210 ",(0,t.jsx)(n.code,{children:".acompletion"})]}),"\n",(0,t.jsx)(n.li,{children:"litellm \u5c06\u5c06\u5176\u8f6c\u6362\u4e3a /v1/messages"}),"\n"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\u8bbe\u7f6e\u60a8\u7684 ",(0,t.jsx)(n.code,{children:"custom_handler.py"})," \u6587\u4ef6"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import litellm\nfrom litellm import CustomLLM, completion, get_llm_provider\n\n\nclass MyCustomLLM(CustomLLM):\n    async def acompletion(self, *args, **kwargs) -> litellm.ModelResponse:\n        return litellm.completion(\n            model="gpt-3.5-turbo",\n            messages=[{"role": "user", "content": "Hello world"}],\n            mock_response="Hi!",\n        )  # type: ignore\n\n\nmy_custom_llm = MyCustomLLM()\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsxs)(n.li,{children:["\u6dfb\u52a0\u5230 ",(0,t.jsx)(n.code,{children:"config.yaml"})]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"\u5728\u4ee5\u4e0b\u914d\u7f6e\u4e2d\uff0c\u6211\u4eec\u4f20\u9012"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["python_filename: ",(0,t.jsx)(n.code,{children:"custom_handler.py"})]}),"\n",(0,t.jsxs)(n.li,{children:["custom_handler_instance_name: ",(0,t.jsx)(n.code,{children:"my_custom_llm"}),"\u3002\u8fd9\u5b9a\u4e49\u5728\u6b65\u9aa41"]}),"\n",(0,t.jsxs)(n.li,{children:["custom_handler: ",(0,t.jsx)(n.code,{children:"custom_handler.my_custom_llm"})]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'model_list:\n  - model_name: "test-model"             \n    litellm_params:\n      model: "openai/text-embedding-ada-002"\n  - model_name: "my-custom-model"\n    litellm_params:\n      model: "my-custom-llm/my-model"\n\nlitellm_settings:\n  custom_provider_map:\n  - {"provider": "my-custom-llm", "custom_handler": custom_handler.my_custom_llm}\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"\u6d4b\u8bd5\u5b83!"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -L -X POST \'http://0.0.0.0:4000/v1/messages\' \\\n-H \'anthropic-version: 2023-06-01\' \\\n-H \'content-type: application/json\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-d \'{\n   "model": "my-custom-model",\n     "max_tokens": 1024,\n     "messages": [{\n         "role": "user",\n         "content": [\n         {\n             "type": "text",\n             "text": "What are the key findings in this document 12?"\n         }]\n     }]\n}\'\n'})}),"\n",(0,t.jsx)(n.p,{children:"\u9884\u671f\u54cd\u5e94"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n    "id": "chatcmpl-Bm4qEp4h4vCe7Zi4Gud1MAxTWgibO",\n    "type": "message",\n    "role": "assistant",\n    "model": "gpt-3.5-turbo-0125",\n    "stop_sequence": null,\n    "usage": {\n        "input_tokens": 18,\n        "output_tokens": 44\n    },\n    "content": [\n        {\n            "type": "text",\n            "text": "Without the specific document being provided, it is not possible to determine the key findings within it. If you can provide the content or a summary of document 12, I would be happy to help identify the key findings."\n        }\n    ],\n    "stop_reason": "end_turn"\n}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"\u989d\u5916\u53c2\u6570",children:"\u989d\u5916\u53c2\u6570"}),"\n",(0,t.jsxs)(n.p,{children:["\u989d\u5916\u53c2\u6570\u5728 ",(0,t.jsx)(n.code,{children:"completion"})," \u6216 ",(0,t.jsx)(n.code,{children:"image_generation"})," \u51fd\u6570\u7684 ",(0,t.jsx)(n.code,{children:"optional_params"})," \u952e\u4e2d\u4f20\u9012\u3002"]}),"\n",(0,t.jsx)(n.p,{children:"\u8bbe\u7f6e\u6b64\u5185\u5bb9\u7684\u65b9\u6cd5\u5982\u4e0b\uff1a"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(i.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import litellm\nfrom litellm import CustomLLM, completion, get_llm_provider\n\n\nclass MyCustomLLM(CustomLLM):\n    def completion(self, *args, **kwargs) -> litellm.ModelResponse:\n        assert kwargs["optional_params"] == {"my_custom_param": "my-custom-param"}  # \ud83d\udc48 \u68c0\u67e5\u6b64\u5904\n        return litellm.completion(\n            model="gpt-3.5-turbo",\n            messages=[{"role": "user", "content": "Hello world"}],\n            mock_response="Hi!",\n        )  # type: ignore\n\nmy_custom_llm = MyCustomLLM()\n\nlitellm.custom_provider_map = [  # \ud83d\udc48 \u5173\u952e\u6b65\u9aa4 - \u6ce8\u518c\u5904\u7406\u5668\n    {"provider": "my-custom-llm", "custom_handler": my_custom_llm}\n]\n\nresp = completion(model="my-custom-llm/my-model", my_custom_param="my-custom-param")\n'})})}),(0,t.jsxs)(i.A,{value:"proxy",label:"\u4ee3\u7406",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\u8bbe\u7f6e\u60a8\u7684 ",(0,t.jsx)(n.code,{children:"custom_handler.py"})," \u6587\u4ef6"]}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import litellm\nfrom litellm import CustomLLM\nfrom litellm.types.utils import ImageResponse, ImageObject\n\n\nclass MyCustomLLM(CustomLLM):\n    async def aimage_generation(self, model: str, prompt: str, model_response: ImageResponse, optional_params: dict, logging_obj: Any, timeout: Optional[Union[float, httpx.Timeout]] = None, client: Optional[AsyncHTTPHandler] = None,) -> ImageResponse:\n        assert optional_params == {"my_custom_param": "my-custom-param"}  # \ud83d\udc48 \u68c0\u67e5\u6b64\u5904\n        return ImageResponse(\n            created=int(time.time()),\n            data=[ImageObject(url="https://example.com/image.png")],\n        )\n\nmy_custom_llm = MyCustomLLM()\n'})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsxs)(n.li,{children:["\u6dfb\u52a0\u5230 ",(0,t.jsx)(n.code,{children:"config.yaml"})]}),"\n"]}),(0,t.jsx)(n.p,{children:"\u5728\u4ee5\u4e0b\u914d\u7f6e\u4e2d\uff0c\u6211\u4eec\u4f20\u9012"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["python_filename: ",(0,t.jsx)(n.code,{children:"custom_handler.py"})]}),"\n",(0,t.jsxs)(n.li,{children:["custom_handler_instance_name: ",(0,t.jsx)(n.code,{children:"my_custom_llm"}),"\u3002\u8fd9\u5b9a\u4e49\u5728\u6b65\u9aa41"]}),"\n",(0,t.jsxs)(n.li,{children:["custom_handler: ",(0,t.jsx)(n.code,{children:"custom_handler.my_custom_llm"})]}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'model_list:\n  - model_name: "test-model"             \n    litellm_params:\n      model: "openai/text-embedding-ada-002"\n  - model_name: "my-custom-model"\n    litellm_params:\n      model: "my-custom-llm/my-model"\n      my_custom_param: "my-custom-param"  # \ud83d\udc48 \u81ea\u5b9a\u4e49\u53c2\u6570\n\nlitellm_settings:\n  custom_provider_map:\n  - {"provider": "my-custom-llm", "custom_handler": custom_handler.my_custom_llm}\n'})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"\u6d4b\u8bd5\u5b83!"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"curl -X POST 'http://0.0.0.0:4000/v1/images/generations' \\\n-H 'Content-Type: application/json' \\\n-H 'Authorization: Bearer sk-1234' \\\n-d '{\n    \"model\": \"my-custom-model\",\n    \"prompt\": \"A cute baby sea otter\",\n}'\n"})})]})]}),"\n",(0,t.jsx)(n.h2,{id:"\u81ea\u5b9a\u4e49\u5904\u7406\u5668\u89c4\u8303",children:"\u81ea\u5b9a\u4e49\u5904\u7406\u5668\u89c4\u8303"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm.types.utils import GenericStreamingChunk, ModelResponse, ImageResponse\nfrom typing import Iterator, AsyncIterator, Any, Optional, Union\nfrom litellm.llms.base import BaseLLM\n\nclass CustomLLMError(Exception):  # \u4f7f\u7528\u6b64\u4e3a\u6240\u6709\u60a8\u7684\u5f02\u5e38\n    def __init__(\n        self,\n        status_code,\n        message,\n    ):\n        self.status_code = status_code\n        self.message = message\n        super().__init__(\n            self.message\n        )  # \u8c03\u7528\u57fa\u7c7b\u6784\u9020\u51fd\u6570\u5e76\u4f20\u9012\u5b83\u9700\u8981\u7684\u53c2\u6570\n\nclass CustomLLM(BaseLLM):\n    def __init__(self) -> None:\n        super().__init__()\n\n    def completion(self, *args, **kwargs) -> ModelResponse:\n        raise CustomLLMError(status_code=500, message="Not implemented yet!")\n\n    def streaming(self, *args, **kwargs) -> Iterator[GenericStreamingChunk]:\n        raise CustomLLMError(status_code=500, message="Not implemented yet!")\n\n    async def acompletion(self, *args, **kwargs) -> ModelResponse:\n        raise CustomLLMError(status_code=500, message="Not implemented yet!")\n\n    async def astreaming(self, *args, **kwargs) -> AsyncIterator[GenericStreamingChunk]:\n        raise CustomLLMError(status_code=500, message="Not implemented yet!")\n\n    def image_generation(\n        self,\n        model: str,\n        prompt: str,\n        model_response: ImageResponse,\n        optional_params: dict,\n        logging_obj: Any,\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        client: Optional[HTTPHandler] = None,\n    ) -> ImageResponse:\n        raise CustomLLMError(status_code=500, message="Not implemented yet!")\n\n    async def aimage_generation(\n        self,\n        model: str,\n        prompt: str,\n        model_response: ImageResponse,\n        optional_params: dict,\n        logging_obj: Any,\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        client: Optional[AsyncHTTPHandler] = None,\n    ) -> ImageResponse:\n        raise CustomLLMError(status_code=500, message="Not implemented yet!")\n\n    def image_edit(\n        self,\n        model: str,\n        image: Any,\n        prompt: str,\n        model_response: ImageResponse,\n        api_key: Optional[str],\n        api_base: Optional[str],\n        optional_params: dict,\n        logging_obj: Any,\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        client: Optional[HTTPHandler] = None,\n    ) -> ImageResponse:\n        raise CustomLLMError(status_code=500, message="Not implemented yet!")\n\n    async def aimage_edit(\n        self,\n        model: str,\n        image: Any,\n        prompt: str,\n        model_response: ImageResponse,\n        api_key: Optional[str],\n        api_base: Optional[str],\n        optional_params: dict,\n        logging_obj: Any,\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        client: Optional[AsyncHTTPHandler] = None,\n    ) -> ImageResponse:\n        raise CustomLLMError(status_code=500, message="Not implemented yet!")\n'})})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}}}]);