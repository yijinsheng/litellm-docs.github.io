"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[44040],{28453(e,n,t){t.d(n,{R:()=>o,x:()=>a});var i=t(96540);const s={},r=i.createContext(s);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:n},e.children)}},76602(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>p});const i=JSON.parse('{"id":"load_test_rpm","title":"\u591a\u5b9e\u4f8b TPM/RPM (litellm.Router)","description":"\u6d4b\u8bd5\u4f60\u5b9a\u4e49\u7684 tpm/rpm \u9650\u5236\u5728\u591a\u4e2a Router \u5b9e\u4f8b\u4e2d\u662f\u5426\u88ab\u9075\u5b88\u3002","source":"@site/docs/load_test_rpm.md","sourceDirName":".","slug":"/load_test_rpm","permalink":"/docs/load_test_rpm","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"LiteLLM SDK vs OpenAI","permalink":"/docs/load_test_sdk"},"next":{"title":"Open WebUI","permalink":"/docs/tutorials/openweb_ui"}}');var s=t(74848),r=t(28453);const o={},a="\u591a\u5b9e\u4f8b TPM/RPM (litellm.Router)",l={},p=[{value:"\u4ee3\u7801",id:"\u4ee3\u7801",level:3},{value:"\u591a\u5b9e\u4f8b TPM/RPM \u8d1f\u8f7d\u6d4b\u8bd5\uff08\u4ee3\u7406\uff09",id:"\u591a\u5b9e\u4f8b-tpmrpm-\u8d1f\u8f7d\u6d4b\u8bd5\u4ee3\u7406",level:2},{value:"1. \u914d\u7f6e\u8bbe\u7f6e",id:"1-\u914d\u7f6e\u8bbe\u7f6e",level:3},{value:"2. \u542f\u52a8\u4e24\u4e2a\u4ee3\u7406\u5b9e\u4f8b",id:"2-\u542f\u52a8\u4e24\u4e2a\u4ee3\u7406\u5b9e\u4f8b",level:3},{value:"3. \u6267\u884c\u6d4b\u8bd5",id:"3-\u6267\u884c\u6d4b\u8bd5",level:3},{value:"\u989d\u5916\u5185\u5bb9 - \u8bbe\u7f6e\u5047\u7684 openai \u670d\u52a1\u5668",id:"\u989d\u5916\u5185\u5bb9---\u8bbe\u7f6e\u5047\u7684-openai-\u670d\u52a1\u5668",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"\u591a\u5b9e\u4f8b-tpmrpm-litellmrouter",children:"\u591a\u5b9e\u4f8b TPM/RPM (litellm.Router)"})}),"\n",(0,s.jsx)(n.p,{children:"\u6d4b\u8bd5\u4f60\u5b9a\u4e49\u7684 tpm/rpm \u9650\u5236\u5728\u591a\u4e2a Router \u5b9e\u4f8b\u4e2d\u662f\u5426\u88ab\u9075\u5b88\u3002"}),"\n",(0,s.jsx)(n.p,{children:"\u5728\u6211\u4eec\u7684\u6d4b\u8bd5\u4e2d\uff1a"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u6bcf\u4e2a\u90e8\u7f72\u7684\u6700\u5927 RPM = \u6bcf\u5206\u949f 100 \u6b21\u8bf7\u6c42"}),"\n",(0,s.jsx)(n.li,{children:"\u8def\u7531\u5668\u6bcf\u5206\u949f\u6700\u5927\u541e\u5410\u91cf = \u6bcf\u5206\u949f 200 \u6b21\u8bf7\u6c42\uff08\u4e24\u4e2a\u90e8\u7f72\uff09"}),"\n",(0,s.jsx)(n.li,{children:"\u6211\u4eec\u5c06\u901a\u8fc7\u8def\u7531\u5668\u53d1\u9001\u7684\u8bf7\u6c42\u6570 = \u6bcf\u5206\u949f 600 \u6b21\u8bf7\u6c42"}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.p,{children:["\u5982\u679c\u4f60\u4e0d\u60f3\u8c03\u7528\u771f\u5b9e\u7684 LLM API \u7aef\u70b9\uff0c\u53ef\u4ee5\u8bbe\u7f6e\u4e00\u4e2a\u5047\u7684 openai \u670d\u52a1\u5668\u3002",(0,s.jsx)(n.a,{href:"#%E9%A2%9D%E5%A4%96---%E8%AE%BE%E7%BD%AE%E5%81%87openai%E6%9C%8D%E5%8A%A1%E5%99%A8",children:"\u67e5\u770b\u4ee3\u7801"})]})}),"\n",(0,s.jsx)(n.h3,{id:"\u4ee3\u7801",children:"\u4ee3\u7801"}),"\n",(0,s.jsx)(n.p,{children:"\u8ba9\u6211\u4eec\u4ee5\u6bcf\u5206\u949f 600 \u6b21\u8bf7\u6c42\u7684\u901f\u5ea6\u51fb\u6253\u8def\u7531\u5668\u3002"}),"\n",(0,s.jsxs)(n.p,{children:["\u590d\u5236\u6b64\u811a\u672c \ud83d\udc47\u3002\u5c06\u5176\u4fdd\u5b58\u4e3a ",(0,s.jsx)(n.code,{children:"test_loadtest_router.py"})," \u5e76\u4f7f\u7528 ",(0,s.jsx)(n.code,{children:"python3 test_loadtest_router.py"})," \u8fd0\u884c\u5b83"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import Router \nimport litellm\nlitellm.suppress_debug_info = True\nlitellm.set_verbose = False\nimport logging\nlogging.basicConfig(level=logging.CRITICAL)\nimport os, random, uuid, time, asyncio\n\n# OpenAI \u548c Anthropic \u6a21\u578b\u7684\u6a21\u578b\u5217\u8868\nmodel_list = [\n    {\n        "model_name": "fake-openai-endpoint",\n        "litellm_params": {\n            "model": "gpt-3.5-turbo",\n            "api_key": "my-fake-key",\n            "api_base": "http://0.0.0.0:8080",\n            "rpm": 100\n        },\n    },\n    {\n        "model_name": "fake-openai-endpoint",\n        "litellm_params": {\n            "model": "gpt-3.5-turbo",\n            "api_key": "my-fake-key",\n            "api_base": "http://0.0.0.0:8081",\n            "rpm": 100\n        },\n    },\n]\n\nrouter_1 = Router(model_list=model_list, num_retries=0, enable_pre_call_checks=True, routing_strategy="simple-shuffle", redis_host=os.getenv("REDIS_HOST"), redis_port=os.getenv("REDIS_PORT"), redis_password=os.getenv("REDIS_PASSWORD"))\nrouter_2 = Router(model_list=model_list, num_retries=0, routing_strategy="simple-shuffle", enable_pre_call_checks=True, redis_host=os.getenv("REDIS_HOST"), redis_port=os.getenv("REDIS_PORT"), redis_password=os.getenv("REDIS_PASSWORD"))\n\n\n\nasync def router_completion_non_streaming():\n  try:\n    client: Router = random.sample([router_1, router_2], 1)[0] # \u968f\u673a\u9009\u62e9\u4e00\u4e2a\u5ba2\u6237\u7aef\n    response = await client.acompletion(\n              model="fake-openai-endpoint", # [\u66f4\u6539\u6b64\u90e8\u5206]\uff08\u5982\u679c\u4f60\u5728\u4ee3\u7406\u4e2d\u8c03\u7528\u7684\u540d\u79f0\u4e0d\u540c\uff09\n              messages=[{"role": "user", "content": f"This is a test: {uuid.uuid4()}"}],\n          )\n    return response\n  except Exception as e:\n    # print(e)\n    return None\n  \nasync def loadtest_fn():\n    start = time.time()\n    n = 600  # \u5e76\u53d1\u4efb\u52a1\u7684\u6570\u91cf\n    tasks = [router_completion_non_streaming() for _ in range(n)]\n    chat_completions = await asyncio.gather(*tasks)\n    successful_completions = [c for c in chat_completions if c is not None]\n    print(n, time.time() - start, len(successful_completions))\n\ndef get_utc_datetime():\n    import datetime as dt\n    from datetime import datetime\n\n    if hasattr(dt, "UTC"):\n        return datetime.now(dt.UTC)  # type: ignore\n    else:\n        return datetime.utcnow()  # type: ignore\n\n\n# \u8fd0\u884c\u4e8b\u4ef6\u5faa\u73af\u4ee5\u6267\u884c\u5f02\u6b65\u51fd\u6570\nasync def parent_fn():\n  for _ in range(10):\n    dt = get_utc_datetime()\n    current_minute = dt.strftime("%H-%M")\n    print(f"\u89e6\u53d1\u65b0\u6279\u6b21 - {current_minute}")\n    await loadtest_fn()\n    await asyncio.sleep(10)\n\nasyncio.run(parent_fn())\n'})}),"\n",(0,s.jsx)(n.h2,{id:"\u591a\u5b9e\u4f8b-tpmrpm-\u8d1f\u8f7d\u6d4b\u8bd5\u4ee3\u7406",children:"\u591a\u5b9e\u4f8b TPM/RPM \u8d1f\u8f7d\u6d4b\u8bd5\uff08\u4ee3\u7406\uff09"}),"\n",(0,s.jsx)(n.p,{children:"\u6d4b\u8bd5\u4f60\u5b9a\u4e49\u7684 tpm/rpm \u9650\u5236\u5728\u591a\u4e2a\u5b9e\u4f8b\u4e2d\u662f\u5426\u88ab\u9075\u5b88\u3002"}),"\n",(0,s.jsxs)(n.p,{children:["\u6700\u5feb\u7684\u65b9\u6cd5\u662f\u901a\u8fc7\u6d4b\u8bd5 ",(0,s.jsx)(n.a,{href:"/docs/proxy/quick_start",children:"\u4ee3\u7406"})," \u6765\u5b9e\u73b0\u3002\u4ee3\u7406\u4f7f\u7528\u4e86 ",(0,s.jsx)(n.a,{href:"/docs/routing",children:"\u8def\u7531\u5668"}),"\uff0c\u56e0\u6b64\u5982\u679c\u4f60\u6b63\u5728\u4f7f\u7528\u5176\u4e2d\u4efb\u4f55\u4e00\u4e2a\uff0c\u6b64\u6d4b\u8bd5\u5e94\u8be5\u5bf9\u4f60\u6709\u6548\u3002"]}),"\n",(0,s.jsx)(n.p,{children:"\u5728\u6211\u4eec\u7684\u6d4b\u8bd5\u4e2d\uff1a"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u6bcf\u4e2a\u90e8\u7f72\u7684\u6700\u5927 RPM = \u6bcf\u5206\u949f 100 \u6b21\u8bf7\u6c42"}),"\n",(0,s.jsx)(n.li,{children:"\u4ee3\u7406\u6bcf\u5206\u949f\u6700\u5927\u541e\u5410\u91cf = \u6bcf\u5206\u949f 200 \u6b21\u8bf7\u6c42\uff08\u4e24\u4e2a\u90e8\u7f72\uff09"}),"\n",(0,s.jsx)(n.li,{children:"\u6211\u4eec\u5c06\u53d1\u9001\u5230\u4ee3\u7406\u7684\u8bf7\u6c42\u6570 = \u6bcf\u5206\u949f 600 \u6b21\u8bf7\u6c42"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"\u56e0\u6b64\u6211\u4eec\u5c06\u4ee5\u6bcf\u5206\u949f 600 \u6b21\u8bf7\u6c42\u7684\u901f\u5ea6\u53d1\u9001\u8bf7\u6c42\uff0c\u4f46\u9884\u671f\u53ea\u6709 200 \u6b21\u8bf7\u6c42\u4f1a\u6210\u529f\u3002"}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.p,{children:["\u5982\u679c\u4f60\u4e0d\u60f3\u8c03\u7528\u771f\u5b9e\u7684 LLM API \u7aef\u70b9\uff0c\u53ef\u4ee5\u8bbe\u7f6e\u4e00\u4e2a\u5047\u7684 openai \u670d\u52a1\u5668\u3002",(0,s.jsx)(n.a,{href:"#%E9%A2%9D%E5%A4%96---%E8%AE%BE%E7%BD%AE%E5%81%87openai%E6%9C%8D%E5%8A%A1%E5%99%A8",children:"\u67e5\u770b\u4ee3\u7801"})]})}),"\n",(0,s.jsx)(n.h3,{id:"1-\u914d\u7f6e\u8bbe\u7f6e",children:"1. \u914d\u7f6e\u8bbe\u7f6e"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"model_list:\n- litellm_params:\n    api_base: http://0.0.0.0:8080\n    api_key: my-fake-key\n    model: openai/my-fake-model\n    rpm: 100\n  model_name: fake-openai-endpoint\n- litellm_params:\n    api_base: http://0.0.0.0:8081\n    api_key: my-fake-key\n    model: openai/my-fake-model-2\n    rpm: 100\n  model_name: fake-openai-endpoint\nrouter_settings:\n  num_retries: 0\n  enable_pre_call_checks: true\n  redis_host: os.environ/REDIS_HOST ## \ud83d\udc48 \u91cd\u8981\uff01\u8bf7\u4f7f\u7528 Redis \u914d\u7f6e\u4ee3\u7406\n  redis_password: os.environ/REDIS_PASSWORD\n  redis_port: os.environ/REDIS_PORT\n  routing_strategy: simple-shuffle # \u63a8\u8350\u7528\u4e8e\u6700\u4f73\u6027\u80fd\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-\u542f\u52a8\u4e24\u4e2a\u4ee3\u7406\u5b9e\u4f8b",children:"2. \u542f\u52a8\u4e24\u4e2a\u4ee3\u7406\u5b9e\u4f8b"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"\u5b9e\u4f8b1"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml --port 4000\n\n## \u8fd0\u884c\u5728 http://0.0.0.0:4000\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"\u5b9e\u4f8b2"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml --port 4001\n\n## \u8fd0\u884c\u5728 http://0.0.0.0:4001\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-\u6267\u884c\u6d4b\u8bd5",children:"3. \u6267\u884c\u6d4b\u8bd5"}),"\n",(0,s.jsx)(n.p,{children:"\u8ba9\u6211\u4eec\u4ee5\u6bcf\u5206\u949f 600 \u6b21\u8bf7\u6c42\u7684\u901f\u5ea6\u51fb\u6253\u4ee3\u7406\u3002"}),"\n",(0,s.jsxs)(n.p,{children:["\u590d\u5236\u6b64\u811a\u672c \ud83d\udc47\u3002\u5c06\u5176\u4fdd\u5b58\u4e3a ",(0,s.jsx)(n.code,{children:"test_loadtest_proxy.py"})," \u5e76\u4f7f\u7528 ",(0,s.jsx)(n.code,{children:"python3 test_loadtest_proxy.py"})," \u8fd0\u884c\u5b83"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from openai import AsyncOpenAI, AsyncAzureOpenAI\nimport random, uuid\nimport time, asyncio, litellm\n# import logging\n# logging.basicConfig(level=logging.DEBUG)\n#### LITELLM \u4ee3\u7406 #### \nlitellm_client = AsyncOpenAI(\n    api_key="sk-1234", # [\u66f4\u6539\u6b64\u90e8\u5206]\n    base_url="http://0.0.0.0:4000"\n)\nlitellm_client_2 = AsyncOpenAI(\n    api_key="sk-1234", # [\u66f4\u6539\u6b64\u90e8\u5206]\n    base_url="http://0.0.0.0:4001"\n)\n\nasync def proxy_completion_non_streaming():\n  try:\n    client = random.sample([litellm_client, litellm_client_2], 1)[0] # \u968f\u673a\u9009\u62e9\u4e00\u4e2a\u5ba2\u6237\u7aef\n    response = await client.chat.completions.create(\n              model="fake-openai-endpoint", # [\u66f4\u6539\u6b64\u90e8\u5206]\uff08\u5982\u679c\u4f60\u5728\u4ee3\u7406\u4e2d\u8c03\u7528\u7684\u540d\u79f0\u4e0d\u540c\uff09\n              messages=[{"role": "user", "content": f"This is a test: {uuid.uuid4()}"}],\n          )\n    return response\n  except Exception as e:\n    # print(e)\n    return None\n  \nasync def loadtest_fn():\n    start = time.time()\n    n = 600  # \u5e76\u53d1\u4efb\u52a1\u7684\u6570\u91cf\n    tasks = [proxy_completion_non_streaming() for _ in range(n)]\n    chat_completions = await asyncio.gather(*tasks)\n    successful_completions = [c for c in chat_completions if c is not None]\n    print(n, time.time() - start, len(successful_completions))\n\ndef get_utc_datetime():\n    import datetime as dt\n    from datetime import datetime\n\n    if hasattr(dt, "UTC"):\n        return datetime.now(dt.UTC)  # type: ignore\n    else:\n        return datetime.utcnow()  # type: ignore\n\n\n# \u8fd0\u884c\u4e8b\u4ef6\u5faa\u73af\u4ee5\u6267\u884c\u5f02\u6b65\u51fd\u6570\nasync def parent_fn():\n  for _ in range(10):\n    dt = get_utc_datetime()\n    current_minute = dt.strftime("%H-%M")\n    print(f"\u89e6\u53d1\u65b0\u6279\u6b21 - {current_minute}")\n    await loadtest_fn()\n    await asyncio.sleep(10)\n\nasyncio.run(parent_fn())\n\n'})}),"\n",(0,s.jsx)(n.h3,{id:"\u989d\u5916\u5185\u5bb9---\u8bbe\u7f6e\u5047\u7684-openai-\u670d\u52a1\u5668",children:"\u989d\u5916\u5185\u5bb9 - \u8bbe\u7f6e\u5047\u7684 openai \u670d\u52a1\u5668"}),"\n",(0,s.jsx)(n.p,{children:"\u8ba9\u6211\u4eec\u8bbe\u7f6e\u4e00\u4e2a\u5e26\u6709\u6bcf\u5206\u949f 100 \u6b21\u8bf7\u6c42\u9650\u5236\u7684\u5047 openai \u670d\u52a1\u5668\u3002"}),"\n",(0,s.jsxs)(n.p,{children:["\u6211\u4eec\u5c06\u6587\u4ef6\u547d\u540d\u4e3a ",(0,s.jsx)(n.code,{children:"fake_openai_server.py"}),"\u3002"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# from sys import path, os\n# path.insert(\n#     0, os.path.abspath("../")\n# )  # \u5c06\u7236\u76ee\u5f55\u6dfb\u52a0\u5230\u7cfb\u7edf\u8def\u5f84\u4e2d\nfrom fastapi import FastAPI, Request, status, HTTPException, Depends\nfrom fastapi.responses import StreamingResponse\nfrom fastapi.security import OAuth2PasswordBearer\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\nfrom fastapi import FastAPI, Request, HTTPException, UploadFile, File\nimport httpx, os, json\nfrom openai import AsyncOpenAI\nfrom typing import Optional\nfrom slowapi import Limiter\nfrom slowapi.util import get_remote_address\nfrom slowapi.errors import RateLimitExceeded\nfrom fastapi import FastAPI, Request, HTTPException\n\nclass ProxyException(Exception):\n    # \u6ce8\u610f\uff1a\u8bf7\u52ff\u4fee\u6539\u6b64\u90e8\u5206\n    # \u7528\u4e8e\u4e0e OPENAI \u5f02\u5e38\u7cbe\u786e\u5bf9\u5e94\n    def __init__(\n        self,\n        message: str,\n        type: str,\n        param: Optional[str],\n        code: Optional[int],\n    ):\n        self.message = message\n        self.type = type\n        self.param = param\n        self.code = code\n\n    def to_dict(self) -> dict:\n        """\u5c06 ProxyException \u5b9e\u4f8b\u8f6c\u6362\u4e3a\u5b57\u5178\u3002"""\n        return {\n            "message": self.message,\n            "type": self.type,\n            "param": self.param,\n            "code": self.code,\n        }\n\nlimiter = Limiter(key_func=get_remote_address)\napp = FastAPI()\napp.state.limiter = limiter\n\n@app.exception_handler(RateLimitExceeded)\nasync def _rate_limit_exceeded_handler(request: Request, exc: RateLimitExceeded):\n    return JSONResponse(status_code=429,\n                        content={"detail": "Rate Limited!"})\n\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=["*"],\n    allow_credentials=True,\n    allow_methods=["*"],\n    allow_headers=["*"],\n)\n\n# \u5b8c\u6210\n@app.post("/chat/completions")\n@app.post("/v1/chat/completions")\n@limiter.limit("100/minute")\nasync def completion(request: Request):\n    # raise HTTPException(status_code=429, detail="Rate Limited!")\n    return {\n        "id": "chatcmpl-123",\n        "object": "chat.completion",\n        "created": 1677652288,\n        "model": None,\n        "system_fingerprint": "fp_44709d6fcb",\n        "choices": [{\n            "index": 0,\n            "message": {\n            "role": "assistant",\n            "content": "\\n\\nHello there, how may I assist you today?",\n            },\n            "logprobs": None,\n            "finish_reason": "stop"\n        }],\n        "usage": {\n            "prompt_tokens": 9,\n            "completion_tokens": 12,\n            "total_tokens": 21\n        }\n    }\n\nif __name__ == "__main__":\n    import socket\n    import uvicorn\n    port = 8080\n    while True:\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        result = sock.connect_ex((\'0.0.0.0\', port))\n        if result != 0:\n            print(f"\u7aef\u53e3 {port} \u53ef\u7528\uff0c\u5f00\u59cb\u670d\u52a1\u5668...")\n            break\n        else:\n            port += 1\n\n    uvicorn.run(app, host="0.0.0.0", port=port)\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python3 fake_openai_server.py\n"})})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);