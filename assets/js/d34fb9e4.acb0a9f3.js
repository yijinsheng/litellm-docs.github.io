"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[20306],{28453(e,n,o){o.d(n,{R:()=>i,x:()=>r});var t=o(96540);const s={},l=t.createContext(s);function i(e){const n=t.useContext(l);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(l.Provider,{value:n},e.children)}},36731(e,n,o){o.r(n),o.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>m,frontMatter:()=>i,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"tutorials/model_fallbacks","title":"Model Fallbacks w/ LiteLLM","description":"\u8fd9\u662f\u5982\u4f55\u4f7f\u7528 LiteLLM \u5728 3 \u4e2a LLM \u63d0\u4f9b\u5546\uff08OpenAI, Anthropic, Azure\uff09\u4e4b\u95f4\u5b9e\u73b0\u6a21\u578b\u56de\u9000\u7684\u65b9\u6cd5\u3002","source":"@site/docs/tutorials/model_fallbacks.md","sourceDirName":"tutorials","slug":"/tutorials/model_fallbacks","permalink":"/docs/tutorials/model_fallbacks","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"\u521b\u5efa\u4f60\u7684\u7b2c\u4e00\u4e2aLLM\u6e38\u4e50\u573a","permalink":"/docs/tutorials/first_playground"},"next":{"title":"\u8d21\u732e\u4ee3\u7801","permalink":"/docs/extras/contributing_code"}}');var s=o(74848),l=o(28453);const i={},r="Model Fallbacks w/ LiteLLM",a={},c=[{value:"1. \u5b89\u88c5 LiteLLM",id:"1-\u5b89\u88c5-litellm",level:2},{value:"2. \u57fa\u672c\u7684\u56de\u9000\u4ee3\u7801",id:"2-\u57fa\u672c\u7684\u56de\u9000\u4ee3\u7801",level:2},{value:"3. \u4e0a\u4e0b\u6587\u7a97\u53e3\u5f02\u5e38",id:"3-\u4e0a\u4e0b\u6587\u7a97\u53e3\u5f02\u5e38",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"model-fallbacks-w-litellm",children:"Model Fallbacks w/ LiteLLM"})}),"\n",(0,s.jsx)(n.p,{children:"\u8fd9\u662f\u5982\u4f55\u4f7f\u7528 LiteLLM \u5728 3 \u4e2a LLM \u63d0\u4f9b\u5546\uff08OpenAI, Anthropic, Azure\uff09\u4e4b\u95f4\u5b9e\u73b0\u6a21\u578b\u56de\u9000\u7684\u65b9\u6cd5\u3002"}),"\n",(0,s.jsx)(n.h2,{id:"1-\u5b89\u88c5-litellm",children:"1. \u5b89\u88c5 LiteLLM"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"!pip install litellm\n"})}),"\n",(0,s.jsx)(n.h2,{id:"2-\u57fa\u672c\u7684\u56de\u9000\u4ee3\u7801",children:"2. \u57fa\u672c\u7684\u56de\u9000\u4ee3\u7801"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import litellm\nfrom litellm import embedding, completion\n\n# \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\nos.environ["OPENAI_API_KEY"] = ""\nos.environ["ANTHROPIC_API_KEY"] = ""\nos.environ["AZURE_API_KEY"] = ""\nos.environ["AZURE_API_BASE"] = ""\nos.environ["AZURE_API_VERSION"] = ""\n\nmodel_fallback_list = ["claude-instant-1", "gpt-3.5-turbo", "chatgpt-test"]\n\nuser_message = "Hello, how are you?"\nmessages = [{ "content": user_message,"role": "user"}]\n\nfor model in model_fallback_list:\n  try:\n      response = completion(model=model, messages=messages)\n  except Exception as e:\n      print(f"\u53d1\u751f\u9519\u8bef: {traceback.format_exc()}")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"3-\u4e0a\u4e0b\u6587\u7a97\u53e3\u5f02\u5e38",children:"3. \u4e0a\u4e0b\u6587\u7a97\u53e3\u5f02\u5e38"}),"\n",(0,s.jsxs)(n.p,{children:["LiteLLM \u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b50\u7c7b ",(0,s.jsx)(n.code,{children:"ContextWindowExceededError"}),"\uff0c\u7528\u4e8e\u5904\u7406\u4e0a\u4e0b\u6587\u7a97\u53e3\u8d85\u51fa\u7684\u9519\u8bef (",(0,s.jsx)(n.a,{href:"https://docs.litellm.ai/docs/exception_mapping",children:"\u6587\u6863"}),")\u3002"]}),"\n",(0,s.jsx)(n.p,{children:"\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7a97\u53e3\u5f02\u5e38\u5b9e\u73b0\u6a21\u578b\u56de\u9000\u3002"}),"\n",(0,s.jsxs)(n.p,{children:["LiteLLM \u8fd8\u66b4\u9732\u4e86 ",(0,s.jsx)(n.code,{children:"get_max_tokens()"})," \u51fd\u6570\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528\u8be5\u51fd\u6570\u6765\u8bc6\u522b\u5df2\u8d85\u51fa\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u3002"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import litellm\nfrom litellm import completion, ContextWindowExceededError, get_max_tokens\n\n# \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\nos.environ["OPENAI_API_KEY"] = ""\nos.environ["COHERE_API_KEY"] = ""\nos.environ["ANTHROPIC_API_KEY"] = ""\nos.environ["AZURE_API_KEY"] = ""\nos.environ["AZURE_API_BASE"] = ""\nos.environ["AZURE_API_VERSION"] = ""\n\ncontext_window_fallback_list = [{"model":"gpt-3.5-turbo-16k", "max_tokens": 16385}, {"model":"gpt-4-32k", "max_tokens": 32768}, {"model": "claude-instant-1", "max_tokens":100000}]\n\nuser_message = "Hello, how are you?"\nmessages = [{ "content": user_message,"role": "user"}]\n\ninitial_model = "command-nightly"\ntry:\n    response = completion(model=initial_model, messages=messages)\nexcept ContextWindowExceededError as e:\n    model_max_tokens = get_max_tokens(initial_model)\n    for model in context_window_fallback_list:\n        if model_max_tokens < model["max_tokens"]:\n            try:\n                response = completion(model=model["model"], messages=messages)\n                return response\n            except ContextWindowExceededError as e:\n                model_max_tokens = get_max_tokens(model["model"])\n                continue\n\nprint(response)\n'})})]})}function m(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);