"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[73099],{83890(n){n.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"gemini_3_flash","metadata":{"permalink":"/blog/gemini_3_flash","source":"@site/blog/gemini_3_flash/index.md","title":"DAY 0 Support: Gemini 3 Flash on LiteLLM","description":"LiteLLM now supports gemini-3-flash-preview and all the new API changes along with it.","date":"2025-12-17T10:00:00.000Z","tags":[{"inline":true,"label":"gemini","permalink":"/blog/tags/gemini"},{"inline":true,"label":"day 0 support","permalink":"/blog/tags/day-0-support"},{"inline":true,"label":"llms","permalink":"/blog/tags/llms"}],"hasTruncateMarker":false,"authors":[{"name":"Sameer Kankute","title":"SWE @ LiteLLM (LLM Translation)","url":"https://www.linkedin.com/in/sameer-kankute/","image_url":"https://pbs.twimg.com/profile_images/2001352686994907136/ONgNuSk5_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/2001352686994907136/ONgNuSk5_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"gemini_3_flash","title":"DAY 0 Support: Gemini 3 Flash on LiteLLM","date":"2025-12-17T10:00:00.000Z","authors":[{"name":"Sameer Kankute","title":"SWE @ LiteLLM (LLM Translation)","url":"https://www.linkedin.com/in/sameer-kankute/","image_url":"https://pbs.twimg.com/profile_images/2001352686994907136/ONgNuSk5_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/2001352686994907136/ONgNuSk5_400x400.jpg"},{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"tags":["gemini","day 0 support","llms"],"hide_table_of_contents":false},"unlisted":false,"nextItem":{"title":"Day 0 Support: Claude 4.5 Opus (+Advanced Features)","permalink":"/blog/anthropic_advanced_features"}},"content":"import Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n\\nLiteLLM now supports `gemini-3-flash-preview` and all the new API changes along with it.\\n\\n:::note\\nIf you only want cost tracking, you need no change in your current Litellm version. But if you want the support for new features introduced along with it like thinking levels, you will need to use v1.80.8-stable.1 or above.\\n:::\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\nghcr.io/berriai/litellm:main-v1.80.8-stable.1\\n```\\n\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.80.8.post1\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n## What\'s New\\n\\n### 1. New Thinking Levels: `thinkingLevel` with MINIMAL & MEDIUM\\n\\nGemini 3 Flash introduces granular thinking control with `thinkingLevel` instead of `thinkingBudget`.\\n- **MINIMAL**: Ultra-lightweight thinking for fast responses\\n- **MEDIUM**: Balanced thinking for complex reasoning  \\n- **HIGH**: Maximum reasoning depth\\n\\nLiteLLM automatically maps the OpenAI `reasoning_effort` parameter to Gemini\'s `thinkingLevel`, so you can use familiar `reasoning_effort` values (`minimal`, `low`, `medium`, `high`) without changing your code!\\n\\n### 2. Thought Signatures\\n\\nLike `gemini-3-pro`, this model also includes thought signatures for tool calls. LiteLLM handles signature extraction and embedding internally. [Learn more about thought signatures](../gemini_3/index.md#thought-signatures).\\n\\n**Edge Case Handling**: If thought signatures are missing in the request, LiteLLM adds a dummy signature ensuring the API call doesn\'t break\\n\\n---\\n## Supported Endpoints\\n\\nLiteLLM provides **full end-to-end support** for Gemini 3 Flash on:\\n\\n- \u2705 `/v1/chat/completions` - OpenAI-compatible chat completions endpoint\\n- \u2705 `/v1/responses` - OpenAI Responses API endpoint (streaming and non-streaming)\\n- \u2705 [`/v1/messages`](../../docs/anthropic_unified) - Anthropic-compatible messages endpoint\\n- \u2705 `/v1/generateContent` \u2013 [Google Gemini API](../../docs/generateContent.md) compatible endpoint \\nAll endpoints support:\\n- Streaming and non-streaming responses\\n- Function calling with thought signatures\\n- Multi-turn conversations\\n- All Gemini 3-specific features\\n- Converstion of provider specific thinking related param to thinkingLevel\\n\\n## Quick Start\\n\\n<Tabs>\\n<TabItem value=\\"sdk\\" label=\\"SDK\\">\\n\\n**Basic Usage with MEDIUM thinking (NEW)**\\n\\n```python\\nfrom litellm import completion\\n\\n# No need to make any changes to your code as we map openai reasoning param to thinkingLevel\\nresponse = completion(\\n    model=\\"gemini/gemini-3-flash-preview\\",\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": \\"Solve this complex math problem: 25 * 4 + 10\\"}],\\n    reasoning_effort=\\"medium\\",  # NEW: MEDIUM thinking level\\n)\\n\\nprint(response.choices[0].message.content)\\n```\\n\\n</TabItem>\\n\\n<TabItem value=\\"proxy\\" label=\\"PROXY\\">\\n\\n**1. Setup config.yaml**\\n\\n```yaml\\nmodel_list:\\n  - model_name: gemini-3-flash\\n    litellm_params:\\n      model: gemini/gemini-3-flash-preview\\n      api_key: os.environ/GEMINI_API_KEY\\n```\\n\\n**2. Start proxy**\\n\\n```bash\\nlitellm --config /path/to/config.yaml\\n```\\n\\n**3. Call with MEDIUM thinking**\\n\\n```bash\\ncurl -X POST http://localhost:4000/v1/chat/completions \\\\\\n  -H \\"Content-Type: application/json\\" \\\\\\n  -H \\"Authorization: Bearer <YOUR-LITELLM-KEY>\\" \\\\\\n  -d \'{\\n    \\"model\\": \\"gemini-3-flash\\",\\n    \\"messages\\": [{\\"role\\": \\"user\\", \\"content\\": \\"Complex reasoning task\\"}],\\n    \\"reasoning_effort\\": \\"medium\\"\\n  }\'\\n``\'\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## All `reasoning_effort` Levels\\n\\n<Tabs>\\n<TabItem value=\\"minimal\\" label=\\"MINIMAL\\">\\n\\n**Ultra-fast, minimal reasoning**\\n\\n```python\\nfrom litellm import completion\\n\\nresponse = completion(\\n    model=\\"gemini/gemini-3-flash-preview\\",\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": \\"What\'s 2+2?\\"}],\\n    reasoning_effort=\\"minimal\\",\\n)\\n```\\n\\n</TabItem>\\n\\n<TabItem value=\\"low\\" label=\\"LOW\\">\\n\\n**Simple instruction following**\\n\\n```python\\nresponse = completion(\\n    model=\\"gemini/gemini-3-flash-preview\\",\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": \\"Write a haiku about coding\\"}],\\n    reasoning_effort=\\"low\\",\\n)\\n```\\n\\n</TabItem>\\n\\n<TabItem value=\\"medium\\" label=\\"MEDIUM (NEW)\\">\\n\\n**Balanced reasoning for complex tasks** \u2728\\n\\n```python\\nresponse = completion(\\n    model=\\"gemini/gemini-3-flash-preview\\",\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": \\"Analyze this dataset and find patterns\\"}],\\n    reasoning_effort=\\"medium\\",  # NEW!\\n)\\n```\\n\\n</TabItem>\\n\\n<TabItem value=\\"high\\" label=\\"HIGH\\">\\n\\n**Maximum reasoning depth**\\n\\n```python\\nresponse = completion(\\n    model=\\"gemini/gemini-3-flash-preview\\",\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": \\"Prove this mathematical theorem\\"}],\\n    reasoning_effort=\\"high\\",\\n)\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Features\\n\\n\u2705 **Thinking Levels**: MINIMAL, LOW, MEDIUM, HIGH  \\n\u2705 **Thought Signatures**: Track reasoning with unique identifiers  \\n\u2705 **Seamless Integration**: Works with existing OpenAI-compatible client  \\n\u2705 **Backward Compatible**: Gemini 2.5 models continue using `thinkingBudget`  \\n\\n---\\n\\n## Installation\\n\\n```bash\\npip install litellm --upgrade\\n```\\n\\n```python\\nimport litellm\\nfrom litellm import completion\\n\\nresponse = completion(\\n    model=\\"gemini/gemini-3-flash-preview\\",\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": \\"Your question here\\"}],\\n    reasoning_effort=\\"medium\\",  # Use MEDIUM thinking\\n)\\nprint(response)\\n```\\n\\n:::note\\nIf using this model via vertex_ai, keep the location as global as this is the only supported location as of now.\\n:::\\n\\n\\n## `reasoning_effort` Mapping for Gemini 3+\\n\\n| reasoning_effort | thinking_level | \\n|------------------|----------------|\\n| `minimal` | `minimal` |\\n| `low` | `low` |\\n| `medium` | `medium` |\\n| `high` | `high` |\\n| `disable` | `minimal` |\\n| `none` | `minimal` |"},{"id":"anthropic_advanced_features","metadata":{"permalink":"/blog/anthropic_advanced_features","source":"@site/blog/anthropic_opus_4_5_and_advanced_features/index.md","title":"Day 0 Support: Claude 4.5 Opus (+Advanced Features)","description":"This guide covers Anthropic\'s latest model (Claude Opus 4.5) and its advanced features now available in LiteLLM: Tool Search, Programmatic Tool Calling, Tool Input Examples, and the Effort Parameter.","date":"2025-11-25T10:00:00.000Z","tags":[{"inline":true,"label":"anthropic","permalink":"/blog/tags/anthropic"},{"inline":true,"label":"claude","permalink":"/blog/tags/claude"},{"inline":true,"label":"tool search","permalink":"/blog/tags/tool-search"},{"inline":true,"label":"programmatic tool calling","permalink":"/blog/tags/programmatic-tool-calling"},{"inline":true,"label":"effort","permalink":"/blog/tags/effort"},{"inline":true,"label":"advanced features","permalink":"/blog/tags/advanced-features"}],"hasTruncateMarker":false,"authors":[{"name":"Sameer Kankute","title":"SWE @ LiteLLM (LLM Translation)","url":"https://www.linkedin.com/in/sameer-kankute/","image_url":"https://pbs.twimg.com/profile_images/2001352686994907136/ONgNuSk5_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/2001352686994907136/ONgNuSk5_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"anthropic_advanced_features","title":"Day 0 Support: Claude 4.5 Opus (+Advanced Features)","date":"2025-11-25T10:00:00.000Z","authors":[{"name":"Sameer Kankute","title":"SWE @ LiteLLM (LLM Translation)","url":"https://www.linkedin.com/in/sameer-kankute/","image_url":"https://pbs.twimg.com/profile_images/2001352686994907136/ONgNuSk5_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/2001352686994907136/ONgNuSk5_400x400.jpg"},{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"tags":["anthropic","claude","tool search","programmatic tool calling","effort","advanced features"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"DAY 0 Support: Gemini 3 Flash on LiteLLM","permalink":"/blog/gemini_3_flash"},"nextItem":{"title":"DAY 0 Support: Gemini 3 on LiteLLM","permalink":"/blog/gemini_3"}},"content":"import Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\nThis guide covers Anthropic\'s latest model (Claude Opus 4.5) and its advanced features now available in LiteLLM: Tool Search, Programmatic Tool Calling, Tool Input Examples, and the Effort Parameter.\\n\\n---\\n\\n| Feature | Supported Models |\\n|---------|-----------------|\\n| Tool Search | Claude Opus 4.5, Sonnet 4.5 |\\n| Programmatic Tool Calling | Claude Opus 4.5, Sonnet 4.5 |\\n| Input Examples | Claude Opus 4.5, Sonnet 4.5 |\\n| Effort Parameter | Claude Opus 4.5 only |\\n\\nSupported Providers: [Anthropic](../../docs/providers/anthropic), [Bedrock](../../docs/providers/bedrock), [Vertex AI](../../docs/providers/vertex_partner#vertex-ai---anthropic-claude), [Azure AI](../../docs/providers/azure_ai).\\n\\n## Usage\\n\\n<Tabs>\\n<TabItem value=\\"sdk\\" label=\\"LiteLLM Python SDK\\">\\n\\n\\n```python\\nimport os\\nfrom litellm import completion\\n\\n# set env - [OPTIONAL] replace with your anthropic key\\nos.environ[\\"ANTHROPIC_API_KEY\\"] = \\"your-api-key\\"\\n\\nmessages = [{\\"role\\": \\"user\\", \\"content\\": \\"Hey! how\'s it going?\\"}]\\n\\n## OPENAI /chat/completions API format\\nresponse = completion(model=\\"claude-opus-4-5-20251101\\", messages=messages)\\nprint(response)\\n\\n```\\n\\n</TabItem>\\n<TabItem value=\\"proxy\\" label=\\"LiteLLM Proxy\\">\\n\\n**1. Setup config.yaml**\\n\\n```yaml\\nmodel_list:\\n  - model_name: claude-4 ### RECEIVED MODEL NAME ###\\n    litellm_params: # all params accepted by litellm.completion() - https://docs.litellm.ai/docs/completion/input\\n      model: claude-opus-4-5-20251101 ### MODEL NAME sent to `litellm.completion()` ###\\n      api_key: \\"os.environ/ANTHROPIC_API_KEY\\" # does os.getenv(\\"ANTHROPIC_API_KEY\\")\\n```\\n\\n**2. Start the proxy**\\n\\n```bash\\nlitellm --config /path/to/config.yaml\\n```\\n\\n**3. Test it!**\\n\\n<Tabs>\\n<TabItem value=\\"curl\\" label=\\"OpenAI Chat Completions\\">\\n```bash\\ncurl --location \'http://0.0.0.0:4000/chat/completions\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--header \'Authorization: Bearer $LITELLM_KEY\' \\\\\\n--data \' {\\n      \\"model\\": \\"claude-4\\",\\n      \\"messages\\": [\\n        {\\n          \\"role\\": \\"user\\",\\n          \\"content\\": \\"what llm are you\\"\\n        }\\n      ]\\n    }\\n\'\\n```\\n</TabItem>\\n<TabItem value=\\"anthropic\\" label=\\"Anthropic /v1/messages API\\">\\n```bash\\ncurl --location \'http://0.0.0.0:4000/v1/messages\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--header \'Authorization: Bearer $LITELLM_KEY\' \\\\\\n--data \' {\\n      \\"model\\": \\"claude-4\\",\\n      \\"max_tokens\\": 1024,\\n      \\"messages\\": [\\n        {\\n          \\"role\\": \\"user\\",\\n          \\"content\\": \\"what llm are you\\"\\n        }\\n      ]\\n    }\\n\'\\n```\\n</TabItem>\\n</Tabs>\\n</TabItem>\\n</Tabs>\\n\\n## Usage - Bedrock\\n\\n:::info\\n\\nLiteLLM uses the boto3 library to authenticate with Bedrock.\\n\\nFor more ways to authenticate with Bedrock, see the [Bedrock documentation](../../docs/providers/bedrock#authentication).\\n\\n:::\\n\\n<Tabs>\\n<TabItem value=\\"sdk\\" label=\\"LiteLLM Python SDK\\">\\n\\n\\n```python\\nimport os\\nfrom litellm import completion\\n\\nos.environ[\\"AWS_ACCESS_KEY_ID\\"] = \\"\\"\\nos.environ[\\"AWS_SECRET_ACCESS_KEY\\"] = \\"\\"\\nos.environ[\\"AWS_REGION_NAME\\"] = \\"\\"\\n\\n## OPENAI /chat/completions API format\\nresponse = completion(\\n  model=\\"bedrock/us.anthropic.claude-opus-4-5-20251101-v1:0\\",\\n  messages=[{ \\"content\\": \\"Hello, how are you?\\",\\"role\\": \\"user\\"}]\\n)\\n```\\n\\n</TabItem>\\n<TabItem value=\\"proxy\\" label=\\"LiteLLM Proxy\\">\\n\\n**1. Setup config.yaml**\\n\\n```yaml\\nmodel_list:\\n  - model_name: claude-4 ### RECEIVED MODEL NAME ###\\n    litellm_params: # all params accepted by litellm.completion() - https://docs.litellm.ai/docs/completion/input\\n      model: bedrock/us.anthropic.claude-opus-4-5-20251101-v1:0 ### MODEL NAME sent to `litellm.completion()` ###\\n      aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID\\n      aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY\\n      aws_region_name: os.environ/AWS_REGION_NAME\\n```\\n\\n**2. Start the proxy**\\n\\n```bash\\nlitellm --config /path/to/config.yaml\\n```\\n\\n**3. Test it!**\\n\\n<Tabs>\\n<TabItem value=\\"curl\\" label=\\"OpenAI Chat Completions\\">\\n```bash\\ncurl --location \'http://0.0.0.0:4000/chat/completions\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--header \'Authorization: Bearer $LITELLM_KEY\' \\\\\\n--data \' {\\n      \\"model\\": \\"claude-4\\",\\n      \\"messages\\": [\\n        {\\n          \\"role\\": \\"user\\",\\n          \\"content\\": \\"what llm are you\\"\\n        }\\n      ]\\n    }\\n\'\\n```\\n</TabItem>\\n<TabItem value=\\"anthropic\\" label=\\"Anthropic /v1/messages API\\">\\n```bash\\ncurl --location \'http://0.0.0.0:4000/v1/messages\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--header \'Authorization: Bearer $LITELLM_KEY\' \\\\\\n--data \' {\\n      \\"model\\": \\"claude-4\\",\\n      \\"max_tokens\\": 1024,\\n      \\"messages\\": [\\n        {\\n          \\"role\\": \\"user\\",\\n          \\"content\\": \\"what llm are you\\"\\n        }\\n      ]\\n    }\\n\'\\n```\\n</TabItem>\\n<TabItem value=\\"invoke\\" label=\\"Bedrock /invoke API\\">\\n```bash\\ncurl --location \'http://0.0.0.0:4000/bedrock/model/claude-4/invoke\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--header \'Authorization: Bearer $LITELLM_KEY\' \\\\\\n--data \' {\\n      \\"max_tokens\\": 1024,\\n      \\"messages\\": [{\\"role\\": \\"user\\", \\"content\\": \\"Hello, how are you?\\"}]\\n    }\'\\n```\\n</TabItem>\\n<TabItem value=\\"converse\\" label=\\"Bedrock /converse API\\">\\n```bash\\ncurl --location \'http://0.0.0.0:4000/bedrock/model/claude-4/converse\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--header \'Authorization: Bearer $LITELLM_KEY\' \\\\\\n--data \' {\\n      \\"messages\\": [{\\"role\\": \\"user\\", \\"content\\": \\"Hello, how are you?\\"}]\\n    }\'\\n```\\n</TabItem>\\n</Tabs>\\n</TabItem>\\n</Tabs>\\n\\n\\n## Usage - Vertex AI\\n\\n\\n<Tabs>\\n<TabItem value=\\"sdk\\" label=\\"LiteLLM Python SDK\\">\\n\\n```python\\nfrom litellm import completion\\nimport json \\n\\n## GET CREDENTIALS \\n## RUN ## \\n# !gcloud auth application-default login - run this to add vertex credentials to your env\\n## OR ## \\nfile_path = \'path/to/vertex_ai_service_account.json\'\\n\\n# Load the JSON file\\nwith open(file_path, \'r\') as file:\\n    vertex_credentials = json.load(file)\\n\\n# Convert to JSON string\\nvertex_credentials_json = json.dumps(vertex_credentials)\\n\\n## COMPLETION CALL \\nresponse = completion(\\n  model=\\"vertex_ai/claude-opus-4-5@20251101\\",\\n  messages=[{ \\"content\\": \\"Hello, how are you?\\",\\"role\\": \\"user\\"}],\\n  vertex_credentials=vertex_credentials_json,\\n  vertex_project=\\"your-project-id\\",\\n  vertex_location=\\"us-east5\\"\\n)\\n```\\n\\n</TabItem>\\n<TabItem value=\\"proxy\\" label=\\"LiteLLM Proxy\\">\\n\\n**1. Setup config.yaml**\\n\\n```yaml\\nmodel_list:\\n  - model_name: claude-4 ### RECEIVED MODEL NAME ###\\n    litellm_params:\\n        model: vertex_ai/claude-opus-4-5@20251101\\n        vertex_credentials: \\"/path/to/service_account.json\\"\\n        vertex_project: \\"your-project-id\\"\\n        vertex_location: \\"us-east5\\"\\n```\\n\\n**2. Start the proxy**\\n\\n```bash\\nlitellm --config /path/to/config.yaml\\n```\\n\\n**3. Test it!**\\n\\n<Tabs>\\n<TabItem value=\\"curl\\" label=\\"OpenAI Chat Completions\\">\\n```bash\\ncurl --location \'http://0.0.0.0:4000/chat/completions\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--header \'Authorization: Bearer $LITELLM_KEY\' \\\\\\n--data \' {\\n      \\"model\\": \\"claude-4\\",\\n      \\"messages\\": [\\n        {\\n          \\"role\\": \\"user\\",\\n          \\"content\\": \\"what llm are you\\"\\n        }\\n      ]\\n    }\\n\'\\n```\\n</TabItem>\\n<TabItem value=\\"anthropic\\" label=\\"Anthropic /v1/messages API\\">\\n```bash\\ncurl --location \'http://0.0.0.0:4000/v1/messages\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--header \'Authorization: Bearer $LITELLM_KEY\' \\\\\\n--data \' {\\n      \\"model\\": \\"claude-4\\",\\n      \\"max_tokens\\": 1024,\\n      \\"messages\\": [\\n        {\\n          \\"role\\": \\"user\\",\\n          \\"content\\": \\"what llm are you\\"\\n        }\\n      ]\\n    }\\n\'\\n```\\n</TabItem>\\n</Tabs>\\n</TabItem>\\n</Tabs>\\n\\n## Usage - Azure Anthropic (Azure Foundry Claude)\\n\\nLiteLLM funnels Azure Claude deployments through the `azure_ai/` provider so Claude Opus models on Azure Foundry keep working with Tool Search, Effort, streaming, and the rest of the advanced feature set. Point `AZURE_AI_API_BASE` to `https://<resource>.services.ai.azure.com/anthropic` (LiteLLM appends `/v1/messages` automatically) and authenticate with `AZURE_AI_API_KEY` or an Azure AD token.\\n\\n<Tabs>\\n<TabItem value=\\"sdk\\" label=\\"LiteLLM Python SDK\\">\\n\\n```python\\nimport os\\nfrom litellm import completion\\n\\n# Configure Azure credentials\\nos.environ[\\"AZURE_AI_API_KEY\\"] = \\"your-azure-ai-api-key\\"\\nos.environ[\\"AZURE_AI_API_BASE\\"] = \\"https://my-resource.services.ai.azure.com/anthropic\\"\\n\\nresponse = completion(\\n    model=\\"azure_ai/claude-opus-4-1\\",\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": \\"Explain how Azure Anthropic hosts Claude Opus differently from the public Anthropic API.\\"}],\\n    max_tokens=1200,\\n    temperature=0.7,\\n    stream=True,\\n)\\n\\nfor chunk in response:\\n    if chunk.choices[0].delta.content:\\n        print(chunk.choices[0].delta.content, end=\\"\\", flush=True)\\n```\\n\\n</TabItem>\\n<TabItem value=\\"proxy\\" label=\\"LiteLLM Proxy\\">\\n\\n**1. Set environment variables**\\n\\n```bash\\nexport AZURE_AI_API_KEY=\\"your-azure-ai-api-key\\"\\nexport AZURE_AI_API_BASE=\\"https://my-resource.services.ai.azure.com/anthropic\\"\\n```\\n\\n**2. Configure the proxy**\\n\\n```yaml\\nmodel_list:\\n  - model_name: claude-4-azure\\n    litellm_params:\\n      model: azure_ai/claude-opus-4-1\\n      api_key: os.environ/AZURE_AI_API_KEY\\n      api_base: os.environ/AZURE_AI_API_BASE\\n```\\n\\n**3. Start LiteLLM**\\n\\n```bash\\nlitellm --config /path/to/config.yaml\\n```\\n\\n**4. Test the Azure Claude route**\\n\\n```bash\\ncurl --location \'http://0.0.0.0:4000/chat/completions\' \\\\\\n  --header \'Content-Type: application/json\' \\\\\\n  --header \'Authorization: Bearer $LITELLM_KEY\' \\\\\\n  --data \'{\\n    \\"model\\": \\"claude-4-azure\\",\\n    \\"messages\\": [\\n      {\\n        \\"role\\": \\"user\\",\\n        \\"content\\": \\"How do I use Claude Opus 4 via Azure Anthropic in LiteLLM?\\"\\n      }\\n    ],\\n    \\"max_tokens\\": 1024\\n  }\'\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n\\n## Tool Search {#tool-search}\\n\\nThis lets Claude work with thousands of tools, by dynamically loading tools on-demand, instead of loading all tools into the context window upfront.\\n\\n### Usage Example\\n\\n<Tabs>\\n<TabItem value=\\"sdk\\" label=\\"LiteLLM Python SDK\\">\\n\\n```python\\nimport litellm\\nimport os\\n\\n# Configure your API key\\nos.environ[\\"ANTHROPIC_API_KEY\\"] = \\"your-api-key\\"\\n\\n# Define your tools with defer_loading\\ntools = [\\n    # Tool search tool (regex variant)\\n    {\\n        \\"type\\": \\"tool_search_tool_regex_20251119\\",\\n        \\"name\\": \\"tool_search_tool_regex\\"\\n    },\\n    # Deferred tools - loaded on-demand\\n    {\\n        \\"type\\": \\"function\\",\\n        \\"function\\": {\\n            \\"name\\": \\"get_weather\\",\\n            \\"description\\": \\"Get the current weather in a given location. Returns temperature and conditions.\\",\\n            \\"parameters\\": {\\n                \\"type\\": \\"object\\",\\n                \\"properties\\": {\\n                    \\"location\\": {\\n                        \\"type\\": \\"string\\",\\n                        \\"description\\": \\"The city and state, e.g. San Francisco, CA\\"\\n                    },\\n                    \\"unit\\": {\\n                        \\"type\\": \\"string\\",\\n                        \\"enum\\": [\\"celsius\\", \\"fahrenheit\\"],\\n                        \\"description\\": \\"Temperature unit\\"\\n                    }\\n                },\\n                \\"required\\": [\\"location\\"]\\n            }\\n        },\\n        \\"defer_loading\\": True  # Load on-demand\\n    },\\n    {\\n        \\"type\\": \\"function\\",\\n        \\"function\\": {\\n            \\"name\\": \\"search_files\\",\\n            \\"description\\": \\"Search through files in the workspace using keywords\\",\\n            \\"parameters\\": {\\n                \\"type\\": \\"object\\",\\n                \\"properties\\": {\\n                    \\"query\\": {\\"type\\": \\"string\\"},\\n                    \\"file_types\\": {\\n                        \\"type\\": \\"array\\",\\n                        \\"items\\": {\\"type\\": \\"string\\"}\\n                    }\\n                },\\n                \\"required\\": [\\"query\\"]\\n            }\\n        },\\n        \\"defer_loading\\": True\\n    },\\n    {\\n        \\"type\\": \\"function\\",\\n        \\"function\\": {\\n            \\"name\\": \\"query_database\\",\\n            \\"description\\": \\"Execute SQL queries against the database\\",\\n            \\"parameters\\": {\\n                \\"type\\": \\"object\\",\\n                \\"properties\\": {\\n                    \\"sql\\": {\\"type\\": \\"string\\"}\\n                },\\n                \\"required\\": [\\"sql\\"]\\n            }\\n        },\\n        \\"defer_loading\\": True\\n    }\\n]\\n\\n# Make a request - Claude will search for and use relevant tools\\nresponse = litellm.completion(\\n    model=\\"anthropic/claude-opus-4-5-20251101\\",\\n    messages=[{\\n        \\"role\\": \\"user\\",\\n        \\"content\\": \\"What\'s the weather like in San Francisco?\\"\\n    }],\\n    tools=tools\\n)\\n\\nprint(\\"Claude\'s response:\\", response.choices[0].message.content)\\nprint(\\"Tool calls:\\", response.choices[0].message.tool_calls)\\n\\n# Check tool search usage\\nif hasattr(response.usage, \'server_tool_use\'):\\n    print(f\\"Tool searches performed: {response.usage.server_tool_use.tool_search_requests}\\")\\n```\\n</TabItem>\\n<TabItem value=\\"proxy\\" label=\\"LiteLLM Proxy\\">\\n\\n1. Setup config.yaml\\n\\n```yaml\\nmodel_list:\\n  - model_name: claude-4\\n    litellm_params:\\n      model: anthropic/claude-opus-4-5-20251101\\n      api_key: os.environ/ANTHROPIC_API_KEY\\n```\\n\\n2. Start the proxy\\n\\n```bash\\nlitellm --config /path/to/config.yaml\\n```\\n\\n3. Test it!\\n\\n\\n```bash\\ncurl --location \'http://0.0.0.0:4000/chat/completions\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--header \'Authorization: Bearer $LITELLM_KEY\' \\\\\\n--data \' {\\n      \\"model\\": \\"claude-4\\",\\n      \\"messages\\": [{\\n        \\"role\\": \\"user\\",\\n        \\"content\\": \\"What\'s the weather like in San Francisco?\\"\\n       }],\\n       \\"tools\\": [\\n        # Tool search tool (regex variant)\\n        {\\n            \\"type\\": \\"tool_search_tool_regex_20251119\\",\\n            \\"name\\": \\"tool_search_tool_regex\\"\\n        },\\n        # Deferred tools - loaded on-demand\\n        {\\n            \\"type\\": \\"function\\",\\n            \\"function\\": {\\n                \\"name\\": \\"get_weather\\",\\n                \\"description\\": \\"Get the current weather in a given location. Returns temperature and conditions.\\",\\n                \\"parameters\\": {\\n                    \\"type\\": \\"object\\",\\n                    \\"properties\\": {\\n                        \\"location\\": {\\n                            \\"type\\": \\"string\\",\\n                            \\"description\\": \\"The city and state, e.g. San Francisco, CA\\"\\n                        },\\n                        \\"unit\\": {\\n                            \\"type\\": \\"string\\",\\n                            \\"enum\\": [\\"celsius\\", \\"fahrenheit\\"],\\n                            \\"description\\": \\"Temperature unit\\"\\n                        }\\n                    },\\n                    \\"required\\": [\\"location\\"]\\n                }\\n            },\\n            \\"defer_loading\\": True  # Load on-demand\\n        },\\n        {\\n            \\"type\\": \\"function\\",\\n            \\"function\\": {\\n                \\"name\\": \\"search_files\\",\\n                \\"description\\": \\"Search through files in the workspace using keywords\\",\\n                \\"parameters\\": {\\n                    \\"type\\": \\"object\\",\\n                    \\"properties\\": {\\n                        \\"query\\": {\\"type\\": \\"string\\"},\\n                        \\"file_types\\": {\\n                            \\"type\\": \\"array\\",\\n                            \\"items\\": {\\"type\\": \\"string\\"}\\n                        }\\n                    },\\n                    \\"required\\": [\\"query\\"]\\n                }\\n            },\\n            \\"defer_loading\\": True\\n        },\\n        {\\n            \\"type\\": \\"function\\",\\n            \\"function\\": {\\n                \\"name\\": \\"query_database\\",\\n                \\"description\\": \\"Execute SQL queries against the database\\",\\n                \\"parameters\\": {\\n                    \\"type\\": \\"object\\",\\n                    \\"properties\\": {\\n                        \\"sql\\": {\\"type\\": \\"string\\"}\\n                    },\\n                    \\"required\\": [\\"sql\\"]\\n                }\\n            },\\n            \\"defer_loading\\": True\\n        }\\n    ]\\n}\\n\'\\n```\\n</TabItem>\\n</Tabs>\\n\\n### BM25 Variant (Natural Language Search)\\n\\nFor natural language queries instead of regex patterns:\\n\\n```python\\ntools = [\\n    {\\n        \\"type\\": \\"tool_search_tool_bm25_20251119\\",  # Natural language variant\\n        \\"name\\": \\"tool_search_tool_bm25\\"\\n    },\\n    # ... your deferred tools\\n]\\n```\\n\\n---\\n\\n## Programmatic Tool Calling {#programmatic-tool-calling}\\n\\nProgrammatic tool calling allows Claude to write code that calls your tools programmatically. [Learn more](https://platform.claude.com/docs/en/agents-and-tools/tool-use/programmatic-tool-calling)\\n\\n<Tabs>\\n<TabItem value=\\"sdk\\" label=\\"LiteLLM Python SDK\\">\\n\\n```python\\nimport litellm\\nimport json\\n\\n# Define tools that can be called programmatically\\ntools = [\\n    # Code execution tool (required for programmatic calling)\\n    {\\n        \\"type\\": \\"code_execution_20250825\\",\\n        \\"name\\": \\"code_execution\\"\\n    },\\n    # Tool that can be called from code\\n    {\\n        \\"type\\": \\"function\\",\\n        \\"function\\": {\\n            \\"name\\": \\"query_database\\",\\n            \\"description\\": \\"Execute a SQL query against the sales database. Returns a list of rows as JSON objects.\\",\\n            \\"parameters\\": {\\n                \\"type\\": \\"object\\",\\n                \\"properties\\": {\\n                    \\"sql\\": {\\n                        \\"type\\": \\"string\\",\\n                        \\"description\\": \\"SQL query to execute\\"\\n                    }\\n                },\\n                \\"required\\": [\\"sql\\"]\\n            }\\n        },\\n        \\"allowed_callers\\": [\\"code_execution_20250825\\"]  # Enable programmatic calling\\n    }\\n]\\n\\n# First request\\nresponse = litellm.completion(\\n    model=\\"anthropic/claude-sonnet-4-5-20250929\\",\\n    messages=[{\\n        \\"role\\": \\"user\\",\\n        \\"content\\": \\"Query sales data for West, East, and Central regions, then tell me which had the highest revenue\\"\\n    }],\\n    tools=tools\\n)\\n\\nprint(\\"Claude\'s response:\\", response.choices[0].message)\\n\\n# Handle tool calls\\nmessages = [\\n    {\\"role\\": \\"user\\", \\"content\\": \\"Query sales data for West, East, and Central regions, then tell me which had the highest revenue\\"},\\n    {\\"role\\": \\"assistant\\", \\"content\\": response.choices[0].message.content, \\"tool_calls\\": response.choices[0].message.tool_calls}\\n]\\n\\n# Process each tool call\\nfor tool_call in response.choices[0].message.tool_calls:\\n    # Check if it\'s a programmatic call\\n    if hasattr(tool_call, \'caller\') and tool_call.caller:\\n        print(f\\"Programmatic call to {tool_call.function.name}\\")\\n        print(f\\"Called from: {tool_call.caller}\\")\\n    \\n    # Simulate tool execution\\n    if tool_call.function.name == \\"query_database\\":\\n        args = json.loads(tool_call.function.arguments)\\n        # Simulate database query\\n        result = json.dumps([\\n            {\\"region\\": \\"West\\", \\"revenue\\": 150000},\\n            {\\"region\\": \\"East\\", \\"revenue\\": 180000},\\n            {\\"region\\": \\"Central\\", \\"revenue\\": 120000}\\n        ])\\n        \\n        messages.append({\\n            \\"role\\": \\"user\\",\\n            \\"content\\": [{\\n                \\"type\\": \\"tool_result\\",\\n                \\"tool_use_id\\": tool_call.id,\\n                \\"content\\": result\\n            }]\\n        })\\n\\n# Get final response\\nfinal_response = litellm.completion(\\n    model=\\"anthropic/claude-sonnet-4-5-20250929\\",\\n    messages=messages,\\n    tools=tools\\n)\\n\\nprint(\\"\\\\nFinal answer:\\", final_response.choices[0].message.content)\\n```\\n\\n</TabItem>\\n<TabItem value=\\"proxy\\" label=\\"LiteLLM Proxy\\">\\n\\n1. Setup config.yaml\\n\\n```yaml\\nmodel_list:\\n  - model_name: claude-4\\n    litellm_params:\\n      model: anthropic/claude-opus-4-5-20251101\\n      api_key: os.environ/ANTHROPIC_API_KEY\\n```\\n\\n2. Start the proxy\\n\\n```bash\\nlitellm --config /path/to/config.yaml\\n```\\n\\n3. Test it!\\n\\n\\n```bash\\ncurl --location \'http://0.0.0.0:4000/chat/completions\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--header \'Authorization: Bearer $LITELLM_KEY\' \\\\\\n--data \' {\\n      \\"model\\": \\"claude-4\\",\\n      \\"messages\\": [{\\n        \\"role\\": \\"user\\",\\n        \\"content\\": \\"Query sales data for West, East, and Central regions, then tell me which had the highest revenue\\"\\n      }],\\n      \\"tools\\": [\\n        # Code execution tool (required for programmatic calling)\\n        {\\n            \\"type\\": \\"code_execution_20250825\\",\\n            \\"name\\": \\"code_execution\\"\\n        },\\n        # Tool that can be called from code\\n        {\\n            \\"type\\": \\"function\\",\\n            \\"function\\": {\\n                \\"name\\": \\"query_database\\",\\n                \\"description\\": \\"Execute a SQL query against the sales database. Returns a list of rows as JSON objects.\\",\\n                \\"parameters\\": {\\n                    \\"type\\": \\"object\\",\\n                    \\"properties\\": {\\n                        \\"sql\\": {\\n                            \\"type\\": \\"string\\",\\n                            \\"description\\": \\"SQL query to execute\\"\\n                        }\\n                    },\\n                    \\"required\\": [\\"sql\\"]\\n                }\\n            },\\n            \\"allowed_callers\\": [\\"code_execution_20250825\\"]  # Enable programmatic calling\\n        }\\n    ]\\n}\\n\'\\n```\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Tool Input Examples {#tool-input-examples}\\n\\nYou can now provide Claude with examples of how to use your tools. [Learn more](https://platform.claude.com/docs/en/agents-and-tools/tool-use/tool-input-examples)\\n\\n\\n<Tabs>\\n<TabItem value=\\"sdk\\" label=\\"LiteLLM Python SDK\\">\\n\\n```python\\nimport litellm\\n\\ntools = [\\n    {\\n        \\"type\\": \\"function\\",\\n        \\"function\\": {\\n            \\"name\\": \\"create_calendar_event\\",\\n            \\"description\\": \\"Create a new calendar event with attendees and reminders\\",\\n            \\"parameters\\": {\\n                \\"type\\": \\"object\\",\\n                \\"properties\\": {\\n                    \\"title\\": {\\"type\\": \\"string\\"},\\n                    \\"start_time\\": {\\n                        \\"type\\": \\"string\\",\\n                        \\"description\\": \\"ISO 8601 format: YYYY-MM-DDTHH:MM:SS\\"\\n                    },\\n                    \\"duration_minutes\\": {\\"type\\": \\"integer\\"},\\n                    \\"attendees\\": {\\n                        \\"type\\": \\"array\\",\\n                        \\"items\\": {\\n                            \\"type\\": \\"object\\",\\n                            \\"properties\\": {\\n                                \\"email\\": {\\"type\\": \\"string\\"},\\n                                \\"optional\\": {\\"type\\": \\"boolean\\"}\\n                            }\\n                        }\\n                    },\\n                    \\"reminders\\": {\\n                        \\"type\\": \\"array\\",\\n                        \\"items\\": {\\n                            \\"type\\": \\"object\\",\\n                            \\"properties\\": {\\n                                \\"minutes_before\\": {\\"type\\": \\"integer\\"},\\n                                \\"method\\": {\\"type\\": \\"string\\", \\"enum\\": [\\"email\\", \\"popup\\"]}\\n                            }\\n                        }\\n                    }\\n                },\\n                \\"required\\": [\\"title\\", \\"start_time\\", \\"duration_minutes\\"]\\n            }\\n        },\\n        # Provide concrete examples\\n        \\"input_examples\\": [\\n            {\\n                \\"title\\": \\"Team Standup\\",\\n                \\"start_time\\": \\"2025-01-15T09:00:00\\",\\n                \\"duration_minutes\\": 30,\\n                \\"attendees\\": [\\n                    {\\"email\\": \\"alice@company.com\\", \\"optional\\": False},\\n                    {\\"email\\": \\"bob@company.com\\", \\"optional\\": False}\\n                ],\\n                \\"reminders\\": [\\n                    {\\"minutes_before\\": 15, \\"method\\": \\"popup\\"}\\n                ]\\n            },\\n            {\\n                \\"title\\": \\"Lunch Break\\",\\n                \\"start_time\\": \\"2025-01-15T12:00:00\\",\\n                \\"duration_minutes\\": 60\\n                # Demonstrates optional fields can be omitted\\n            }\\n        ]\\n    }\\n]\\n\\nresponse = litellm.completion(\\n    model=\\"anthropic/claude-sonnet-4-5-20250929\\",\\n    messages=[{\\n        \\"role\\": \\"user\\",\\n        \\"content\\": \\"Schedule a team meeting for tomorrow at 2pm for 45 minutes with john@company.com and sarah@company.com\\"\\n    }],\\n    tools=tools\\n)\\n\\nprint(\\"Tool call:\\", response.choices[0].message.tool_calls[0].function.arguments)\\n```\\n\\n</TabItem>\\n<TabItem value=\\"proxy\\" label=\\"LiteLLM Proxy\\">\\n\\n1. Setup config.yaml\\n\\n```yaml\\nmodel_list:\\n  - model_name: claude-4\\n    litellm_params:\\n      model: anthropic/claude-opus-4-5-20251101\\n      api_key: os.environ/ANTHROPIC_API_KEY\\n```\\n\\n2. Start the proxy\\n\\n```bash\\nlitellm --config /path/to/config.yaml\\n```\\n\\n3. Test it!\\n\\n\\n```bash\\ncurl --location \'http://0.0.0.0:4000/chat/completions\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--header \'Authorization: Bearer $LITELLM_KEY\' \\\\\\n--data \' {\\n      \\"model\\": \\"claude-4\\",\\n      \\"messages\\": [{\\n        \\"role\\": \\"user\\",\\n        \\"content\\": \\"Schedule a team meeting for tomorrow at 2pm for 45 minutes with john@company.com and sarah@company.com\\"\\n      }],\\n      \\"tools\\": [\\n    {\\n        \\"type\\": \\"function\\",\\n        \\"function\\": {\\n            \\"name\\": \\"create_calendar_event\\",\\n            \\"description\\": \\"Create a new calendar event with attendees and reminders\\",\\n            \\"parameters\\": {\\n                \\"type\\": \\"object\\",\\n                \\"properties\\": {\\n                    \\"title\\": {\\"type\\": \\"string\\"},\\n                    \\"start_time\\": {\\n                        \\"type\\": \\"string\\",\\n                        \\"description\\": \\"ISO 8601 format: YYYY-MM-DDTHH:MM:SS\\"\\n                    },\\n                    \\"duration_minutes\\": {\\"type\\": \\"integer\\"},\\n                    \\"attendees\\": {\\n                        \\"type\\": \\"array\\",\\n                        \\"items\\": {\\n                            \\"type\\": \\"object\\",\\n                            \\"properties\\": {\\n                                \\"email\\": {\\"type\\": \\"string\\"},\\n                                \\"optional\\": {\\"type\\": \\"boolean\\"}\\n                            }\\n                        }\\n                    },\\n                    \\"reminders\\": {\\n                        \\"type\\": \\"array\\",\\n                        \\"items\\": {\\n                            \\"type\\": \\"object\\",\\n                            \\"properties\\": {\\n                                \\"minutes_before\\": {\\"type\\": \\"integer\\"},\\n                                \\"method\\": {\\"type\\": \\"string\\", \\"enum\\": [\\"email\\", \\"popup\\"]}\\n                            }\\n                        }\\n                    }\\n                },\\n                \\"required\\": [\\"title\\", \\"start_time\\", \\"duration_minutes\\"]\\n            }\\n        },\\n        # Provide concrete examples\\n        \\"input_examples\\": [\\n            {\\n                \\"title\\": \\"Team Standup\\",\\n                \\"start_time\\": \\"2025-01-15T09:00:00\\",\\n                \\"duration_minutes\\": 30,\\n                \\"attendees\\": [\\n                    {\\"email\\": \\"alice@company.com\\", \\"optional\\": False},\\n                    {\\"email\\": \\"bob@company.com\\", \\"optional\\": False}\\n                ],\\n                \\"reminders\\": [\\n                    {\\"minutes_before\\": 15, \\"method\\": \\"popup\\"}\\n                ]\\n            },\\n            {\\n                \\"title\\": \\"Lunch Break\\",\\n                \\"start_time\\": \\"2025-01-15T12:00:00\\",\\n                \\"duration_minutes\\": 60\\n                # Demonstrates optional fields can be omitted\\n            }\\n        ]\\n    }\\n]\\n}\\n\'\\n```\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Effort Parameter: Control Token Usage {#effort-parameter}\\n\\nControl how much effort Claude puts into its response using the `reasoning_effort` parameter. This allows you to trade off between response thoroughness and token efficiency.\\n\\n:::info\\nLiteLLM automatically maps `reasoning_effort` to Anthropic\'s `output_config` format and adds the required `effort-2025-11-24` beta header for Claude Opus 4.5.\\n:::\\n\\nPotential values for `reasoning_effort` parameter: `\\"high\\"`, `\\"medium\\"`, `\\"low\\"`.\\n\\n### Usage Example\\n\\n<Tabs>\\n<TabItem value=\\"sdk\\" label=\\"LiteLLM Python SDK\\">\\n\\n```python\\nimport litellm\\n\\nmessage = \\"Analyze the trade-offs between microservices and monolithic architectures\\"\\n\\n# High effort (default) - Maximum capability\\nresponse_high = litellm.completion(\\n    model=\\"anthropic/claude-opus-4-5-20251101\\",\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": message}],\\n    reasoning_effort=\\"high\\"\\n)\\n\\nprint(\\"High effort response:\\")\\nprint(response_high.choices[0].message.content)\\nprint(f\\"Tokens used: {response_high.usage.completion_tokens}\\\\n\\")\\n\\n# Medium effort - Balanced approach\\nresponse_medium = litellm.completion(\\n    model=\\"anthropic/claude-opus-4-5-20251101\\",\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": message}],\\n    reasoning_effort=\\"medium\\"\\n)\\n\\nprint(\\"Medium effort response:\\")\\nprint(response_medium.choices[0].message.content)\\nprint(f\\"Tokens used: {response_medium.usage.completion_tokens}\\\\n\\")\\n\\n# Low effort - Maximum efficiency\\nresponse_low = litellm.completion(\\n    model=\\"anthropic/claude-opus-4-5-20251101\\",\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": message}],\\n    reasoning_effort=\\"low\\"\\n)\\n\\nprint(\\"Low effort response:\\")\\nprint(response_low.choices[0].message.content)\\nprint(f\\"Tokens used: {response_low.usage.completion_tokens}\\\\n\\")\\n\\n# Compare token usage\\nprint(\\"Token Comparison:\\")\\nprint(f\\"High:   {response_high.usage.completion_tokens} tokens\\")\\nprint(f\\"Medium: {response_medium.usage.completion_tokens} tokens\\")\\nprint(f\\"Low:    {response_low.usage.completion_tokens} tokens\\")\\n```\\n\\n</TabItem>\\n<TabItem value=\\"proxy\\" label=\\"LiteLLM Proxy\\">\\n\\n1. Setup config.yaml\\n\\n```yaml\\nmodel_list:\\n  - model_name: claude-4\\n    litellm_params:\\n      model: anthropic/claude-opus-4-5-20251101\\n      api_key: os.environ/ANTHROPIC_API_KEY\\n```\\n\\n2. Start the proxy\\n\\n```bash\\nlitellm --config /path/to/config.yaml\\n```\\n\\n3. Test it!\\n\\n```bash\\ncurl --location \'http://0.0.0.0:4000/chat/completions\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--header \'Authorization: Bearer $LITELLM_KEY\' \\\\\\n--data \' {\\n      \\"model\\": \\"claude-4\\",\\n      \\"messages\\": [{\\n        \\"role\\": \\"user\\",\\n        \\"content\\": \\"Analyze the trade-offs between microservices and monolithic architectures\\"\\n      }],\\n      \\"reasoning_effort\\": \\"high\\"\\n    }\\n\'\\n```\\n</TabItem>\\n</Tabs>"},{"id":"gemini_3","metadata":{"permalink":"/blog/gemini_3","source":"@site/blog/gemini_3/index.md","title":"DAY 0 Support: Gemini 3 on LiteLLM","description":"This guide covers common questions and best practices for using gemini-3-pro-preview with LiteLLM Proxy and SDK.","date":"2025-11-19T10:00:00.000Z","tags":[{"inline":true,"label":"gemini","permalink":"/blog/tags/gemini"},{"inline":true,"label":"day 0 support","permalink":"/blog/tags/day-0-support"},{"inline":true,"label":"llms","permalink":"/blog/tags/llms"}],"hasTruncateMarker":false,"authors":[{"name":"Sameer Kankute","title":"SWE @ LiteLLM (LLM Translation)","url":"https://www.linkedin.com/in/sameer-kankute/","image_url":"https://pbs.twimg.com/profile_images/2001352686994907136/ONgNuSk5_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/2001352686994907136/ONgNuSk5_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"gemini_3","title":"DAY 0 Support: Gemini 3 on LiteLLM","date":"2025-11-19T10:00:00.000Z","authors":[{"name":"Sameer Kankute","title":"SWE @ LiteLLM (LLM Translation)","url":"https://www.linkedin.com/in/sameer-kankute/","image_url":"https://pbs.twimg.com/profile_images/2001352686994907136/ONgNuSk5_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/2001352686994907136/ONgNuSk5_400x400.jpg"},{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"tags":["gemini","day 0 support","llms"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Day 0 Support: Claude 4.5 Opus (+Advanced Features)","permalink":"/blog/anthropic_advanced_features"}},"content":"import Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n:::info\\n\\nThis guide covers common questions and best practices for using `gemini-3-pro-preview` with LiteLLM Proxy and SDK.\\n\\n:::\\n\\n## Quick Start\\n\\n<Tabs>\\n<TabItem value=\\"sdk\\" label=\\"Python SDK\\">\\n\\n```python\\nfrom litellm import completion\\nimport os\\n\\nos.environ[\\"GEMINI_API_KEY\\"] = \\"your-api-key\\"\\n\\nresponse = completion(\\n    model=\\"gemini/gemini-3-pro-preview\\",\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": \\"Hello!\\"}],\\n    reasoning_effort=\\"low\\"\\n)\\n\\nprint(response.choices[0].message.content)\\n```\\n\\n</TabItem>\\n<TabItem value=\\"proxy\\" label=\\"LiteLLM Proxy\\">\\n\\n**1. Add to config.yaml:**\\n\\n```yaml\\nmodel_list:\\n  - model_name: gemini-3-pro-preview\\n    litellm_params:\\n      model: gemini/gemini-3-pro-preview\\n      api_key: os.environ/GEMINI_API_KEY\\n```\\n\\n**2. Start proxy:**\\n\\n```bash\\nlitellm --config /path/to/config.yaml\\n```\\n\\n**3. Make request:**\\n\\n```bash\\ncurl http://0.0.0.0:4000/v1/chat/completions \\\\\\n  -H \\"Content-Type: application/json\\" \\\\\\n  -H \\"Authorization: Bearer sk-1234\\" \\\\\\n  -d \'{\\n    \\"model\\": \\"gemini-3-pro-preview\\",\\n    \\"messages\\": [{\\"role\\": \\"user\\", \\"content\\": \\"Hello!\\"}],\\n    \\"reasoning_effort\\": \\"low\\"\\n  }\'\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n## Supported Endpoints\\n\\nLiteLLM provides **full end-to-end support** for Gemini 3 Pro Preview on:\\n\\n- \u2705 `/v1/chat/completions` - OpenAI-compatible chat completions endpoint\\n- \u2705 `/v1/responses` - OpenAI Responses API endpoint (streaming and non-streaming)\\n- \u2705 [`/v1/messages`](../../docs/anthropic_unified) - Anthropic-compatible messages endpoint\\n- \u2705 `/v1/generateContent` \u2013 [Google Gemini API](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini#rest) compatible endpoint (for code, see: `client.models.generate_content(...)`)\\n\\nAll endpoints support:\\n- Streaming and non-streaming responses\\n- Function calling with thought signatures\\n- Multi-turn conversations\\n- All Gemini 3-specific features\\n\\n## Thought Signatures\\n\\n#### What are Thought Signatures?\\n\\nThought signatures are encrypted representations of the model\'s internal reasoning process. They\'re essential for maintaining context across multi-turn conversations, especially with function calling.\\n\\n#### How Thought Signatures Work\\n\\n1. **Automatic Extraction**: When Gemini 3 returns a function call, LiteLLM automatically extracts the `thought_signature` from the response\\n2. **Storage**: Thought signatures are stored in `provider_specific_fields.thought_signature` of tool calls\\n3. **Automatic Preservation**: When you include the assistant\'s message in conversation history, LiteLLM automatically preserves and returns thought signatures to Gemini\\n\\n## Example: Multi-Turn Function Calling\\n\\n#### Streaming with Thought Signatures\\n\\nWhen using streaming mode with `stream_chunk_builder()`, thought signatures are now automatically preserved:\\n\\n<Tabs>\\n<TabItem value=\\"streaming\\" label=\\"Streaming SDK\\">\\n\\n```python\\nimport os\\nimport litellm\\nfrom litellm import completion\\n\\nos.environ[\\"GEMINI_API_KEY\\"] = \\"your-api-key\\"\\n\\nMODEL = \\"gemini/gemini-3-pro-preview\\"\\n\\nmessages = [\\n    {\\"role\\": \\"system\\", \\"content\\": \\"You are a helpful assistant. Use the calculate tool.\\"},\\n    {\\"role\\": \\"user\\", \\"content\\": \\"What is 2+2?\\"},\\n]\\n\\ntools = [{\\n    \\"type\\": \\"function\\",\\n    \\"function\\": {\\n        \\"name\\": \\"calculate\\",\\n        \\"description\\": \\"Calculate a mathematical expression\\",\\n        \\"parameters\\": {\\n            \\"type\\": \\"object\\",\\n            \\"properties\\": {\\"expression\\": {\\"type\\": \\"string\\"}},\\n            \\"required\\": [\\"expression\\"],\\n        },\\n    },\\n}]\\n\\nprint(\\"Step 1: Sending request with stream=True...\\")\\nresponse = completion(\\n    model=MODEL,\\n    messages=messages,\\n    stream=True,\\n    tools=tools,\\n    reasoning_effort=\\"low\\"\\n)\\n\\n# Collect all chunks\\nchunks = []\\nfor part in response:\\n    chunks.append(part)\\n\\n# Reconstruct message using stream_chunk_builder\\n# Thought signatures are now preserved automatically!\\nfull_response = litellm.stream_chunk_builder(chunks, messages=messages)\\nprint(f\\"Full response: {full_response}\\")\\n\\nassistant_msg = full_response.choices[0].message\\n\\n# \u2705 Thought signature is now preserved in provider_specific_fields\\nif assistant_msg.tool_calls and assistant_msg.tool_calls[0].provider_specific_fields:\\n    thought_sig = assistant_msg.tool_calls[0].provider_specific_fields.get(\\"thought_signature\\")\\n    print(f\\"Thought signature preserved: {thought_sig is not None}\\")\\n\\n# Append assistant message (includes thought signatures automatically)\\nmessages.append(assistant_msg)\\n\\n# Mock tool execution\\nmessages.append({\\n    \\"role\\": \\"tool\\",\\n    \\"content\\": \\"4\\",\\n    \\"tool_call_id\\": assistant_msg.tool_calls[0].id\\n})\\n\\nprint(\\"\\\\nStep 2: Sending tool result back to model...\\")\\nresponse_2 = completion(\\n    model=MODEL,\\n    messages=messages,\\n    stream=True,\\n    tools=tools,\\n    reasoning_effort=\\"low\\"\\n)\\n\\nfor part in response_2:\\n    if part.choices[0].delta.content:\\n        print(part.choices[0].delta.content, end=\\"\\")\\nprint()  # New line\\n```\\n\\n**Key Points:**\\n- \u2705 `stream_chunk_builder()` now preserves `provider_specific_fields` including thought signatures\\n- \u2705 Thought signatures are automatically included when appending `assistant_msg` to conversation history\\n- \u2705 Multi-turn conversations work seamlessly with streaming\\n\\n</TabItem>\\n<TabItem value=\\"sdk\\" label=\\"Non-Streaming SDK\\">\\n\\n```python\\nfrom openai import OpenAI\\nimport json\\n\\nclient = OpenAI(api_key=\\"sk-1234\\", base_url=\\"http://localhost:4000\\")\\n\\n# Define tools\\ntools = [\\n    {\\n        \\"type\\": \\"function\\",\\n        \\"function\\": {\\n            \\"name\\": \\"get_weather\\",\\n            \\"description\\": \\"Get the current weather\\",\\n            \\"parameters\\": {\\n                \\"type\\": \\"object\\",\\n                \\"properties\\": {\\n                    \\"location\\": {\\"type\\": \\"string\\"}\\n                },\\n                \\"required\\": [\\"location\\"]\\n            }\\n        }\\n    }\\n]\\n\\n# Step 1: Initial request\\nmessages = [{\\"role\\": \\"user\\", \\"content\\": \\"What\'s the weather in Tokyo?\\"}]\\n\\nresponse = client.chat.completions.create(\\n    model=\\"gemini-3-pro-preview\\",\\n    messages=messages,\\n    tools=tools,\\n    reasoning_effort=\\"low\\"\\n)\\n\\n# Step 2: Append assistant message (thought signatures automatically preserved)\\nmessages.append(response.choices[0].message)\\n\\n# Step 3: Execute tool and append result\\nfor tool_call in response.choices[0].message.tool_calls:\\n    if tool_call.function.name == \\"get_weather\\":\\n        result = {\\"temperature\\": 30, \\"unit\\": \\"celsius\\"}\\n        messages.append({\\n            \\"role\\": \\"tool\\",\\n            \\"content\\": json.dumps(result),\\n            \\"tool_call_id\\": tool_call.id\\n        })\\n\\n# Step 4: Follow-up request (thought signatures automatically included)\\nresponse2 = client.chat.completions.create(\\n    model=\\"gemini-3-pro-preview\\",\\n    messages=messages,\\n    tools=tools,\\n    reasoning_effort=\\"low\\"\\n)\\n\\nprint(response2.choices[0].message.content)\\n```\\n\\n**Key Points:**\\n- \u2705 Thought signatures are automatically extracted from `response.choices[0].message.tool_calls[].provider_specific_fields.thought_signature`\\n- \u2705 When you append `response.choices[0].message` to your conversation history, thought signatures are automatically preserved\\n- \u2705 You don\'t need to manually extract or manage thought signatures\\n\\n</TabItem>\\n<TabItem value=\\"proxy\\" label=\\"cURL\\">\\n\\n```bash\\n# Step 1: Initial request\\ncurl http://localhost:4000/v1/chat/completions \\\\\\n  -H \\"Content-Type: application/json\\" \\\\\\n  -H \\"Authorization: Bearer sk-1234\\" \\\\\\n  -d \'{\\n    \\"model\\": \\"gemini-3-pro-preview\\",\\n    \\"messages\\": [\\n      {\\"role\\": \\"user\\", \\"content\\": \\"What\'\\\\\'\'s the weather in Tokyo?\\"}\\n    ],\\n    \\"tools\\": [\\n      {\\n        \\"type\\": \\"function\\",\\n        \\"function\\": {\\n          \\"name\\": \\"get_weather\\",\\n          \\"description\\": \\"Get the current weather\\",\\n          \\"parameters\\": {\\n            \\"type\\": \\"object\\",\\n            \\"properties\\": {\\n              \\"location\\": {\\"type\\": \\"string\\"}\\n            },\\n            \\"required\\": [\\"location\\"]\\n          }\\n        }\\n      }\\n    ],\\n    \\"reasoning_effort\\": \\"low\\"\\n  }\'\\n```\\n\\n**Response includes thought signature:**\\n\\n```json\\n{\\n  \\"choices\\": [{\\n    \\"message\\": {\\n      \\"role\\": \\"assistant\\",\\n      \\"tool_calls\\": [{\\n        \\"id\\": \\"call_abc123\\",\\n        \\"type\\": \\"function\\",\\n        \\"function\\": {\\n          \\"name\\": \\"get_weather\\",\\n          \\"arguments\\": \\"{\\\\\\"location\\\\\\": \\\\\\"Tokyo\\\\\\"}\\"\\n        },\\n        \\"provider_specific_fields\\": {\\n          \\"thought_signature\\": \\"CpcHAdHtim9+q4rstcbvQC0ic4x1/vqQlCJWgE+UZ6dTLYGHMMBkF/AxqL5UmP6SY46uYC8t4BTFiXG5zkw6EMJ...\\"\\n        }\\n      }]\\n    }\\n  }]\\n}\\n```\\n\\n```bash\\n# Step 2: Follow-up request (include assistant message with thought signature)\\ncurl http://localhost:4000/v1/chat/completions \\\\\\n  -H \\"Content-Type: application/json\\" \\\\\\n  -H \\"Authorization: Bearer sk-1234\\" \\\\\\n  -d \'{\\n    \\"model\\": \\"gemini-3-pro-preview\\",\\n    \\"messages\\": [\\n      {\\"role\\": \\"user\\", \\"content\\": \\"What\'\\\\\'\'s the weather in Tokyo?\\"},\\n      {\\n        \\"role\\": \\"assistant\\",\\n        \\"content\\": null,\\n        \\"tool_calls\\": [{\\n          \\"id\\": \\"call_abc123\\",\\n          \\"type\\": \\"function\\",\\n          \\"function\\": {\\n            \\"name\\": \\"get_weather\\",\\n            \\"arguments\\": \\"{\\\\\\"location\\\\\\": \\\\\\"Tokyo\\\\\\"}\\"\\n          },\\n          \\"provider_specific_fields\\": {\\n            \\"thought_signature\\": \\"CpcHAdHtim9+q4rstcbvQC0ic4x1/vqQlCJWgE+UZ6dTLYGHMMBkF/AxqL5UmP6SY46uYC8t4BTFiXG5zkw6EMJ...\\"\\n          }\\n        }]\\n      },\\n      {\\n        \\"role\\": \\"tool\\",\\n        \\"content\\": \\"{\\\\\\"temperature\\\\\\": 30, \\\\\\"unit\\\\\\": \\\\\\"celsius\\\\\\"}\\",\\n        \\"tool_call_id\\": \\"call_abc123\\"\\n      }\\n    ],\\n    \\"tools\\": [...],\\n    \\"reasoning_effort\\": \\"low\\"\\n  }\'\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n#### Important Notes on Thought Signatures\\n\\n1. **Automatic Handling**: LiteLLM automatically extracts and preserves thought signatures. You don\'t need to manually manage them.\\n\\n2. **Parallel Function Calls**: When the model makes parallel function calls, only the **first function call** has a thought signature.\\n\\n3. **Sequential Function Calls**: In multi-step function calling, each step\'s first function call has its own thought signature that must be preserved.\\n\\n4. **Required for Context**: Thought signatures are essential for maintaining reasoning context. Without them, the model may lose context of its previous reasoning.\\n\\n## Conversation History: Switching from Non-Gemini-3 Models\\n\\n#### Common Question: Will switching from a non-Gemini-3 model to Gemini-3 break conversation history?\\n\\n**Answer: No!** LiteLLM automatically handles this by adding dummy thought signatures when needed.\\n\\n#### How It Works\\n\\nWhen you switch from a model that doesn\'t use thought signatures (e.g., `gemini-2.5-flash`) to Gemini 3, LiteLLM:\\n\\n1. **Detects missing signatures**: Identifies assistant messages with tool calls that lack thought signatures\\n2. **Adds dummy signature**: Automatically injects a dummy thought signature (`skip_thought_signature_validator`) for compatibility\\n3. **Maintains conversation flow**: Your conversation history continues to work seamlessly\\n\\n#### Example: Switching Models Mid-Conversation\\n\\n<Tabs>\\n<TabItem value=\\"sdk\\" label=\\"Python SDK\\">\\n\\n```python\\nfrom openai import OpenAI\\n\\nclient = OpenAI(api_key=\\"sk-1234\\", base_url=\\"http://localhost:4000\\")\\n\\n# Step 1: Start with gemini-2.5-flash (no thought signatures)\\nmessages = [{\\"role\\": \\"user\\", \\"content\\": \\"What\'s the weather?\\"}]\\n\\nresponse1 = client.chat.completions.create(\\n    model=\\"gemini-2.5-flash\\",\\n    messages=messages,\\n    tools=[...],\\n    reasoning_effort=\\"low\\"\\n)\\n\\n# Append assistant message (no tool call thought signature from gemini-2.5-flash)\\nmessages.append(response1.choices[0].message)\\n\\n# Step 2: Switch to gemini-3-pro-preview\\n# LiteLLM automatically adds dummy thought signature to the previous assistant message\\nresponse2 = client.chat.completions.create(\\n    model=\\"gemini-3-pro-preview\\",  # \ud83d\udc48 Switched model\\n    messages=messages,  # \ud83d\udc48 Same conversation history\\n    tools=[...],\\n    reasoning_effort=\\"low\\"\\n)\\n\\n# \u2705 Works seamlessly! No errors, no breaking changes\\nprint(response2.choices[0].message.content)\\n```\\n\\n</TabItem>\\n<TabItem value=\\"proxy\\" label=\\"cURL\\">\\n\\n```bash\\n# Step 1: Start with gemini-2.5-flash\\ncurl http://localhost:4000/v1/chat/completions \\\\\\n  -H \\"Content-Type: application/json\\" \\\\\\n  -H \\"Authorization: Bearer sk-1234\\" \\\\\\n  -d \'{\\n    \\"model\\": \\"gemini-2.5-flash\\",\\n    \\"messages\\": [{\\"role\\": \\"user\\", \\"content\\": \\"What\'\\\\\'\'s the weather?\\"}],\\n    \\"tools\\": [...],\\n    \\"reasoning_effort\\": \\"low\\"\\n  }\'\\n\\n# Step 2: Switch to gemini-3-pro-preview with same conversation history\\n# LiteLLM automatically handles the missing thought signature\\ncurl http://localhost:4000/v1/chat/completions \\\\\\n  -H \\"Content-Type: application/json\\" \\\\\\n  -H \\"Authorization: Bearer sk-1234\\" \\\\\\n  -d \'{\\n    \\"model\\": \\"gemini-3-pro-preview\\",  # \ud83d\udc48 Switched model\\n    \\"messages\\": [\\n      {\\"role\\": \\"user\\", \\"content\\": \\"What\'\\\\\'\'s the weather?\\"},\\n      {\\n        \\"role\\": \\"assistant\\",\\n        \\"tool_calls\\": [...]  # \ud83d\udc48 No thought_signature from gemini-2.5-flash\\n      }\\n    ],\\n    \\"tools\\": [...],\\n    \\"reasoning_effort\\": \\"low\\"\\n  }\'\\n# \u2705 Works! LiteLLM adds dummy signature automatically\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n#### Dummy Signature Details\\n\\nThe dummy signature used is: `base64(\\"skip_thought_signature_validator\\")`\\n\\nThis is the recommended approach by Google for handling conversation history from models that don\'t support thought signatures. It allows Gemini 3 to:\\n- Accept the conversation history without validation errors\\n- Continue the conversation seamlessly\\n- Maintain context across model switches\\n\\n## Thinking Level Parameter\\n\\n#### How `reasoning_effort` Maps to `thinking_level`\\n\\nFor Gemini 3 Pro Preview, LiteLLM automatically maps `reasoning_effort` to the new `thinking_level` parameter:\\n\\n| `reasoning_effort` | `thinking_level` | Notes |\\n|-------------------|------------------|-------|\\n| `\\"minimal\\"` | `\\"low\\"` | Maps to low thinking level |\\n| `\\"low\\"` | `\\"low\\"` | Default for most use cases |\\n| `\\"medium\\"` | `\\"high\\"` | Medium not available yet, maps to high |\\n| `\\"high\\"` | `\\"high\\"` | Maximum reasoning depth |\\n| `\\"disable\\"` | `\\"low\\"` | Gemini 3 cannot fully disable thinking |\\n| `\\"none\\"` | `\\"low\\"` | Gemini 3 cannot fully disable thinking |\\n\\n#### Default Behavior\\n\\nIf you don\'t specify `reasoning_effort`, LiteLLM automatically sets `thinking_level=\\"low\\"` for Gemini 3 models, to avoid high costs. \\n\\n### Example Usage\\n\\n<Tabs>\\n<TabItem value=\\"sdk\\" label=\\"Python SDK\\">\\n\\n```python\\nfrom litellm import completion\\n\\n# Low thinking level (faster, lower cost)\\nresponse = completion(\\n    model=\\"gemini/gemini-3-pro-preview\\",\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": \\"What\'s the weather?\\"}],\\n    reasoning_effort=\\"low\\"  # Maps to thinking_level=\\"low\\"\\n)\\n\\n# High thinking level (deeper reasoning, higher cost)\\nresponse = completion(\\n    model=\\"gemini/gemini-3-pro-preview\\",\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": \\"Solve this complex math problem step by step.\\"}],\\n    reasoning_effort=\\"high\\"  # Maps to thinking_level=\\"high\\"\\n)\\n```\\n\\n</TabItem>\\n<TabItem value=\\"proxy\\" label=\\"LiteLLM Proxy\\">\\n\\n```bash\\n# Low thinking level\\ncurl http://localhost:4000/v1/chat/completions \\\\\\n  -H \\"Content-Type: application/json\\" \\\\\\n  -H \\"Authorization: Bearer sk-1234\\" \\\\\\n  -d \'{\\n    \\"model\\": \\"gemini-3-pro-preview\\",\\n    \\"messages\\": [{\\"role\\": \\"user\\", \\"content\\": \\"What\'\\\\\'\'s the weather?\\"}],\\n    \\"reasoning_effort\\": \\"low\\"\\n  }\'\\n\\n# High thinking level\\ncurl http://localhost:4000/v1/chat/completions \\\\\\n  -H \\"Content-Type: application/json\\" \\\\\\n  -H \\"Authorization: Bearer sk-1234\\" \\\\\\n  -d \'{\\n    \\"model\\": \\"gemini-3-pro-preview\\",\\n    \\"messages\\": [{\\"role\\": \\"user\\", \\"content\\": \\"Solve this complex problem.\\"}],\\n    \\"reasoning_effort\\": \\"high\\"\\n  }\'\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n## Important Notes\\n\\n1. **Gemini 3 Cannot Disable Thinking**: Unlike Gemini 2.5 models, Gemini 3 cannot fully disable thinking. Even when you set `reasoning_effort=\\"none\\"` or `\\"disable\\"`, it maps to `thinking_level=\\"low\\"`.\\n\\n2. **Temperature Recommendation**: For Gemini 3 models, LiteLLM defaults `temperature` to `1.0` and strongly recommends keeping it at this default. Setting `temperature < 1.0` can cause:\\n   - Infinite loops\\n   - Degraded reasoning performance\\n   - Failure on complex tasks\\n\\n3. **Automatic Defaults**: If you don\'t specify `reasoning_effort`, LiteLLM automatically sets `thinking_level=\\"low\\"` for optimal performance.\\n\\n## Cost Tracking: Prompt Caching & Context Window\\n\\nLiteLLM provides comprehensive cost tracking for Gemini 3 Pro Preview, including support for prompt caching and tiered pricing based on context window size.\\n\\n### Prompt Caching Cost Tracking\\n\\nGemini 3 supports prompt caching, which allows you to cache frequently used prompt prefixes to reduce costs. LiteLLM automatically tracks and calculates costs for:\\n\\n- **Cache Hit Tokens**: Tokens that are read from cache (charged at a lower rate)\\n- **Cache Creation Tokens**: Tokens that are written to cache (one-time cost)\\n- **Text Tokens**: Regular prompt tokens that are processed normally\\n\\n#### How It Works\\n\\nLiteLLM extracts caching information from the `prompt_tokens_details` field in the usage object:\\n\\n```python\\n{\\n  \\"usage\\": {\\n    \\"prompt_tokens\\": 50000,\\n    \\"completion_tokens\\": 1000,\\n    \\"total_tokens\\": 51000,\\n    \\"prompt_tokens_details\\": {\\n      \\"cached_tokens\\": 30000,  # Cache hit tokens\\n      \\"cache_creation_tokens\\": 5000,  # Tokens written to cache\\n      \\"text_tokens\\": 15000  # Regular processed tokens\\n    }\\n  }\\n}\\n```\\n\\n### Context Window Tiered Pricing\\n\\nGemini 3 Pro Preview supports up to 1M tokens of context, with tiered pricing that automatically applies when your prompt exceeds 200k tokens.\\n\\n#### Automatic Tier Detection\\n\\nLiteLLM automatically detects when your prompt exceeds the 200k token threshold and applies the appropriate tiered pricing:\\n\\n```python\\nfrom litellm import completion_cost\\n\\n# Example: Small prompt (< 200k tokens)\\nresponse_small = completion(\\n    model=\\"gemini/gemini-3-pro-preview\\",\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": \\"Hello!\\"}]\\n)\\n# Uses base pricing: $0.000002/input token, $0.000012/output token\\n\\n# Example: Large prompt (> 200k tokens)\\nresponse_large = completion(\\n    model=\\"gemini/gemini-3-pro-preview\\",\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": \\"...\\" * 250000}]  # 250k tokens\\n)\\n# Automatically uses tiered pricing: $0.000004/input token, $0.000018/output token\\n```\\n\\n#### Cost Breakdown\\n\\nThe cost calculation includes:\\n\\n1. **Text Processing Cost**: Regular tokens processed at base or tiered rate\\n2. **Cache Read Cost**: Cached tokens read at discounted rate\\n3. **Cache Creation Cost**: One-time cost for writing tokens to cache (applies tiered rate if above 200k)\\n4. **Output Cost**: Generated tokens at base or tiered rate\\n\\n### Example: Viewing Cost Breakdown\\n\\nYou can view the detailed cost breakdown using LiteLLM\'s cost tracking:\\n\\n```python\\nfrom litellm import completion, completion_cost\\n\\nresponse = completion(\\n    model=\\"gemini/gemini-3-pro-preview\\",\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": \\"Explain prompt caching\\"}],\\n    caching=True  # Enable prompt caching\\n)\\n\\n# Get total cost\\ntotal_cost = completion_cost(completion_response=response)\\nprint(f\\"Total cost: ${total_cost:.6f}\\")\\n\\n# Access usage details\\nusage = response.usage\\nprint(f\\"Prompt tokens: {usage.prompt_tokens}\\")\\nprint(f\\"Completion tokens: {usage.completion_tokens}\\")\\n\\n# Access caching details\\nif usage.prompt_tokens_details:\\n    print(f\\"Cache hit tokens: {usage.prompt_tokens_details.cached_tokens}\\")\\n    print(f\\"Cache creation tokens: {usage.prompt_tokens_details.cache_creation_tokens}\\")\\n    print(f\\"Text tokens: {usage.prompt_tokens_details.text_tokens}\\")\\n```\\n\\n### Cost Optimization Tips\\n\\n1. **Use Prompt Caching**: For repeated prompt prefixes, enable caching to reduce costs by up to 90% for cached portions\\n2. **Monitor Context Size**: Be aware that prompts above 200k tokens use tiered pricing (2x for input, 1.5x for output)\\n3. **Cache Management**: Cache creation tokens are charged once when writing to cache, then subsequent reads are much cheaper\\n4. **Track Usage**: Use LiteLLM\'s built-in cost tracking to monitor spending across different token types\\n\\n### Integration with LiteLLM Proxy\\n\\nWhen using LiteLLM Proxy, all cost tracking is automatically logged and available through:\\n\\n- **Usage Logs**: Detailed token and cost breakdowns in proxy logs\\n- **Budget Management**: Set budgets and alerts based on actual usage\\n- **Analytics Dashboard**: View cost trends and breakdowns by token type\\n\\n```yaml\\n# config.yaml\\nmodel_list:\\n  - model_name: gemini-3-pro-preview\\n    litellm_params:\\n      model: gemini/gemini-3-pro-preview\\n      api_key: os.environ/GEMINI_API_KEY\\n\\nlitellm_settings:\\n  # Enable detailed cost tracking\\n  success_callback: [\\"langfuse\\"]  # or your preferred logging service\\n```\\n\\n## Using with Claude Code CLI\\n\\nYou can use `gemini-3-pro-preview` with **Claude Code CLI** - Anthropic\'s command-line interface. This allows you to use Gemini 3 Pro Preview with Claude Code\'s native syntax and workflows.\\n\\n### Setup\\n\\n**1. Add Gemini 3 Pro Preview to your `config.yaml`:**\\n\\n```yaml\\nmodel_list:\\n  - model_name: gemini-3-pro-preview\\n    litellm_params:\\n      model: gemini/gemini-3-pro-preview\\n      api_key: os.environ/GEMINI_API_KEY\\n\\nlitellm_settings:\\n  master_key: os.environ/LITELLM_MASTER_KEY\\n```\\n\\n**2. Set environment variables:**\\n\\n```bash\\nexport GEMINI_API_KEY=\\"your-gemini-api-key\\"\\nexport LITELLM_MASTER_KEY=\\"sk-1234567890\\"  # Generate a secure key\\n```\\n\\n**3. Start LiteLLM Proxy:**\\n\\n```bash\\nlitellm --config /path/to/config.yaml\\n\\n# RUNNING on http://0.0.0.0:4000\\n```\\n\\n**4. Configure Claude Code to use LiteLLM Proxy:**\\n\\n```bash\\nexport ANTHROPIC_BASE_URL=\\"http://0.0.0.0:4000\\"\\nexport ANTHROPIC_AUTH_TOKEN=\\"$LITELLM_MASTER_KEY\\"\\n```\\n\\n**5. Use Gemini 3 Pro Preview with Claude Code:**\\n\\n```bash\\n# Claude Code will use gemini-3-pro-preview from your LiteLLM proxy\\nclaude --model gemini-3-pro-preview\\n\\n```\\n\\n### Example Usage\\n\\nOnce configured, you can interact with Gemini 3 Pro Preview using Claude Code\'s native interface:\\n\\n```bash\\n$ claude --model gemini-3-pro-preview\\n> Explain how thought signatures work in multi-turn conversations.\\n\\n# Gemini 3 Pro Preview responds through Claude Code interface\\n```\\n\\n### Benefits\\n\\n- \u2705 **Native Claude Code Experience**: Use Gemini 3 Pro Preview with Claude Code\'s familiar CLI interface\\n- \u2705 **Unified Authentication**: Single API key for all models through LiteLLM proxy\\n- \u2705 **Cost Tracking**: All usage tracked through LiteLLM\'s centralized logging\\n- \u2705 **Seamless Model Switching**: Easily switch between Claude and Gemini models\\n- \u2705 **Full Feature Support**: All Gemini 3 features (thought signatures, function calling, etc.) work through Claude Code\\n\\n### Troubleshooting\\n\\n**Claude Code not finding the model:**\\n- Ensure the model name in Claude Code matches exactly: `gemini-3-pro-preview`\\n- Verify your proxy is running: `curl http://0.0.0.0:4000/health`\\n- Check that `ANTHROPIC_BASE_URL` points to your LiteLLM proxy\\n\\n**Authentication errors:**\\n- Verify `ANTHROPIC_AUTH_TOKEN` matches your LiteLLM master key\\n- Ensure `GEMINI_API_KEY` is set correctly\\n- Check LiteLLM proxy logs for detailed error messages\\n\\n## Responses API Support\\n\\nLiteLLM fully supports the OpenAI Responses API for Gemini 3 Pro Preview, including both streaming and non-streaming modes. The Responses API provides a structured way to handle multi-turn conversations with function calling, and LiteLLM automatically preserves thought signatures throughout the conversation.\\n\\n### Example: Using Responses API with Gemini 3\\n\\n<Tabs>\\n<TabItem value=\\"sdk\\" label=\\"Non-Streaming\\">\\n\\n```python\\nfrom openai import OpenAI\\nimport json\\n\\nclient = OpenAI()\\n\\n# 1. Define a list of callable tools for the model\\ntools = [\\n    {\\n        \\"type\\": \\"function\\",\\n        \\"name\\": \\"get_horoscope\\",\\n        \\"description\\": \\"Get today\'s horoscope for an astrological sign.\\",\\n        \\"parameters\\": {\\n            \\"type\\": \\"object\\",\\n            \\"properties\\": {\\n                \\"sign\\": {\\n                    \\"type\\": \\"string\\",\\n                    \\"description\\": \\"An astrological sign like Taurus or Aquarius\\",\\n                },\\n            },\\n            \\"required\\": [\\"sign\\"],\\n        },\\n    },\\n]\\n\\ndef get_horoscope(sign):\\n    return f\\"{sign}: Next Tuesday you will befriend a baby otter.\\"\\n\\n# Create a running input list we will add to over time\\ninput_list = [\\n    {\\"role\\": \\"user\\", \\"content\\": \\"What is my horoscope? I am an Aquarius.\\"}\\n]\\n\\n# 2. Prompt the model with tools defined\\nresponse = client.responses.create(\\n    model=\\"gemini-3-pro-preview\\",\\n    tools=tools,\\n    input=input_list,\\n)\\n\\n# Save function call outputs for subsequent requests\\ninput_list += response.output\\n\\nfor item in response.output:\\n    if item.type == \\"function_call\\":\\n        if item.name == \\"get_horoscope\\":\\n            # 3. Execute the function logic for get_horoscope\\n            horoscope = get_horoscope(json.loads(item.arguments))\\n            \\n            # 4. Provide function call results to the model\\n            input_list.append({\\n                \\"type\\": \\"function_call_output\\",\\n                \\"call_id\\": item.call_id,\\n                \\"output\\": json.dumps({\\n                  \\"horoscope\\": horoscope\\n                })\\n            })\\n\\nprint(\\"Final input:\\")\\nprint(input_list)\\n\\nresponse = client.responses.create(\\n    model=\\"gemini-3-pro-preview\\",\\n    instructions=\\"Respond only with a horoscope generated by a tool.\\",\\n    tools=tools,\\n    input=input_list,\\n)\\n\\n# 5. The model should be able to give a response!\\nprint(\\"Final output:\\")\\nprint(response.model_dump_json(indent=2))\\nprint(\\"\\\\n\\" + response.output_text)\\n```\\n\\n**Key Points:**\\n- \u2705 Thought signatures are automatically preserved in function calls\\n- \u2705 Works seamlessly with multi-turn conversations\\n- \u2705 All Gemini 3-specific features are fully supported\\n\\n</TabItem>\\n<TabItem value=\\"streaming\\" label=\\"Streaming\\">\\n\\n```python\\nfrom openai import OpenAI\\nimport json\\n\\nclient = OpenAI()\\n\\ntools = [\\n    {\\n        \\"type\\": \\"function\\",\\n        \\"name\\": \\"get_horoscope\\",\\n        \\"description\\": \\"Get today\'s horoscope for an astrological sign.\\",\\n        \\"parameters\\": {\\n            \\"type\\": \\"object\\",\\n            \\"properties\\": {\\n                \\"sign\\": {\\n                    \\"type\\": \\"string\\",\\n                    \\"description\\": \\"An astrological sign like Taurus or Aquarius\\",\\n                },\\n            },\\n            \\"required\\": [\\"sign\\"],\\n        },\\n    },\\n]\\n\\ndef get_horoscope(sign):\\n    return f\\"{sign}: Next Tuesday you will befriend a baby otter.\\"\\n\\ninput_list = [\\n    {\\"role\\": \\"user\\", \\"content\\": \\"What is my horoscope? I am an Aquarius.\\"}\\n]\\n\\n# Streaming mode\\nresponse = client.responses.create(\\n    model=\\"gemini-3-pro-preview\\",\\n    tools=tools,\\n    input=input_list,\\n    stream=True,\\n)\\n\\n# Collect all chunks\\nchunks = []\\nfor chunk in response:\\n    chunks.append(chunk)\\n    # Process streaming chunks as they arrive\\n    print(chunk)\\n\\n# Thought signatures are automatically preserved in streaming mode\\n```\\n\\n**Key Points:**\\n- \u2705 Streaming mode fully supported\\n- \u2705 Thought signatures preserved across streaming chunks\\n- \u2705 Real-time processing of function calls and responses\\n\\n</TabItem>\\n</Tabs>\\n\\n### Responses API Benefits\\n\\n- \u2705 **Structured Output**: Responses API provides a clear structure for handling function calls and multi-turn conversations\\n- \u2705 **Thought Signature Preservation**: LiteLLM automatically preserves thought signatures in both streaming and non-streaming modes\\n- \u2705 **Seamless Integration**: Works with existing OpenAI SDK patterns\\n- \u2705 **Full Feature Support**: All Gemini 3 features (thought signatures, function calling, reasoning) are fully supported\\n\\n\\n## Best Practices\\n\\n#### 1. Always Include Thought Signatures in Conversation History\\n\\nWhen building multi-turn conversations with function calling:\\n\\n\u2705 **Do:**\\n```python\\n# Append the full assistant message (includes thought signatures)\\nmessages.append(response.choices[0].message)\\n```\\n\\n\u274c **Don\'t:**\\n```python\\n# Don\'t manually construct assistant messages without thought signatures\\nmessages.append({\\n    \\"role\\": \\"assistant\\",\\n    \\"tool_calls\\": [...]  # Missing thought signatures!\\n})\\n```\\n\\n#### 2. Use Appropriate Thinking Levels\\n\\n- **`reasoning_effort=\\"low\\"`**: For simple queries, quick responses, cost optimization\\n- **`reasoning_effort=\\"high\\"`**: For complex problems requiring deep reasoning\\n\\n#### 3. Keep Temperature at Default\\n\\nFor Gemini 3 models, always use `temperature=1.0` (default). Lower temperatures can cause issues.\\n\\n#### 4. Handle Model Switches Gracefully\\n\\nWhen switching from non-Gemini-3 to Gemini-3:\\n- \u2705 LiteLLM automatically handles missing thought signatures\\n- \u2705 No manual intervention needed\\n- \u2705 Conversation history continues seamlessly\\n\\n\\n## Troubleshooting\\n\\n#### Issue: Missing Thought Signatures\\n\\n**Symptom**: Error when including assistant messages in conversation history\\n\\n**Solution**: Ensure you\'re appending the full assistant message from the response:\\n```python\\nmessages.append(response.choices[0].message)  # \u2705 Includes thought signatures\\n```\\n\\n#### Issue: Conversation Breaks When Switching Models\\n\\n**Symptom**: Errors when switching from gemini-2.5-flash to gemini-3-pro-preview\\n\\n**Solution**: This should work automatically! LiteLLM adds dummy signatures. If you see errors, ensure you\'re using the latest LiteLLM version.\\n\\n#### Issue: Infinite Loops or Poor Performance\\n\\n**Symptom**: Model gets stuck or produces poor results\\n\\n**Solution**: \\n- Ensure `temperature=1.0` (default for Gemini 3)\\n- Check that `reasoning_effort` is set appropriately\\n- Verify you\'re using the correct model name: `gemini/gemini-3-pro-preview`\\n\\n## Additional Resources\\n\\n- [Gemini Provider Documentation](../gemini.md)\\n- [Thought Signatures Guide](../gemini.md#thought-signatures)\\n- [Reasoning Content Documentation](../../reasoning_content.md)\\n- [Function Calling Guide](../../function_calling.md)"}]}}')}}]);