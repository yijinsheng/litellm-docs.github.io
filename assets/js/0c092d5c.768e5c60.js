"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[18367],{7227(e,n,l){l.d(n,{A:()=>o});l(96540);var s=l(18215);const t="tabItem_Ymn6";var a=l(74848);function o({children:e,hidden:n,className:l}){return(0,a.jsx)("div",{role:"tabpanel",className:(0,s.A)(t,l),hidden:n,children:e})}},28453(e,n,l){l.d(n,{R:()=>o,x:()=>r});var s=l(96540);const t={},a=s.createContext(t);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(a.Provider,{value:n},e.children)}},32682(e,n,l){l.r(n),l.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>m});const s=JSON.parse('{"id":"providers/vllm","title":"VLLM","description":"LiteLLM supports all models on VLLM.","source":"@site/docs/providers/vllm.md","sourceDirName":"providers","slug":"/providers/vllm","permalink":"/docs/providers/vllm","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Vercel AI\u7f51\u5173","permalink":"/docs/providers/vercel_ai_gateway"},"next":{"title":"vLLM - \u6279\u91cf\u5904\u7406 + \u6587\u4ef6 API","permalink":"/docs/providers/vllm_batches"}}');var t=l(74848),a=l(28453),o=l(49489),r=l(7227);const i={},d="VLLM",c={},m=[{value:"Usage - litellm.completion (calling OpenAI compatible endpoint)",id:"usage---litellmcompletion-calling-openai-compatible-endpoint",level:2},{value:"Usage -  LiteLLM Proxy Server (calling OpenAI compatible endpoint)",id:"usage----litellm-proxy-server-calling-openai-compatible-endpoint",level:2},{value:"Reasoning Effort",id:"reasoning-effort",level:2},{value:"Embeddings",id:"embeddings",level:2},{value:"Rerank",id:"rerank",level:2},{value:"Async Usage",id:"async-usage",level:3},{value:"Send Video URL to VLLM",id:"send-video-url-to-vllm",level:2},{value:"(Deprecated) for <code>vllm pip package</code>",id:"deprecated-for-vllm-pip-package",level:2},{value:"Using - <code>litellm.completion</code>",id:"using---litellmcompletion",level:3},{value:"Batch Completion",id:"batch-completion",level:3},{value:"Prompt Templates",id:"prompt-templates",level:3},{value:"Models we already have Prompt Templates for",id:"models-we-already-have-prompt-templates-for",level:4},{value:"Custom prompt templates",id:"custom-prompt-templates",level:4}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"vllm",children:"VLLM"})}),"\n",(0,t.jsx)(n.p,{children:"LiteLLM supports all models on VLLM."}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Property"}),(0,t.jsx)(n.th,{children:"Details"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Description"}),(0,t.jsxs)(n.td,{children:["vLLM is a fast and easy-to-use library for LLM inference and serving. ",(0,t.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/index.html",children:"Docs"})]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Provider Route on LiteLLM"}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"hosted_vllm/"})," (for OpenAI compatible server), ",(0,t.jsx)(n.code,{children:"vllm/"})," ([DEPRECATED] for vLLM sdk usage)"]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Provider Doc"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/index.html",children:"vLLM \u2197"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Supported Endpoints"}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"/chat/completions"}),", ",(0,t.jsx)(n.code,{children:"/embeddings"}),", ",(0,t.jsx)(n.code,{children:"/completions"}),", ",(0,t.jsx)(n.code,{children:"/rerank"}),", ",(0,t.jsx)(n.code,{children:"/audio/transcriptions"})]})]})]})]}),"\n",(0,t.jsx)(n.h1,{id:"quick-start",children:"Quick Start"}),"\n",(0,t.jsx)(n.h2,{id:"usage---litellmcompletion-calling-openai-compatible-endpoint",children:"Usage - litellm.completion (calling OpenAI compatible endpoint)"}),"\n",(0,t.jsx)(n.p,{children:"vLLM Provides an OpenAI compatible endpoints - here's how to call it with LiteLLM"}),"\n",(0,t.jsx)(n.p,{children:"In order to use litellm to call a hosted vllm server add the following to your completion call"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'model="hosted_vllm/<your-vllm-model-name>"'})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:'api_base = "your-hosted-vllm-server"'})}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import litellm \n\nresponse = litellm.completion(\n            model="hosted_vllm/facebook/opt-125m", # pass the vllm model name\n            messages=messages,\n            api_base="https://hosted-vllm-api.co",\n            temperature=0.2,\n            max_tokens=80)\n\nprint(response)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"usage----litellm-proxy-server-calling-openai-compatible-endpoint",children:"Usage -  LiteLLM Proxy Server (calling OpenAI compatible endpoint)"}),"\n",(0,t.jsx)(n.p,{children:"Here's how to call an OpenAI-Compatible Endpoint with the LiteLLM Proxy Server"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Modify the config.yaml"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: my-model\n    litellm_params:\n      model: hosted_vllm/facebook/opt-125m  # add hosted_vllm/ prefix to route as OpenAI provider\n      api_base: https://hosted-vllm-api.co      # add api base for OpenAI compatible provider\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start the proxy"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"$ litellm --config /path/to/config.yaml\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Send Request to LiteLLM Proxy Server"}),"\n"]}),"\n",(0,t.jsxs)(o.A,{children:[(0,t.jsx)(r.A,{value:"openai",label:"OpenAI Python v1.0.0+",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import openai\nclient = openai.OpenAI(\n    api_key="sk-1234",             # pass litellm proxy key, if you\'re using virtual keys\n    base_url="http://0.0.0.0:4000" # litellm-proxy-base url\n)\n\nresponse = client.chat.completions.create(\n    model="my-model",\n    messages = [\n        {\n            "role": "user",\n            "content": "what llm are you"\n        }\n    ],\n)\n\nprint(response)\n'})})}),(0,t.jsx)(r.A,{value:"curl",label:"curl",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:'curl --location \'http://0.0.0.0:4000/chat/completions\' \\\n    --header \'Authorization: Bearer sk-1234\' \\\n    --header \'Content-Type: application/json\' \\\n    --data \'{\n    "model": "my-model",\n    "messages": [\n        {\n        "role": "user",\n        "content": "what llm are you"\n        }\n    ],\n}\'\n'})})})]}),"\n",(0,t.jsx)(n.h2,{id:"reasoning-effort",children:"Reasoning Effort"}),"\n",(0,t.jsxs)(o.A,{children:[(0,t.jsx)(r.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion\n\nresponse = completion(\n    model="hosted_vllm/gpt-oss-120b",\n    messages=[{"role": "user", "content": "whats 2 + 2"}],\n    reasoning_effort="high",\n    api_base="https://hosted-vllm-api.co",\n)\nprint(response)\n'})})}),(0,t.jsxs)(r.A,{value:"proxy",label:"PROXY",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Setup config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: gpt-oss-120b\n    litellm_params:\n      model: hosted_vllm/gpt-oss-120b\n      api_base: https://hosted-vllm-api.co\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start the proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Test it!"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl http://0.0.0.0:4000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{"model": "gpt-oss-120b", "messages": [{"role": "user", "content": "whats 2 + 2"}], "reasoning_effort": "high"}\'\n'})})]})]}),"\n",(0,t.jsx)(n.h2,{id:"embeddings",children:"Embeddings"}),"\n",(0,t.jsxs)(o.A,{children:[(0,t.jsx)(r.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import embedding   \nimport os\n\nos.environ["HOSTED_VLLM_API_BASE"] = "http://localhost:8000"\n\n\nembedding = embedding(model="hosted_vllm/facebook/opt-125m", input=["Hello world"])\n\nprint(embedding)\n'})})}),(0,t.jsxs)(r.A,{value:"proxy",label:"PROXY",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Setup config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n    - model_name: my-model\n      litellm_params:\n        model: hosted_vllm/facebook/opt-125m  # add hosted_vllm/ prefix to route as OpenAI provider\n        api_base: https://hosted-vllm-api.co      # add api base for OpenAI compatible provider\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start the proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"$ litellm --config /path/to/config.yaml\n\n# RUNNING on http://0.0.0.0:4000\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Test it!"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"curl -L -X POST 'http://0.0.0.0:4000/embeddings' \\\n-H 'Authorization: Bearer sk-1234' \\\n-H 'Content-Type: application/json' \\\n-d '{\"input\": [\"hello world\"], \"model\": \"my-model\"}'\n"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"/docs/proxy/user_keys#embeddings",children:"See OpenAI SDK/Langchain/etc. examples"})})]})]}),"\n",(0,t.jsx)(n.h2,{id:"rerank",children:"Rerank"}),"\n",(0,t.jsxs)(o.A,{children:[(0,t.jsxs)(r.A,{value:"sdk",label:"SDK",children:[(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import rerank\nimport os\n\nos.environ["HOSTED_VLLM_API_BASE"] = "http://localhost:8000"\nos.environ["HOSTED_VLLM_API_KEY"] = ""  # [optional], if your VLLM server requires an API key\n\nquery = "What is the capital of the United States?"\ndocuments = [\n    "Carson City is the capital city of the American state of Nevada.",\n    "The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.",\n    "Washington, D.C. is the capital of the United States.",\n    "Capital punishment has existed in the United States since before it was a country.",\n]\n\nresponse = rerank(\n    model="hosted_vllm/your-rerank-model",\n    query=query,\n    documents=documents,\n    top_n=3,\n)\nprint(response)\n'})}),(0,t.jsx)(n.h3,{id:"async-usage",children:"Async Usage"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import arerank\nimport os, asyncio\n\nos.environ["HOSTED_VLLM_API_BASE"] = "http://localhost:8000"\nos.environ["HOSTED_VLLM_API_KEY"] = ""  # [optional], if your VLLM server requires an API key\n\nasync def test_async_rerank(): \n    query = "What is the capital of the United States?"\n    documents = [\n        "Carson City is the capital city of the American state of Nevada.",\n        "The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.",\n        "Washington, D.C. is the capital of the United States.",\n        "Capital punishment has existed in the United States since before it was a country.",\n    ]\n\n    response = await arerank(\n        model="hosted_vllm/your-rerank-model",\n        query=query,\n        documents=documents,\n        top_n=3,\n    )\n    print(response)\n\nasyncio.run(test_async_rerank())\n'})})]}),(0,t.jsxs)(r.A,{value:"proxy",label:"PROXY",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Setup config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n    - model_name: my-rerank-model\n      litellm_params:\n        model: hosted_vllm/your-rerank-model  # add hosted_vllm/ prefix to route as VLLM provider\n        api_base: http://localhost:8000      # add api base for your VLLM server\n        # api_key: your-api-key             # [optional] if your VLLM server requires authentication\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start the proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"$ litellm --config /path/to/config.yaml\n\n# RUNNING on http://0.0.0.0:4000\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Test it!"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -L -X POST \'http://0.0.0.0:4000/rerank\' \\\n-H \'Authorization: Bearer sk-1234\' \\\n-H \'Content-Type: application/json\' \\\n-d \'{\n    "model": "my-rerank-model",\n    "query": "What is the capital of the United States?",\n    "documents": [\n        "Carson City is the capital city of the American state of Nevada.",\n        "The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.",\n        "Washington, D.C. is the capital of the United States.",\n        "Capital punishment has existed in the United States since before it was a country."\n    ],\n    "top_n": 3\n}\'\n'})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"/docs/rerank#litellm-proxy-usage",children:"See OpenAI SDK/Langchain/etc. examples"})})]})]}),"\n",(0,t.jsx)(n.h2,{id:"send-video-url-to-vllm",children:"Send Video URL to VLLM"}),"\n",(0,t.jsxs)(n.p,{children:["Example Implementation from VLLM ",(0,t.jsx)(n.a,{href:"https://github.com/vllm-project/vllm/pull/10020",children:"here"})]}),"\n",(0,t.jsxs)(o.A,{children:[(0,t.jsxs)(r.A,{value:"files_message",label:"(Unified) Files Message",children:[(0,t.jsxs)(n.p,{children:["Use this to send a video url to VLLM + Gemini in the same format, using OpenAI's ",(0,t.jsx)(n.code,{children:"files"})," message type."]}),(0,t.jsx)(n.p,{children:"There are two ways to send a video url to VLLM:"}),(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Pass the video url directly"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'{"type": "file", "file": {"file_id": video_url}},\n'})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Pass the video data as base64"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'{"type": "file", "file": {"file_data": f"data:video/mp4;base64,{video_data_base64}"}}\n'})}),(0,t.jsxs)(o.A,{children:[(0,t.jsx)(r.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion\n\nmessages=[\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "text",\n                "text": "Summarize the following video"\n            },\n            {\n                "type": "file",\n                "file": {\n                    "file_id": "https://www.youtube.com/watch?v=dQw4w9WgXcQ"\n                }\n            }\n        ]\n    }\n]\n\n# call vllm \nos.environ["HOSTED_VLLM_API_BASE"] = "https://hosted-vllm-api.co"\nos.environ["HOSTED_VLLM_API_KEY"] = "" # [optional], if your VLLM server requires an API key\nresponse = completion(\n    model="hosted_vllm/qwen", # pass the vllm model name\n    messages=messages,\n)\n\n# call gemini \nos.environ["GEMINI_API_KEY"] = "your-gemini-api-key"\nresponse = completion(\n    model="gemini/gemini-1.5-flash", # pass the gemini model name\n    messages=messages,\n)\n\nprint(response)\n'})})}),(0,t.jsxs)(r.A,{value:"proxy",label:"PROXY",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Setup config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n    - model_name: my-model\n      litellm_params:\n        model: hosted_vllm/qwen  # add hosted_vllm/ prefix to route as OpenAI provider\n        api_base: https://hosted-vllm-api.co      # add api base for OpenAI compatible provider\n    - model_name: my-gemini-model\n      litellm_params:\n        model: gemini/gemini-1.5-flash  # add gemini/ prefix to route as Google AI Studio provider\n        api_key: os.environ/GEMINI_API_KEY\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start the proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"$ litellm --config /path/to/config.yaml\n\n# RUNNING on http://0.0.0.0:4000\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Test it!"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://0.0.0.0:4000/chat/completions \\\n-H "Authorization: Bearer sk-1234" \\\n-H "Content-Type: application/json" \\\n-d \'{\n    "model": "my-model",\n    "messages": [\n        {"role": "user", "content": \n            [\n                {"type": "text", "text": "Summarize the following video"},\n                {"type": "file", "file": {"file_id": "https://www.youtube.com/watch?v=dQw4w9WgXcQ"}}\n            ]\n        }\n    ]\n}\'\n'})})]})]})]}),(0,t.jsxs)(r.A,{value:"video_url",label:"(VLLM-specific) Video Message",children:[(0,t.jsxs)(n.p,{children:["Use this to send a video url to VLLM in it's native message format (",(0,t.jsx)(n.code,{children:"video_url"}),")."]}),(0,t.jsx)(n.p,{children:"There are two ways to send a video url to VLLM:"}),(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Pass the video url directly"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'{"type": "video_url", "video_url": {"url": video_url}},\n'})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Pass the video data as base64"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'{"type": "video_url", "video_url": {"url": f"data:video/mp4;base64,{video_data_base64}"}}\n'})}),(0,t.jsxs)(o.A,{children:[(0,t.jsx)(r.A,{value:"sdk",label:"SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion\n\nresponse = completion(\n            model="hosted_vllm/qwen", # pass the vllm model name\n            messages=[\n                {\n                    "role": "user",\n                    "content": [\n                        {\n                            "type": "text",\n                            "text": "Summarize the following video"\n                        },\n                        {\n                            "type": "video_url",\n                            "video_url": {\n                                "url": "https://www.youtube.com/watch?v=dQw4w9WgXcQ"\n                            }\n                        }\n                    ]\n                }\n            ],\n            api_base="https://hosted-vllm-api.co")\n\nprint(response)\n'})})}),(0,t.jsxs)(r.A,{value:"proxy",label:"PROXY",children:[(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Setup config.yaml"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n    - model_name: my-model\n      litellm_params:\n        model: hosted_vllm/qwen  # add hosted_vllm/ prefix to route as OpenAI provider\n        api_base: https://hosted-vllm-api.co      # add api base for OpenAI compatible provider\n"})}),(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Start the proxy"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"$ litellm --config /path/to/config.yaml\n\n# RUNNING on http://0.0.0.0:4000\n"})}),(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Test it!"}),"\n"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://0.0.0.0:4000/chat/completions \\\n-H "Authorization: Bearer sk-1234" \\\n-H "Content-Type: application/json" \\\n-d \'{\n    "model": "my-model",\n    "messages": [\n        {"role": "user", "content": \n            [\n                {"type": "text", "text": "Summarize the following video"},\n                {"type": "video_url", "video_url": {"url": "https://www.youtube.com/watch?v=dQw4w9WgXcQ"}}\n            ]\n        }\n    ]\n}\'\n'})})]})]})]})]}),"\n",(0,t.jsxs)(n.h2,{id:"deprecated-for-vllm-pip-package",children:["(Deprecated) for ",(0,t.jsx)(n.code,{children:"vllm pip package"})]}),"\n",(0,t.jsxs)(n.h3,{id:"using---litellmcompletion",children:["Using - ",(0,t.jsx)(n.code,{children:"litellm.completion"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"pip install litellm vllm\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import litellm \n\nresponse = litellm.completion(\n            model="vllm/facebook/opt-125m", # add a vllm prefix so litellm knows the custom_llm_provider==vllm\n            messages=messages,\n            temperature=0.2,\n            max_tokens=80)\n\nprint(response)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"batch-completion",children:"Batch Completion"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import batch_completion\n\nmodel_name = "facebook/opt-125m"\nprovider = "vllm"\nmessages = [[{"role": "user", "content": "Hey, how\'s it going"}] for _ in range(5)]\n\nresponse_list = batch_completion(\n            model=model_name, \n            custom_llm_provider=provider, # can easily switch to huggingface, replicate, together ai, sagemaker, etc.\n            messages=messages,\n            temperature=0.2,\n            max_tokens=80,\n        )\nprint(response_list)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"prompt-templates",children:"Prompt Templates"}),"\n",(0,t.jsx)(n.p,{children:"For models with special prompt templates (e.g. Llama2), we format the prompt to fit their template."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"What if we don't support a model you need?"}),"\nYou can also specify you're own custom prompt formatting, in case we don't have your model covered yet."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Does this mean you have to specify a prompt for all models?"}),"\nNo. By default we'll concatenate your message content to make a prompt (expected format for Bloom, T-5, Llama-2 base models, etc.)"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Default Prompt Template"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def default_pt(messages):\n    return " ".join(message["content"] for message in messages)\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://github.com/BerriAI/litellm/blob/main/litellm/llms/prompt_templates/factory.py",children:"Code for how prompt templates work in LiteLLM"})}),"\n",(0,t.jsx)(n.h4,{id:"models-we-already-have-prompt-templates-for",children:"Models we already have Prompt Templates for"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Model Name"}),(0,t.jsx)(n.th,{children:"Works for Models"}),(0,t.jsx)(n.th,{children:"Function Call"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"meta-llama/Llama-2-7b-chat"}),(0,t.jsx)(n.td,{children:"All meta-llama llama2 chat models"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='vllm/meta-llama/Llama-2-7b', messages=messages, api_base=\"your_api_endpoint\")"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"tiiuae/falcon-7b-instruct"}),(0,t.jsx)(n.td,{children:"All falcon instruct models"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='vllm/tiiuae/falcon-7b-instruct', messages=messages, api_base=\"your_api_endpoint\")"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"mosaicml/mpt-7b-chat"}),(0,t.jsx)(n.td,{children:"All mpt chat models"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='vllm/mosaicml/mpt-7b-chat', messages=messages, api_base=\"your_api_endpoint\")"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"codellama/CodeLlama-34b-Instruct-hf"}),(0,t.jsx)(n.td,{children:"All codellama instruct models"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='vllm/codellama/CodeLlama-34b-Instruct-hf', messages=messages, api_base=\"your_api_endpoint\")"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"WizardLM/WizardCoder-Python-34B-V1.0"}),(0,t.jsx)(n.td,{children:"All wizardcoder models"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='vllm/WizardLM/WizardCoder-Python-34B-V1.0', messages=messages, api_base=\"your_api_endpoint\")"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Phind/Phind-CodeLlama-34B-v2"}),(0,t.jsx)(n.td,{children:"All phind-codellama models"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"completion(model='vllm/Phind/Phind-CodeLlama-34B-v2', messages=messages, api_base=\"your_api_endpoint\")"})})]})]})]}),"\n",(0,t.jsx)(n.h4,{id:"custom-prompt-templates",children:"Custom prompt templates"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Create your own custom prompt template works \nlitellm.register_prompt_template(\n\tmodel="togethercomputer/LLaMA-2-7B-32K",\n\troles={\n            "system": {\n                "pre_message": "[INST] <<SYS>>\\n",\n                "post_message": "\\n<</SYS>>\\n [/INST]\\n"\n            },\n            "user": { \n                "pre_message": "[INST] ",\n                "post_message": " [/INST]\\n"\n            }, \n            "assistant": {\n                "pre_message": "\\n",\n                "post_message": "\\n",\n            }\n        } # tell LiteLLM how you want to map the openai messages to this model\n)\n\ndef test_vllm_custom_model():\n    model = "vllm/togethercomputer/LLaMA-2-7B-32K"\n    response = completion(model=model, messages=messages)\n    print(response[\'choices\'][0][\'message\'][\'content\'])\n    return response\n\ntest_vllm_custom_model()\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://github.com/BerriAI/litellm/blob/6b3cb1898382f2e4e80fd372308ea232868c78d1/litellm/utils.py#L1414",children:"Implementation Code"})})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},49489(e,n,l){l.d(n,{A:()=>L});var s=l(96540),t=l(18215),a=l(24245),o=l(56347),r=l(36494),i=l(62814),d=l(45167),c=l(69900);function m(e){return s.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,s.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function p(e){const{values:n,children:l}=e;return(0,s.useMemo)(()=>{const e=n??function(e){return m(e).map(({props:{value:e,label:n,attributes:l,default:s}})=>({value:e,label:n,attributes:l,default:s}))}(l);return function(e){const n=(0,d.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,l])}function h({value:e,tabValues:n}){return n.some(n=>n.value===e)}function u({queryString:e=!1,groupId:n}){const l=(0,o.W6)(),t=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,i.aZ)(t),(0,s.useCallback)(e=>{if(!t)return;const n=new URLSearchParams(l.location.search);n.set(t,e),l.replace({...l.location,search:n.toString()})},[t,l])]}function x(e){const{defaultValue:n,queryString:l=!1,groupId:t}=e,a=p(e),[o,i]=(0,s.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!h({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const l=n.find(e=>e.default)??n[0];if(!l)throw new Error("Unexpected error: 0 tabValues");return l.value}({defaultValue:n,tabValues:a})),[d,m]=u({queryString:l,groupId:t}),[x,j]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[l,t]=(0,c.Dv)(n);return[l,(0,s.useCallback)(e=>{n&&t.set(e)},[n,t])]}({groupId:t}),g=(()=>{const e=d??x;return h({value:e,tabValues:a})?e:null})();(0,r.A)(()=>{g&&i(g)},[g]);return{selectedValue:o,selectValue:(0,s.useCallback)(e=>{if(!h({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);i(e),m(e),j(e)},[m,j,a]),tabValues:a}}var j=l(11062);const g="tabList__CuJ",v="tabItem_LNqP";var f=l(74848);function y({className:e,block:n,selectedValue:l,selectValue:s,tabValues:o}){const r=[],{blockElementScrollPositionUntilNextRender:i}=(0,a.a_)(),d=e=>{const n=e.currentTarget,t=r.indexOf(n),a=o[t].value;a!==l&&(i(n),s(a))},c=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const l=r.indexOf(e.currentTarget)+1;n=r[l]??r[0];break}case"ArrowLeft":{const l=r.indexOf(e.currentTarget)-1;n=r[l]??r[r.length-1];break}}n?.focus()};return(0,f.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,t.A)("tabs",{"tabs--block":n},e),children:o.map(({value:e,label:n,attributes:s})=>(0,f.jsx)("li",{role:"tab",tabIndex:l===e?0:-1,"aria-selected":l===e,ref:e=>{r.push(e)},onKeyDown:c,onClick:d,...s,className:(0,t.A)("tabs__item",v,s?.className,{"tabs__item--active":l===e}),children:n??e},e))})}function b({lazy:e,children:n,selectedValue:l}){const a=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=a.find(e=>e.props.value===l);return e?(0,s.cloneElement)(e,{className:(0,t.A)("margin-top--md",e.props.className)}):null}return(0,f.jsx)("div",{className:"margin-top--md",children:a.map((e,n)=>(0,s.cloneElement)(e,{key:n,hidden:e.props.value!==l}))})}function _(e){const n=x(e);return(0,f.jsxs)("div",{className:(0,t.A)("tabs-container",g),children:[(0,f.jsx)(y,{...n,...e}),(0,f.jsx)(b,{...n,...e})]})}function L(e){const n=(0,j.A)();return(0,f.jsx)(_,{...e,children:m(e.children)},String(n))}}}]);