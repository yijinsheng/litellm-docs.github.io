"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[20786],{28453(e,n,t){t.d(n,{R:()=>o,x:()=>r});var i=t(96540);const l={},c=i.createContext(l);function o(e){const n=i.useContext(c);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:o(e.components),i.createElement(c.Provider,{value:n},e.children)}},34340(e,n,t){t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>s});const i=JSON.parse('{"id":"caching/caching_api","title":"Hosted Cache - api.litellm.ai","description":"\u4f7f\u7528 api.litellm.ai \u5bf9 completion() \u548c embedding() \u54cd\u5e94\u8fdb\u884c\u7f13\u5b58\u3002","source":"@site/docs/caching/caching_api.md","sourceDirName":"caching","slug":"/caching/caching_api","permalink":"/docs/caching/caching_api","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{}}');var l=t(74848),c=t(28453);const o={},r="Hosted Cache - api.litellm.ai",a={},s=[{value:"\u5feb\u901f\u5f00\u59cb\u4f7f\u7528 - \u8865\u5168",id:"\u5feb\u901f\u5f00\u59cb\u4f7f\u7528---\u8865\u5168",level:2},{value:"\u4f7f\u7528 - \u5d4c\u5165",id:"\u4f7f\u7528---\u5d4c\u5165",level:2},{value:"\u6d41\u5f0f\u7f13\u5b58",id:"\u6d41\u5f0f\u7f13\u5b58",level:2},{value:"\u4f7f\u7528\u65b9\u6cd5",id:"\u4f7f\u7528\u65b9\u6cd5",level:3}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,c.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"hosted-cache---apilitellmai",children:"Hosted Cache - api.litellm.ai"})}),"\n",(0,l.jsxs)(n.p,{children:["\u4f7f\u7528 api.litellm.ai \u5bf9 ",(0,l.jsx)(n.code,{children:"completion()"})," \u548c ",(0,l.jsx)(n.code,{children:"embedding()"})," \u54cd\u5e94\u8fdb\u884c\u7f13\u5b58\u3002"]}),"\n",(0,l.jsx)(n.h2,{id:"\u5feb\u901f\u5f00\u59cb\u4f7f\u7528---\u8865\u5168",children:"\u5feb\u901f\u5f00\u59cb\u4f7f\u7528 - \u8865\u5168"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import litellm\nfrom litellm import completion\nfrom litellm.caching.caching import Cache\nlitellm.cache = Cache(type="hosted")  # \u521d\u59cb\u5316\u7f13\u5b58\u4ee5\u4f7f\u7528 api.litellm.ai\n\n# \u53d1\u9001\u8865\u5168\u8bf7\u6c42\nresponse1 = completion(\n    model="gpt-3.5-turbo", \n    messages=[{"role": "user", "content": "Tell me a joke."}],\n    caching=True\n)\n\nresponse2 = completion(\n    model="gpt-3.5-turbo", \n    messages=[{"role": "user", "content": "Tell me a joke."}],\n    caching=True\n)\n# response1 == response2\uff0cresponse1 \u5df2\u88ab\u7f13\u5b58\n'})}),"\n",(0,l.jsx)(n.h2,{id:"\u4f7f\u7528---\u5d4c\u5165",children:"\u4f7f\u7528 - \u5d4c\u5165"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import time\nimport litellm\nfrom litellm import completion, embedding\nfrom litellm.caching.caching import Cache\nlitellm.cache = Cache(type="hosted")\n\nstart_time = time.time()\nembedding1 = embedding(model="text-embedding-ada-002", input=["hello from litellm"*5], caching=True)\nend_time = time.time()\nprint(f"\u5d4c\u5165 1 \u54cd\u5e94\u65f6\u95f4: {end_time - start_time} \u79d2")\n\nstart_time = time.time()\nembedding2 = embedding(model="text-embedding-ada-002", input=["hello from litellm"*5], caching=True)\nend_time = time.time()\nprint(f"\u5d4c\u5165 2 \u54cd\u5e94\u65f6\u95f4: {end_time - start_time} \u79d2")\n'})}),"\n",(0,l.jsx)(n.h2,{id:"\u6d41\u5f0f\u7f13\u5b58",children:"\u6d41\u5f0f\u7f13\u5b58"}),"\n",(0,l.jsx)(n.p,{children:"LiteLLM \u53ef\u4ee5\u4e3a\u60a8\u7f13\u5b58\u6d41\u5f0f\u54cd\u5e94\u3002"}),"\n",(0,l.jsx)(n.h3,{id:"\u4f7f\u7528\u65b9\u6cd5",children:"\u4f7f\u7528\u65b9\u6cd5"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import litellm\nimport time\nfrom litellm import completion\nfrom litellm.caching.caching import Cache\n\nlitellm.cache = Cache(type="hosted")\n\n# \u53d1\u9001\u8865\u5168\u8bf7\u6c42\nresponse1 = completion(\n    model="gpt-3.5-turbo", \n    messages=[{"role": "user", "content": "Tell me a joke."}], \n    stream=True,\n    caching=True)\nfor chunk in response1:\n    print(chunk)\n\ntime.sleep(1)  # \u7f13\u5b58\u5f02\u6b65\u66f4\u65b0\n\nresponse2 = completion(\n    model="gpt-3.5-turbo", \n    messages=[{"role": "user", "content": "Tell me a joke."}], \n    stream=True,\n    caching=True)\nfor chunk in response2:\n    print(chunk)\n'})})]})}function d(e={}){const{wrapper:n}={...(0,c.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(m,{...e})}):m(e)}}}]);