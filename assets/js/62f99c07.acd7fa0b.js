"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[6299],{4329(e,n,t){t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>m,frontMatter:()=>l,metadata:()=>s,toc:()=>p});const s=JSON.parse('{"id":"proxy/user_keys","title":"Langchain, OpenAI SDK, LlamaIndex, Instructor, Curl examples","description":"LiteLLM Proxy is OpenAI-Compatible, and supports:","source":"@site/docs/proxy/user_keys.md","sourceDirName":"proxy","slug":"/proxy/user_keys","permalink":"/docs/proxy/user_keys","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Email Notifications","permalink":"/docs/proxy/email"},"next":{"title":"\u5ba2\u6237\u7aef LLM \u8eab\u4efd\u9a8c\u8bc1","permalink":"/docs/proxy/clientside_auth"}}');var a=t(74848),o=t(28453),i=t(49489),r=t(7227);const l={},c="Langchain, OpenAI SDK, LlamaIndex, Instructor, Curl examples",d={},p=[{value:"<code>/chat/completions</code>",id:"chatcompletions",level:2},{value:"Request Format",id:"request-format",level:3},{value:"Using Tags for Categorization and Tracking",id:"using-tags-for-categorization-and-tracking",level:2},{value:"Tag Benefits",id:"tag-benefits",level:3},{value:"Response Format",id:"response-format",level:3},{value:"<strong>Streaming</strong>",id:"streaming",level:3},{value:"Function Calling",id:"function-calling",level:3},{value:"<code>/embeddings</code>",id:"embeddings",level:2},{value:"Request Format",id:"request-format-1",level:3},{value:"Response Format",id:"response-format-1",level:3},{value:"<code>/moderations</code>",id:"moderations",level:2},{value:"Request Format",id:"request-format-2",level:3},{value:"Response Format",id:"response-format-2",level:3},{value:"Using with OpenAI compatible projects",id:"using-with-openai-compatible-projects",level:2},{value:"Start the LiteLLM proxy",id:"start-the-litellm-proxy",level:4},{value:"1. Clone the repo",id:"1-clone-the-repo",level:4},{value:"2. Modify Librechat&#39;s <code>docker-compose.yml</code>",id:"2-modify-librechats-docker-composeyml",level:4},{value:"3. Save fake OpenAI key in Librechat&#39;s <code>.env</code>",id:"3-save-fake-openai-key-in-librechats-env",level:4},{value:"4. Run LibreChat:",id:"4-run-librechat",level:4},{value:"Using with Vertex, Boto3, Anthropic SDK (Native format)",id:"using-with-vertex-boto3-anthropic-sdk-native-format",level:2},{value:"Advanced",id:"advanced",level:2},{value:"(BETA) Batch Completions - pass multiple models",id:"beta-batch-completions---pass-multiple-models",level:3},{value:"Expected Request Format",id:"expected-request-format",level:4},{value:"Expected Response Format",id:"expected-response-format",level:4},{value:"Expected Response Format",id:"expected-response-format-1",level:4}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"langchain-openai-sdk-llamaindex-instructor-curl-examples",children:"Langchain, OpenAI SDK, LlamaIndex, Instructor, Curl examples"})}),"\n",(0,a.jsxs)(n.p,{children:["LiteLLM Proxy is ",(0,a.jsx)(n.strong,{children:"OpenAI-Compatible"}),", and supports:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"/chat/completions"}),"\n",(0,a.jsx)(n.li,{children:"/embeddings"}),"\n",(0,a.jsx)(n.li,{children:"/completions"}),"\n",(0,a.jsx)(n.li,{children:"/image/generations"}),"\n",(0,a.jsx)(n.li,{children:"/moderations"}),"\n",(0,a.jsx)(n.li,{children:"/audio/transcriptions"}),"\n",(0,a.jsx)(n.li,{children:"/audio/speech"}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://docs.litellm.ai/docs/assistants",children:"Assistants API endpoints"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://docs.litellm.ai/docs/batches",children:"Batches API endpoints"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://docs.litellm.ai/docs/fine_tuning",children:"Fine-Tuning API endpoints"})}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["LiteLLM Proxy is ",(0,a.jsx)(n.strong,{children:"Azure OpenAI-compatible"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"/chat/completions"}),"\n",(0,a.jsx)(n.li,{children:"/completions"}),"\n",(0,a.jsx)(n.li,{children:"/embeddings"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["LiteLLM Proxy is ",(0,a.jsx)(n.strong,{children:"Anthropic-compatible"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"/messages"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["LiteLLM Proxy is ",(0,a.jsx)(n.strong,{children:"Vertex AI compatible"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"../vertex_ai",children:"Supports ALL Vertex Endpoints"})}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"This doc covers:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"/chat/completion"}),"\n",(0,a.jsx)(n.li,{children:"/embedding"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["These are ",(0,a.jsx)(n.strong,{children:"selected examples"}),". LiteLLM Proxy is ",(0,a.jsx)(n.strong,{children:"OpenAI-Compatible"}),", it works with any project that calls OpenAI. Just change the ",(0,a.jsx)(n.code,{children:"base_url"}),", ",(0,a.jsx)(n.code,{children:"api_key"})," and ",(0,a.jsx)(n.code,{children:"model"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["To pass provider-specific args, ",(0,a.jsx)(n.a,{href:"https://docs.litellm.ai/docs/completion/provider_specific_params#proxy-usage",children:"go here"})]}),"\n",(0,a.jsxs)(n.p,{children:["To drop unsupported params (E.g. frequency_penalty for bedrock with librechat), ",(0,a.jsx)(n.a,{href:"https://docs.litellm.ai/docs/completion/drop_params#openai-proxy-usage",children:"go here"})]}),"\n",(0,a.jsx)(n.admonition,{type:"info",children:(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Input, Output, Exceptions are mapped to the OpenAI format for all supported models"})})}),"\n",(0,a.jsx)(n.p,{children:"How to send requests to the proxy, pass metadata, allow users to pass in their OpenAI API key"}),"\n",(0,a.jsx)(n.h2,{id:"chatcompletions",children:(0,a.jsx)(n.code,{children:"/chat/completions"})}),"\n",(0,a.jsx)(n.h3,{id:"request-format",children:"Request Format"}),"\n",(0,a.jsxs)(i.A,{children:[(0,a.jsxs)(r.A,{value:"openai",label:"OpenAI Python v1.0.0+",children:[(0,a.jsxs)(n.p,{children:["Set ",(0,a.jsx)(n.code,{children:'extra_body={"metadata": { }}'})," to ",(0,a.jsx)(n.code,{children:"metadata"})," you want to pass"]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import openai\nclient = openai.OpenAI(\n    api_key="anything",\n    base_url="http://0.0.0.0:4000"\n)\n\n# request sent to model set on litellm proxy, `litellm --model`\nresponse = client.chat.completions.create(\n    model="gpt-3.5-turbo",\n    messages = [\n        {\n            "role": "user",\n            "content": "this is a test request, write a short poem"\n        }\n    ],\n    extra_body={ # pass in any provider-specific param, if not supported by openai, https://docs.litellm.ai/docs/completion/input#provider-specific-params\n        "metadata": { # \ud83d\udc48 use for logging additional params (e.g. to langfuse)\n            "generation_name": "ishaan-generation-openai-client",\n            "generation_id": "openai-client-gen-id22",\n            "trace_id": "openai-client-trace-id22",\n            "trace_user_id": "openai-client-user-id2"\n        }\n    }\n)\n\nprint(response)\n'})})]}),(0,a.jsx)(r.A,{value:"litellm_sdk",label:"LiteLLM Python SDK",children:(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"../providers/litellm_proxy#send-all-sdk-requests-to-litellm-proxy",children:(0,a.jsx)(n.strong,{children:"\ud83d\udc49 Go Here"})})})}),(0,a.jsxs)(r.A,{value:"azureopenai",label:"AzureOpenAI Python",children:[(0,a.jsxs)(n.p,{children:["Set ",(0,a.jsx)(n.code,{children:'extra_body={"metadata": { }}'})," to ",(0,a.jsx)(n.code,{children:"metadata"})," you want to pass"]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import openai\nclient = openai.AzureOpenAI(\n    api_key="anything",\n    base_url="http://0.0.0.0:4000"\n)\n\n# request sent to model set on litellm proxy, `litellm --model`\nresponse = client.chat.completions.create(\n    model="gpt-3.5-turbo",\n    messages = [\n        {\n            "role": "user",\n            "content": "this is a test request, write a short poem"\n        }\n    ],\n    extra_body={ # pass in any provider-specific param, if not supported by openai, https://docs.litellm.ai/docs/completion/input#provider-specific-params\n        "metadata": { # \ud83d\udc48 use for logging additional params (e.g. to langfuse)\n            "generation_name": "ishaan-generation-openai-client",\n            "generation_id": "openai-client-gen-id22",\n            "trace_id": "openai-client-trace-id22",\n            "trace_user_id": "openai-client-user-id2"\n        }\n    }\n)\n\nprint(response)\n'})})]}),(0,a.jsx)(r.A,{value:"LlamaIndex",label:"LlamaIndex",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import os, dotenv\n\nfrom llama_index.llms import AzureOpenAI\nfrom llama_index.embeddings import AzureOpenAIEmbedding\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n\nllm = AzureOpenAI(\n    engine="azure-gpt-3.5",               # model_name on litellm proxy\n    temperature=0.0,\n    azure_endpoint="http://0.0.0.0:4000", # litellm proxy endpoint\n    api_key="sk-1234",                    # litellm proxy API Key\n    api_version="2023-07-01-preview",\n)\n\nembed_model = AzureOpenAIEmbedding(\n    deployment_name="azure-embedding-model",\n    azure_endpoint="http://0.0.0.0:4000",\n    api_key="sk-1234",\n    api_version="2023-07-01-preview",\n)\n\n\ndocuments = SimpleDirectoryReader("llama_index_data").load_data()\nservice_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query("What did the author do growing up?")\nprint(response)\n\n'})})}),(0,a.jsxs)(r.A,{value:"Curl",label:"Curl Request",children:[(0,a.jsxs)(n.p,{children:["Pass ",(0,a.jsx)(n.code,{children:"metadata"})," as part of the request body"]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:'curl --location \'http://0.0.0.0:4000/chat/completions\' \\\n    --header \'Content-Type: application/json\' \\\n    --data \'{\n    "model": "gpt-3.5-turbo",\n    "messages": [\n        {\n        "role": "user",\n        "content": "what llm are you"\n        }\n    ],\n    "metadata": {\n        "generation_name": "ishaan-test-generation",\n        "generation_id": "gen-id22",\n        "trace_id": "trace-id22",\n        "trace_user_id": "user-id2"\n    }\n}\'\n'})})]}),(0,a.jsx)(r.A,{value:"langchain",label:"Langchain",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate,\n)\nfrom langchain.schema import HumanMessage, SystemMessage\nimport os \n\nos.environ["OPENAI_API_KEY"] = "anything"\n\nchat = ChatOpenAI(\n    openai_api_base="http://0.0.0.0:4000",\n    model = "gpt-3.5-turbo",\n    temperature=0.1,\n    extra_body={\n        "metadata": {\n            "generation_name": "ishaan-generation-langchain-client",\n            "generation_id": "langchain-client-gen-id22",\n            "trace_id": "langchain-client-trace-id22",\n            "trace_user_id": "langchain-client-user-id2"\n        }\n    }\n)\n\nmessages = [\n    SystemMessage(\n        content="You are a helpful assistant that im using to make a test request to."\n    ),\n    HumanMessage(\n        content="test from litellm. tell me why it\'s amazing in 1 sentence"\n    ),\n]\nresponse = chat(messages)\n\nprint(response)\n'})})}),(0,a.jsx)(r.A,{value:"langchain js",label:"Langchain JS",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-js",children:'import { ChatOpenAI } from "@langchain/openai";\n\n\nconst model = new ChatOpenAI({\n  modelName: "gpt-4",\n  openAIApiKey: "sk-1234",\n  modelKwargs: {"metadata": "hello world"} // \ud83d\udc48 PASS Additional params here\n}, {\n  basePath: "http://0.0.0.0:4000",\n});\n\nconst message = await model.invoke("Hi there!");\n\nconsole.log(message);\n\n'})})}),(0,a.jsx)(r.A,{value:"openai JS",label:"OpenAI JS",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-js",children:'const { OpenAI } = require(\'openai\');\n\nconst openai = new OpenAI({\n  apiKey: "sk-1234", // This is the default and can be omitted\n  baseURL: "http://0.0.0.0:4000"\n});\n\nasync function main() {\n  const chatCompletion = await openai.chat.completions.create({\n    messages: [{ role: \'user\', content: \'Say this is a test\' }],\n    model: \'gpt-3.5-turbo\',\n  }, {"metadata": {\n            "generation_name": "ishaan-generation-openaijs-client",\n            "generation_id": "openaijs-client-gen-id22",\n            "trace_id": "openaijs-client-trace-id22",\n            "trace_user_id": "openaijs-client-user-id2"\n        }});\n}\n\nmain();\n\n'})})}),(0,a.jsx)(r.A,{value:"anthropic-py",label:"Anthropic Python SDK",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import os\n\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    base_url="http://localhost:4000", # proxy endpoint\n    api_key="sk-test-proxy-key-123", # litellm proxy virtual key (example)\n)\n\nmessage = client.messages.create(\n    max_tokens=1024,\n    messages=[\n        {\n            "role": "user",\n            "content": "Hello, Claude",\n        }\n    ],\n    model="claude-3-opus-20240229",\n)\nprint(message.content)\n'})})}),(0,a.jsx)(r.A,{value:"mistral-py",label:"Mistral Python SDK",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import os\nfrom mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ChatMessage\n\n\nclient = MistralClient(api_key="sk-1234", endpoint="http://0.0.0.0:4000")\nchat_response = client.chat(\n    model="mistral-small-latest",\n    messages=[\n        {"role": "user", "content": "this is a test request, write a short poem"}\n    ],\n)\nprint(chat_response.choices[0].message.content)\n'})})}),(0,a.jsx)(r.A,{value:"instructor",label:"Instructor",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\nimport instructor\nfrom pydantic import BaseModel\n\nmy_proxy_api_key = "" # e.g. sk-1234 - LITELLM KEY\nmy_proxy_base_url = "" # e.g. http://0.0.0.0:4000 - LITELLM PROXY BASE URL\n\n# This enables response_model keyword\n# from client.chat.completions.create\n## WORKS ACROSS OPENAI/ANTHROPIC/VERTEXAI/ETC. - all LITELLM SUPPORTED MODELS!\nclient = instructor.from_openai(OpenAI(api_key=my_proxy_api_key, base_url=my_proxy_base_url))\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\nuser = client.chat.completions.create(\n    model="gemini-pro-flash",\n    response_model=UserDetail,\n    messages=[\n        {"role": "user", "content": "Extract Jason is 25 years old"},\n    ]\n)\n\nassert isinstance(user, UserDetail)\nassert user.name == "Jason"\nassert user.age == 25\n'})})})]}),"\n",(0,a.jsx)(n.h2,{id:"using-tags-for-categorization-and-tracking",children:"Using Tags for Categorization and Tracking"}),"\n",(0,a.jsx)(n.p,{children:"Tags allow you to categorize, filter, and track your LLM requests. Add tags to your metadata for better organization and analytics."}),"\n",(0,a.jsxs)(i.A,{children:[(0,a.jsx)(r.A,{value:"openai-python",label:"OpenAI Python",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import openai\nclient = openai.OpenAI(\n    api_key="anything",\n    base_url="http://0.0.0.0:4000"\n)\n\nresponse = client.chat.completions.create(\n    model="gpt-3.5-turbo",\n    messages=[{"role": "user", "content": "Hello!"}],\n    extra_body={\n        "metadata": {\n            "tags": ["production", "customer-support", "urgent"],\n            "generation_name": "support-bot",\n            "trace_user_id": "user-123"\n        }\n    }\n)\n'})})}),(0,a.jsx)(r.A,{value:"langchain-python",label:"LangChain Python",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage\n\nchat = ChatOpenAI(\n    openai_api_base="http://0.0.0.0:4000",\n    model="gpt-4o",\n    extra_body={\n        "metadata": {\n            "tags": ["langchain-integration", "content-gen"],\n            "trace_user_id": "user-456"\n        }\n    }\n)\n\nresponse = chat.invoke([HumanMessage(content="Generate a blog post")])\n'})})}),(0,a.jsx)(r.A,{value:"curl",label:"Curl",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'curl --location \'http://0.0.0.0:4000/chat/completions\' \\\n    --header \'Content-Type: application/json\' \\\n    --data \'{\n    "model": "gpt-3.5-turbo",\n    "messages": [{"role": "user", "content": "Hello!"}],\n    "metadata": {\n        "tags": ["api-test", "development"],\n        "trace_user_id": "test-user"\n    }\n}\'\n'})})}),(0,a.jsx)(r.A,{value:"openai-js",label:"OpenAI JS",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-js",children:'const { OpenAI } = require(\'openai\');\n\nconst openai = new OpenAI({\n  apiKey: "sk-1234",\n  baseURL: "http://0.0.0.0:4000"\n});\n\nasync function main() {\n  const response = await openai.chat.completions.create({\n    messages: [{ role: \'user\', content: \'Hello!\' }],\n    model: \'gpt-3.5-turbo\',\n    metadata: {\n      tags: ["javascript-client", "api-test"],\n      trace_user_id: "js-user-789"\n    }\n  });\n}\n'})})})]}),"\n",(0,a.jsx)(n.h3,{id:"tag-benefits",children:"Tag Benefits"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cost Tracking"}),": Monitor spending by project/team/feature"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Analytics"}),": Filter requests by tags in logs and dashboards"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Routing"}),": Use tags for conditional model routing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Debugging"}),": Easier troubleshooting with categorized requests"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"response-format",children:"Response Format"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "id": "chatcmpl-8c5qbGTILZa1S4CK3b31yj5N40hFN",\n  "choices": [\n    {\n      "finish_reason": "stop",\n      "index": 0,\n      "message": {\n        "content": "As an AI language model, I do not have a physical form or personal preferences. However, I am programmed to assist with various topics and provide information on a wide range of subjects. Is there something specific you would like assistance with?",\n        "role": "assistant"\n      }\n    }\n  ],\n  "created": 1704089632,\n  "model": "gpt-35-turbo",\n  "object": "chat.completion",\n  "system_fingerprint": null,\n  "usage": {\n    "completion_tokens": 47,\n    "prompt_tokens": 12,\n    "total_tokens": 59\n  },\n  "_response_ms": 1753.426\n}\n\n'})}),"\n",(0,a.jsx)(n.h3,{id:"streaming",children:(0,a.jsx)(n.strong,{children:"Streaming"})}),"\n",(0,a.jsxs)(i.A,{children:[(0,a.jsx)(r.A,{value:"curl",label:"curl",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'curl http://0.0.0.0:4000/v1/chat/completions \\\n-H "Content-Type: application/json" \\\n-H "Authorization: Bearer $OPTIONAL_YOUR_PROXY_KEY" \\\n-d \'{\n  "model": "gpt-4-turbo",\n  "messages": [\n    {\n      "role": "user",\n      "content": "this is a test request, write a short poem"\n    }\n  ],\n  "stream": true\n}\'\n'})})}),(0,a.jsx)(r.A,{value:"sdk",label:"SDK",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\nclient = OpenAI(\n    api_key="sk-1234", # [OPTIONAL] set if you set one on proxy, else set ""\n    base_url="http://0.0.0.0:4000",\n)\n\nmessages = [{"role": "user", "content": "this is a test request, write a short poem"}]\ncompletion = client.chat.completions.create(\n  model="gpt-4o",\n  messages=messages,\n  stream=True\n)\n\nprint(completion)\n\n'})})})]}),"\n",(0,a.jsx)(n.h3,{id:"function-calling",children:"Function Calling"}),"\n",(0,a.jsx)(n.p,{children:"Here's some examples of doing function calling with the proxy."}),"\n",(0,a.jsxs)(n.p,{children:["You can use the proxy for function calling with ",(0,a.jsx)(n.strong,{children:"any"})," openai-compatible project."]}),"\n",(0,a.jsxs)(i.A,{children:[(0,a.jsx)(r.A,{value:"curl",label:"curl",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'curl http://0.0.0.0:4000/v1/chat/completions \\\n-H "Content-Type: application/json" \\\n-H "Authorization: Bearer $OPTIONAL_YOUR_PROXY_KEY" \\\n-d \'{\n  "model": "gpt-4-turbo",\n  "messages": [\n    {\n      "role": "user",\n      "content": "What\'\\\'\'s the weather like in Boston today?"\n    }\n  ],\n  "tools": [\n    {\n      "type": "function",\n      "function": {\n        "name": "get_current_weather",\n        "description": "Get the current weather in a given location",\n        "parameters": {\n          "type": "object",\n          "properties": {\n            "location": {\n              "type": "string",\n              "description": "The city and state, e.g. San Francisco, CA"\n            },\n            "unit": {\n              "type": "string",\n              "enum": ["celsius", "fahrenheit"]\n            }\n          },\n          "required": ["location"]\n        }\n      }\n    }\n  ],\n  "tool_choice": "auto"\n}\'\n'})})}),(0,a.jsx)(r.A,{value:"sdk",label:"SDK",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\nclient = OpenAI(\n    api_key="sk-1234", # [OPTIONAL] set if you set one on proxy, else set ""\n    base_url="http://0.0.0.0:4000",\n)\n\ntools = [\n  {\n    "type": "function",\n    "function": {\n      "name": "get_current_weather",\n      "description": "Get the current weather in a given location",\n      "parameters": {\n        "type": "object",\n        "properties": {\n          "location": {\n            "type": "string",\n            "description": "The city and state, e.g. San Francisco, CA",\n          },\n          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},\n        },\n        "required": ["location"],\n      },\n    }\n  }\n]\nmessages = [{"role": "user", "content": "What\'s the weather like in Boston today?"}]\ncompletion = client.chat.completions.create(\n  model="gpt-4o", # use \'model_name\' from config.yaml\n  messages=messages,\n  tools=tools,\n  tool_choice="auto"\n)\n\nprint(completion)\n\n'})})})]}),"\n",(0,a.jsx)(n.h2,{id:"embeddings",children:(0,a.jsx)(n.code,{children:"/embeddings"})}),"\n",(0,a.jsx)(n.h3,{id:"request-format-1",children:"Request Format"}),"\n",(0,a.jsx)(n.p,{children:"Input, Output and Exceptions are mapped to the OpenAI format for all supported models"}),"\n",(0,a.jsxs)(i.A,{children:[(0,a.jsx)(r.A,{value:"openai",label:"OpenAI Python v1.0.0+",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import openai\nfrom openai import OpenAI\n\n# set base_url to your proxy server\n# set api_key to send to proxy server\nclient = OpenAI(api_key="<proxy-api-key>", base_url="http://0.0.0.0:4000")\n\nresponse = client.embeddings.create(\n    input=["hello from litellm"],\n    model="text-embedding-ada-002"\n)\n\nprint(response)\n\n'})})}),(0,a.jsx)(r.A,{value:"Curl",label:"Curl Request",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:'curl --location \'http://0.0.0.0:4000/embeddings\' \\\n  --header \'Content-Type: application/json\' \\\n  --data \' {\n  "model": "text-embedding-ada-002",\n  "input": ["write a litellm poem"]\n  }\'\n'})})}),(0,a.jsx)(r.A,{value:"langchain-embedding",label:"Langchain Embeddings",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.embeddings import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model="sagemaker-embeddings", openai_api_base="http://0.0.0.0:4000", openai_api_key="temp-key")\n\n\ntext = "This is a test document."\n\nquery_result = embeddings.embed_query(text)\n\nprint(f"SAGEMAKER EMBEDDINGS")\nprint(query_result[:5])\n\nembeddings = OpenAIEmbeddings(model="bedrock-embeddings", openai_api_base="http://0.0.0.0:4000", openai_api_key="temp-key")\n\ntext = "This is a test document."\n\nquery_result = embeddings.embed_query(text)\n\nprint(f"BEDROCK EMBEDDINGS")\nprint(query_result[:5])\n\nembeddings = OpenAIEmbeddings(model="bedrock-titan-embeddings", openai_api_base="http://0.0.0.0:4000", openai_api_key="temp-key")\n\ntext = "This is a test document."\n\nquery_result = embeddings.embed_query(text)\n\nprint(f"TITAN EMBEDDINGS")\nprint(query_result[:5])\n'})})})]}),"\n",(0,a.jsx)(n.h3,{id:"response-format-1",children:"Response Format"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "object": "list",\n  "data": [\n    {\n      "object": "embedding",\n      "embedding": [\n        0.0023064255,\n        -0.009327292,\n        .... \n        -0.0028842222,\n      ],\n      "index": 0\n    }\n  ],\n  "model": "text-embedding-ada-002",\n  "usage": {\n    "prompt_tokens": 8,\n    "total_tokens": 8\n  }\n}\n\n'})}),"\n",(0,a.jsx)(n.h2,{id:"moderations",children:(0,a.jsx)(n.code,{children:"/moderations"})}),"\n",(0,a.jsx)(n.h3,{id:"request-format-2",children:"Request Format"}),"\n",(0,a.jsx)(n.p,{children:"Input, Output and Exceptions are mapped to the OpenAI format for all supported models"}),"\n",(0,a.jsxs)(i.A,{children:[(0,a.jsx)(r.A,{value:"openai",label:"OpenAI Python v1.0.0+",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import openai\nfrom openai import OpenAI\n\n# set base_url to your proxy server\n# set api_key to send to proxy server\nclient = OpenAI(api_key="<proxy-api-key>", base_url="http://0.0.0.0:4000")\n\nresponse = client.moderations.create(\n    input="hello from litellm",\n    model="text-moderation-stable"\n)\n\nprint(response)\n\n'})})}),(0,a.jsx)(r.A,{value:"Curl",label:"Curl Request",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"curl --location 'http://0.0.0.0:4000/moderations' \\\n    --header 'Content-Type: application/json' \\\n    --header 'Authorization: Bearer sk-1234' \\\n    --data '{\"input\": \"Sample text goes here\", \"model\": \"text-moderation-stable\"}'\n"})})})]}),"\n",(0,a.jsx)(n.h3,{id:"response-format-2",children:"Response Format"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "id": "modr-8sFEN22QCziALOfWTa77TodNLgHwA",\n  "model": "text-moderation-007",\n  "results": [\n    {\n      "categories": {\n        "harassment": false,\n        "harassment/threatening": false,\n        "hate": false,\n        "hate/threatening": false,\n        "self-harm": false,\n        "self-harm/instructions": false,\n        "self-harm/intent": false,\n        "sexual": false,\n        "sexual/minors": false,\n        "violence": false,\n        "violence/graphic": false\n      },\n      "category_scores": {\n        "harassment": 0.000019947197870351374,\n        "harassment/threatening": 5.5971017900446896e-6,\n        "hate": 0.000028560316422954202,\n        "hate/threatening": 2.2631787999216613e-8,\n        "self-harm": 2.9121162015144364e-7,\n        "self-harm/instructions": 9.314219084899378e-8,\n        "self-harm/intent": 8.093739012338119e-8,\n        "sexual": 0.00004414955765241757,\n        "sexual/minors": 0.0000156943697220413,\n        "violence": 0.00022354527027346194,\n        "violence/graphic": 8.804164281173144e-6\n      },\n      "flagged": false\n    }\n  ]\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"using-with-openai-compatible-projects",children:"Using with OpenAI compatible projects"}),"\n",(0,a.jsxs)(n.p,{children:["Set ",(0,a.jsx)(n.code,{children:"base_url"})," to the LiteLLM Proxy server"]}),"\n",(0,a.jsxs)(i.A,{children:[(0,a.jsx)(r.A,{value:"openai",label:"OpenAI v1.0.0+",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import openai\nclient = openai.OpenAI(\n    api_key="anything",\n    base_url="http://0.0.0.0:4000"\n)\n\n# request sent to model set on litellm proxy, `litellm --model`\nresponse = client.chat.completions.create(model="gpt-3.5-turbo", messages = [\n    {\n        "role": "user",\n        "content": "this is a test request, write a short poem"\n    }\n])\n\nprint(response)\n\n'})})}),(0,a.jsxs)(r.A,{value:"librechat",label:"LibreChat",children:[(0,a.jsx)(n.h4,{id:"start-the-litellm-proxy",children:"Start the LiteLLM proxy"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"litellm --model gpt-3.5-turbo\n\n#INFO: Proxy running on http://0.0.0.0:4000\n"})}),(0,a.jsx)(n.h4,{id:"1-clone-the-repo",children:"1. Clone the repo"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"git clone https://github.com/danny-avila/LibreChat.git\n"})}),(0,a.jsxs)(n.h4,{id:"2-modify-librechats-docker-composeyml",children:["2. Modify Librechat's ",(0,a.jsx)(n.code,{children:"docker-compose.yml"})]}),(0,a.jsxs)(n.p,{children:["LiteLLM Proxy is running on port ",(0,a.jsx)(n.code,{children:"4000"}),", set ",(0,a.jsx)(n.code,{children:"4000"})," as the proxy below"]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"OPENAI_REVERSE_PROXY=http://host.docker.internal:4000/v1/chat/completions\n"})}),(0,a.jsxs)(n.h4,{id:"3-save-fake-openai-key-in-librechats-env",children:["3. Save fake OpenAI key in Librechat's ",(0,a.jsx)(n.code,{children:".env"})]}),(0,a.jsxs)(n.p,{children:["Copy Librechat's ",(0,a.jsx)(n.code,{children:".env.example"})," to ",(0,a.jsx)(n.code,{children:".env"})," and overwrite the default OPENAI_API_KEY (by default it requires the user to pass a key)."]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-env",children:"OPENAI_API_KEY=sk-1234\n"})}),(0,a.jsx)(n.h4,{id:"4-run-librechat",children:"4. Run LibreChat:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"docker compose up\n"})})]}),(0,a.jsxs)(r.A,{value:"continue-dev",label:"ContinueDev",children:[(0,a.jsxs)(n.p,{children:["Continue-Dev brings ChatGPT to VSCode. See how to ",(0,a.jsx)(n.a,{href:"https://continue.dev/docs/quickstart",children:"install it here"}),"."]}),(0,a.jsxs)(n.p,{children:["In the ",(0,a.jsx)(n.a,{href:"https://continue.dev/docs/reference/Models/openai",children:"config.py"})," set this as your default model."]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'  default=OpenAI(\n      api_key="IGNORED",\n      model="fake-model-name",\n      context_length=2048, # customize if needed for your model\n      api_base="http://localhost:4000" # your proxy server url\n  ),\n'})}),(0,a.jsxs)(n.p,{children:["Credits ",(0,a.jsx)(n.a,{href:"https://github.com/ollama/ollama/issues/305#issuecomment-1751848077",children:"@vividfog"})," for this tutorial."]})]}),(0,a.jsx)(r.A,{value:"aider",label:"Aider",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"$ pip install aider \n\n$ aider --openai-api-base http://0.0.0.0:4000 --openai-api-key fake-key\n"})})}),(0,a.jsxs)(r.A,{value:"autogen",label:"AutoGen",children:[(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"pip install pyautogen\n"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from autogen import AssistantAgent, UserProxyAgent, oai\nconfig_list=[\n    {\n        "model": "my-fake-model",\n        "api_base": "http://localhost:4000",  #litellm compatible endpoint\n        "api_type": "open_ai",\n        "api_key": "NULL", # just a placeholder\n    }\n]\n\nresponse = oai.Completion.create(config_list=config_list, prompt="Hi")\nprint(response) # works fine\n\nllm_config={\n    "config_list": config_list,\n}\n\nassistant = AssistantAgent("assistant", llm_config=llm_config)\nuser_proxy = UserProxyAgent("user_proxy")\nuser_proxy.initiate_chat(assistant, message="Plot a chart of META and TESLA stock price change YTD.", config_list=config_list)\n'})}),(0,a.jsxs)(n.p,{children:["Credits ",(0,a.jsx)(n.a,{href:"https://github.com/microsoft/autogen/issues/45#issuecomment-1749921972",children:"@victordibia"})," for this tutorial."]})]}),(0,a.jsxs)(r.A,{value:"guidance",label:"guidance",children:[(0,a.jsxs)(n.p,{children:["A guidance language for controlling large language models.\n",(0,a.jsx)(n.a,{href:"https://github.com/guidance-ai/guidance",children:"https://github.com/guidance-ai/guidance"})]}),(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"NOTE:"})," Guidance sends additional params like ",(0,a.jsx)(n.code,{children:"stop_sequences"})," which can cause some models to fail if they don't support it."]}),(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Fix"}),": Start your proxy using the ",(0,a.jsx)(n.code,{children:"--drop_params"})," flag"]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"litellm --model ollama/codellama --temperature 0.3 --max_tokens 2048 --drop_params\n"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import guidance\n\n# set api_base to your proxy\n# set api_key to anything\ngpt4 = guidance.llms.OpenAI(\"gpt-4\", api_base=\"http://0.0.0.0:4000\", api_key=\"anything\")\n\nexperts = guidance('''\n{{#system~}}\nYou are a helpful and terse assistant.\n{{~/system}}\n\n{{#user~}}\nI want a response to the following question:\n{{query}}\nName 3 world-class experts (past or present) who would be great at answering this?\nDon't answer the question yet.\n{{~/user}}\n\n{{#assistant~}}\n{{gen 'expert_names' temperature=0 max_tokens=300}}\n{{~/assistant}}\n''', llm=gpt4)\n\nresult = experts(query='How can I be more productive?')\nprint(result)\n"})})]})]}),"\n",(0,a.jsx)(n.h2,{id:"using-with-vertex-boto3-anthropic-sdk-native-format",children:"Using with Vertex, Boto3, Anthropic SDK (Native format)"}),"\n",(0,a.jsxs)(n.p,{children:["\ud83d\udc49 ",(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.a,{href:"/docs/pass_through/vertex_ai",children:"Here's how to use litellm proxy with Vertex, boto3, Anthropic SDK - in the native format"})})]}),"\n",(0,a.jsx)(n.h2,{id:"advanced",children:"Advanced"}),"\n",(0,a.jsx)(n.h3,{id:"beta-batch-completions---pass-multiple-models",children:"(BETA) Batch Completions - pass multiple models"}),"\n",(0,a.jsx)(n.p,{children:"Use this when you want to send 1 request to N Models"}),"\n",(0,a.jsx)(n.h4,{id:"expected-request-format",children:"Expected Request Format"}),"\n",(0,a.jsxs)(n.p,{children:["Pass model as a string of comma separated value of models. Example ",(0,a.jsx)(n.code,{children:'"model"="llama3,gpt-3.5-turbo"'})]}),"\n",(0,a.jsxs)(n.p,{children:["This same request will be sent to the following model groups on the ",(0,a.jsx)(n.a,{href:"https://docs.litellm.ai/docs/proxy/configs",children:"litellm proxy config.yaml"})]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.code,{children:'model_name="llama3"'})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.code,{children:'model_name="gpt-3.5-turbo"'})}),"\n"]}),"\n",(0,a.jsxs)(i.A,{children:[(0,a.jsxs)(r.A,{value:"openai-py",label:"OpenAI Python SDK",children:[(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import openai\n\nclient = openai.OpenAI(api_key="sk-1234", base_url="http://0.0.0.0:4000")\n\nresponse = client.chat.completions.create(\n    model="gpt-3.5-turbo,llama3",\n    messages=[\n        {"role": "user", "content": "this is a test request, write a short poem"}\n    ],\n)\n\nprint(response)\n'})}),(0,a.jsx)(n.h4,{id:"expected-response-format",children:"Expected Response Format"}),(0,a.jsxs)(n.p,{children:["Get a list of responses when ",(0,a.jsx)(n.code,{children:"model"})," is passed as a list"]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"[\n    ChatCompletion(\n        id='chatcmpl-9NoYhS2G0fswot0b6QpoQgmRQMaIf',\n        choices=[\n            Choice(\n                finish_reason='stop',\n                index=0,\n                logprobs=None,\n                message=ChatCompletionMessage(\n                    content='In the depths of my soul, a spark ignites\\nA light that shines so pure and bright\\nIt dances and leaps, refusing to die\\nA flame of hope that reaches the sky\\n\\nIt warms my heart and fills me with bliss\\nA reminder that in darkness, there is light to kiss\\nSo I hold onto this fire, this guiding light\\nAnd let it lead me through the darkest night.',\n                    role='assistant',\n                    function_call=None,\n                    tool_calls=None\n                )\n            )\n        ],\n        created=1715462919,\n        model='gpt-3.5-turbo-0125',\n        object='chat.completion',\n        system_fingerprint=None,\n        usage=CompletionUsage(\n            completion_tokens=83,\n            prompt_tokens=17,\n            total_tokens=100\n        )\n    ),\n    ChatCompletion(\n        id='chatcmpl-4ac3e982-da4e-486d-bddb-ed1d5cb9c03c',\n        choices=[\n            Choice(\n                finish_reason='stop',\n                index=0,\n                logprobs=None,\n                message=ChatCompletionMessage(\n                    content=\"A test request, and I'm delighted!\\nHere's a short poem, just for you:\\n\\nMoonbeams dance upon the sea,\\nA path of light, for you to see.\\nThe stars up high, a twinkling show,\\nA night of wonder, for all to know.\\n\\nThe world is quiet, save the night,\\nA peaceful hush, a gentle light.\\nThe world is full, of beauty rare,\\nA treasure trove, beyond compare.\\n\\nI hope you enjoyed this little test,\\nA poem born, of whimsy and jest.\\nLet me know, if there's anything else!\",\n                    role='assistant',\n                    function_call=None,\n                    tool_calls=None\n                )\n            )\n        ],\n        created=1715462919,\n        model='groq/llama3-8b-8192',\n        object='chat.completion',\n        system_fingerprint='fp_a2c8d063cb',\n        usage=CompletionUsage(\n            completion_tokens=120,\n            prompt_tokens=20,\n            total_tokens=140\n        )\n    )\n]\n"})})]}),(0,a.jsxs)(r.A,{value:"curl",label:"Curl",children:[(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:'curl --location \'http://localhost:4000/chat/completions\' \\\n    --header \'Authorization: Bearer sk-1234\' \\\n    --header \'Content-Type: application/json\' \\\n    --data \'{\n    "model": "llama3,gpt-3.5-turbo",\n    "max_tokens": 10,\n    "user": "litellm2",\n    "messages": [\n        {\n        "role": "user",\n        "content": "is litellm getting better"\n        }\n    ]\n}\'\n'})}),(0,a.jsx)(n.h4,{id:"expected-response-format-1",children:"Expected Response Format"}),(0,a.jsxs)(n.p,{children:["Get a list of responses when ",(0,a.jsx)(n.code,{children:"model"})," is passed as a list"]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'[\n  {\n    "id": "chatcmpl-3dbd5dd8-7c82-4ca3-bf1f-7c26f497cf2b",\n    "choices": [\n      {\n        "finish_reason": "length",\n        "index": 0,\n        "message": {\n          "content": "The Elder Scrolls IV: Oblivion!\\n\\nReleased",\n          "role": "assistant"\n        }\n      }\n    ],\n    "created": 1715459876,\n    "model": "groq/llama3-8b-8192",\n    "object": "chat.completion",\n    "system_fingerprint": "fp_179b0f92c9",\n    "usage": {\n      "completion_tokens": 10,\n      "prompt_tokens": 12,\n      "total_tokens": 22\n    }\n  },\n  {\n    "id": "chatcmpl-9NnldUfFLmVquFHSX4yAtjCw8PGei",\n    "choices": [\n      {\n        "finish_reason": "length",\n        "index": 0,\n        "message": {\n          "content": "TES4 could refer to The Elder Scrolls IV:",\n          "role": "assistant"\n        }\n      }\n    ],\n    "created": 1715459877,\n    "model": "gpt-3.5-turbo-0125",\n    "object": "chat.completion",\n    "system_fingerprint": null,\n    "usage": {\n      "completion_tokens": 10,\n      "prompt_tokens": 9,\n      "total_tokens": 19\n    }\n  }\n]\n'})})]})]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},7227(e,n,t){t.d(n,{A:()=>i});t(96540);var s=t(18215);const a="tabItem_Ymn6";var o=t(74848);function i({children:e,hidden:n,className:t}){return(0,o.jsx)("div",{role:"tabpanel",className:(0,s.A)(a,t),hidden:n,children:e})}},28453(e,n,t){t.d(n,{R:()=>i,x:()=>r});var s=t(96540);const a={},o=s.createContext(a);function i(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),s.createElement(o.Provider,{value:n},e.children)}},49489(e,n,t){t.d(n,{A:()=>A});var s=t(96540),a=t(18215),o=t(24245),i=t(56347),r=t(36494),l=t(62814),c=t(45167),d=t(69900);function p(e){return s.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,s.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function h(e){const{values:n,children:t}=e;return(0,s.useMemo)(()=>{const e=n??function(e){return p(e).map(({props:{value:e,label:n,attributes:t,default:s}})=>({value:e,label:n,attributes:t,default:s}))}(t);return function(e){const n=(0,c.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}function u({queryString:e=!1,groupId:n}){const t=(0,i.W6)(),a=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,l.aZ)(a),(0,s.useCallback)(e=>{if(!a)return;const n=new URLSearchParams(t.location.search);n.set(a,e),t.replace({...t.location,search:n.toString()})},[a,t])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,o=h(e),[i,l]=(0,s.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:o})),[c,p]=u({queryString:t,groupId:a}),[g,x]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,a]=(0,d.Dv)(n);return[t,(0,s.useCallback)(e=>{n&&a.set(e)},[n,a])]}({groupId:a}),j=(()=>{const e=c??g;return m({value:e,tabValues:o})?e:null})();(0,r.A)(()=>{j&&l(j)},[j]);return{selectedValue:i,selectValue:(0,s.useCallback)(e=>{if(!m({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);l(e),p(e),x(e)},[p,x,o]),tabValues:o}}var x=t(11062);const j="tabList__CuJ",y="tabItem_LNqP";var f=t(74848);function b({className:e,block:n,selectedValue:t,selectValue:s,tabValues:i}){const r=[],{blockElementScrollPositionUntilNextRender:l}=(0,o.a_)(),c=e=>{const n=e.currentTarget,a=r.indexOf(n),o=i[a].value;o!==t&&(l(n),s(o))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=r.indexOf(e.currentTarget)+1;n=r[t]??r[0];break}case"ArrowLeft":{const t=r.indexOf(e.currentTarget)-1;n=r[t]??r[r.length-1];break}}n?.focus()};return(0,f.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":n},e),children:i.map(({value:e,label:n,attributes:s})=>(0,f.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{r.push(e)},onKeyDown:d,onClick:c,...s,className:(0,a.A)("tabs__item",y,s?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function _({lazy:e,children:n,selectedValue:t}){const o=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=o.find(e=>e.props.value===t);return e?(0,s.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,f.jsx)("div",{className:"margin-top--md",children:o.map((e,n)=>(0,s.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function v(e){const n=g(e);return(0,f.jsxs)("div",{className:(0,a.A)("tabs-container",j),children:[(0,f.jsx)(b,{...n,...e}),(0,f.jsx)(_,{...n,...e})]})}function A(e){const n=(0,x.A)();return(0,f.jsx)(v,{...e,children:p(e.children)},String(n))}}}]);