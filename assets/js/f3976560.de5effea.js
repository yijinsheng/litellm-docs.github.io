"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[47795],{7227(e,n,l){l.d(n,{A:()=>t});l(96540);var o=l(18215);const s="tabItem_Ymn6";var r=l(74848);function t({children:e,hidden:n,className:l}){return(0,r.jsx)("div",{role:"tabpanel",className:(0,o.A)(s,l),hidden:n,children:e})}},28453(e,n,l){l.d(n,{R:()=>t,x:()=>i});var o=l(96540);const s={},r=o.createContext(s);function t(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),o.createElement(r.Provider,{value:n},e.children)}},49489(e,n,l){l.d(n,{A:()=>A});var o=l(96540),s=l(18215),r=l(24245),t=l(56347),i=l(36494),a=l(62814),c=l(45167),p=l(69900);function d(e){return o.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function u(e){const{values:n,children:l}=e;return(0,o.useMemo)(()=>{const e=n??function(e){return d(e).map(({props:{value:e,label:n,attributes:l,default:o}})=>({value:e,label:n,attributes:l,default:o}))}(l);return function(e){const n=(0,c.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,l])}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}function h({queryString:e=!1,groupId:n}){const l=(0,t.W6)(),s=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,a.aZ)(s),(0,o.useCallback)(e=>{if(!s)return;const n=new URLSearchParams(l.location.search);n.set(s,e),l.replace({...l.location,search:n.toString()})},[s,l])]}function g(e){const{defaultValue:n,queryString:l=!1,groupId:s}=e,r=u(e),[t,a]=(0,o.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const l=n.find(e=>e.default)??n[0];if(!l)throw new Error("Unexpected error: 0 tabValues");return l.value}({defaultValue:n,tabValues:r})),[c,d]=h({queryString:l,groupId:s}),[g,x]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[l,s]=(0,p.Dv)(n);return[l,(0,o.useCallback)(e=>{n&&s.set(e)},[n,s])]}({groupId:s}),y=(()=>{const e=c??g;return m({value:e,tabValues:r})?e:null})();(0,i.A)(()=>{y&&a(y)},[y]);return{selectedValue:t,selectValue:(0,o.useCallback)(e=>{if(!m({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);a(e),d(e),x(e)},[d,x,r]),tabValues:r}}var x=l(11062);const y="tabList__CuJ",v="tabItem_LNqP";var j=l(74848);function b({className:e,block:n,selectedValue:l,selectValue:o,tabValues:t}){const i=[],{blockElementScrollPositionUntilNextRender:a}=(0,r.a_)(),c=e=>{const n=e.currentTarget,s=i.indexOf(n),r=t[s].value;r!==l&&(a(n),o(r))},p=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const l=i.indexOf(e.currentTarget)+1;n=i[l]??i[0];break}case"ArrowLeft":{const l=i.indexOf(e.currentTarget)-1;n=i[l]??i[i.length-1];break}}n?.focus()};return(0,j.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":n},e),children:t.map(({value:e,label:n,attributes:o})=>(0,j.jsx)("li",{role:"tab",tabIndex:l===e?0:-1,"aria-selected":l===e,ref:e=>{i.push(e)},onKeyDown:p,onClick:c,...o,className:(0,s.A)("tabs__item",v,o?.className,{"tabs__item--active":l===e}),children:n??e},e))})}function f({lazy:e,children:n,selectedValue:l}){const r=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=r.find(e=>e.props.value===l);return e?(0,o.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,j.jsx)("div",{className:"margin-top--md",children:r.map((e,n)=>(0,o.cloneElement)(e,{key:n,hidden:e.props.value!==l}))})}function _(e){const n=g(e);return(0,j.jsxs)("div",{className:(0,s.A)("tabs-container",y),children:[(0,j.jsx)(b,{...n,...e}),(0,j.jsx)(f,{...n,...e})]})}function A(e){const n=(0,x.A)();return(0,j.jsx)(_,{...e,children:d(e.children)},String(n))}},94717(e,n,l){l.r(n),l.d(n,{assets:()=>p,contentTitle:()=>c,default:()=>m,frontMatter:()=>a,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"type":"mdx","permalink":"/","source":"@site/src/pages/index.md","title":"LiteLLM - Getting Started","description":"https://github.com/BerriAI/litellm","frontMatter":{},"unlisted":false}');var s=l(74848),r=l(28453),t=l(49489),i=l(7227);const a={},c="LiteLLM - Getting Started",p={},d=[{value:"<strong>Call 100+ LLMs using the OpenAI Input/Output Format</strong>",id:"call-100-llms-using-the-openai-inputoutput-format",level:2},{value:"How to use LiteLLM",id:"how-to-use-litellm",level:2},{value:"<strong>When to use LiteLLM Proxy Server (LLM Gateway)</strong>",id:"when-to-use-litellm-proxy-server-llm-gateway",level:3},{value:"<strong>When to use LiteLLM Python SDK</strong>",id:"when-to-use-litellm-python-sdk",level:3},{value:"<strong>LiteLLM Python SDK</strong>",id:"litellm-python-sdk",level:2},{value:"Basic usage",id:"basic-usage",level:3},{value:"Responses API",id:"responses-api",level:3},{value:"Streaming",id:"streaming",level:3},{value:"Exception handling",id:"exception-handling",level:3},{value:"Logging Observability - Log LLM Input/Output (Docs)",id:"logging-observability---log-llm-inputoutput-docs",level:3},{value:"Track Costs, Usage, Latency for streaming",id:"track-costs-usage-latency-for-streaming",level:3},{value:"<strong>LiteLLM Proxy Server (LLM Gateway)</strong>",id:"litellm-proxy-server-llm-gateway",level:2},{value:"\ud83d\udcd6 Proxy Endpoints - Swagger Docs",id:"-proxy-endpoints---swagger-docs",level:3},{value:"Quick Start Proxy - CLI",id:"quick-start-proxy---cli",level:3},{value:"Step 1: Start litellm proxy",id:"step-1-start-litellm-proxy",level:4},{value:"Step 1. CREATE config.yaml",id:"step-1-create-configyaml",level:3},{value:"Step 2. RUN Docker Image",id:"step-2-run-docker-image",level:3},{value:"Step 2: Make ChatCompletions Request to Proxy",id:"step-2-make-chatcompletions-request-to-proxy",level:4},{value:"More details",id:"more-details",level:2}];function u(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"litellm---getting-started",children:"LiteLLM - Getting Started"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://github.com/BerriAI/litellm",children:"https://github.com/BerriAI/litellm"})}),"\n",(0,s.jsx)(n.h2,{id:"call-100-llms-using-the-openai-inputoutput-format",children:(0,s.jsx)(n.strong,{children:"Call 100+ LLMs using the OpenAI Input/Output Format"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Translate inputs to provider's ",(0,s.jsx)(n.code,{children:"completion"}),", ",(0,s.jsx)(n.code,{children:"embedding"}),", and ",(0,s.jsx)(n.code,{children:"image_generation"})," endpoints"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://docs.litellm.ai/docs/completion/output",children:"Consistent output"}),", text responses will always be available at ",(0,s.jsx)(n.code,{children:"['choices'][0]['message']['content']"})]}),"\n",(0,s.jsxs)(n.li,{children:["Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - ",(0,s.jsx)(n.a,{href:"https://docs.litellm.ai/docs/routing",children:"Router"})]}),"\n",(0,s.jsxs)(n.li,{children:["Track spend & set budgets per project ",(0,s.jsx)(n.a,{href:"https://docs.litellm.ai/docs/simple_proxy",children:"LiteLLM Proxy Server"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"how-to-use-litellm",children:"How to use LiteLLM"}),"\n",(0,s.jsx)(n.p,{children:"You can use litellm through either:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"#litellm-proxy-server-llm-gateway",children:"LiteLLM Proxy Server"})," - Server (LLM Gateway) to call 100+ LLMs, load balance, cost tracking across projects"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"#basic-usage",children:"LiteLLM python SDK"})," - Python Client to call 100+ LLMs, load balance, cost tracking"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"when-to-use-litellm-proxy-server-llm-gateway",children:(0,s.jsx)(n.strong,{children:"When to use LiteLLM Proxy Server (LLM Gateway)"})}),"\n",(0,s.jsxs)(n.admonition,{type:"tip",children:[(0,s.jsxs)(n.p,{children:["Use LiteLLM Proxy Server if you want a ",(0,s.jsx)(n.strong,{children:"central service (LLM Gateway) to access multiple LLMs"})]}),(0,s.jsx)(n.p,{children:"Typically used by Gen AI Enablement /  ML PLatform Teams"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"LiteLLM Proxy gives you a unified interface to access multiple LLMs (100+ LLMs)"}),"\n",(0,s.jsx)(n.li,{children:"Track LLM Usage and setup guardrails"}),"\n",(0,s.jsx)(n.li,{children:"Customize Logging, Guardrails, Caching per project"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"when-to-use-litellm-python-sdk",children:(0,s.jsx)(n.strong,{children:"When to use LiteLLM Python SDK"})}),"\n",(0,s.jsxs)(n.admonition,{type:"tip",children:[(0,s.jsxs)(n.p,{children:["Use LiteLLM Python SDK if you want to use LiteLLM in your ",(0,s.jsx)(n.strong,{children:"python code"})]}),(0,s.jsx)(n.p,{children:"Typically used by developers building llm projects"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"LiteLLM SDK gives you a unified interface to access multiple LLMs (100+ LLMs)"}),"\n",(0,s.jsxs)(n.li,{children:["Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - ",(0,s.jsx)(n.a,{href:"https://docs.litellm.ai/docs/routing",children:"Router"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"litellm-python-sdk",children:(0,s.jsx)(n.strong,{children:"LiteLLM Python SDK"})}),"\n",(0,s.jsx)(n.h3,{id:"basic-usage",children:"Basic usage"}),"\n",(0,s.jsx)("a",{target:"_blank",href:"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb",children:(0,s.jsx)("img",{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"pip install litellm\n"})}),"\n",(0,s.jsxs)(t.A,{children:[(0,s.jsx)(i.A,{value:"openai",label:"OpenAI",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\n## set ENV variables\nos.environ["OPENAI_API_KEY"] = "your-api-key"\n\nresponse = completion(\n  model="gpt-3.5-turbo",\n  messages=[{ "content": "Hello, how are you?","role": "user"}]\n)\n'})})}),(0,s.jsx)(i.A,{value:"anthropic",label:"Anthropic",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\n## set ENV variables\nos.environ["ANTHROPIC_API_KEY"] = "your-api-key"\n\nresponse = completion(\n  model="claude-2",\n  messages=[{ "content": "Hello, how are you?","role": "user"}]\n)\n'})})}),(0,s.jsx)(i.A,{value:"vertex",label:"VertexAI",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\n# auth: run \'gcloud auth application-default\'\nos.environ["VERTEX_PROJECT"] = "hardy-device-386718"\nos.environ["VERTEX_LOCATION"] = "us-central1"\n\nresponse = completion(\n  model="chat-bison",\n  messages=[{ "content": "Hello, how are you?","role": "user"}]\n)\n'})})}),(0,s.jsx)(i.A,{value:"nvidia",label:"NVIDIA",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\n## set ENV variables\nos.environ["NVIDIA_NIM_API_KEY"] = "nvidia_api_key"\nos.environ["NVIDIA_NIM_API_BASE"] = "nvidia_nim_endpoint_url"\n\nresponse = completion(\n  model="nvidia_nim/<model_name>",\n  messages=[{ "content": "Hello, how are you?","role": "user"}]\n)\n'})})}),(0,s.jsx)(i.A,{value:"hugging",label:"HuggingFace",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\nos.environ["HUGGINGFACE_API_KEY"] = "huggingface_api_key"\n\n# e.g. Call \'WizardLM/WizardCoder-Python-34B-V1.0\' hosted on HF Inference endpoints\nresponse = completion(\n  model="huggingface/WizardLM/WizardCoder-Python-34B-V1.0",\n  messages=[{ "content": "Hello, how are you?","role": "user"}],\n  api_base="https://my-endpoint.huggingface.cloud"\n)\n\nprint(response)\n'})})}),(0,s.jsx)(i.A,{value:"azure",label:"Azure OpenAI",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\n## set ENV variables\nos.environ["AZURE_API_KEY"] = ""\nos.environ["AZURE_API_BASE"] = ""\nos.environ["AZURE_API_VERSION"] = ""\n\n# azure call\nresponse = completion(\n  "azure/<your_deployment_name>",\n  messages = [{ "content": "Hello, how are you?","role": "user"}]\n)\n'})})}),(0,s.jsx)(i.A,{value:"ollama",label:"Ollama",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion\n\nresponse = completion(\n            model="ollama/llama2",\n            messages = [{ "content": "Hello, how are you?","role": "user"}],\n            api_base="http://localhost:11434"\n)\n'})})}),(0,s.jsx)(i.A,{value:"or",label:"Openrouter",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\n## set ENV variables\nos.environ["OPENROUTER_API_KEY"] = "openrouter_api_key"\n\nresponse = completion(\n  model="openrouter/google/palm-2-chat-bison",\n  messages = [{ "content": "Hello, how are you?","role": "user"}],\n)\n'})})}),(0,s.jsx)(i.A,{value:"novita",label:"Novita AI",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\n## set ENV variables. Visit https://novita.ai/settings/key-management to get your API key\nos.environ["NOVITA_API_KEY"] = "novita-api-key"\n\nresponse = completion(\n  model="novita/deepseek/deepseek-r1",\n  messages=[{ "content": "Hello, how are you?","role": "user"}]\n)\n'})})})]}),"\n",(0,s.jsx)(n.h3,{id:"responses-api",children:"Responses API"}),"\n",(0,s.jsxs)(n.p,{children:["Use ",(0,s.jsx)(n.code,{children:"litellm.responses()"})," for advanced models that support reasoning content like GPT-5, o3, etc."]}),"\n",(0,s.jsxs)(t.A,{children:[(0,s.jsx)(i.A,{value:"openai-responses",label:"OpenAI",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import responses\nimport os\n\n## set ENV variables\nos.environ["OPENAI_API_KEY"] = "your-api-key"\n\nresponse = responses(\n  model="gpt-5-mini",\n  messages=[{ "content": "What is the capital of France?","role": "user"}],\n  reasoning_effort="medium"\n)\n\nprint(response)\nprint(response.choices[0].message.content) # response\nprint(response.choices[0].message.reasoning_content) # reasoning\n\n'})})}),(0,s.jsx)(i.A,{value:"anthropic-responses",label:"Anthropic (Claude)",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import responses\nimport os\n\n## set ENV variables\nos.environ["ANTHROPIC_API_KEY"] = "your-api-key"\n\nresponse = responses(\n  model="claude-3.5-sonnet",\n  messages=[{ "content": "What is the capital of France?","role": "user"}]\n)\n'})})}),(0,s.jsx)(i.A,{value:"vertex-responses",label:"VertexAI",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import responses\nimport os\n\n# auth: run \'gcloud auth application-default\'\nos.environ["VERTEX_PROJECT"] = "jr-smith-386718"\nos.environ["VERTEX_LOCATION"] = "us-central1"\n\nresponse = responses(\n  model="chat-bison",\n  messages=[{ "content": "What is the capital of France?","role": "user"}]\n)\n'})})}),(0,s.jsx)(i.A,{value:"azure-responses",label:"Azure OpenAI",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import responses\nimport os\n\n## set ENV variables\nos.environ["AZURE_API_KEY"] = ""\nos.environ["AZURE_API_BASE"] = ""\nos.environ["AZURE_API_VERSION"] = ""\n\n# azure call\nresponse = responses(\n  "azure/<your_deployment_name>",\n  messages = [{ "content": "What is the capital of France?","role": "user"}]\n)\n\nprint(response)\n'})})})]}),"\n",(0,s.jsx)(n.h3,{id:"streaming",children:"Streaming"}),"\n",(0,s.jsxs)(n.p,{children:["Set ",(0,s.jsx)(n.code,{children:"stream=True"})," in the ",(0,s.jsx)(n.code,{children:"completion"})," args."]}),"\n",(0,s.jsxs)(t.A,{children:[(0,s.jsx)(i.A,{value:"openai",label:"OpenAI",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\n## set ENV variables\nos.environ["OPENAI_API_KEY"] = "your-api-key"\n\nresponse = completion(\n  model="gpt-3.5-turbo",\n  messages=[{ "content": "Hello, how are you?","role": "user"}],\n  stream=True,\n)\n'})})}),(0,s.jsx)(i.A,{value:"anthropic",label:"Anthropic",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\n## set ENV variables\nos.environ["ANTHROPIC_API_KEY"] = "your-api-key"\n\nresponse = completion(\n  model="claude-2",\n  messages=[{ "content": "Hello, how are you?","role": "user"}],\n  stream=True,\n)\n'})})}),(0,s.jsx)(i.A,{value:"vertex",label:"VertexAI",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\n# auth: run \'gcloud auth application-default\'\nos.environ["VERTEX_PROJECT"] = "hardy-device-386718"\nos.environ["VERTEX_LOCATION"] = "us-central1"\n\nresponse = completion(\n  model="chat-bison",\n  messages=[{ "content": "Hello, how are you?","role": "user"}],\n  stream=True,\n)\n'})})}),(0,s.jsx)(i.A,{value:"nvidia",label:"NVIDIA",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\n## set ENV variables\nos.environ["NVIDIA_NIM_API_KEY"] = "nvidia_api_key"\nos.environ["NVIDIA_NIM_API_BASE"] = "nvidia_nim_endpoint_url"\n\nresponse = completion(\n  model="nvidia_nim/<model_name>",\n  messages=[{ "content": "Hello, how are you?","role": "user"}]\n  stream=True,\n)\n'})})}),(0,s.jsx)(i.A,{value:"hugging",label:"HuggingFace",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\nos.environ["HUGGINGFACE_API_KEY"] = "huggingface_api_key"\n\n# e.g. Call \'WizardLM/WizardCoder-Python-34B-V1.0\' hosted on HF Inference endpoints\nresponse = completion(\n  model="huggingface/WizardLM/WizardCoder-Python-34B-V1.0",\n  messages=[{ "content": "Hello, how are you?","role": "user"}],\n  api_base="https://my-endpoint.huggingface.cloud",\n  stream=True,\n)\n\nprint(response)\n'})})}),(0,s.jsx)(i.A,{value:"azure",label:"Azure OpenAI",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\n## set ENV variables\nos.environ["AZURE_API_KEY"] = ""\nos.environ["AZURE_API_BASE"] = ""\nos.environ["AZURE_API_VERSION"] = ""\n\n# azure call\nresponse = completion(\n  "azure/<your_deployment_name>",\n  messages = [{ "content": "Hello, how are you?","role": "user"}],\n  stream=True,\n)\n'})})}),(0,s.jsx)(i.A,{value:"ollama",label:"Ollama",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion\n\nresponse = completion(\n            model="ollama/llama2",\n            messages = [{ "content": "Hello, how are you?","role": "user"}],\n            api_base="http://localhost:11434",\n            stream=True,\n)\n'})})}),(0,s.jsx)(i.A,{value:"or",label:"Openrouter",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\n## set ENV variables\nos.environ["OPENROUTER_API_KEY"] = "openrouter_api_key"\n\nresponse = completion(\n  model="openrouter/google/palm-2-chat-bison",\n  messages = [{ "content": "Hello, how are you?","role": "user"}],\n  stream=True,\n)\n'})})}),(0,s.jsx)(i.A,{value:"novita",label:"Novita AI",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\n## set ENV variables. Visit https://novita.ai/settings/key-management to get your API key\nos.environ["NOVITA_API_KEY"] = "novita_api_key"\n\nresponse = completion(\n  model="novita/deepseek/deepseek-r1",\n  messages = [{ "content": "Hello, how are you?","role": "user"}],\n  stream=True,\n)\n'})})})]}),"\n",(0,s.jsx)(n.h3,{id:"exception-handling",children:"Exception handling"}),"\n",(0,s.jsx)(n.p,{children:"LiteLLM maps exceptions across all supported providers to the OpenAI exceptions. All our exceptions inherit from OpenAI's exception types, so any error-handling you have for that, should work out of the box with LiteLLM."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from openai.error import OpenAIError\nfrom litellm import completion\n\nos.environ["ANTHROPIC_API_KEY"] = "bad-key"\ntry:\n    # some code\n    completion(model="claude-instant-1", messages=[{"role": "user", "content": "Hey, how\'s it going?"}])\nexcept OpenAIError as e:\n    print(e)\n'})}),"\n",(0,s.jsxs)(n.h3,{id:"logging-observability---log-llm-inputoutput-docs",children:["Logging Observability - Log LLM Input/Output (",(0,s.jsx)(n.a,{href:"https://docs.litellm.ai/docs/observability/callbacks",children:"Docs"}),")"]}),"\n",(0,s.jsx)(n.p,{children:"LiteLLM exposes pre defined callbacks to send data to MLflow, Lunary, Langfuse, Helicone, Promptlayer, Traceloop, Slack"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from litellm import completion\n\n## set env variables for logging tools (API key set up is not required when using MLflow)\nos.environ["LUNARY_PUBLIC_KEY"] = "your-lunary-public-key" # get your key at https://app.lunary.ai/settings\nos.environ["HELICONE_API_KEY"] = "your-helicone-key"\nos.environ["LANGFUSE_PUBLIC_KEY"] = ""\nos.environ["LANGFUSE_SECRET_KEY"] = ""\n\nos.environ["OPENAI_API_KEY"]\n\n# set callbacks\nlitellm.success_callback = ["lunary", "mlflow", "langfuse", "helicone"] # log input/output to lunary, mlflow, langfuse, helicone\n\n#openai call\nresponse = completion(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hi \ud83d\udc4b - i\'m openai"}])\n'})}),"\n",(0,s.jsx)(n.h3,{id:"track-costs-usage-latency-for-streaming",children:"Track Costs, Usage, Latency for streaming"}),"\n",(0,s.jsxs)(n.p,{children:["Use a callback function for this - more info on custom callbacks: ",(0,s.jsx)(n.a,{href:"https://docs.litellm.ai/docs/observability/custom_callback",children:"https://docs.litellm.ai/docs/observability/custom_callback"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import litellm\n\n# track_cost_callback\ndef track_cost_callback(\n    kwargs,                 # kwargs to completion\n    completion_response,    # response from completion\n    start_time, end_time    # start/end time\n):\n    try:\n      response_cost = kwargs.get("response_cost", 0)\n      print("streaming response_cost", response_cost)\n    except:\n        pass\n# set callback\nlitellm.success_callback = [track_cost_callback] # set custom callback function\n\n# litellm.completion() call\nresponse = completion(\n    model="gpt-3.5-turbo",\n    messages=[\n        {\n            "role": "user",\n            "content": "Hi \ud83d\udc4b - i\'m openai"\n        }\n    ],\n    stream=True\n)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"litellm-proxy-server-llm-gateway",children:(0,s.jsx)(n.strong,{children:"LiteLLM Proxy Server (LLM Gateway)"})}),"\n",(0,s.jsx)(n.p,{children:"Track spend across multiple projects/people"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://github.com/BerriAI/litellm/assets/29436595/47c97d5e-b9be-4839-b28c-43d7f4f10033",alt:"ui_3"})}),"\n",(0,s.jsx)(n.p,{children:"The proxy provides:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth",children:"Hooks for auth"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.litellm.ai/docs/proxy/logging#step-1---create-your-custom-litellm-callback-class",children:"Hooks for logging"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.litellm.ai/docs/proxy/virtual_keys#tracking-spend",children:"Cost tracking"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.litellm.ai/docs/proxy/users#set-rate-limits",children:"Rate Limiting"})}),"\n"]}),"\n",(0,s.jsxs)(n.h3,{id:"-proxy-endpoints---swagger-docs",children:["\ud83d\udcd6 Proxy Endpoints - ",(0,s.jsx)(n.a,{href:"https://litellm-api.up.railway.app/",children:"Swagger Docs"})]}),"\n",(0,s.jsxs)(n.p,{children:["Go here for a complete tutorial with keys + rate limits - ",(0,s.jsx)(n.a,{href:"./proxy/docker_quick_start.md",children:(0,s.jsx)(n.strong,{children:"here"})})]}),"\n",(0,s.jsx)(n.h3,{id:"quick-start-proxy---cli",children:"Quick Start Proxy - CLI"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"pip install 'litellm[proxy]'\n"})}),"\n",(0,s.jsx)(n.h4,{id:"step-1-start-litellm-proxy",children:"Step 1: Start litellm proxy"}),"\n",(0,s.jsxs)(t.A,{children:[(0,s.jsx)(i.A,{label:"pip package",value:"pip",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"$ litellm --model huggingface/bigcode/starcoder\n\n#INFO: Proxy running on http://0.0.0.0:4000\n"})})}),(0,s.jsxs)(i.A,{label:"Docker container",value:"docker",children:[(0,s.jsx)(n.h3,{id:"step-1-create-configyaml",children:"Step 1. CREATE config.yaml"}),(0,s.jsxs)(n.p,{children:["Example ",(0,s.jsx)(n.code,{children:"litellm_config.yaml"})]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'model_list:\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n      model: azure/<your-azure-model-deployment>\n      api_base: os.environ/AZURE_API_BASE # runs os.getenv("AZURE_API_BASE")\n      api_key: os.environ/AZURE_API_KEY # runs os.getenv("AZURE_API_KEY")\n      api_version: "2023-07-01-preview"\n\nlitellm_settings:\n  master_key: sk-1234\n  database_url: postgres://\n'})}),(0,s.jsx)(n.h3,{id:"step-2-run-docker-image",children:"Step 2. RUN Docker Image"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"docker run \\\n    -v $(pwd)/litellm_config.yaml:/app/config.yaml \\\n    -e AZURE_API_KEY=d6*********** \\\n    -e AZURE_API_BASE=https://openai-***********/ \\\n    -p 4000:4000 \\\n    docker.litellm.ai/berriai/litellm:main-latest \\\n    --config /app/config.yaml --detailed_debug\n"})})]})]}),"\n",(0,s.jsx)(n.h4,{id:"step-2-make-chatcompletions-request-to-proxy",children:"Step 2: Make ChatCompletions Request to Proxy"}),"\n",(0,s.jsxs)(t.A,{children:[(0,s.jsx)(i.A,{value:"chat-completions",label:"Chat Completions",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai # openai v1.0.0+\nclient = openai.OpenAI(api_key="anything",base_url="http://0.0.0.0:4000") # set proxy to base_url\n# request sent to model set on litellm proxy, `litellm --model`\nresponse = client.chat.completions.create(model="gpt-3.5-turbo", messages = [\n    {\n        "role": "user",\n        "content": "this is a test request, write a short poem"\n    }\n])\n\nprint(response)\n'})})}),(0,s.jsx)(i.A,{value:"responses-api",label:"Responses API",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI(\n    api_key="sk-1234",\n    base_url="http://0.0.0.0:4000"\n)\n\nresponse = client.responses.create(\n  model="gpt-5",\n  input="Tell me a three sentence bedtime story about a unicorn."\n)\n\nprint(response)\n'})})})]}),"\n",(0,s.jsx)(n.h2,{id:"more-details",children:"More details"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"../../docs/exception_mapping",children:"exception mapping"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"../../docs/proxy/docker_quick_start",children:"E2E Tutorial for LiteLLM Proxy Server"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"../../docs/proxy/virtual_keys",children:"proxy virtual keys & spend management"})}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}}}]);