"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[18924],{42986(e){e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"v1-80-11","metadata":{"permalink":"/release_notes/v1-80-11","source":"@site/release_notes/v1.80.11-stable/index.md","title":"[Preview] v1.80.11 - Google Interactions API","description":"Deploy this version","date":"2025-12-20T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"[Preview] v1.80.11 - Google Interactions API","slug":"v1-80-11","date":"2025-12-20T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"nextItem":{"title":"[Preview] v1.80.10.rc.1 - Agent Gateway: Azure Foundry & Bedrock AgentCore","permalink":"/release_notes/v1-80-10"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.80.11.rc.1\\n```\\n\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.80.11\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights\\n\\n- **Gemini 3 Flash Preview** - [Day 0 support for Google\'s Gemini 3 Flash Preview with reasoning capabilities](../../docs/providers/gemini)\\n- **Stability AI Image Generation** - [New provider for Stability AI image generation and editing](../../docs/providers/stability)\\n- **LiteLLM Content Filter** - [Built-in guardrails for harmful content, bias, and PII detection with image support](../../docs/proxy/guardrails/litellm_content_filter)\\n- **New Provider: Venice.ai** - Support for Venice.ai API via providers.json\\n- **Unified Skills API** - [Skills API works across Anthropic, Vertex, Azure, and Bedrock](../../docs/skills)\\n- **Azure Sentinel Logging** - [New logging integration for Azure Sentinel](../../docs/observability/azure_sentinel)\\n- **Guardrails Load Balancing** - [Load balance between multiple guardrail providers](../../docs/proxy/guardrails)\\n- **Email Budget Alerts** - [Send email notifications when budgets are reached](../../docs/proxy/email)\\n- **Cloudzero Integration on UI** - Setup your Cloudzero Integration Directly on the UI\\n\\n---\\n\\n### Cloudzero Integration on UI\\n\\n<Image\\nimg={require(\'../../img/ui_cloudzero.png\')}\\nstyle={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n\\nUsers can now configure their Cloudzero Integration directly on the UI.\\n\\n---\\n### Performance: 50% Reduction in Memory Usage and Import Latency for the LiteLLM SDK\\n\\nWe\'ve completely restructured `litellm.__init__.py` to defer heavy imports until they\'re actually needed, implementing lazy loading for **109 components**.\\n\\nThis refactoring includes **41 provider config classes**, **40 utility functions**, cache implementations (Redis, DualCache, InMemoryCache), HTTP handlers, logging, types, and other heavy dependencies. Heavy libraries like tiktoken and boto3 are now loaded on-demand rather than eagerly at import time.\\n\\nThis makes LiteLLM especially beneficial for serverless functions, Lambda deployments, and containerized environments where cold start times and memory footprint matter.\\n\\n---\\n\\n## New Providers and Endpoints\\n\\n### New Providers (5 new providers)\\n\\n| Provider | Supported LiteLLM Endpoints | Description |\\n| -------- | ------------------- | ----------- |\\n| [Stability AI](../../docs/providers/stability) | `/images/generations`, `/images/edits` | Stable Diffusion 3, SD3.5, image editing and generation |\\n| Venice.ai | `/chat/completions`, `/messages`, `/responses` | Venice.ai API integration via providers.json |\\n| [Pydantic AI Agents](../../docs/providers/pydantic_ai_agent) | `/a2a` | Pydantic AI agents for A2A protocol workflows |\\n| [VertexAI Agent Engine](../../docs/providers/vertex_ai_agent_engine) | `/a2a` | Google Vertex AI Agent Engine for agentic workflows |\\n| [LinkUp Search](../../docs/search/linkup) | `/search` | LinkUp web search API integration |\\n\\n### New LLM API Endpoints (2 new endpoints)\\n\\n| Endpoint | Method | Description | Documentation |\\n| -------- | ------ | ----------- | ------------- |\\n| `/interactions` | POST | Google Interactions API for conversational AI | [Docs](../../docs/interactions) |\\n| `/search` | POST | RAG Search API with rerankers | [Docs](../../docs/search/index) |\\n\\n---\\n\\n## New Models / Updated Models\\n\\n#### New Model Support (55+ new models)\\n\\n| Provider | Model | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Features |\\n| -------- | ----- | -------------- | ------------------- | -------------------- | -------- |\\n| Gemini | `gemini/gemini-3-flash-preview` | 1M | $0.50 | $3.00 | Reasoning, vision, audio, video, PDF |\\n| Vertex AI | `vertex_ai/gemini-3-flash-preview` | 1M | $0.50 | $3.00 | Reasoning, vision, audio, video, PDF |\\n| Azure AI | `azure_ai/deepseek-v3.2` | 164K | $0.58 | $1.68 | Reasoning, function calling, caching |\\n| Azure AI | `azure_ai/cohere-rerank-v4.0-pro` | 32K | $0.0025/query | - | Rerank |\\n| Azure AI | `azure_ai/cohere-rerank-v4.0-fast` | 32K | $0.002/query | - | Rerank |\\n| OpenRouter | `openrouter/openai/gpt-5.2` | 400K | $1.75 | $14.00 | Reasoning, vision, caching |\\n| OpenRouter | `openrouter/openai/gpt-5.2-pro` | 400K | $21.00 | $168.00 | Reasoning, vision |\\n| OpenRouter | `openrouter/mistralai/devstral-2512` | 262K | $0.15 | $0.60 | Function calling |\\n| OpenRouter | `openrouter/mistralai/ministral-3b-2512` | 131K | $0.10 | $0.10 | Function calling, vision |\\n| OpenRouter | `openrouter/mistralai/ministral-8b-2512` | 262K | $0.15 | $0.15 | Function calling, vision |\\n| OpenRouter | `openrouter/mistralai/ministral-14b-2512` | 262K | $0.20 | $0.20 | Function calling, vision |\\n| OpenRouter | `openrouter/mistralai/mistral-large-2512` | 262K | $0.50 | $1.50 | Function calling, vision |\\n| OpenAI | `gpt-4o-transcribe-diarize` | 16K | $6.00/audio | - | Audio transcription with diarization |\\n| OpenAI | `gpt-image-1.5-2025-12-16` | - | Various | Various | Image generation |\\n| Stability | `stability/sd3-large` | - | - | $0.065/image | Image generation |\\n| Stability | `stability/sd3.5-large` | - | - | $0.065/image | Image generation |\\n| Stability | `stability/stable-image-ultra` | - | - | $0.08/image | Image generation |\\n| Stability | `stability/inpaint` | - | - | $0.005/image | Image editing |\\n| Stability | `stability/outpaint` | - | - | $0.004/image | Image editing |\\n| Bedrock | `stability.stable-conservative-upscale-v1:0` | - | - | $0.40/image | Image upscaling |\\n| Bedrock | `stability.stable-creative-upscale-v1:0` | - | - | $0.60/image | Image upscaling |\\n| Vertex AI | `vertex_ai/deepseek-ai/deepseek-ocr-maas` | - | $0.30 | $1.20 | OCR |\\n| LinkUp | `linkup/search` | - | $5.87/1K queries | - | Web search |\\n| LinkUp | `linkup/search-deep` | - | $58.67/1K queries | - | Deep web search |\\n| GitHub Copilot | 20+ models | Various | - | - | Chat completions |\\n\\n#### Features\\n\\n- **[Gemini](../../docs/providers/gemini)**\\n    - Add Gemini 3 Flash Preview day 0 support with reasoning - [PR #18135](https://github.com/BerriAI/litellm/pull/18135)\\n    - Support extra_headers in batch embeddings - [PR #18004](https://github.com/BerriAI/litellm/pull/18004)\\n    - Propagate token usage when generating images - [PR #17987](https://github.com/BerriAI/litellm/pull/17987)\\n    - Use JSON instead of form-data for image edit requests - [PR #18012](https://github.com/BerriAI/litellm/pull/18012)\\n    - Fix web search requests count - [PR #17921](https://github.com/BerriAI/litellm/pull/17921)\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Use dynamic max_tokens based on model - [PR #17900](https://github.com/BerriAI/litellm/pull/17900)\\n    - Fix claude-3-7-sonnet max_tokens to 64K default - [PR #17979](https://github.com/BerriAI/litellm/pull/17979)\\n    - Add OpenAI-compatible API with modify_params=True - [PR #17106](https://github.com/BerriAI/litellm/pull/17106)\\n- **[Vertex AI](../../docs/providers/vertex)**\\n    - Add Gemini 3 Flash Preview support - [PR #18164](https://github.com/BerriAI/litellm/pull/18164)\\n    - Add reasoning support for gemini-3-flash-preview - [PR #18175](https://github.com/BerriAI/litellm/pull/18175)\\n    - Fix image edit credential source - [PR #18121](https://github.com/BerriAI/litellm/pull/18121)\\n    - Pass credentials to PredictionServiceClient for custom endpoints - [PR #17757](https://github.com/BerriAI/litellm/pull/17757)\\n    - Fix multimodal embeddings for text + base64 image combinations - [PR #18172](https://github.com/BerriAI/litellm/pull/18172)\\n    - Add OCR support for DeepSeek model - [PR #17971](https://github.com/BerriAI/litellm/pull/17971)\\n- **[Azure AI](../../docs/providers/azure_ai)**\\n    - Add Azure Cohere 4 reranking models - [PR #17961](https://github.com/BerriAI/litellm/pull/17961)\\n    - Add Azure DeepSeek V3.2 versions - [PR #18019](https://github.com/BerriAI/litellm/pull/18019)\\n    - Return AzureAnthropicConfig for Claude models in get_provider_chat_config - [PR #18086](https://github.com/BerriAI/litellm/pull/18086)\\n- **[Fireworks AI](../../docs/providers/fireworks_ai)**\\n    - Add reasoning param support for Fireworks AI models - [PR #17967](https://github.com/BerriAI/litellm/pull/17967)\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Add Qwen 2 and Qwen 3 to get_bedrock_model_id - [PR #18100](https://github.com/BerriAI/litellm/pull/18100)\\n    - Remove ttl field when routing to bedrock - [PR #18049](https://github.com/BerriAI/litellm/pull/18049)\\n    - Add Bedrock Stability image edit models - [PR #18254](https://github.com/BerriAI/litellm/pull/18254)\\n- **[Perplexity](../../docs/providers/perplexity)**\\n    - Use API-provided cost instead of manual calculation - [PR #17887](https://github.com/BerriAI/litellm/pull/17887)\\n- **[OpenAI](../../docs/providers/openai)**\\n    - Add diarize model for audio transcription - [PR #18117](https://github.com/BerriAI/litellm/pull/18117)\\n    - Add gpt-image-1.5-2025-12-16 in model cost map - [PR #18107](https://github.com/BerriAI/litellm/pull/18107)\\n    - Fix cost calculation of gpt-image-1 model - [PR #17966](https://github.com/BerriAI/litellm/pull/17966)\\n- **[GitHub Copilot](../../docs/providers/github_copilot)**\\n    - Add github_copilot model info - [PR #17858](https://github.com/BerriAI/litellm/pull/17858)\\n- **[Custom LLM](../../docs/providers/custom_llm_server)**\\n    - Add image_edit and aimage_edit support - [PR #17999](https://github.com/BerriAI/litellm/pull/17999)\\n\\n### Bug Fixes\\n\\n- **[Gemini](../../docs/providers/gemini)**\\n    - Fix pricing for Gemini 3 Flash on Vertex AI - [PR #18202](https://github.com/BerriAI/litellm/pull/18202)\\n    - Add output_cost_per_image_token for gemini-2.5-flash-image models - [PR #18156](https://github.com/BerriAI/litellm/pull/18156)\\n    - Fix properties should be non-empty for OBJECT type - [PR #18237](https://github.com/BerriAI/litellm/pull/18237)\\n- **[Qwen](../../docs/providers/fireworks_ai)**\\n    - Add qwen3-embedding-8b input per token price - [PR #18018](https://github.com/BerriAI/litellm/pull/18018)\\n- **General**\\n    - Fix image URL handling - [PR #18139](https://github.com/BerriAI/litellm/pull/18139)\\n    - Support Signed URLs with Query Parameters in Image Processing - [PR #17976](https://github.com/BerriAI/litellm/pull/17976)\\n    - Add none to encoding_format instead of omitting it - [PR #18042](https://github.com/BerriAI/litellm/pull/18042)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n\\n- **[Responses API](../../docs/response_api)**\\n    - Add provider specific tools support - [PR #17980](https://github.com/BerriAI/litellm/pull/17980)\\n    - Add custom headers support - [PR #18036](https://github.com/BerriAI/litellm/pull/18036)\\n    - Fix tool calls transformation in completion bridge - [PR #18226](https://github.com/BerriAI/litellm/pull/18226)\\n    - Use list format with input_text for tool results - [PR #18257](https://github.com/BerriAI/litellm/pull/18257)\\n    - Add cost tracking in background mode - [PR #18236](https://github.com/BerriAI/litellm/pull/18236)\\n    - Fix Claude code responses API bridge errors - [PR #18194](https://github.com/BerriAI/litellm/pull/18194)\\n- **[Chat Completions API](../../docs/completion/input)**\\n    - Add support for agent skills - [PR #18031](https://github.com/BerriAI/litellm/pull/18031)\\n- **[Skills API](../../docs/skills)**\\n    - Unified Skills API works across Anthropic, Vertex, Azure, Bedrock - [PR #18232](https://github.com/BerriAI/litellm/pull/18232)\\n- **[Search API](../../docs/search/index)**\\n    - Add new RAG Search API with rerankers - [PR #18217](https://github.com/BerriAI/litellm/pull/18217)\\n- **[Interactions API](../../docs/interactions)**\\n    - Add Google Interactions API on SDK and AI Gateway - [PR #18079](https://github.com/BerriAI/litellm/pull/18079), [PR #18081](https://github.com/BerriAI/litellm/pull/18081)\\n- **[Image Edit API](../../docs/image_edits)**\\n    - Add drop_params support and fix Vertex AI config - [PR #18077](https://github.com/BerriAI/litellm/pull/18077)\\n- **General**\\n    - Skip adding beta headers for Vertex AI as it is not supported - [PR #18037](https://github.com/BerriAI/litellm/pull/18037)\\n    - Fix managed files endpoint - [PR #18046](https://github.com/BerriAI/litellm/pull/18046)\\n    - Allow base_model for non-Azure providers in proxy - [PR #18038](https://github.com/BerriAI/litellm/pull/18038)\\n\\n#### Bugs\\n\\n- **General**\\n    - Fix basemodel import in guardrail translation - [PR #17977](https://github.com/BerriAI/litellm/pull/17977)\\n    - Fix No module named \'fastapi\' error - [PR #18239](https://github.com/BerriAI/litellm/pull/18239)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n\\n- **Virtual Keys**\\n    - Add master key rotation for credentials table - [PR #17952](https://github.com/BerriAI/litellm/pull/17952)\\n    - Fix tag management to preserve encrypted fields in litellm_params - [PR #17484](https://github.com/BerriAI/litellm/pull/17484)\\n    - Fix key delete and regenerate permissions - [PR #18214](https://github.com/BerriAI/litellm/pull/18214)\\n- **Models + Endpoints**\\n    - Add Models Conditional Rendering in UI - [PR #18071](https://github.com/BerriAI/litellm/pull/18071)\\n    - Add Health Check Model for Wildcard Model in UI - [PR #18269](https://github.com/BerriAI/litellm/pull/18269)\\n    - Auto Resolve Vector Store Embedding Model Config - [PR #18167](https://github.com/BerriAI/litellm/pull/18167)\\n- **Vector Stores**\\n    - Add Milvus Vector Store UI support - [PR #18030](https://github.com/BerriAI/litellm/pull/18030)\\n    - Persist Vector Store Settings in Team Update - [PR #18274](https://github.com/BerriAI/litellm/pull/18274)\\n- **Logs & Spend**\\n    - Add LiteLLM Overhead to Logs - [PR #18033](https://github.com/BerriAI/litellm/pull/18033)\\n    - Show LiteLLM Overhead in Logs UI - [PR #18034](https://github.com/BerriAI/litellm/pull/18034)\\n    - Resolve Team ID to Team Alias in Usage Page - [PR #18275](https://github.com/BerriAI/litellm/pull/18275)\\n    - Fix Usage Page Top Key View Button Visibility - [PR #18203](https://github.com/BerriAI/litellm/pull/18203)\\n- **SSO & Health**\\n    - Add SSO Readiness Health Check - [PR #18078](https://github.com/BerriAI/litellm/pull/18078)\\n    - Fix /health/test_connection to resolve env variables like /chat/completions - [PR #17752](https://github.com/BerriAI/litellm/pull/17752)\\n- **CloudZero**\\n    - Add CloudZero Cost Tracking UI - [PR #18163](https://github.com/BerriAI/litellm/pull/18163)\\n    - Add Delete CloudZero Settings Route and UI - [PR #18168](https://github.com/BerriAI/litellm/pull/18168), [PR #18170](https://github.com/BerriAI/litellm/pull/18170)\\n- **General**\\n    - Update UI path handling for non-root Docker - [PR #17989](https://github.com/BerriAI/litellm/pull/17989)\\n\\n#### Bugs\\n\\n- **UI Fixes**\\n    - Fix Login Page Failed To Parse JSON Error - [PR #18159](https://github.com/BerriAI/litellm/pull/18159)\\n    - Fix new user route user_id collision handling - [PR #17559](https://github.com/BerriAI/litellm/pull/17559)\\n    - Fix Callback Environment Variables Casing - [PR #17912](https://github.com/BerriAI/litellm/pull/17912)\\n\\n---\\n\\n## AI Integrations\\n\\n### Logging\\n\\n- **[Azure Sentinel](../../docs/observability/azure_sentinel)**\\n    - Add new Azure Sentinel Logger integration - [PR #18146](https://github.com/BerriAI/litellm/pull/18146)\\n- **[Prometheus](../../docs/proxy/logging#prometheus)**\\n    - Add extraction of top level metadata for custom labels - [PR #18087](https://github.com/BerriAI/litellm/pull/18087)\\n- **[Langfuse](../../docs/proxy/logging#langfuse)**\\n    - Fix not working log_failure_event - [PR #18234](https://github.com/BerriAI/litellm/pull/18234)\\n- **[Arize Phoenix](../../docs/observability/phoenix_integration)**\\n    - Fix nested spans - [PR #18102](https://github.com/BerriAI/litellm/pull/18102)\\n- **General**\\n    - Change extra_headers to additional_headers - [PR #17950](https://github.com/BerriAI/litellm/pull/17950)\\n\\n### Guardrails\\n\\n- **[LiteLLM Content Filter](../../docs/proxy/guardrails/litellm_content_filter)**\\n    - Add built-in guardrails for harmful content, bias, etc. - [PR #18029](https://github.com/BerriAI/litellm/pull/18029)\\n    - Add support for running content filters on images - [PR #18044](https://github.com/BerriAI/litellm/pull/18044)\\n    - Add support for Brazil PII field - [PR #18076](https://github.com/BerriAI/litellm/pull/18076)\\n    - Add configurable guardrail options for content filtering - [PR #18007](https://github.com/BerriAI/litellm/pull/18007)\\n- **[Guardrails API](../../docs/adding_provider/generic_guardrail_api)**\\n    - Support LLM tool call response checks on `/chat/completions`, `/v1/responses`, `/v1/messages` - [PR #17619](https://github.com/BerriAI/litellm/pull/17619)\\n    - Add guardrails load balancing - [PR #18181](https://github.com/BerriAI/litellm/pull/18181)\\n    - Fix guardrails for passthrough endpoint - [PR #18109](https://github.com/BerriAI/litellm/pull/18109)\\n    - Add headers to metadata for guardrails on pass-through endpoints - [PR #17992](https://github.com/BerriAI/litellm/pull/17992)\\n    - Various fixes for guardrail on OpenRouter models - [PR #18085](https://github.com/BerriAI/litellm/pull/18085)\\n- **[Lakera](../../docs/proxy/guardrails/lakera_ai)**\\n    - Add monitor mode for Lakera - [PR #18084](https://github.com/BerriAI/litellm/pull/18084)\\n- **[Pillar Security](../../docs/proxy/guardrails/pillar_security)**\\n    - Add masking support and MCP call support - [PR #17959](https://github.com/BerriAI/litellm/pull/17959)\\n- **[Bedrock Guardrails](../../docs/proxy/guardrails/bedrock)**\\n    - Add support for Bedrock image guardrails - [PR #18115](https://github.com/BerriAI/litellm/pull/18115)\\n    - Guardrails block action takes precedence over masking - [PR #17968](https://github.com/BerriAI/litellm/pull/17968)\\n\\n### Secret Managers\\n\\n- **[HashiCorp Vault](../../docs/secret_managers/hashicorp_vault)**\\n    - Add documentation for configurable Vault mount - [PR #18082](https://github.com/BerriAI/litellm/pull/18082)\\n    - Add per-team Vault configuration - [PR #18150](https://github.com/BerriAI/litellm/pull/18150)\\n- **UI**\\n    - Add secret manager settings controls to team management UI - [PR #18149](https://github.com/BerriAI/litellm/pull/18149)\\n\\n---\\n\\n## Spend Tracking, Budgets and Rate Limiting\\n\\n- **Email Budget Alerts** - Send email notifications when budgets are reached - [PR #17995](https://github.com/BerriAI/litellm/pull/17995)\\n\\n---\\n\\n## MCP Gateway\\n\\n- **Auth Header Propagation** - Add MCP auth header propagation - [PR #17963](https://github.com/BerriAI/litellm/pull/17963)\\n- **Fix deepcopy error** - Fix MCP tool call deepcopy error when processing requests - [PR #18010](https://github.com/BerriAI/litellm/pull/18010)\\n- **Fix list tool** - Fix MCP list_tools not working without database connection - [PR #18161](https://github.com/BerriAI/litellm/pull/18161)\\n\\n---\\n\\n## Agent Gateway (A2A)\\n\\n- **New Provider: Agent Gateway** - Add pydantic ai agents support - [PR #18013](https://github.com/BerriAI/litellm/pull/18013)\\n- **VertexAI Agent Engine** - Add Vertex AI Agent Engine provider - [PR #18014](https://github.com/BerriAI/litellm/pull/18014)\\n- **Fix model extraction** - Fix get_model_from_request() to extract model ID from Vertex AI passthrough URLs - [PR #18097](https://github.com/BerriAI/litellm/pull/18097)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n- **Lazy Imports** - Use per-attribute lazy imports and extract shared constants - [PR #17994](https://github.com/BerriAI/litellm/pull/17994)\\n- **Lazy Load HTTP Handlers** - Lazy load http handlers - [PR #17997](https://github.com/BerriAI/litellm/pull/17997)\\n- **Lazy Load Caches** - Lazy load caches - [PR #18001](https://github.com/BerriAI/litellm/pull/18001)\\n- **Lazy Load Types** - Lazy load bedrock types, .types.utils, GuardrailItem - [PR #18053](https://github.com/BerriAI/litellm/pull/18053), [PR #18054](https://github.com/BerriAI/litellm/pull/18054), [PR #18072](https://github.com/BerriAI/litellm/pull/18072)\\n- **Lazy Load Configs** - Lazy load 41 configuration classes - [PR #18267](https://github.com/BerriAI/litellm/pull/18267)\\n- **Lazy Load Client Decorators** - Lazy load heavy client decorator imports - [PR #18064](https://github.com/BerriAI/litellm/pull/18064)\\n- **Prisma Build Time** - Download Prisma binaries at build time instead of runtime for security restricted environments - [PR #17695](https://github.com/BerriAI/litellm/pull/17695)\\n- **Docker Alpine** - Add libsndfile to Alpine image for ARM64 audio processing - [PR #18092](https://github.com/BerriAI/litellm/pull/18092)\\n- **Security** - Prevent LiteLLM API key leakage on /health endpoint failures - [PR #18133](https://github.com/BerriAI/litellm/pull/18133)\\n\\n---\\n\\n## Documentation Updates\\n\\n- **SAP Docs** - Update SAP documentation - [PR #17974](https://github.com/BerriAI/litellm/pull/17974)\\n- **Pydantic AI Agents** - Add docs on using pydantic ai agents with LiteLLM A2A gateway - [PR #18026](https://github.com/BerriAI/litellm/pull/18026)\\n- **Vertex AI Agent Engine** - Add Vertex AI Agent Engine documentation - [PR #18027](https://github.com/BerriAI/litellm/pull/18027)\\n- **Router Order** - Add router order parameter documentation - [PR #18045](https://github.com/BerriAI/litellm/pull/18045)\\n- **Secret Manager Settings** - Improve secret manager settings documentation - [PR #18235](https://github.com/BerriAI/litellm/pull/18235)\\n- **Gemini 3 Flash** - Add version requirement in Gemini 3 Flash blog - [PR #18227](https://github.com/BerriAI/litellm/pull/18227)\\n- **README** - Expand Responses API section and update endpoints - [PR #17354](https://github.com/BerriAI/litellm/pull/17354)\\n- **Amazon Nova** - Add Amazon Nova to sidebar and supported models - [PR #18220](https://github.com/BerriAI/litellm/pull/18220)\\n- **Benchmarks** - Add infrastructure recommendations to benchmarks documentation - [PR #18264](https://github.com/BerriAI/litellm/pull/18264)\\n- **Broken Links** - Fix broken link corrections - [PR #18104](https://github.com/BerriAI/litellm/pull/18104)\\n- **README Fixes** - Various README improvements - [PR #18206](https://github.com/BerriAI/litellm/pull/18206)\\n\\n---\\n\\n## Infrastructure / CI/CD\\n\\n- **PR Templates** - Add LiteLLM team PR template and CI/CD rules - [PR #17983](https://github.com/BerriAI/litellm/pull/17983), [PR #17985](https://github.com/BerriAI/litellm/pull/17985)\\n- **Issue Labeling** - Improve issue labeling with component dropdown and more provider keywords - [PR #17957](https://github.com/BerriAI/litellm/pull/17957)\\n- **PR Template Cleanup** - Remove redundant fields from PR template - [PR #17956](https://github.com/BerriAI/litellm/pull/17956)\\n- **Dependencies** - Bump altcha-lib from 1.3.0 to 1.4.1 - [PR #18017](https://github.com/BerriAI/litellm/pull/18017)\\n\\n---\\n\\n## New Contributors\\n\\n* @dongbin-lunark made their first contribution in [PR #17757](https://github.com/BerriAI/litellm/pull/17757)\\n* @qdrddr made their first contribution in [PR #18004](https://github.com/BerriAI/litellm/pull/18004)\\n* @donicrosby made their first contribution in [PR #17962](https://github.com/BerriAI/litellm/pull/17962)\\n* @NicolaivdSmagt made their first contribution in [PR #17992](https://github.com/BerriAI/litellm/pull/17992)\\n* @Reapor-Yurnero made their first contribution in [PR #18085](https://github.com/BerriAI/litellm/pull/18085)\\n* @jk-f5 made their first contribution in [PR #18086](https://github.com/BerriAI/litellm/pull/18086)\\n* @castrapel made their first contribution in [PR #18077](https://github.com/BerriAI/litellm/pull/18077)\\n* @dtikhonov made their first contribution in [PR #17484](https://github.com/BerriAI/litellm/pull/17484)\\n* @opleonnn made their first contribution in [PR #18175](https://github.com/BerriAI/litellm/pull/18175)\\n* @eurogig made their first contribution in [PR #18084](https://github.com/BerriAI/litellm/pull/18084)\\n\\n---\\n\\n## Full Changelog\\n\\n**[View complete changelog on GitHub](https://github.com/BerriAI/litellm/compare/v1.80.10-nightly...v1.80.11)**"},{"id":"v1-80-10","metadata":{"permalink":"/release_notes/v1-80-10","source":"@site/release_notes/v1.80.10-stable/index.md","title":"[Preview] v1.80.10.rc.1 - Agent Gateway: Azure Foundry & Bedrock AgentCore","description":"Deploy this version","date":"2025-12-13T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"[Preview] v1.80.10.rc.1 - Agent Gateway: Azure Foundry & Bedrock AgentCore","slug":"v1-80-10","date":"2025-12-13T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"[Preview] v1.80.11 - Google Interactions API","permalink":"/release_notes/v1-80-11"},"nextItem":{"title":"v1.80.8-stable - Introducing A2A Agent Gateway","permalink":"/release_notes/v1-80-8"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.80.10.rc.1\\n```\\n\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.80.10\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights\\n\\n- **Agent (A2A) Gateway with Cost Tracking** - [Track agent costs per query, per token pricing, and view agent usage in the dashboard](../../docs/a2a_cost_tracking)\\n- **2 New Agent Providers** - [LangGraph Agents](../../docs/providers/langgraph) and [Azure AI Foundry Agents](../../docs/providers/azure_ai_agents) for agentic workflows\\n- **New Provider: SAP Gen AI Hub** - [Full support for SAP Generative AI Hub with chat completions](../../docs/providers/sap)\\n- **New Bedrock Writer Models** - Add Palmyra-X4 and Palmyra-X5 models on Bedrock\\n- **OpenAI GPT-5.2 Models** - Full support for GPT-5.2, GPT-5.2-pro, and Azure GPT-5.2 models with reasoning support\\n- **227 New Fireworks AI Models** - Comprehensive model coverage for Fireworks AI platform\\n- **MCP Support on /chat/completions** - [Use MCP servers directly via chat completions endpoint](../../docs/mcp)\\n- **Performance Improvements** - Reduced memory leaks by 50%\\n\\n---\\n\\n### Agent Gateway - 4 New Agent Providers\\n\\n<Image\\nimg={require(\'../../img/a2a_gateway2.png\')}\\nstyle={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n\\n<br/>\\n\\nThis release adds support for agents from the following providers:\\n- **LangGraph Agents** - Deploy and manage LangGraph-based agents\\n- **Azure AI Foundry Agents** - Enterprise agent deployments on Azure\\n- **Bedrock AgentCore** - AWS Bedrock agent integration\\n- **A2A Agents** - Agent-to-Agent protocol support\\n\\nAI Gateway admins can now add agents from any of these providers, and developers can invoke them through a unified interface using the A2A protocol.\\n\\nFor all agent requests running through the AI Gateway, LiteLLM automatically tracks request/response logs, cost, and token usage. \\n\\n### Agent (A2A) Usage UI\\n\\n<Image\\nimg={require(\'../../img/agent_usage.png\')}\\nstyle={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n\\nUsers can now filter usage statistics by agents, providing the same granular filtering capabilities available for teams, organizations, and customers.\\n\\n**Details:**\\n\\n- Filter usage analytics, spend logs, and activity metrics by agent ID\\n- View breakdowns on a per-agent basis\\n- Consistent filtering experience across all usage and analytics views\\n\\n---\\n\\n## New Providers and Endpoints\\n\\n### New Providers (5 new providers)\\n\\n| Provider | Supported LiteLLM Endpoints | Description |\\n| -------- | ------------------- | ----------- |\\n| [SAP Gen AI Hub](../../docs/providers/sap) | `/chat/completions`, `/messages`, `/responses` | SAP Generative AI Hub integration for enterprise AI |\\n| [LangGraph](../../docs/providers/langgraph) | `/chat/completions`, `/messages`, `/responses`, `/a2a` | LangGraph agents for agentic workflows |\\n| [Azure AI Foundry Agents](../../docs/providers/azure_ai_agents) | `/chat/completions`, `/messages`, `/responses`, `/a2a` | Azure AI Foundry Agents for enterprise agent deployments |\\n| [Voyage AI Rerank](../../docs/providers/voyage) | `/rerank` | Voyage AI rerank models support |\\n| [Fireworks AI Rerank](../../docs/providers/fireworks_ai) | `/rerank` | Fireworks AI rerank endpoint support |\\n\\n### New LLM API Endpoints (4 new endpoints)\\n\\n| Endpoint | Method | Description | Documentation |\\n| -------- | ------ | ----------- | ------------- |\\n| `/containers/{id}/files` | GET | List files in a container | [Docs](../../docs/container_files) |\\n| `/containers/{id}/files/{file_id}` | GET | Retrieve container file metadata | [Docs](../../docs/container_files) |\\n| `/containers/{id}/files/{file_id}` | DELETE | Delete a file from a container | [Docs](../../docs/container_files) |\\n| `/containers/{id}/files/{file_id}/content` | GET | Retrieve container file content | [Docs](../../docs/container_files) |\\n\\n---\\n\\n## New Models / Updated Models\\n\\n#### New Model Support (270+ new models)\\n\\n| Provider | Model | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Features |\\n| -------- | ----- | -------------- | ------------------- | -------------------- | -------- |\\n| OpenAI | `gpt-5.2` | 400K | $1.75 | $14.00 | Reasoning, vision, PDF, caching |\\n| OpenAI | `gpt-5.2-pro` | 400K | $21.00 | $168.00 | Reasoning, web search, vision |\\n| Azure | `azure/gpt-5.2` | 400K | $1.75 | $14.00 | Reasoning, vision, PDF, caching |\\n| Azure | `azure/gpt-5.2-pro` | 400K | $21.00 | $168.00 | Reasoning, web search |\\n| Bedrock | `us.writer.palmyra-x4-v1:0` | 128K | $2.50 | $10.00 | Function calling, PDF input |\\n| Bedrock | `us.writer.palmyra-x5-v1:0` | 1M | $0.60 | $6.00 | Function calling, PDF input |\\n| Bedrock | `eu.anthropic.claude-opus-4-5-20251101-v1:0` | 200K | $5.00 | $25.00 | Reasoning, computer use, vision |\\n| Bedrock | `google.gemma-3-12b-it` | 128K | $0.10 | $0.30 | Audio input |\\n| Bedrock | `moonshot.kimi-k2-thinking` | 128K | $0.60 | $2.50 | Reasoning |\\n| Bedrock | `nvidia.nemotron-nano-12b-v2` | 128K | $0.20 | $0.60 | Vision |\\n| Bedrock | `qwen.qwen3-next-80b-a3b` | 128K | $0.15 | $1.20 | Function calling |\\n| Vertex AI | `vertex_ai/deepseek-ai/deepseek-v3.2-maas` | 164K | $0.56 | $1.68 | Reasoning, caching |\\n| Mistral | `mistral/codestral-2508` | 256K | $0.30 | $0.90 | Function calling |\\n| Mistral | `mistral/devstral-2512` | 256K | $0.40 | $2.00 | Function calling |\\n| Mistral | `mistral/labs-devstral-small-2512` | 256K | $0.10 | $0.30 | Function calling |\\n| Cerebras | `cerebras/zai-glm-4.6` | 128K | - | - | Chat completions |\\n| NVIDIA NIM | `nvidia_nim/ranking/nvidia/llama-3.2-nv-rerankqa-1b-v2` | - | Free | Free | Rerank |\\n| Voyage | `voyage/rerank-2.5` | 32K | $0.05/1K tokens | - | Rerank |\\n| Fireworks AI | 227 new models | Various | Various | Various | Full model catalog |\\n\\n#### Features\\n\\n- **[OpenAI](../../docs/providers/openai)**\\n    - Add support for OpenAI GPT-5.2 models with reasoning_effort=\'xhigh\' - [PR #17836](https://github.com/BerriAI/litellm/pull/17836), [PR #17875](https://github.com/BerriAI/litellm/pull/17875)\\n    - Include \'user\' param for responses API models - [PR #17648](https://github.com/BerriAI/litellm/pull/17648)\\n    - Use optimized async http client for text completions - [PR #17831](https://github.com/BerriAI/litellm/pull/17831)\\n- **[Azure](../../docs/providers/azure)**\\n    - Add Azure GPT-5.2 models support - [PR #17866](https://github.com/BerriAI/litellm/pull/17866)\\n- **[Azure AI](../../docs/providers/azure_ai)**\\n    - Fix Azure AI Anthropic api-key header and passthrough cost calculation - [PR #17656](https://github.com/BerriAI/litellm/pull/17656)\\n    - Remove unsupported params from Azure AI Anthropic requests - [PR #17822](https://github.com/BerriAI/litellm/pull/17822)\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Prevent duplicate tool_result blocks with same tool - [PR #17632](https://github.com/BerriAI/litellm/pull/17632)\\n    - Handle partial JSON chunks in streaming responses - [PR #17493](https://github.com/BerriAI/litellm/pull/17493)\\n    - Preserve server_tool_use and web_search_tool_result in multi-turn conversations - [PR #17746](https://github.com/BerriAI/litellm/pull/17746)\\n    - Capture web_search_tool_result in streaming for multi-turn conversations - [PR #17798](https://github.com/BerriAI/litellm/pull/17798)\\n    - Add retrieve batches and retrieve file content support - [PR #17700](https://github.com/BerriAI/litellm/pull/17700)\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Add new Bedrock OSS models to model list - [PR #17638](https://github.com/BerriAI/litellm/pull/17638)\\n    - Add Bedrock Writer models (Palmyra-X4, Palmyra-X5) - [PR #17685](https://github.com/BerriAI/litellm/pull/17685)\\n    - Add EU Claude Opus 4.5 model - [PR #17897](https://github.com/BerriAI/litellm/pull/17897)\\n    - Add serviceTier support for Converse API - [PR #17810](https://github.com/BerriAI/litellm/pull/17810)\\n    - Fix header forwarding with custom API for Bedrock embeddings - [PR #17872](https://github.com/BerriAI/litellm/pull/17872)\\n- **[Gemini](../../docs/providers/gemini)**\\n    - Add support for computer use for Gemini - [PR #17756](https://github.com/BerriAI/litellm/pull/17756)\\n    - Handle context window errors - [PR #17751](https://github.com/BerriAI/litellm/pull/17751)\\n    - Add speechConfig to GenerationConfig for Gemini TTS - [PR #17851](https://github.com/BerriAI/litellm/pull/17851)\\n- **[Vertex AI](../../docs/providers/vertex)**\\n    - Add DeepSeek-V3.2 model support - [PR #17770](https://github.com/BerriAI/litellm/pull/17770)\\n    - Preserve systemInstructions for generate content request - [PR #17803](https://github.com/BerriAI/litellm/pull/17803)\\n- **[Mistral](../../docs/providers/mistral)**\\n    - Add Codestral 2508, Devstral 2512 models - [PR #17801](https://github.com/BerriAI/litellm/pull/17801)\\n- **[Cerebras](../../docs/providers/cerebras)**\\n    - Add zai-glm-4.6 model support - [PR #17683](https://github.com/BerriAI/litellm/pull/17683)\\n    - Fix context window errors not recognized - [PR #17587](https://github.com/BerriAI/litellm/pull/17587)\\n- **[DeepSeek](../../docs/providers/deepseek)**\\n    - Add native support for thinking and reasoning_effort params - [PR #17712](https://github.com/BerriAI/litellm/pull/17712)\\n- **[NVIDIA NIM Rerank](../../docs/providers/nvidia_nim_rerank)**\\n    - Add llama-3.2-nv-rerankqa-1b-v2 rerank model - [PR #17670](https://github.com/BerriAI/litellm/pull/17670)\\n- **[Fireworks AI](../../docs/providers/fireworks_ai)**\\n    - Add 227 new Fireworks AI models - [PR #17692](https://github.com/BerriAI/litellm/pull/17692)\\n- **[Dashscope](../../docs/providers/dashscope)**\\n    - Fix default base_url error - [PR #17584](https://github.com/BerriAI/litellm/pull/17584)\\n\\n### Bug Fixes\\n\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Fix missing content in Anthropic to OpenAI conversion - [PR #17693](https://github.com/BerriAI/litellm/pull/17693)\\n    - Avoid error when we have just the tool_calls in input - [PR #17753](https://github.com/BerriAI/litellm/pull/17753)\\n- **[Azure](../../docs/providers/azure)**\\n    - Fix error about encoding video id for Azure - [PR #17708](https://github.com/BerriAI/litellm/pull/17708)\\n- **[Azure AI](../../docs/providers/azure_ai)**\\n    - Fix LLM provider for azure_ai in model map - [PR #17805](https://github.com/BerriAI/litellm/pull/17805)\\n- **[Watsonx](../../docs/providers/watsonx)**\\n    - Fix Watsonx Audio Transcription to only send supported params to API - [PR #17840](https://github.com/BerriAI/litellm/pull/17840)\\n- **[Router](../../docs/routing)**\\n    - Handle tools=None in completion requests - [PR #17684](https://github.com/BerriAI/litellm/pull/17684)\\n    - Add minimum request threshold for error rate cooldown - [PR #17464](https://github.com/BerriAI/litellm/pull/17464)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n\\n- **[Responses API](../../docs/response_api)**\\n    - Add usage details in responses usage object - [PR #17641](https://github.com/BerriAI/litellm/pull/17641)\\n    - Fix error for response API polling - [PR #17654](https://github.com/BerriAI/litellm/pull/17654)\\n    - Fix streaming tool_calls being dropped when text + tool_calls - [PR #17652](https://github.com/BerriAI/litellm/pull/17652)\\n    - Transform image content in tool results for Responses API - [PR #17799](https://github.com/BerriAI/litellm/pull/17799)\\n    - Fix responses api not applying tpm rate limits on api keys - [PR #17707](https://github.com/BerriAI/litellm/pull/17707)\\n- **[Containers API](../../docs/containers)**\\n    - Allow using LIST, Create Containers using custom-llm-provider - [PR #17740](https://github.com/BerriAI/litellm/pull/17740)\\n    - Add new container API file management + UI Interface - [PR #17745](https://github.com/BerriAI/litellm/pull/17745)\\n- **[Rerank API](../../docs/rerank)**\\n    - Add support for forwarding client headers in /rerank endpoint - [PR #17873](https://github.com/BerriAI/litellm/pull/17873)\\n- **[Files API](../../docs/files_endpoints)**\\n    - Add support for expires_after param in Files endpoint - [PR #17860](https://github.com/BerriAI/litellm/pull/17860)\\n- **[Video API](../../docs/videos)**\\n    - Use litellm params for all videos APIs - [PR #17732](https://github.com/BerriAI/litellm/pull/17732)\\n    - Respect videos content db creds - [PR #17771](https://github.com/BerriAI/litellm/pull/17771)\\n- **[Embeddings API](../../docs/proxy/embedding)**\\n    - Fix handling token array input decoding for embeddings - [PR #17468](https://github.com/BerriAI/litellm/pull/17468)\\n- **[Chat Completions API](../../docs/completion/input)**\\n    - Add v0 target storage support - store files in Azure AI storage and use with chat completions API - [PR #17758](https://github.com/BerriAI/litellm/pull/17758)\\n- **[generateContent API](../../docs/providers/gemini)**\\n    - Support model names with slashes on Gemini generateContent endpoints - [PR #17743](https://github.com/BerriAI/litellm/pull/17743)\\n- **General**\\n    - Use audio content for caching - [PR #17651](https://github.com/BerriAI/litellm/pull/17651)\\n    - Return 403 exception when calling GET responses API - [PR #17629](https://github.com/BerriAI/litellm/pull/17629)\\n    - Add nested field removal support to additional_drop_params - [PR #17711](https://github.com/BerriAI/litellm/pull/17711)\\n    - Async post_call_streaming_iterator_hook now properly iterates async generators - [PR #17626](https://github.com/BerriAI/litellm/pull/17626)\\n\\n#### Bugs\\n\\n- **General**\\n    - Fix handle string content in is_cached_message - [PR #17853](https://github.com/BerriAI/litellm/pull/17853)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n\\n- **UI Settings**\\n    - Add Get and Update Backend Routes for UI Settings - [PR #17689](https://github.com/BerriAI/litellm/pull/17689)\\n    - UI Settings page implementation - [PR #17697](https://github.com/BerriAI/litellm/pull/17697)\\n    - Ensure Model Page honors UI Settings - [PR #17804](https://github.com/BerriAI/litellm/pull/17804)\\n    - Add All Proxy Models to Default User Settings - [PR #17902](https://github.com/BerriAI/litellm/pull/17902)\\n- **Agent & Usage UI**\\n    - Daily Agent Usage Backend - [PR #17781](https://github.com/BerriAI/litellm/pull/17781)\\n    - Agent Usage UI - [PR #17797](https://github.com/BerriAI/litellm/pull/17797)\\n    - Add agent cost tracking on UI - [PR #17899](https://github.com/BerriAI/litellm/pull/17899)\\n    - New Badge for Agent Usage - [PR #17883](https://github.com/BerriAI/litellm/pull/17883)\\n    - Usage Entity labels for filtering - [PR #17896](https://github.com/BerriAI/litellm/pull/17896)\\n    - Agent Usage Page minor fixes - [PR #17901](https://github.com/BerriAI/litellm/pull/17901)\\n    - Usage Page View Select component - [PR #17854](https://github.com/BerriAI/litellm/pull/17854)\\n    - Usage Page Components refactor - [PR #17848](https://github.com/BerriAI/litellm/pull/17848)\\n- **Logs & Spend**\\n    - Enhanced spend analytics in logs view - [PR #17623](https://github.com/BerriAI/litellm/pull/17623)\\n    - Add user info delete modal for user management - [PR #17625](https://github.com/BerriAI/litellm/pull/17625)\\n    - Show request and response details in logs view - [PR #17928](https://github.com/BerriAI/litellm/pull/17928)\\n- **Virtual Keys**\\n    - Fix x-litellm-key-spend header update - [PR #17864](https://github.com/BerriAI/litellm/pull/17864)\\n- **Models & Endpoints**\\n    - Model Hub Useful Links Rearrange - [PR #17859](https://github.com/BerriAI/litellm/pull/17859)\\n    - Create Team Model Dropdown honors Organization\'s Models - [PR #17834](https://github.com/BerriAI/litellm/pull/17834)\\n- **SSO & Auth**\\n    - Allow upserting user role when SSO provider role changes - [PR #17754](https://github.com/BerriAI/litellm/pull/17754)\\n    - Allow fetching role from generic SSO provider (Keycloak) - [PR #17787](https://github.com/BerriAI/litellm/pull/17787)\\n    - JWT Auth - allow selecting team_id from request header - [PR #17884](https://github.com/BerriAI/litellm/pull/17884)\\n    - Remove SSO Config Values from Config Table on SSO Update - [PR #17668](https://github.com/BerriAI/litellm/pull/17668)\\n- **Teams**\\n    - Attach team to org table - [PR #17832](https://github.com/BerriAI/litellm/pull/17832)\\n    - Expose the team alias when authenticating - [PR #17725](https://github.com/BerriAI/litellm/pull/17725)\\n- **MCP Server Management**\\n    - Add extra_headers and allowed_tools to UpdateMCPServerRequest - [PR #17940](https://github.com/BerriAI/litellm/pull/17940)\\n- **Notifications**\\n    - Show progress and pause on hover for Notifications - [PR #17942](https://github.com/BerriAI/litellm/pull/17942)\\n- **General**\\n    - Allow Root Path to Redirect when Docs not on Root Path - [PR #16843](https://github.com/BerriAI/litellm/pull/16843)\\n    - Show UI version number on top left near logo - [PR #17891](https://github.com/BerriAI/litellm/pull/17891)\\n    - Re-organize left navigation with correct categories and agents on root - [PR #17890](https://github.com/BerriAI/litellm/pull/17890)\\n    - UI Playground - allow custom model names in model selector dropdown - [PR #17892](https://github.com/BerriAI/litellm/pull/17892)\\n\\n#### Bugs\\n\\n- **UI Fixes**\\n    - Fix links + old login page deprecation message - [PR #17624](https://github.com/BerriAI/litellm/pull/17624)\\n    - Filtering for Chat UI Endpoint Selector - [PR #17567](https://github.com/BerriAI/litellm/pull/17567)\\n    - Race Condition Handling in SCIM v2 - [PR #17513](https://github.com/BerriAI/litellm/pull/17513)\\n    - Make /litellm_model_cost_map public - [PR #16795](https://github.com/BerriAI/litellm/pull/16795)\\n    - Custom Callback on UI - [PR #17522](https://github.com/BerriAI/litellm/pull/17522)\\n    - Add User Writable Directory to Non Root Docker for Logo - [PR #17180](https://github.com/BerriAI/litellm/pull/17180)\\n    - Swap URL Input and Display Name inputs - [PR #17682](https://github.com/BerriAI/litellm/pull/17682)\\n    - Change deprecation banner to only show on /sso/key/generate - [PR #17681](https://github.com/BerriAI/litellm/pull/17681)\\n    - Change credential encryption to only affect db credentials - [PR #17741](https://github.com/BerriAI/litellm/pull/17741)\\n- **Auth & Routes**\\n    - Return 403 instead of 503 for unauthorized routes - [PR #17723](https://github.com/BerriAI/litellm/pull/17723)\\n    - AI Gateway Auth - allow using wildcard patterns for public routes - [PR #17686](https://github.com/BerriAI/litellm/pull/17686)\\n\\n---\\n\\n## AI Integrations\\n\\n### New Integrations (4 new integrations)\\n\\n| Integration | Type | Description |\\n| ----------- | ---- | ----------- |\\n| [SumoLogic](../../docs/proxy/logging#sumologic) | Logging | Native webhook integration for SumoLogic - [PR #17630](https://github.com/BerriAI/litellm/pull/17630) |\\n| [Arize Phoenix](../../docs/proxy/arize_phoenix_prompts) | Prompt Management | Arize Phoenix OSS prompt management integration - [PR #17750](https://github.com/BerriAI/litellm/pull/17750) |\\n| [Sendgrid](../../docs/proxy/email) | Email | Sendgrid email notifications integration - [PR #17775](https://github.com/BerriAI/litellm/pull/17775) |\\n| [Onyx](../../docs/proxy/guardrails/onyx_security) | Guardrails | Onyx guardrail hooks integration - [PR #16591](https://github.com/BerriAI/litellm/pull/16591) |\\n\\n### Logging\\n\\n- **[Langfuse](../../docs/proxy/logging#langfuse)**\\n    - Propagate Langfuse trace_id - [PR #17669](https://github.com/BerriAI/litellm/pull/17669)\\n    - Prefer standard trace id for Langfuse logging - [PR #17791](https://github.com/BerriAI/litellm/pull/17791)\\n    - Move query params to create_pass_through_route call in Langfuse passthrough - [PR #17660](https://github.com/BerriAI/litellm/pull/17660)\\n    - Add support for custom masking function - [PR #17826](https://github.com/BerriAI/litellm/pull/17826)\\n- **[Prometheus](../../docs/proxy/logging#prometheus)**\\n    - Add \'exception_status\' to prometheus logger - [PR #17847](https://github.com/BerriAI/litellm/pull/17847)\\n- **[OpenTelemetry](../../docs/proxy/logging#otel)**\\n    - Add latency metrics (TTFT, TPOT, Total Generation Time) to OTEL payload - [PR #17888](https://github.com/BerriAI/litellm/pull/17888)\\n- **General**\\n    - Add polling via cache feature for async logging - [PR #16862](https://github.com/BerriAI/litellm/pull/16862)\\n\\n### Guardrails\\n\\n- **[HiddenLayer](../../docs/proxy/guardrails/hiddenlayer)**\\n    - Add HiddenLayer Guardrail Hooks - [PR #17728](https://github.com/BerriAI/litellm/pull/17728)\\n- **[Pillar Security](../../docs/proxy/guardrails/pillar_security)**\\n    - Add opt-in evidence results for Pillar Security guardrail during monitoring - [PR #17812](https://github.com/BerriAI/litellm/pull/17812)\\n- **[PANW Prisma AIRS](../../docs/proxy/guardrails/panw_prisma_airs)**\\n    - Add configurable fail-open, timeout, and app_user tracking - [PR #17785](https://github.com/BerriAI/litellm/pull/17785)\\n- **[Presidio](../../docs/proxy/guardrails/pii_masking_v2)**\\n    - Add support for configurable confidence score thresholds and scope in Presidio PII masking - [PR #17817](https://github.com/BerriAI/litellm/pull/17817)\\n- **[LiteLLM Content Filter](../../docs/proxy/guardrails/litellm_content_filter)**\\n    - Mask all regex pattern matches, not just first - [PR #17727](https://github.com/BerriAI/litellm/pull/17727)\\n- **[Regex Guardrails](../../docs/proxy/guardrails/secret_detection)**\\n    - Add enhanced regex pattern matching for guardrails - [PR #17915](https://github.com/BerriAI/litellm/pull/17915)\\n- **[Gray Swan Guardrail](../../docs/proxy/guardrails/grayswan)**\\n    - Add passthrough mode for model response - [PR #17102](https://github.com/BerriAI/litellm/pull/17102)\\n\\n### Prompt Management\\n\\n- **General**\\n    - New API for integrating prompt management providers - [PR #17829](https://github.com/BerriAI/litellm/pull/17829)\\n\\n---\\n\\n## Spend Tracking, Budgets and Rate Limiting\\n\\n- **Service Tier Pricing** - Extract service_tier from response/usage for OpenAI flex pricing - [PR #17748](https://github.com/BerriAI/litellm/pull/17748)\\n- **Agent Cost Tracking** - Track agent_id in SpendLogs - [PR #17795](https://github.com/BerriAI/litellm/pull/17795)\\n- **Tag Activity** - Deduplicate /tag/daily/activity metadata - [PR #16764](https://github.com/BerriAI/litellm/pull/16764)\\n- **Rate Limiting** - Dynamic Rate Limiter - allow specifying ttl for in memory cache - [PR #17679](https://github.com/BerriAI/litellm/pull/17679)\\n\\n---\\n\\n## MCP Gateway\\n\\n- **Chat Completions Integration** - Add support for using MCPs on /chat/completions - [PR #17747](https://github.com/BerriAI/litellm/pull/17747)\\n- **UI Session Permissions** - Fix UI session MCP permissions across real teams - [PR #17620](https://github.com/BerriAI/litellm/pull/17620)\\n- **OAuth Callback** - Fix MCP OAuth callback routing and URL handling - [PR #17789](https://github.com/BerriAI/litellm/pull/17789)\\n- **Tool Name Prefix** - Fix MCP tool name prefix - [PR #17908](https://github.com/BerriAI/litellm/pull/17908)\\n\\n---\\n\\n## Agent Gateway (A2A)\\n\\n- **Cost Per Query** - Add cost per query for agent invocations - [PR #17774](https://github.com/BerriAI/litellm/pull/17774)\\n- **Token Counting** - Add token counting non streaming + streaming - [PR #17779](https://github.com/BerriAI/litellm/pull/17779)\\n- **Cost Per Token** - Add cost per token pricing for A2A - [PR #17780](https://github.com/BerriAI/litellm/pull/17780)\\n- **LangGraph Provider** - Add LangGraph provider for Agent Gateway - [PR #17783](https://github.com/BerriAI/litellm/pull/17783)\\n- **Bedrock & LangGraph Agents** - Allow using Bedrock AgentCore, LangGraph agents with A2A Gateway - [PR #17786](https://github.com/BerriAI/litellm/pull/17786)\\n- **Agent Management** - Allow adding LangGraph, Bedrock Agent Core agents - [PR #17802](https://github.com/BerriAI/litellm/pull/17802)\\n- **Azure Foundry Agents** - Add Azure AI Foundry Agents support - [PR #17845](https://github.com/BerriAI/litellm/pull/17845)\\n- **Azure Foundry UI** - Allow adding Azure Foundry Agents on UI - [PR #17909](https://github.com/BerriAI/litellm/pull/17909)\\n- **Azure Foundry Fixes** - Ensure Azure Foundry agents work correctly - [PR #17943](https://github.com/BerriAI/litellm/pull/17943)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n- **Memory Leak Fix** - Cut memory leak in half - [PR #17784](https://github.com/BerriAI/litellm/pull/17784)\\n- **Spend Logs Memory** - Reduce memory accumulation of spend_logs - [PR #17742](https://github.com/BerriAI/litellm/pull/17742)\\n- **Router Optimization** - Replace time.perf_counter() with time.time() - [PR #17881](https://github.com/BerriAI/litellm/pull/17881)\\n- **Filter Internal Params** - Filter internal params in fallback code - [PR #17941](https://github.com/BerriAI/litellm/pull/17941)\\n- **Gunicorn Suggestion** - Suggest Gunicorn instead of uvicorn when using max_requests_before_restart - [PR #17788](https://github.com/BerriAI/litellm/pull/17788)\\n- **Pydantic Warnings** - Mitigate PydanticDeprecatedSince20 warnings - [PR #17657](https://github.com/BerriAI/litellm/pull/17657)\\n- **Python 3.14 Support** - Add Python 3.14 support via grpcio version constraints - [PR #17666](https://github.com/BerriAI/litellm/pull/17666)\\n- **OpenAI Package** - Bump openai package to 2.9.0 - [PR #17818](https://github.com/BerriAI/litellm/pull/17818)\\n\\n---\\n\\n## Documentation Updates\\n\\n- **Contributing** - Update clone instructions to recommend forking first - [PR #17637](https://github.com/BerriAI/litellm/pull/17637)\\n- **Getting Started** - Improve Getting Started page and SDK documentation structure - [PR #17614](https://github.com/BerriAI/litellm/pull/17614)\\n- **JSON Mode** - Make it clearer how to get Pydantic model output - [PR #17671](https://github.com/BerriAI/litellm/pull/17671)\\n- **drop_params** - Update litellm docs for drop_params - [PR #17658](https://github.com/BerriAI/litellm/pull/17658)\\n- **Environment Variables** - Document missing environment variables and fix incorrect types - [PR #17649](https://github.com/BerriAI/litellm/pull/17649)\\n- **SumoLogic** - Add SumoLogic integration documentation - [PR #17647](https://github.com/BerriAI/litellm/pull/17647)\\n- **SAP Gen AI** - Add SAP Gen AI provider documentation - [PR #17667](https://github.com/BerriAI/litellm/pull/17667)\\n- **Authentication** - Add Note for Authentication - [PR #17733](https://github.com/BerriAI/litellm/pull/17733)\\n- **Known Issues** - Adding known issues to 1.80.5-stable docs - [PR #17738](https://github.com/BerriAI/litellm/pull/17738)\\n- **Supported Endpoints** - Fix Supported Endpoints page - [PR #17710](https://github.com/BerriAI/litellm/pull/17710)\\n- **Token Count** - Document token count endpoint - [PR #17772](https://github.com/BerriAI/litellm/pull/17772)\\n- **Overview** - Made litellm proxy and SDK difference cleaner in overview with a table - [PR #17790](https://github.com/BerriAI/litellm/pull/17790)\\n- **Containers API** - Add docs for containers files API + code interpreter on LiteLLM - [PR #17749](https://github.com/BerriAI/litellm/pull/17749)\\n- **Target Storage** - Add documentation for target storage - [PR #17882](https://github.com/BerriAI/litellm/pull/17882)\\n- **Agent Usage** - Agent Usage documentation - [PR #17931](https://github.com/BerriAI/litellm/pull/17931), [PR #17932](https://github.com/BerriAI/litellm/pull/17932), [PR #17934](https://github.com/BerriAI/litellm/pull/17934)\\n- **Cursor Integration** - Cursor Integration documentation - [PR #17855](https://github.com/BerriAI/litellm/pull/17855), [PR #17939](https://github.com/BerriAI/litellm/pull/17939)\\n- **A2A Cost Tracking** - A2A cost tracking docs - [PR #17913](https://github.com/BerriAI/litellm/pull/17913)\\n- **Azure Search** - Update azure search docs - [PR #17726](https://github.com/BerriAI/litellm/pull/17726)\\n- **Milvus Client** - Fix milvus client docs - [PR #17736](https://github.com/BerriAI/litellm/pull/17736)\\n- **Streaming Logging** - Remove streaming logging doc - [PR #17739](https://github.com/BerriAI/litellm/pull/17739)\\n- **Integration Docs** - Update integration docs location - [PR #17644](https://github.com/BerriAI/litellm/pull/17644)\\n- **Links** - Updated docs links for mistral and anthropic - [PR #17852](https://github.com/BerriAI/litellm/pull/17852)\\n- **Community** - Add community doc link - [PR #17734](https://github.com/BerriAI/litellm/pull/17734)\\n- **Pricing** - Update pricing for global.anthropic.claude-haiku-4-5-20251001-v1:0 - [PR #17703](https://github.com/BerriAI/litellm/pull/17703)\\n- **gpt-image-1-mini** - Correct model type for gpt-image-1-mini - [PR #17635](https://github.com/BerriAI/litellm/pull/17635)\\n\\n---\\n\\n## Infrastructure / Deployment\\n\\n- **Docker** - Use python instead of wget for healthcheck in docker-compose.yml - [PR #17646](https://github.com/BerriAI/litellm/pull/17646)\\n- **Helm Chart** - Add extraResources support for Helm chart deployments - [PR #17627](https://github.com/BerriAI/litellm/pull/17627)\\n- **Helm Versioning** - Add semver prerelease suffix to helm chart versions - [PR #17678](https://github.com/BerriAI/litellm/pull/17678)\\n- **Database Schema** - Add storage_backend and storage_url columns to schema.prisma for target storage feature - [PR #17936](https://github.com/BerriAI/litellm/pull/17936)\\n\\n---\\n\\n## New Contributors\\n\\n* @xianzongxie-stripe made their first contribution in [PR #16862](https://github.com/BerriAI/litellm/pull/16862)\\n* @krisxia0506 made their first contribution in [PR #17637](https://github.com/BerriAI/litellm/pull/17637)\\n* @chetanchoudhary-sumo made their first contribution in [PR #17630](https://github.com/BerriAI/litellm/pull/17630)\\n* @kevinmarx made their first contribution in [PR #17632](https://github.com/BerriAI/litellm/pull/17632)\\n* @expruc made their first contribution in [PR #17627](https://github.com/BerriAI/litellm/pull/17627)\\n* @rcII made their first contribution in [PR #17626](https://github.com/BerriAI/litellm/pull/17626)\\n* @tamirkiviti13 made their first contribution in [PR #16591](https://github.com/BerriAI/litellm/pull/16591)\\n* @Eric84626 made their first contribution in [PR #17629](https://github.com/BerriAI/litellm/pull/17629)\\n* @vasilisazayka made their first contribution in [PR #16053](https://github.com/BerriAI/litellm/pull/16053)\\n* @juliettech13 made their first contribution in [PR #17663](https://github.com/BerriAI/litellm/pull/17663)\\n* @jason-nance made their first contribution in [PR #17660](https://github.com/BerriAI/litellm/pull/17660)\\n* @yisding made their first contribution in [PR #17671](https://github.com/BerriAI/litellm/pull/17671)\\n* @emilsvennesson made their first contribution in [PR #17656](https://github.com/BerriAI/litellm/pull/17656)\\n* @kumekay made their first contribution in [PR #17646](https://github.com/BerriAI/litellm/pull/17646)\\n* @chenzhaofei01 made their first contribution in [PR #17584](https://github.com/BerriAI/litellm/pull/17584)\\n* @shivamrawat1 made their first contribution in [PR #17733](https://github.com/BerriAI/litellm/pull/17733)\\n* @ephrimstanley made their first contribution in [PR #17723](https://github.com/BerriAI/litellm/pull/17723)\\n* @hwittenborn made their first contribution in [PR #17743](https://github.com/BerriAI/litellm/pull/17743)\\n* @peterkc made their first contribution in [PR #17727](https://github.com/BerriAI/litellm/pull/17727)\\n* @saisurya237 made their first contribution in [PR #17725](https://github.com/BerriAI/litellm/pull/17725)\\n* @Ashton-Sidhu made their first contribution in [PR #17728](https://github.com/BerriAI/litellm/pull/17728)\\n* @CyrusTC made their first contribution in [PR #17810](https://github.com/BerriAI/litellm/pull/17810)\\n* @jichmi made their first contribution in [PR #17703](https://github.com/BerriAI/litellm/pull/17703)\\n* @ryan-crabbe made their first contribution in [PR #17852](https://github.com/BerriAI/litellm/pull/17852)\\n* @nlineback made their first contribution in [PR #17851](https://github.com/BerriAI/litellm/pull/17851)\\n* @butnarurazvan made their first contribution in [PR #17468](https://github.com/BerriAI/litellm/pull/17468)\\n* @yoshi-p27 made their first contribution in [PR #17915](https://github.com/BerriAI/litellm/pull/17915)\\n\\n---\\n\\n## Full Changelog\\n\\n**[View complete changelog on GitHub](https://github.com/BerriAI/litellm/compare/v1.80.8.rc.1...v1.80.10)**"},{"id":"v1-80-8","metadata":{"permalink":"/release_notes/v1-80-8","source":"@site/release_notes/v1.80.8-stable/index.md","title":"v1.80.8-stable - Introducing A2A Agent Gateway","description":"Deploy this version","date":"2025-12-06T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.80.8-stable - Introducing A2A Agent Gateway","slug":"v1-80-8","date":"2025-12-06T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"[Preview] v1.80.10.rc.1 - Agent Gateway: Azure Foundry & Bedrock AgentCore","permalink":"/release_notes/v1-80-10"},"nextItem":{"title":"v1.80.5-stable - Gemini 3.0 Support","permalink":"/release_notes/v1-80-5"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.80.8-stable\\n```\\n\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.80.8\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights\\n\\n- **Agent Gateway (A2A)** - [Invoke agents through the AI Gateway with request/response logging and access controls](../../docs/a2a)\\n- **Guardrails API v2** - [Generic Guardrail API with streaming support, structured messages, and tool call checks](../../docs/adding_provider/generic_guardrail_api)\\n- **Customer (End User) Usage UI** - [Track and visualize end-user spend directly in the dashboard](../../docs/proxy/customer_usage)\\n- **vLLM Batch + Files API** - [Support for batch and files API with vLLM deployments](../../docs/batches)\\n- **Dynamic Rate Limiting on Teams** - [Enable dynamic rate limits and priority reservation on team-level](../../docs/proxy/team_budgets)\\n- **Google Cloud Chirp3 HD** - [New text-to-speech provider with Chirp3 HD voices](../../docs/text_to_speech)\\n\\n---\\n\\n### Agent Gateway (A2A)\\n\\n<Image \\n  img={require(\'../../img/a2a_gateway.png\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n\\n<br/>\\n\\nThis release introduces **A2A Agent Gateway** for LiteLLM, allowing you to invoke and manage A2A agents with the same controls you have for LLM APIs.\\n\\nAs a **LiteLLM Gateway Admin**, you can now do the following:\\n    - **Request/Response Logging** - Every agent invocation is logged to the Logs page with full request and response tracking.\\n    - **Access Control** - Control which Team/Key can access which agents.\\n\\nAs a developer, you can continue using the A2A SDK, all you need to do is point you `A2AClient` to the LiteLLM proxy URL and your API key.\\n\\n**Works with the A2A SDK:**\\n\\n```python\\nfrom a2a.client import A2AClient\\n\\nclient = A2AClient(\\n    base_url=\\"http://localhost:4000\\",  # Your LiteLLM proxy\\n    api_key=\\"sk-1234\\"                   # LiteLLM API key\\n)\\n\\nresponse = client.send_message(\\n    agent_id=\\"my-agent\\",\\n    message=\\"What\'s the status of my order?\\"\\n)\\n```\\n\\nGet started with Agent Gateway here: [Agent Gateway Documentation](../../docs/a2a)\\n\\n---\\n\\n### Customer (End User) Usage UI\\n\\n<Image\\nimg={require(\'../../img/customer_usage.png\')}\\nstyle={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n\\nUsers can now filter usage statistics by customers, providing the same granular filtering capabilities available for teams and organizations.\\n\\n**Details:**\\n\\n- Filter usage analytics, spend logs, and activity metrics by customer ID\\n- View customer-level breakdowns alongside existing team and user-level filters\\n- Consistent filtering experience across all usage and analytics views\\n\\n---\\n\\n## New Providers and Endpoints\\n\\n### New Providers (5 new providers)\\n\\n| Provider | Supported LiteLLM Endpoints | Description |\\n| -------- | ------------------- | ----------- |\\n| **[Z.AI (Zhipu AI)](../../docs/providers/zai)** | `/v1/chat/completions`, `/v1/responses`, `/v1/messages` | Built-in support for Zhipu AI GLM models |\\n| **[RAGFlow](../../docs/providers/ragflow)** | `/v1/chat/completions`, `/v1/responses`, `/v1/messages`, `/v1/vector_stores` | RAG-based chat completions with vector store support |\\n| **[PublicAI](../../docs/providers/publicai)** | `/v1/chat/completions`, `/v1/responses`, `/v1/messages` | OpenAI-compatible provider via JSON config |\\n| **[Google Cloud Chirp3 HD](../../docs/text_to_speech)** | `/v1/audio/speech`, `/v1/audio/speech/stream` | Text-to-speech with Google Cloud Chirp3 HD voices |\\n\\n### New LLM API Endpoints (2 new endpoints)\\n\\n| Endpoint | Method | Description | Documentation |\\n| -------- | ------ | ----------- | ------------- |\\n| `/v1/agents/invoke` | POST | Invoke A2A agents through the AI Gateway | [Agent Gateway](../../docs/a2a) |\\n| `/cursor/chat/completions` | POST | Cursor BYOK endpoint - accepts Responses API input, returns Chat Completions output | [Cursor Integration](../../docs/tutorials/cursor_integration) |\\n\\n---\\n\\n## New Models / Updated Models\\n\\n#### New Model Support (33 new models)\\n\\n| Provider | Model | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Features |\\n| -------- | ----- | -------------- | ------------------- | -------------------- | -------- |\\n| OpenAI | `gpt-5.1-codex-max` | 400K | $1.25 | $10.00 | Reasoning, vision, PDF input, responses API |\\n| Azure | `azure/gpt-5.1-codex-max` | 400K | $1.25 | $10.00 | Reasoning, vision, PDF input, responses API |\\n| Anthropic | `claude-opus-4-5` | 200K | $5.00 | $25.00 | Computer use, reasoning, vision |\\n| Bedrock | `global.anthropic.claude-opus-4-5-20251101-v1:0` | 200K | $5.00 | $25.00 | Computer use, reasoning, vision |\\n| Bedrock | `amazon.nova-2-lite-v1:0` | 1M | $0.30 | $2.50 | Reasoning, vision, video, PDF input |\\n| Bedrock | `amazon.titan-image-generator-v2:0` | - | - | $0.008/image | Image generation |\\n| Fireworks | `fireworks_ai/deepseek-v3p2` | 164K | $1.20 | $1.20 | Function calling, response schema |\\n| Fireworks | `fireworks_ai/kimi-k2-instruct-0905` | 262K | $0.60 | $2.50 | Function calling, response schema |\\n| DeepSeek | `deepseek/deepseek-v3.2` | 164K | $0.28 | $0.40 | Reasoning, function calling |\\n| Mistral | `mistral/mistral-large-3` | 256K | $0.50 | $1.50 | Function calling, vision |\\n| Azure AI | `azure_ai/mistral-large-3` | 256K | $0.50 | $1.50 | Function calling, vision |\\n| Moonshot | `moonshot/kimi-k2-0905-preview` | 262K | $0.60 | $2.50 | Function calling, web search |\\n| Moonshot | `moonshot/kimi-k2-turbo-preview` | 262K | $1.15 | $8.00 | Function calling, web search |\\n| Moonshot | `moonshot/kimi-k2-thinking-turbo` | 262K | $1.15 | $8.00 | Function calling, web search |\\n| OpenRouter | `openrouter/deepseek/deepseek-v3.2` | 164K | $0.28 | $0.40 | Reasoning, function calling |\\n| Databricks | `databricks/databricks-claude-haiku-4-5` | 200K | $1.00 | $5.00 | Reasoning, function calling |\\n| Databricks | `databricks/databricks-claude-opus-4` | 200K | $15.00 | $75.00 | Reasoning, function calling |\\n| Databricks | `databricks/databricks-claude-opus-4-1` | 200K | $15.00 | $75.00 | Reasoning, function calling |\\n| Databricks | `databricks/databricks-claude-opus-4-5` | 200K | $5.00 | $25.00 | Reasoning, function calling |\\n| Databricks | `databricks/databricks-claude-sonnet-4` | 200K | $3.00 | $15.00 | Reasoning, function calling |\\n| Databricks | `databricks/databricks-claude-sonnet-4-1` | 200K | $3.00 | $15.00 | Reasoning, function calling |\\n| Databricks | `databricks/databricks-gemini-2-5-flash` | 1M | $0.30 | $2.50 | Function calling |\\n| Databricks | `databricks/databricks-gemini-2-5-pro` | 1M | $1.25 | $10.00 | Function calling |\\n| Databricks | `databricks/databricks-gpt-5` | 400K | $1.25 | $10.00 | Function calling |\\n| Databricks | `databricks/databricks-gpt-5-1` | 400K | $1.25 | $10.00 | Function calling |\\n| Databricks | `databricks/databricks-gpt-5-mini` | 400K | $0.25 | $2.00 | Function calling |\\n| Databricks | `databricks/databricks-gpt-5-nano` | 400K | $0.05 | $0.40 | Function calling |\\n| Vertex AI | `vertex_ai/chirp` | - | $30.00/1M chars | - | Text-to-speech (Chirp3 HD) |\\n| Z.AI | `zai/glm-4.6` | 200K | $0.60 | $2.20 | Function calling |\\n| Z.AI | `zai/glm-4.5` | 128K | $0.60 | $2.20 | Function calling |\\n| Z.AI | `zai/glm-4.5v` | 128K | $0.60 | $1.80 | Function calling, vision |\\n| Z.AI | `zai/glm-4.5-flash` | 128K | Free | Free | Function calling |\\n| Vertex AI | `vertex_ai/bge-large-en-v1.5` | - | - | - | BGE Embeddings |\\n\\n#### Features\\n\\n- **[OpenAI](../../docs/providers/openai)**\\n    - Add `gpt-5.1-codex-max` model pricing and configuration - [PR #17541](https://github.com/BerriAI/litellm/pull/17541)\\n    - Add xhigh reasoning effort for gpt-5.1-codex-max - [PR #17585](https://github.com/BerriAI/litellm/pull/17585)\\n    - Add clear error message for empty LLM endpoint responses - [PR #17445](https://github.com/BerriAI/litellm/pull/17445)\\n\\n- **[Azure OpenAI](../../docs/providers/azure/azure)**\\n    - Allow reasoning_effort=\'none\' for Azure gpt-5.1 models - [PR #17311](https://github.com/BerriAI/litellm/pull/17311)\\n\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Add `claude-opus-4-5` alias to pricing data - [PR #17313](https://github.com/BerriAI/litellm/pull/17313)\\n    - Parse `<budget:thinking>` blocks for opus 4.5 - [PR #17534](https://github.com/BerriAI/litellm/pull/17534)\\n    - Update new Anthropic features as reviewed - [PR #17142](https://github.com/BerriAI/litellm/pull/17142)\\n    - Skip empty text blocks in Anthropic system messages - [PR #17442](https://github.com/BerriAI/litellm/pull/17442)\\n\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Add Nova embedding support - [PR #17253](https://github.com/BerriAI/litellm/pull/17253)\\n    - Add support for Bedrock Qwen 2 imported model - [PR #17461](https://github.com/BerriAI/litellm/pull/17461)\\n    - Bedrock OpenAI model support - [PR #17368](https://github.com/BerriAI/litellm/pull/17368)\\n    - Add support for file content download for Bedrock batches - [PR #17470](https://github.com/BerriAI/litellm/pull/17470)\\n    - Make streaming chunk size configurable in Bedrock API - [PR #17357](https://github.com/BerriAI/litellm/pull/17357)\\n    - Add experimental latest-user filtering for Bedrock - [PR #17282](https://github.com/BerriAI/litellm/pull/17282)\\n    - Handle Cohere v4 embed response dictionary format - [PR #17220](https://github.com/BerriAI/litellm/pull/17220)\\n    - Remove not compatible beta header from Bedrock - [PR #17301](https://github.com/BerriAI/litellm/pull/17301)\\n    - Add model price and details for Global Opus 4.5 Bedrock endpoint - [PR #17380](https://github.com/BerriAI/litellm/pull/17380)\\n\\n- **[Gemini (Google AI Studio + Vertex AI)](../../docs/providers/gemini)**\\n    - Add better handling in image generation for Gemini models - [PR #17292](https://github.com/BerriAI/litellm/pull/17292)\\n    - Fix reasoning_content showing duplicate content in streaming responses - [PR #17266](https://github.com/BerriAI/litellm/pull/17266)\\n    - Handle partial JSON chunks after first valid chunk - [PR #17496](https://github.com/BerriAI/litellm/pull/17496)\\n    - Fix Gemini 3 last chunk thinking block - [PR #17403](https://github.com/BerriAI/litellm/pull/17403)\\n    - Fix Gemini image_tokens treated as text tokens in cost calculation - [PR #17554](https://github.com/BerriAI/litellm/pull/17554)\\n    - Make sure that media resolution is only for Gemini 3 model - [PR #17137](https://github.com/BerriAI/litellm/pull/17137)\\n\\n- **[Vertex AI](../../docs/providers/vertex)**\\n    - Add Google Cloud Chirp3 HD support on /speech - [PR #17391](https://github.com/BerriAI/litellm/pull/17391)\\n    - Add BGE Embeddings support - [PR #17362](https://github.com/BerriAI/litellm/pull/17362)\\n    - Handle global location for Vertex AI image generation endpoint - [PR #17255](https://github.com/BerriAI/litellm/pull/17255)\\n    - Add Google Private API Endpoint to Vertex AI fields - [PR #17382](https://github.com/BerriAI/litellm/pull/17382)\\n\\n- **[Z.AI (Zhipu AI)](../../docs/providers/zai)**\\n    - Add Z.AI as built-in provider - [PR #17307](https://github.com/BerriAI/litellm/pull/17307)\\n\\n- **[GitHub Copilot](../../docs/providers/github_copilot)**\\n    - Add Embedding API support - [PR #17278](https://github.com/BerriAI/litellm/pull/17278)\\n    - Preserve encrypted_content in reasoning items for multi-turn conversations - [PR #17130](https://github.com/BerriAI/litellm/pull/17130)\\n\\n- **[Databricks](../../docs/providers/databricks)**\\n    - Update Databricks model pricing and add new models - [PR #17277](https://github.com/BerriAI/litellm/pull/17277)\\n\\n- **[OVHcloud](../../docs/providers/ovhcloud)**\\n    - Add support of audio transcription for OVHcloud - [PR #17305](https://github.com/BerriAI/litellm/pull/17305)\\n\\n- **[Mistral](../../docs/providers/mistral)**\\n    - Add Mistral Large 3 model support - [PR #17547](https://github.com/BerriAI/litellm/pull/17547)\\n\\n- **[Moonshot](../../docs/providers/moonshot)**\\n    - Fix missing Moonshot turbo models and fix incorrect pricing - [PR #17432](https://github.com/BerriAI/litellm/pull/17432)\\n\\n- **[Together AI](../../docs/providers/togetherai)**\\n    - Add context window exception mapping for Together AI - [PR #17284](https://github.com/BerriAI/litellm/pull/17284)\\n\\n- **[WatsonX](../../docs/providers/watsonx/index)**\\n    - Allow passing zen_api_key dynamically - [PR #16655](https://github.com/BerriAI/litellm/pull/16655)\\n    - Fix Watsonx Audio Transcription API - [PR #17326](https://github.com/BerriAI/litellm/pull/17326)\\n    - Fix audio transcriptions, don\'t force content type in request headers - [PR #17546](https://github.com/BerriAI/litellm/pull/17546)\\n\\n- **[Fireworks AI](../../docs/providers/fireworks_ai)**\\n    - Add new model `fireworks_ai/kimi-k2-instruct-0905` - [PR #17328](https://github.com/BerriAI/litellm/pull/17328)\\n    - Add `fireworks/deepseek-v3p2` - [PR #17395](https://github.com/BerriAI/litellm/pull/17395)\\n\\n- **[DeepSeek](../../docs/providers/deepseek)**\\n    - Support Deepseek 3.2 with Reasoning - [PR #17384](https://github.com/BerriAI/litellm/pull/17384)\\n\\n- **[Nova Lite 2](../../docs/providers/bedrock)**\\n    - Add Nova Lite 2 reasoning support with reasoningConfig - [PR #17371](https://github.com/BerriAI/litellm/pull/17371)\\n\\n- **[Ollama](../../docs/providers/ollama)**\\n    - Fix auth not working with ollama.com - [PR #17191](https://github.com/BerriAI/litellm/pull/17191)\\n\\n- **[Groq](../../docs/providers/groq)**\\n    - Fix supports_response_schema before using json_tool_call workaround - [PR #17438](https://github.com/BerriAI/litellm/pull/17438)\\n\\n- **[vLLM](../../docs/providers/vllm)**\\n    - Fix empty response + vLLM streaming - [PR #17516](https://github.com/BerriAI/litellm/pull/17516)\\n\\n- **[Azure AI](../../docs/providers/azure_ai)**\\n    - Migrate Anthropic provider to Azure AI - [PR #17202](https://github.com/BerriAI/litellm/pull/17202)\\n    - Fix GA path for Azure OpenAI realtime models - [PR #17260](https://github.com/BerriAI/litellm/pull/17260)\\n\\n- **[Bedrock TwelveLabs](../../docs/providers/bedrock#twelvelabs-pegasus---video-understanding)**\\n    - Add support for TwelveLabs Pegasus video understanding - [PR #17193](https://github.com/BerriAI/litellm/pull/17193)\\n\\n### Bug Fixes\\n\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Fix extra_headers in messages API bedrock invoke - [PR #17271](https://github.com/BerriAI/litellm/pull/17271)\\n    - Fix Bedrock models in model map - [PR #17419](https://github.com/BerriAI/litellm/pull/17419)\\n    - Make Bedrock converse messages respect modify_params as expected - [PR #17427](https://github.com/BerriAI/litellm/pull/17427)\\n    - Fix Anthropic beta headers for Bedrock imported Qwen models - [PR #17467](https://github.com/BerriAI/litellm/pull/17467)\\n    - Preserve usage from JSON response for OpenAI provider in Bedrock - [PR #17589](https://github.com/BerriAI/litellm/pull/17589)\\n\\n- **[SambaNova](../../docs/providers/sambanova)**\\n    - Fix acompletion throws error with SambaNova models - [PR #17217](https://github.com/BerriAI/litellm/pull/17217)\\n\\n- **General**\\n    - Fix AttributeError when metadata is null in request body - [PR #17306](https://github.com/BerriAI/litellm/pull/17306)\\n    - Fix 500 error for malformed request - [PR #17291](https://github.com/BerriAI/litellm/pull/17291)\\n    - Respect custom LLM provider in header - [PR #17290](https://github.com/BerriAI/litellm/pull/17290)\\n    - Replace deprecated .dict() with .model_dump() in streaming_handler - [PR #17359](https://github.com/BerriAI/litellm/pull/17359)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n\\n- **[Responses API](../../docs/response_api)**\\n    - Add cost tracking for responses API - [PR #17258](https://github.com/BerriAI/litellm/pull/17258)\\n    - Map output_tokens_details of responses API to completion_tokens_details - [PR #17458](https://github.com/BerriAI/litellm/pull/17458)\\n    - Add image generation support for Responses API - [PR #16586](https://github.com/BerriAI/litellm/pull/16586)\\n\\n- **[Batch API](../../docs/batches)**\\n    - Add vLLM batch+files API support - [PR #15823](https://github.com/BerriAI/litellm/pull/15823)\\n    - Fix optional parameter default value - [PR #17434](https://github.com/BerriAI/litellm/pull/17434)\\n    - Add status parameter as optional for FileObject - [PR #17431](https://github.com/BerriAI/litellm/pull/17431)\\n\\n- **[Video Generation API](../../docs/videos)**\\n    - Add passthrough cost tracking for Veo - [PR #17296](https://github.com/BerriAI/litellm/pull/17296)\\n\\n- **[OCR API](../../docs/ocr)**\\n    - Add missing OCR and aOCR to CallTypes enum - [PR #17435](https://github.com/BerriAI/litellm/pull/17435)\\n\\n- **General**\\n    - Support routing to only websearch supported deployments - [PR #17500](https://github.com/BerriAI/litellm/pull/17500)\\n\\n#### Bugs\\n\\n- **General**\\n    - Fix streaming error validation - [PR #17242](https://github.com/BerriAI/litellm/pull/17242)\\n    - Add length validation for empty tool_calls in delta - [PR #17523](https://github.com/BerriAI/litellm/pull/17523)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n\\n- **New Login Page**\\n    - New Login Page UI - [PR #17443](https://github.com/BerriAI/litellm/pull/17443)\\n    - Refactor /login route - [PR #17379](https://github.com/BerriAI/litellm/pull/17379)\\n    - Add auto_redirect_to_sso to UI Config - [PR #17399](https://github.com/BerriAI/litellm/pull/17399)\\n    - Add Auto Redirect to SSO to New Login Page - [PR #17451](https://github.com/BerriAI/litellm/pull/17451)\\n\\n- **Customer (End User) Usage**\\n    - Customer (end user) Usage feature - [PR #17498](https://github.com/BerriAI/litellm/pull/17498)\\n    - Customer Usage UI - [PR #17506](https://github.com/BerriAI/litellm/pull/17506)\\n    - Add Info Banner for Customer Usage - [PR #17598](https://github.com/BerriAI/litellm/pull/17598)\\n\\n- **Virtual Keys**\\n    - Standardize API Key vs Virtual Key in UI - [PR #17325](https://github.com/BerriAI/litellm/pull/17325)\\n    - Add User Alias Column to Internal User Table - [PR #17321](https://github.com/BerriAI/litellm/pull/17321)\\n    - Delete Credential Enhancements - [PR #17317](https://github.com/BerriAI/litellm/pull/17317)\\n\\n- **Models + Endpoints**\\n    - Show all credential values on Edit Credential Modal - [PR #17397](https://github.com/BerriAI/litellm/pull/17397)\\n    - Change Edit Team Models Shown to Match Create Team - [PR #17394](https://github.com/BerriAI/litellm/pull/17394)\\n    - Support Images in Compare UI - [PR #17562](https://github.com/BerriAI/litellm/pull/17562)\\n\\n- **Callbacks**\\n    - Show all callbacks on UI - [PR #16335](https://github.com/BerriAI/litellm/pull/16335)\\n    - Credentials to use React Query - [PR #17465](https://github.com/BerriAI/litellm/pull/17465)\\n\\n- **Management Routes**\\n    - Allow admin viewer to access global tag usage - [PR #17501](https://github.com/BerriAI/litellm/pull/17501)\\n    - Allow wildcard routes for nonproxy admin (SCIM) - [PR #17178](https://github.com/BerriAI/litellm/pull/17178)\\n    - Return 404 when a user is not found on /user/info - [PR #16850](https://github.com/BerriAI/litellm/pull/16850)\\n\\n- **OCI Configuration**\\n    - Enable Oracle Cloud Infrastructure configuration via UI - [PR #17159](https://github.com/BerriAI/litellm/pull/17159)\\n\\n#### Bugs\\n\\n- **UI Fixes**\\n    - Fix Request and Response Panel JSONViewer - [PR #17233](https://github.com/BerriAI/litellm/pull/17233)\\n    - Adding Button Loading States to Edit Settings - [PR #17236](https://github.com/BerriAI/litellm/pull/17236)\\n    - Fix Various Text, button state, and test changes - [PR #17237](https://github.com/BerriAI/litellm/pull/17237)\\n    - Fix Fallbacks Immediately Deleting before API resolves - [PR #17238](https://github.com/BerriAI/litellm/pull/17238)\\n    - Remove Feature Flags - [PR #17240](https://github.com/BerriAI/litellm/pull/17240)\\n    - Fix metadata tags and model name display in UI for Azure passthrough - [PR #17258](https://github.com/BerriAI/litellm/pull/17258)\\n    - Change labeling around Vertex Fields - [PR #17383](https://github.com/BerriAI/litellm/pull/17383)\\n    - Remove second scrollbar when sidebar is expanded + tooltip z index - [PR #17436](https://github.com/BerriAI/litellm/pull/17436)\\n    - Fix Select in Edit Membership Modal - [PR #17524](https://github.com/BerriAI/litellm/pull/17524)\\n    - Change useAuthorized Hook to redirect to new Login Page - [PR #17553](https://github.com/BerriAI/litellm/pull/17553)\\n\\n- **SSO**\\n    - Fix the generic SSO provider - [PR #17227](https://github.com/BerriAI/litellm/pull/17227)\\n    - Clear SSO integration for all users - [PR #17287](https://github.com/BerriAI/litellm/pull/17287)\\n    - Fix SSO users not added to Entra synced team - [PR #17331](https://github.com/BerriAI/litellm/pull/17331)\\n\\n- **Auth / JWT**\\n    - JWT Auth - Allow using regular OIDC flow with user info endpoints - [PR #17324](https://github.com/BerriAI/litellm/pull/17324)\\n    - Fix litellm user auth not passing issue - [PR #17342](https://github.com/BerriAI/litellm/pull/17342)\\n    - Add other routes in JWT auth - [PR #17345](https://github.com/BerriAI/litellm/pull/17345)\\n    - Fix new org team validate against org - [PR #17333](https://github.com/BerriAI/litellm/pull/17333)\\n    - Fix litellm_enterprise ensure imported routes exist - [PR #17337](https://github.com/BerriAI/litellm/pull/17337)\\n    - Use organization.members instead of deprecated organization field - [PR #17557](https://github.com/BerriAI/litellm/pull/17557)\\n\\n- **Organizations/Teams**\\n    - Fix organization max budget not enforced - [PR #17334](https://github.com/BerriAI/litellm/pull/17334)\\n    - Fix budget update to allow null max_budget - [PR #17545](https://github.com/BerriAI/litellm/pull/17545)\\n\\n---\\n\\n## AI Integrations (2 new integrations)\\n\\n### Logging (1 new integration)\\n\\n#### New Integration\\n\\n- **[Weave](../../docs/proxy/logging)**\\n    - Basic Weave OTEL integration - [PR #17439](https://github.com/BerriAI/litellm/pull/17439)\\n\\n#### Improvements & Fixes\\n\\n- **[DataDog](../../docs/proxy/logging#datadog)**\\n    - Fix Datadog callback regression when ddtrace is installed - [PR #17393](https://github.com/BerriAI/litellm/pull/17393)\\n\\n- **[Arize Phoenix](../../docs/observability/arize_integration)**\\n    - Fix clean arize-phoenix traces - [PR #16611](https://github.com/BerriAI/litellm/pull/16611)\\n\\n- **[MLflow](../../docs/proxy/logging#mlflow)**\\n    - Fix MLflow streaming spans for Anthropic passthrough - [PR #17288](https://github.com/BerriAI/litellm/pull/17288)\\n\\n- **[Langfuse](../../docs/proxy/logging#langfuse)**\\n    - Fix Langfuse logger test mock setup - [PR #17591](https://github.com/BerriAI/litellm/pull/17591)\\n\\n- **General**\\n    - Improve PII anonymization handling in logging callbacks - [PR #17207](https://github.com/BerriAI/litellm/pull/17207)\\n\\n### Guardrails (1 new integration)\\n\\n#### New Integration\\n\\n- **[Generic Guardrail API](../../docs/adding_provider/generic_guardrail_api)**\\n    - Generic Guardrail API - allows guardrail providers to add INSTANT support for LiteLLM w/out PR to repo - [PR #17175](https://github.com/BerriAI/litellm/pull/17175)\\n    - Guardrails API V2 - user api key metadata, session id, specify input type (request/response), image support - [PR #17338](https://github.com/BerriAI/litellm/pull/17338)\\n    - Guardrails API - add streaming support - [PR #17400](https://github.com/BerriAI/litellm/pull/17400)\\n    - Guardrails API - support tool call checks on OpenAI `/chat/completions`, OpenAI `/responses`, Anthropic `/v1/messages` - [PR #17459](https://github.com/BerriAI/litellm/pull/17459)\\n    - Guardrails API - new `structured_messages` param - [PR #17518](https://github.com/BerriAI/litellm/pull/17518)\\n    - Correctly map a v1/messages call to the anthropic unified guardrail - [PR #17424](https://github.com/BerriAI/litellm/pull/17424)\\n    - Support during_call event type for unified guardrails - [PR #17514](https://github.com/BerriAI/litellm/pull/17514)\\n\\n#### Improvements & Fixes\\n\\n- **[Noma Guardrail](../../docs/proxy/guardrails/noma_security)**\\n    - Refactor Noma guardrail to use shared Responses transformation and include system instructions - [PR #17315](https://github.com/BerriAI/litellm/pull/17315)\\n\\n- **[Presidio](../../docs/proxy/guardrails/pii_masking_v2)**\\n    - Handle empty content and error dict responses in guardrails - [PR #17489](https://github.com/BerriAI/litellm/pull/17489)\\n    - Fix Presidio guardrail test TypeError and license base64 decoding error - [PR #17538](https://github.com/BerriAI/litellm/pull/17538)\\n\\n- **[Tool Permissions](../../docs/proxy/guardrails/tool_permission)**\\n    - Add regex-based tool_name/tool_type matching for tool-permission - [PR #17164](https://github.com/BerriAI/litellm/pull/17164)\\n    - Add images for tool permission guardrail documentation - [PR #17322](https://github.com/BerriAI/litellm/pull/17322)\\n\\n- **[AIM Guardrails](../../docs/proxy/guardrails/aim_security)**\\n    - Fix AIM guardrail tests - [PR #17499](https://github.com/BerriAI/litellm/pull/17499)\\n\\n- **[Bedrock Guardrails](../../docs/proxy/guardrails/bedrock)**\\n    - Fix Bedrock Guardrail indent and import - [PR #17378](https://github.com/BerriAI/litellm/pull/17378)\\n\\n- **General Guardrails**\\n    - Mask all matching keywords in content filter - [PR #17521](https://github.com/BerriAI/litellm/pull/17521)\\n    - Ensure guardrail metadata is preserved in request_data - [PR #17593](https://github.com/BerriAI/litellm/pull/17593)\\n    - Fix apply_guardrail method and improve test isolation - [PR #17555](https://github.com/BerriAI/litellm/pull/17555)\\n\\n### Secret Managers\\n\\n- **[CyberArk](../../docs/secret_managers/cyberark)**\\n    - Allow setting SSL verify to false - [PR #17433](https://github.com/BerriAI/litellm/pull/17433)\\n\\n- **General**\\n    - Make email and secret manager operations independent in key management hooks - [PR #17551](https://github.com/BerriAI/litellm/pull/17551)\\n\\n---\\n\\n## Spend Tracking, Budgets and Rate Limiting\\n\\n- **Rate Limiting**\\n    - Parallel Request Limiter with /messages - [PR #17426](https://github.com/BerriAI/litellm/pull/17426)\\n    - Allow using dynamic rate limit/priority reservation on teams - [PR #17061](https://github.com/BerriAI/litellm/pull/17061)\\n    - Dynamic Rate Limiter - Fix token count increases/decreases by 1 instead of actual count + Redis TTL - [PR #17558](https://github.com/BerriAI/litellm/pull/17558)\\n\\n- **Spend Logs**\\n    - Deprecate `spend/logs` & add `spend/logs/v2` - [PR #17167](https://github.com/BerriAI/litellm/pull/17167)\\n    - Optimize SpendLogs queries to use timestamp filtering for index usage - [PR #17504](https://github.com/BerriAI/litellm/pull/17504)\\n\\n- **Enforce User Param**\\n    - Enforce support of enforce_user_param to OpenAI post endpoints - [PR #17407](https://github.com/BerriAI/litellm/pull/17407)\\n\\n---\\n\\n## MCP Gateway\\n\\n- **MCP Configuration**\\n    - Remove URL format validation for MCP server endpoints - [PR #17270](https://github.com/BerriAI/litellm/pull/17270)\\n    - Add stack trace to MCP error message - [PR #17269](https://github.com/BerriAI/litellm/pull/17269)\\n\\n- **MCP Tool Results**\\n    - Preserve tool metadata in CallToolResult - [PR #17561](https://github.com/BerriAI/litellm/pull/17561)\\n\\n---\\n\\n## Agent Gateway (A2A)\\n\\n- **Agent Invocation**\\n    - Allow invoking agents through AI Gateway - [PR #17440](https://github.com/BerriAI/litellm/pull/17440)\\n    - Allow tracking request/response in \\"Logs\\" Page - [PR #17449](https://github.com/BerriAI/litellm/pull/17449)\\n\\n- **Agent Access Control**\\n    - Enforce Allowed agents by key, team + add agent access groups on backend - [PR #17502](https://github.com/BerriAI/litellm/pull/17502)\\n\\n- **Agent Gateway UI**\\n    - Allow testing agents on UI - [PR #17455](https://github.com/BerriAI/litellm/pull/17455)\\n    - Set allowed agents by key, team - [PR #17511](https://github.com/BerriAI/litellm/pull/17511)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n- **Audio/Speech Performance**\\n    - Fix `/audio/speech` performance by using `shared_sessions` - [PR #16739](https://github.com/BerriAI/litellm/pull/16739)\\n\\n- **Memory Optimization**\\n    - Prevent memory leak in aiohttp connection pooling - [PR #17388](https://github.com/BerriAI/litellm/pull/17388)\\n    - Lazy-load utils to reduce memory + import time - [PR #17171](https://github.com/BerriAI/litellm/pull/17171)\\n\\n- **Database**\\n    - Update default database connection number - [PR #17353](https://github.com/BerriAI/litellm/pull/17353)\\n    - Update default proxy_batch_write_at number - [PR #17355](https://github.com/BerriAI/litellm/pull/17355)\\n    - Add background health checks to db - [PR #17528](https://github.com/BerriAI/litellm/pull/17528)\\n\\n- **Proxy Caching**\\n    - Fix proxy caching between requests in aiohttp transport - [PR #17122](https://github.com/BerriAI/litellm/pull/17122)\\n\\n- **Session Management**\\n    - Fix session consistency, move Lasso API version away from source code - [PR #17316](https://github.com/BerriAI/litellm/pull/17316)\\n    - Conditionally pass enable_cleanup_closed to aiohttp TCPConnector - [PR #17367](https://github.com/BerriAI/litellm/pull/17367)\\n\\n- **Vector Store**\\n    - Fix vector store configuration synchronization failure - [PR #17525](https://github.com/BerriAI/litellm/pull/17525)\\n\\n---\\n\\n## Documentation Updates\\n\\n- **Provider Documentation**\\n    - Add Azure AI Foundry documentation for Claude models - [PR #17104](https://github.com/BerriAI/litellm/pull/17104)\\n    - Document responses and embedding API for GitHub Copilot - [PR #17456](https://github.com/BerriAI/litellm/pull/17456)\\n    - Add gpt-5.1-codex-max to OpenAI provider documentation - [PR #17602](https://github.com/BerriAI/litellm/pull/17602)\\n    - Update Instructions For Phoenix Integration - [PR #17373](https://github.com/BerriAI/litellm/pull/17373)\\n\\n- **Guides**\\n    - Add guide on how to debug gateway error vs provider error - [PR #17387](https://github.com/BerriAI/litellm/pull/17387)\\n    - Agent Gateway documentation - [PR #17454](https://github.com/BerriAI/litellm/pull/17454)\\n    - A2A Permission management documentation - [PR #17515](https://github.com/BerriAI/litellm/pull/17515)\\n    - Update docs to link agent hub - [PR #17462](https://github.com/BerriAI/litellm/pull/17462)\\n\\n- **Projects**\\n    - Add Google ADK and Harbor to projects - [PR #17352](https://github.com/BerriAI/litellm/pull/17352)\\n    - Add Microsoft Agent Lightning to projects - [PR #17422](https://github.com/BerriAI/litellm/pull/17422)\\n\\n- **Cleanup**\\n    - Cleanup: Remove orphan docs pages and Docusaurus template files - [PR #17356](https://github.com/BerriAI/litellm/pull/17356)\\n    - Remove `source .env` from docs - [PR #17466](https://github.com/BerriAI/litellm/pull/17466)\\n\\n---\\n\\n## Infrastructure / CI/CD\\n\\n- **Helm Chart**\\n    - Add ingress-only labels - [PR #17348](https://github.com/BerriAI/litellm/pull/17348)\\n\\n- **Docker**\\n    - Add retry logic to apk package installation in Dockerfile.non_root - [PR #17596](https://github.com/BerriAI/litellm/pull/17596)\\n    - Chainguard fixes - [PR #17406](https://github.com/BerriAI/litellm/pull/17406)\\n\\n- **OpenAPI Schema**\\n    - Refactor add_schema_to_components to move definitions to components/schemas - [PR #17389](https://github.com/BerriAI/litellm/pull/17389)\\n\\n- **Security**\\n    - Fix security vulnerability: update mdast-util-to-hast to 13.2.1 - [PR #17601](https://github.com/BerriAI/litellm/pull/17601)\\n    - Bump jws from 3.2.2 to 3.2.3 - [PR #17494](https://github.com/BerriAI/litellm/pull/17494)\\n\\n---\\n\\n## New Contributors\\n\\n* @weichiet made their first contribution in [PR #17242](https://github.com/BerriAI/litellm/pull/17242)\\n* @AndyForest made their first contribution in [PR #17220](https://github.com/BerriAI/litellm/pull/17220)\\n* @omkar806 made their first contribution in [PR #17217](https://github.com/BerriAI/litellm/pull/17217)\\n* @v0rtex20k made their first contribution in [PR #17178](https://github.com/BerriAI/litellm/pull/17178)\\n* @hxomer made their first contribution in [PR #17207](https://github.com/BerriAI/litellm/pull/17207)\\n* @orgersh92 made their first contribution in [PR #17316](https://github.com/BerriAI/litellm/pull/17316)\\n* @dannykopping made their first contribution in [PR #17313](https://github.com/BerriAI/litellm/pull/17313)\\n* @rioiart made their first contribution in [PR #17333](https://github.com/BerriAI/litellm/pull/17333)\\n* @codgician made their first contribution in [PR #17278](https://github.com/BerriAI/litellm/pull/17278)\\n* @epistoteles made their first contribution in [PR #17277](https://github.com/BerriAI/litellm/pull/17277)\\n* @kothamah made their first contribution in [PR #17368](https://github.com/BerriAI/litellm/pull/17368)\\n* @flozonn made their first contribution in [PR #17371](https://github.com/BerriAI/litellm/pull/17371)\\n* @richardmcsong made their first contribution in [PR #17389](https://github.com/BerriAI/litellm/pull/17389)\\n* @matt-greathouse made their first contribution in [PR #17384](https://github.com/BerriAI/litellm/pull/17384)\\n* @mossbanay made their first contribution in [PR #17380](https://github.com/BerriAI/litellm/pull/17380)\\n* @mhielpos-asapp made their first contribution in [PR #17376](https://github.com/BerriAI/litellm/pull/17376)\\n* @Joilence made their first contribution in [PR #17367](https://github.com/BerriAI/litellm/pull/17367)\\n* @deepaktammali made their first contribution in [PR #17357](https://github.com/BerriAI/litellm/pull/17357)\\n* @axiomofjoy made their first contribution in [PR #16611](https://github.com/BerriAI/litellm/pull/16611)\\n* @DevajMody made their first contribution in [PR #17445](https://github.com/BerriAI/litellm/pull/17445)\\n* @andrewtruong made their first contribution in [PR #17439](https://github.com/BerriAI/litellm/pull/17439)\\n* @AnasAbdelR made their first contribution in [PR #17490](https://github.com/BerriAI/litellm/pull/17490)\\n* @dominicfeliton made their first contribution in [PR #17516](https://github.com/BerriAI/litellm/pull/17516)\\n* @kristianmitk made their first contribution in [PR #17504](https://github.com/BerriAI/litellm/pull/17504)\\n* @rgshr made their first contribution in [PR #17130](https://github.com/BerriAI/litellm/pull/17130)\\n* @dominicfallows made their first contribution in [PR #17489](https://github.com/BerriAI/litellm/pull/17489)\\n* @irfansofyana made their first contribution in [PR #17467](https://github.com/BerriAI/litellm/pull/17467)\\n* @GusBricker made their first contribution in [PR #17191](https://github.com/BerriAI/litellm/pull/17191)\\n* @OlivverX made their first contribution in [PR #17255](https://github.com/BerriAI/litellm/pull/17255)\\n* @withsmilo made their first contribution in [PR #17585](https://github.com/BerriAI/litellm/pull/17585)\\n\\n---\\n\\n## Full Changelog\\n\\n**[View complete changelog on GitHub](https://github.com/BerriAI/litellm/compare/v1.80.7-nightly...v1.80.8)**"},{"id":"v1-80-5","metadata":{"permalink":"/release_notes/v1-80-5","source":"@site/release_notes/v1.80.5-stable/index.md","title":"v1.80.5-stable - Gemini 3.0 Support","description":"Deploy this version","date":"2025-11-22T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.80.5-stable - Gemini 3.0 Support","slug":"v1-80-5","date":"2025-11-22T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.80.8-stable - Introducing A2A Agent Gateway","permalink":"/release_notes/v1-80-8"},"nextItem":{"title":"v1.80.0-stable - Introducing Agent Hub: Register, Publish, and Share Agents","permalink":"/release_notes/v1-80-0"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.80.5-stable\\n```\\n\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.80.5\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights\\n\\n- **Gemini 3** - [Day-0 support for Gemini 3 models with thought signatures](../../blog/gemini_3)\\n- **Prompt Management** - [Full prompt versioning support with UI for editing, testing, and version history](../../docs/proxy/litellm_prompt_management)\\n- **MCP Hub** - [Publish and discover MCP servers within your organization](../../docs/proxy/ai_hub#mcp-servers)\\n- **Model Compare UI** - [Side-by-side model comparison interface for testing](../../docs/proxy/model_compare_ui)\\n- **Batch API Spend Tracking** - [Granular spend tracking with custom metadata for batch and file creation requests](../../docs/proxy/cost_tracking#-custom-spend-log-metadata)\\n- **AWS IAM Secret Manager** - [IAM role authentication support for AWS Secret Manager](../../docs/secret_managers/aws_secret_manager#iam-role-assumption)\\n- **Logging Callback Controls** - [Admin-level controls to prevent callers from disabling logging callbacks in compliance environments](../../docs/proxy/dynamic_logging#disabling-dynamic-callback-management-enterprise)\\n- **Proxy CLI JWT Authentication** - [Enable developers to authenticate to LiteLLM AI Gateway using the Proxy CLI](../../docs/proxy/cli_sso)\\n- **Batch API Routing** - [Route batch operations to different provider accounts using model-specific credentials from your config.yaml](../../docs/batches#multi-account--model-based-routing)\\n\\n---\\n\\n### Prompt Management\\n\\n<Image \\n  img={require(\'../../img/prompt_history.png\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n\\n<br/>\\n<br/>\\n\\nThis release introduces **LiteLLM Prompt Studio** - a comprehensive prompt management solution built directly into the LiteLLM UI. Create, test, and version your prompts without leaving your browser.\\n\\nYou can now do the following on LiteLLM Prompt Studio:\\n\\n- **Create & Test Prompts**: Build prompts with developer messages (system instructions) and test them in real-time with an interactive chat interface\\n- **Dynamic Variables**: Use `{{variable_name}}` syntax to create reusable prompt templates with automatic variable detection\\n- **Version Control**: Automatic versioning for every prompt update with complete version history tracking and rollback capabilities\\n- **Prompt Studio**: Edit prompts in a dedicated studio environment with live testing and preview\\n\\n**API Integration:**\\n\\nUse your prompts in any application with simple API calls:\\n\\n```python\\nresponse = client.chat.completions.create(\\n    model=\\"gpt-4\\",\\n    extra_body={\\n        \\"prompt_id\\": \\"your-prompt-id\\",\\n        \\"prompt_version\\": 2,  # Optional: specify version\\n        \\"prompt_variables\\": {\\"name\\": \\"value\\"}  # Optional: pass variables\\n    }\\n)\\n```\\n\\nGet started here: [LiteLLM Prompt Management Documentation](../../docs/proxy/litellm_prompt_management)\\n\\n---\\n\\n### Performance \u2013 `/realtime` 182\xd7 Lower p99 Latency\\n\\nThis update reduces `/realtime` latency by removing redundant encodings on the hot path, reusing shared SSL contexts, and caching formatting strings that were being regenerated twice per request despite rarely changing.\\n\\n#### Results\\n\\n| Metric          | Before    | After     | Improvement                |\\n| --------------- | --------- | --------- | -------------------------- |\\n| Median latency  | 2,200 ms  | **59 ms** | **\u221297% (~37\xd7 faster)**     |\\n| p95 latency     | 8,500 ms  | **67 ms** | **\u221299% (~127\xd7 faster)**    |\\n| p99 latency     | 18,000 ms | **99 ms** | **\u221299% (~182\xd7 faster)**    |\\n| Average latency | 3,214 ms  | **63 ms** | **\u221298% (~51\xd7 faster)**     |\\n| RPS             | 165       | **1,207** | **+631% (~7.3\xd7 increase)** |\\n\\n\\n#### Test Setup\\n\\n| Category | Specification |\\n|----------|---------------|\\n| **Load Testing** | Locust: 1,000 concurrent users, 500 ramp-up |\\n| **System** | 4 vCPUs, 8 GB RAM, 4 workers, 4 instances |\\n| **Database** | PostgreSQL (Redis unused) |\\n| **Configuration** | [config.yaml](https://gist.github.com/AlexsanderHamir/420fb44c31c00b4f17a99588637f01ec) |\\n| **Load Script** | [no_cache_hits.py](https://gist.github.com/AlexsanderHamir/73b83ada21d9b84d4fe09665cf1745f5) |\\n\\n---\\n\\n### Model Compare UI\\n\\nNew interactive playground UI enables side-by-side comparison of multiple LLM models, making it easy to evaluate and compare model responses.\\n\\n**Features:**\\n- Compare responses from multiple models in real-time\\n- Side-by-side view with synchronized scrolling\\n- Support for all LiteLLM-supported models\\n- Cost tracking per model\\n- Response time comparison\\n- Pre-configured prompts for quick and easy testing\\n\\n**Details:**\\n\\n- **Parameterization**: Configure API keys, endpoints, models, and model parameters, as well as interaction types (chat completions, embeddings, etc.)\\n\\n- **Model Comparison**: Compare up to 3 different models simultaneously with side-by-side response views\\n\\n- **Comparison Metrics**: View detailed comparison information including:\\n\\n  - Time To First Token\\n  - Input / Output / Reasoning Tokens\\n  - Total Latency\\n  - Cost (if enabled in config)\\n\\n- **Safety Filters**: Configure and test guardrails (safety filters) directly in the playground interface\\n\\n[Get Started with Model Compare](../../docs/proxy/model_compare_ui)\\n\\n## New Providers and Endpoints\\n\\n### New Providers\\n\\n| Provider | Supported Endpoints | Description |\\n| -------- | ------------------- | ----------- |\\n| **[Docker Model Runner](../../docs/providers/docker_model_runner)** | `/v1/chat/completions` | Run LLM models in Docker containers |\\n\\n---\\n\\n## New Models / Updated Models\\n\\n#### New Model Support\\n\\n| Provider | Model | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Features |\\n| -------- | ----- | -------------- | ------------------- | -------------------- | -------- |\\n| Azure | `azure/gpt-5.1` | 272K | $1.38 | $11.00 | Reasoning, vision, PDF input, responses API |\\n| Azure | `azure/gpt-5.1-2025-11-13` | 272K | $1.38 | $11.00 | Reasoning, vision, PDF input, responses API |\\n| Azure | `azure/gpt-5.1-codex` | 272K | $1.38 | $11.00 | Responses API, reasoning, vision |\\n| Azure | `azure/gpt-5.1-codex-2025-11-13` | 272K | $1.38 | $11.00 | Responses API, reasoning, vision |\\n| Azure | `azure/gpt-5.1-codex-mini` | 272K | $0.275 | $2.20 | Responses API, reasoning, vision |\\n| Azure | `azure/gpt-5.1-codex-mini-2025-11-13` | 272K | $0.275 | $2.20 | Responses API, reasoning, vision |\\n| Azure EU | `azure/eu/gpt-5-2025-08-07` | 272K | $1.375 | $11.00 | Reasoning, vision, PDF input |\\n| Azure EU | `azure/eu/gpt-5-mini-2025-08-07` | 272K | $0.275 | $2.20 | Reasoning, vision, PDF input |\\n| Azure EU | `azure/eu/gpt-5-nano-2025-08-07` | 272K | $0.055 | $0.44 | Reasoning, vision, PDF input |\\n| Azure EU | `azure/eu/gpt-5.1` | 272K | $1.38 | $11.00 | Reasoning, vision, PDF input, responses API |\\n| Azure EU | `azure/eu/gpt-5.1-codex` | 272K | $1.38 | $11.00 | Responses API, reasoning, vision |\\n| Azure EU | `azure/eu/gpt-5.1-codex-mini` | 272K | $0.275 | $2.20 | Responses API, reasoning, vision |\\n| Gemini | `gemini-3-pro-preview` | 2M | $1.25 | $5.00 | Reasoning, vision, function calling |\\n| Gemini | `gemini-3-pro-image` | 2M | $1.25 | $5.00 | Image generation, reasoning |\\n| OpenRouter | `openrouter/deepseek/deepseek-v3p1-terminus` | 164K | $0.20 | $0.40 | Function calling, reasoning |\\n| OpenRouter | `openrouter/moonshot/kimi-k2-instruct` | 262K | $0.60 | $2.50 | Function calling, web search |\\n| OpenRouter | `openrouter/gemini/gemini-3-pro-preview` | 2M | $1.25 | $5.00 | Reasoning, vision, function calling |\\n| XAI | `xai/grok-4.1-fast` | 2M | $0.20 | $0.50 | Reasoning, function calling |\\n| Together AI | `together_ai/z-ai/glm-4.6` | 203K | $0.40 | $1.75 | Function calling, reasoning |\\n| Cerebras | `cerebras/gpt-oss-120b` | 131K | $0.60 | $0.60 | Function calling |\\n| Bedrock | `anthropic.claude-sonnet-4-5-20250929-v1:0` | 200K | $3.00 | $15.00 | Computer use, reasoning, vision |\\n\\n#### Features\\n\\n- **[Gemini (Google AI Studio + Vertex AI)](../../docs/providers/gemini)**\\n    - Add Day 0 gemini-3-pro-preview support - [PR #16719](https://github.com/BerriAI/litellm/pull/16719)\\n    - Add support for Gemini 3 Pro Image model - [PR #16938](https://github.com/BerriAI/litellm/pull/16938)\\n    - Add reasoning_content to streaming responses with tools enabled - [PR #16854](https://github.com/BerriAI/litellm/pull/16854)\\n    - Add includeThoughts=True for Gemini 3 reasoning_effort - [PR #16838](https://github.com/BerriAI/litellm/pull/16838)\\n    - Support thought signatures for Gemini 3 in responses API - [PR #16872](https://github.com/BerriAI/litellm/pull/16872)\\n    - Correct wrong system message handling for gemma - [PR #16767](https://github.com/BerriAI/litellm/pull/16767)\\n    - Gemini 3 Pro Image: capture image_tokens and support cost_per_output_image - [PR #16912](https://github.com/BerriAI/litellm/pull/16912)\\n    - Fix missing costs for gemini-2.5-flash-image - [PR #16882](https://github.com/BerriAI/litellm/pull/16882)\\n    - Gemini 3 thought signatures in tool call id - [PR #16895](https://github.com/BerriAI/litellm/pull/16895)\\n\\n- **[Azure](../../docs/providers/azure)**\\n    - Add azure gpt-5.1 models - [PR #16817](https://github.com/BerriAI/litellm/pull/16817)\\n    - Add Azure models 2025 11 to cost maps - [PR #16762](https://github.com/BerriAI/litellm/pull/16762)\\n    - Update Azure Pricing - [PR #16371](https://github.com/BerriAI/litellm/pull/16371)\\n    - Add SSML Support for Azure Text-to-Speech (AVA) - [PR #16747](https://github.com/BerriAI/litellm/pull/16747)\\n\\n- **[OpenAI](../../docs/providers/openai)**\\n    - Support GPT-5.1 reasoning.effort=\'none\' in proxy - [PR #16745](https://github.com/BerriAI/litellm/pull/16745)\\n    - Add gpt-5.1-codex and gpt-5.1-codex-mini models to documentation - [PR #16735](https://github.com/BerriAI/litellm/pull/16735)\\n    - Inherit BaseVideoConfig to enable async content response for OpenAI video - [PR #16708](https://github.com/BerriAI/litellm/pull/16708)\\n\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Add support for `strict` parameter in Anthropic tool schemas - [PR #16725](https://github.com/BerriAI/litellm/pull/16725)\\n    - Add image as url support to anthropic - [PR #16868](https://github.com/BerriAI/litellm/pull/16868)\\n    - Add thought signature support to v1/messages api - [PR #16812](https://github.com/BerriAI/litellm/pull/16812)\\n    - Anthropic - support Structured Outputs `output_format` for Claude 4.5 sonnet and Opus 4.1 - [PR #16949](https://github.com/BerriAI/litellm/pull/16949)\\n\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Haiku 4.5 correct Bedrock configs - [PR #16732](https://github.com/BerriAI/litellm/pull/16732)\\n    - Ensure consistent chunk IDs in Bedrock streaming responses - [PR #16596](https://github.com/BerriAI/litellm/pull/16596)\\n    - Add Claude 4.5 to US Gov Cloud - [PR #16957](https://github.com/BerriAI/litellm/pull/16957)\\n    - Fix images being dropped from tool results for bedrock - [PR #16492](https://github.com/BerriAI/litellm/pull/16492)\\n\\n- **[Vertex AI](../../docs/providers/vertex)**\\n    - Add Vertex AI Image Edit Support - [PR #16828](https://github.com/BerriAI/litellm/pull/16828)\\n    - Update veo 3 pricing and add prod models - [PR #16781](https://github.com/BerriAI/litellm/pull/16781)\\n    - Fix Video download for veo3 - [PR #16875](https://github.com/BerriAI/litellm/pull/16875)\\n\\n- **[Snowflake](../../docs/providers/snowflake)**\\n    - Snowflake provider support: added embeddings, PAT, account_id - [PR #15727](https://github.com/BerriAI/litellm/pull/15727)\\n\\n- **[OCI](../../docs/providers/oci)**\\n    - Add oci_endpoint_id Parameter for OCI Dedicated Endpoints - [PR #16723](https://github.com/BerriAI/litellm/pull/16723)\\n\\n- **[XAI](../../docs/providers/xai)**\\n    - Add support for Grok 4.1 Fast models - [PR #16936](https://github.com/BerriAI/litellm/pull/16936)\\n\\n- **[Together AI](../../docs/providers/togetherai)**\\n    - Add GLM 4.6 from together.ai - [PR #16942](https://github.com/BerriAI/litellm/pull/16942)\\n\\n- **[Cerebras](../../docs/providers/cerebras)**\\n    - Fix Cerebras GPT-OSS-120B model name - [PR #16939](https://github.com/BerriAI/litellm/pull/16939)\\n\\n### Bug Fixes\\n\\n- **[OpenAI](../../docs/providers/openai)**\\n    - Fix for 16863 - openai conversion from responses to completions - [PR #16864](https://github.com/BerriAI/litellm/pull/16864)\\n    - Revert \\"Make all gpt-5 and reasoning models to responses by default\\" - [PR #16849](https://github.com/BerriAI/litellm/pull/16849)\\n\\n- **General**\\n    - Get custom_llm_provider from query param - [PR #16731](https://github.com/BerriAI/litellm/pull/16731)\\n    - Fix optional param mapping - [PR #16852](https://github.com/BerriAI/litellm/pull/16852)\\n    - Add None check for litellm_params - [PR #16754](https://github.com/BerriAI/litellm/pull/16754)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n\\n- **[Responses API](../../docs/response_api)**\\n    - Add Responses API support for gpt-5.1-codex model - [PR #16845](https://github.com/BerriAI/litellm/pull/16845)\\n    - Add managed files support for responses API - [PR #16733](https://github.com/BerriAI/litellm/pull/16733)\\n    - Add extra_body support for response supported api params from chat completion - [PR #16765](https://github.com/BerriAI/litellm/pull/16765)\\n\\n- **[Batch API](../../docs/batches)**\\n    - Support /delete for files + support /cancel for batches - [PR #16387](https://github.com/BerriAI/litellm/pull/16387)\\n    - Add config based routing support for batches and files - [PR #16872](https://github.com/BerriAI/litellm/pull/16872)\\n    - Populate spend_logs_metadata in batch and files endpoints - [PR #16921](https://github.com/BerriAI/litellm/pull/16921)\\n\\n- **[Search APIs](../../docs/search)**\\n    - Search APIs - error in firecrawl-search \\"Invalid request body\\" - [PR #16943](https://github.com/BerriAI/litellm/pull/16943)\\n\\n- **[Vector Stores](../../docs/vector_stores)**\\n    - Fix vector store create issue - [PR #16804](https://github.com/BerriAI/litellm/pull/16804)\\n    - Team vector-store permissions now respected for key access - [PR #16639](https://github.com/BerriAI/litellm/pull/16639)\\n\\n- **[Audio Transcription](../../docs/audio_transcription)**\\n    - Fix audio transcription cost tracking - [PR #16478](https://github.com/BerriAI/litellm/pull/16478)\\n    - Add missing shared_sessions to audio/transcriptions - [PR #16858](https://github.com/BerriAI/litellm/pull/16858)\\n\\n- **[Video Generation API](../../docs/video_generation)**\\n    - Fix videos tagging - [PR #16770](https://github.com/BerriAI/litellm/pull/16770)\\n\\n#### Bugs\\n\\n- **General**\\n    - Responses API cost tracking with custom deployment names - [PR #16778](https://github.com/BerriAI/litellm/pull/16778)\\n    - Trim logged response strings in spend-logs - [PR #16654](https://github.com/BerriAI/litellm/pull/16654)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n\\n- **Proxy CLI Auth**\\n    - Allow using JWTs for signing in with Proxy CLI - [PR #16756](https://github.com/BerriAI/litellm/pull/16756)\\n\\n- **Virtual Keys**\\n    - Fix Key Model Alias Not Working - [PR #16896](https://github.com/BerriAI/litellm/pull/16896)\\n\\n- **Models + Endpoints**\\n    - Add additional model settings to chat models in test key - [PR #16793](https://github.com/BerriAI/litellm/pull/16793)\\n    - Deactivate delete button on model table for config models - [PR #16787](https://github.com/BerriAI/litellm/pull/16787)\\n    - Change Public Model Hub to use proxyBaseUrl - [PR #16892](https://github.com/BerriAI/litellm/pull/16892)\\n    - Add JSON Viewer to request/response panel - [PR #16687](https://github.com/BerriAI/litellm/pull/16687)\\n    - Standarize icon images - [PR #16837](https://github.com/BerriAI/litellm/pull/16837)\\n\\n- **Teams**\\n    - Teams table empty state - [PR #16738](https://github.com/BerriAI/litellm/pull/16738)\\n\\n- **Fallbacks**\\n    - Fallbacks icon button tooltips and delete with friction - [PR #16737](https://github.com/BerriAI/litellm/pull/16737)\\n\\n- **MCP Servers**\\n    - Delete user and MCP Server Modal, MCP Table Tooltips - [PR #16751](https://github.com/BerriAI/litellm/pull/16751)\\n\\n- **Callbacks**\\n    - Expose backend endpoint for callbacks settings - [PR #16698](https://github.com/BerriAI/litellm/pull/16698)\\n    - Edit add callbacks route to use data from backend - [PR #16699](https://github.com/BerriAI/litellm/pull/16699)\\n\\n- **Usage & Analytics**\\n    - Allow partial matches for user ID in User Table - [PR #16952](https://github.com/BerriAI/litellm/pull/16952)\\n\\n- **General UI**\\n    - Allow setting base_url in API reference docs - [PR #16674](https://github.com/BerriAI/litellm/pull/16674)\\n    - Change /public fields to honor server root path - [PR #16930](https://github.com/BerriAI/litellm/pull/16930)\\n    - Correct ui build - [PR #16702](https://github.com/BerriAI/litellm/pull/16702)\\n    - Enable automatic dark/light mode based on system preference - [PR #16748](https://github.com/BerriAI/litellm/pull/16748)\\n\\n#### Bugs\\n\\n- **UI Fixes**\\n    - Fix flaky tests due to antd Notification Manager - [PR #16740](https://github.com/BerriAI/litellm/pull/16740)\\n    - Fix UI MCP Tool Test Regression - [PR #16695](https://github.com/BerriAI/litellm/pull/16695)\\n    - Fix edit logging settings not appearing - [PR #16798](https://github.com/BerriAI/litellm/pull/16798)\\n    - Add css to truncate long request ids in request viewer - [PR #16665](https://github.com/BerriAI/litellm/pull/16665)\\n    - Remove azure/ prefix in Placeholder for Azure in Add Model - [PR #16597](https://github.com/BerriAI/litellm/pull/16597)\\n    - Remove UI Session Token from user/info return - [PR #16851](https://github.com/BerriAI/litellm/pull/16851)\\n    - Remove console logs and errors from model tab - [PR #16455](https://github.com/BerriAI/litellm/pull/16455)\\n    - Change Bulk Invite User Roles to Match Backend - [PR #16906](https://github.com/BerriAI/litellm/pull/16906)\\n    - Mock Tremor\'s Tooltip to Fix Flaky UI Tests - [PR #16786](https://github.com/BerriAI/litellm/pull/16786)\\n    - Fix e2e ui playwright test - [PR #16799](https://github.com/BerriAI/litellm/pull/16799)\\n    - Fix Tests in CI/CD - [PR #16972](https://github.com/BerriAI/litellm/pull/16972)\\n\\n- **SSO**\\n    - Ensure `role` from SSO provider is used when a user is inserted onto LiteLLM - [PR #16794](https://github.com/BerriAI/litellm/pull/16794)\\n    - Docs - SSO - Manage User Roles via Azure App Roles - [PR #16796](https://github.com/BerriAI/litellm/pull/16796)\\n\\n- **Auth**\\n    - Ensure Team Tags works when using JWT Auth - [PR #16797](https://github.com/BerriAI/litellm/pull/16797)\\n    - Fix key never expires - [PR #16692](https://github.com/BerriAI/litellm/pull/16692)\\n\\n- **Swagger UI**\\n    - Fixes Swagger UI resolver errors for chat completion endpoints caused by Pydantic v2 `$defs` not being properly exposed in the OpenAPI schema - [PR #16784](https://github.com/BerriAI/litellm/pull/16784)\\n\\n---\\n\\n## AI Integrations\\n\\n### Logging\\n\\n- **[Arize Phoenix](../../docs/observability/arize_phoenix)**\\n    - Fix arize phoenix logging - [PR #16301](https://github.com/BerriAI/litellm/pull/16301)\\n    - Arize Phoenix - root span logging - [PR #16949](https://github.com/BerriAI/litellm/pull/16949)\\n\\n- **[Langfuse](../../docs/proxy/logging#langfuse)**\\n    - Filter secret fields form Langfuse - [PR #16842](https://github.com/BerriAI/litellm/pull/16842)\\n\\n- **General**\\n    - Exclude litellm_credential_name from Sensitive Data Masker (Updated) - [PR #16958](https://github.com/BerriAI/litellm/pull/16958)\\n    - Allow admins to disable, dynamic callback controls - [PR #16750](https://github.com/BerriAI/litellm/pull/16750)\\n\\n### Guardrails\\n\\n- **[IBM Guardrails](../../docs/proxy/guardrails)**\\n    - Fix IBM Guardrails optional params, add extra_headers field - [PR #16771](https://github.com/BerriAI/litellm/pull/16771)\\n\\n- **[Noma Guardrail](../../docs/proxy/guardrails)**\\n    - Use LiteLLM key alias as fallback Noma applicationId in NomaGuardrail - [PR #16832](https://github.com/BerriAI/litellm/pull/16832)\\n    - Allow custom violation message for tool-permission guardrail - [PR #16916](https://github.com/BerriAI/litellm/pull/16916)\\n\\n- **[Grayswan Guardrail](../../docs/proxy/guardrails)**\\n    - Grayswan guardrail passthrough on flagged - [PR #16891](https://github.com/BerriAI/litellm/pull/16891)\\n\\n- **General Guardrails**\\n    - Fix prompt injection not working - [PR #16701](https://github.com/BerriAI/litellm/pull/16701)\\n\\n### Prompt Management\\n\\n- **[Prompt Management](../../docs/proxy/prompt_management)**\\n    - Allow specifying just prompt_id in a request to a model - [PR #16834](https://github.com/BerriAI/litellm/pull/16834)\\n    - Add support for versioning prompts - [PR #16836](https://github.com/BerriAI/litellm/pull/16836)\\n    - Allow storing prompt version in DB - [PR #16848](https://github.com/BerriAI/litellm/pull/16848)\\n    - Add UI for editing the prompts - [PR #16853](https://github.com/BerriAI/litellm/pull/16853)\\n    - Allow testing prompts with Chat UI - [PR #16898](https://github.com/BerriAI/litellm/pull/16898)\\n    - Allow viewing version history - [PR #16901](https://github.com/BerriAI/litellm/pull/16901)\\n    - Allow specifying prompt version in code - [PR #16929](https://github.com/BerriAI/litellm/pull/16929)\\n    - UI, allow seeing model, prompt id for Prompt - [PR #16932](https://github.com/BerriAI/litellm/pull/16932)\\n    - Show \\"get code\\" section for prompt management + minor polish of showing version history - [PR #16941](https://github.com/BerriAI/litellm/pull/16941)\\n\\n### Secret Managers\\n\\n- **[AWS Secrets Manager](../../docs/secret_managers)**\\n    - Adds IAM role assumption support for AWS Secret Manager - [PR #16887](https://github.com/BerriAI/litellm/pull/16887)\\n\\n---\\n\\n## MCP Gateway\\n\\n- **MCP Hub** - Publish/discover MCP Servers within a company - [PR #16857](https://github.com/BerriAI/litellm/pull/16857)\\n- **MCP Resources** - MCP resources support - [PR #16800](https://github.com/BerriAI/litellm/pull/16800)\\n- **MCP OAuth** - Docs - mcp oauth flow details - [PR #16742](https://github.com/BerriAI/litellm/pull/16742)\\n- **MCP Lifecycle** - Drop MCPClient.connect and use run_with_session lifecycle - [PR #16696](https://github.com/BerriAI/litellm/pull/16696)\\n- **MCP Server IDs** - Add mcp server ids - [PR #16904](https://github.com/BerriAI/litellm/pull/16904)\\n- **MCP URL Format** - Fix mcp url format - [PR #16940](https://github.com/BerriAI/litellm/pull/16940)\\n\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n- **Realtime Endpoint Performance** - Fix bottlenecks degrading realtime endpoint performance - [PR #16670](https://github.com/BerriAI/litellm/pull/16670)\\n- **SSL Context Caching** - Cache SSL contexts to prevent excessive memory allocation - [PR #16955](https://github.com/BerriAI/litellm/pull/16955)\\n- **Cache Optimization** - Fix cache cooldown key generation - [PR #16954](https://github.com/BerriAI/litellm/pull/16954)\\n- **Router Cache** - Fix routing for requests with same cacheable prefix but different user messages - [PR #16951](https://github.com/BerriAI/litellm/pull/16951)\\n- **Redis Event Loop** - Fix redis event loop closed at first call - [PR #16913](https://github.com/BerriAI/litellm/pull/16913)\\n- **Dependency Management** - Upgrade pydantic to version 2.11.0 - [PR #16909](https://github.com/BerriAI/litellm/pull/16909)\\n\\n---\\n\\n## Documentation Updates\\n\\n- **Provider Documentation**\\n    - Add missing details to benchmark comparison - [PR #16690](https://github.com/BerriAI/litellm/pull/16690)\\n    - Fix anthropic pass-through endpoint - [PR #16883](https://github.com/BerriAI/litellm/pull/16883)\\n    - Cleanup repo and improve AI docs - [PR #16775](https://github.com/BerriAI/litellm/pull/16775)\\n\\n- **API Documentation**\\n    - Add docs related to openai metadata - [PR #16872](https://github.com/BerriAI/litellm/pull/16872)\\n    - Update docs with all supported endpoints and cost tracking - [PR #16872](https://github.com/BerriAI/litellm/pull/16872)\\n\\n- **General Documentation**\\n    - Add mini-swe-agent to Projects built on LiteLLM - [PR #16971](https://github.com/BerriAI/litellm/pull/16971)\\n\\n---\\n\\n## Infrastructure / CI/CD\\n\\n- **UI Testing**\\n    - Break e2e_ui_testing into build, unit, and e2e steps - [PR #16783](https://github.com/BerriAI/litellm/pull/16783)\\n    - Building UI for Testing - [PR #16968](https://github.com/BerriAI/litellm/pull/16968)\\n    - CI/CD Fixes - [PR #16937](https://github.com/BerriAI/litellm/pull/16937)\\n\\n- **Dependency Management**\\n    - Bump js-yaml from 3.14.1 to 3.14.2 in /tests/proxy_admin_ui_tests/ui_unit_tests - [PR #16755](https://github.com/BerriAI/litellm/pull/16755)\\n    - Bump js-yaml from 3.14.1 to 3.14.2 - [PR #16802](https://github.com/BerriAI/litellm/pull/16802)\\n\\n- **Migration**\\n    - Migration job labels - [PR #16831](https://github.com/BerriAI/litellm/pull/16831)\\n\\n- **Config**\\n    - This yaml actually works - [PR #16757](https://github.com/BerriAI/litellm/pull/16757)\\n\\n- **Release Notes**\\n    - Add perf improvements on embeddings to release notes - [PR #16697](https://github.com/BerriAI/litellm/pull/16697)\\n    - Docs - v1.80.0 - [PR #16694](https://github.com/BerriAI/litellm/pull/16694)\\n\\n- **Investigation**\\n    - Investigate issue root cause - [PR #16859](https://github.com/BerriAI/litellm/pull/16859)\\n\\n---\\n\\n## New Contributors\\n\\n* @mattmorgis made their first contribution in [PR #16371](https://github.com/BerriAI/litellm/pull/16371)\\n* @mmandic-coatue made their first contribution in [PR #16732](https://github.com/BerriAI/litellm/pull/16732)\\n* @Bradley-Butcher made their first contribution in [PR #16725](https://github.com/BerriAI/litellm/pull/16725)\\n* @BenjaminLevy made their first contribution in [PR #16757](https://github.com/BerriAI/litellm/pull/16757)\\n* @CatBraaain made their first contribution in [PR #16767](https://github.com/BerriAI/litellm/pull/16767)\\n* @tushar8408 made their first contribution in [PR #16831](https://github.com/BerriAI/litellm/pull/16831)\\n* @nbsp1221 made their first contribution in [PR #16845](https://github.com/BerriAI/litellm/pull/16845)\\n* @idola9 made their first contribution in [PR #16832](https://github.com/BerriAI/litellm/pull/16832)\\n* @nkukard made their first contribution in [PR #16864](https://github.com/BerriAI/litellm/pull/16864)\\n* @alhuang10 made their first contribution in [PR #16852](https://github.com/BerriAI/litellm/pull/16852)\\n* @sebslight made their first contribution in [PR #16838](https://github.com/BerriAI/litellm/pull/16838)\\n* @TsurumaruTsuyoshi made their first contribution in [PR #16905](https://github.com/BerriAI/litellm/pull/16905)\\n* @cyberjunk made their first contribution in [PR #16492](https://github.com/BerriAI/litellm/pull/16492)\\n* @colinlin-stripe made their first contribution in [PR #16895](https://github.com/BerriAI/litellm/pull/16895)\\n* @sureshdsk made their first contribution in [PR #16883](https://github.com/BerriAI/litellm/pull/16883)\\n* @eiliyaabedini made their first contribution in [PR #16875](https://github.com/BerriAI/litellm/pull/16875)\\n* @justin-tahara made their first contribution in [PR #16957](https://github.com/BerriAI/litellm/pull/16957)\\n* @wangsoft made their first contribution in [PR #16913](https://github.com/BerriAI/litellm/pull/16913)\\n* @dsduenas made their first contribution in [PR #16891](https://github.com/BerriAI/litellm/pull/16891)\\n\\n---\\n\\n## Known Issues\\n* `/audit` and `/user/available_users` routes return 404. Fixed in [PR #17337](https://github.com/BerriAI/litellm/pull/17337)\\n\\n---\\n\\n## Full Changelog\\n\\n**[View complete changelog on GitHub](https://github.com/BerriAI/litellm/compare/v1.80.0-nightly...v1.80.5.rc.2)**"},{"id":"v1-80-0","metadata":{"permalink":"/release_notes/v1-80-0","source":"@site/release_notes/v1.80.0-stable/index.md","title":"v1.80.0-stable - Introducing Agent Hub: Register, Publish, and Share Agents","description":"Deploy this version","date":"2025-11-15T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.80.0-stable - Introducing Agent Hub: Register, Publish, and Share Agents","slug":"v1-80-0","date":"2025-11-15T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.80.5-stable - Gemini 3.0 Support","permalink":"/release_notes/v1-80-5"},"nextItem":{"title":"v1.79.3-stable - Built-in Guardrails on AI Gateway","permalink":"/release_notes/v1-79-3"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.80.0-stable\\n```\\n\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.80.0\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights\\n\\n- **\ud83c\udd95 Agent Hub Support** - Register and make agents public for your organization\\n- **RunwayML Provider** - Complete video generation, image generation, and text-to-speech support\\n- **GPT-5.1 Family Support** - Day-0 support for OpenAI\'s latest GPT-5.1 and GPT-5.1-Codex models\\n- **Prometheus OSS** - Prometheus metrics now available in open-source version\\n- **Vector Store Files API** - Complete OpenAI-compatible Vector Store Files API with full CRUD operations\\n- **Embeddings Performance** - O(1) lookup optimization for router embeddings with shared sessions\\n\\n---\\n\\n### Agent Hub \\n\\n<Image img={require(\'../../img/agent_hub_clean.png\')} />  \\n\\nThis release adds support for registering and making agents public for your organization. This is great for **Proxy Admins** who want a central place to make agents built in their organization, discoverable to their users. \\n\\nHere\'s the flow: \\n1. Add agent to litellm. \\n2. Make it public. \\n3. Allow anyone to discover it on the public AI Hub page.\\n\\n[**Get Started with Agent Hub**](../../docs/proxy/ai_hub)\\n\\n\\n### Performance \u2013 `/embeddings` 13\xd7 Lower p95 Latency\\n\\nThis update significantly improves `/embeddings` latency by routing it through the same optimized pipeline as `/chat/completions`, benefiting from all previously applied networking optimizations.\\n\\n### Results\\n\\n| Metric | Before | After | Improvement |\\n| --- | --- | --- | --- |\\n| p95 latency | 5,700 ms | **430 ms** | \u221292% (~13\xd7 faster)** |\\n| p99 latency | 7,200 ms | **780 ms** | \u221289% |\\n| Average latency | 844 ms | **262 ms** | \u221269% |\\n| Median latency | 290 ms | **230 ms** | \u221221% |\\n| RPS | 1,216.7 | **1,219.7** | **+0.25%** |\\n\\n### Test Setup\\n\\n| Category | Specification |\\n| --- | --- |\\n| **Load Testing** | Locust: 1,000 concurrent users, 500 ramp-up |\\n| **System** | 4 vCPUs, 8 GB RAM, 4 workers, 4 instances |\\n| **Database** | PostgreSQL (Redis unused) |\\n| **Configuration** | [config.yaml](https://gist.github.com/AlexsanderHamir/550791675fd752befcac6a9e44024652) |\\n| **Load Script** | [no_cache_hits.py](https://gist.github.com/AlexsanderHamir/99d673bf74cdd81fd39f59fa9048f2e8) |\\n\\n---\\n\\n### \ud83c\udd95 RunwayML\\n\\nComplete integration for RunwayML\'s Gen-4 family of models, supporting video generation, image generation, and text-to-speech.\\n\\n**Supported Endpoints:**\\n- `/v1/videos` - Video generation (Gen-4 Turbo, Gen-4 Aleph, Gen-3A Turbo)\\n- `/v1/images/generations` - Image generation (Gen-4 Image, Gen-4 Image Turbo)\\n- `/v1/audio/speech` - Text-to-speech (ElevenLabs Multilingual v2)\\n\\n**Quick Start:**\\n\\n```bash showLineNumbers title=\\"Generate Video with RunwayML\\"\\ncurl --location \'http://localhost:4000/v1/videos\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--header \'Authorization: Bearer sk-1234\' \\\\\\n--data \'{\\n    \\"model\\": \\"runwayml/gen4_turbo\\",\\n    \\"prompt\\": \\"A high quality demo video of litellm ai gateway\\",\\n    \\"input_reference\\": \\"https://example.com/image.jpg\\",\\n    \\"seconds\\": 5,\\n    \\"size\\": \\"1280x720\\"\\n}\'\\n```\\n\\n[Get Started with RunwayML](../../docs/providers/runwayml/videos)\\n\\n---\\n\\n### Prometheus Metrics - Open Source\\n\\nPrometheus metrics are now available in the open-source version of LiteLLM, providing comprehensive observability for your AI Gateway without requiring an enterprise license.\\n\\n**Quick Start:**\\n\\n```yaml\\nlitellm_settings:\\n  success_callback: [\\"prometheus\\"]\\n  failure_callback: [\\"prometheus\\"]\\n```\\n\\n[Get Started with Prometheus](../../docs/proxy/logging#prometheus)\\n\\n---\\n\\n### Vector Store Files API\\n\\nComplete OpenAI-compatible Vector Store Files API now stable, enabling full file lifecycle management within vector stores.\\n\\n**Supported Endpoints:**\\n- `POST /v1/vector_stores/{vector_store_id}/files` - Create vector store file\\n- `GET /v1/vector_stores/{vector_store_id}/files` - List vector store files\\n- `GET /v1/vector_stores/{vector_store_id}/files/{file_id}` - Retrieve vector store file\\n- `GET /v1/vector_stores/{vector_store_id}/files/{file_id}/content` - Retrieve file content\\n- `DELETE /v1/vector_stores/{vector_store_id}/files/{file_id}` - Delete vector store file\\n- `DELETE /v1/vector_stores/{vector_store_id}` - Delete vector store\\n\\n**Quick Start:**\\n\\n```bash showLineNumbers title=\\"Create Vector Store File\\"\\ncurl --location \'http://localhost:4000/v1/vector_stores/vs_123/files\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--header \'Authorization: Bearer sk-1234\' \\\\\\n--data \'{\\n    \\"file_id\\": \\"file_abc\\"\\n}\'\\n```\\n\\n[Get Started with Vector Stores](../../docs/vector_store_files)\\n\\n---\\n\\n## New Providers and Endpoints\\n\\n### New Providers\\n\\n| Provider | Supported Endpoints | Description |\\n| -------- | ------------------- | ----------- |\\n| **[RunwayML](../../docs/providers/runwayml/videos)** | `/v1/videos`, `/v1/images/generations`, `/v1/audio/speech` | Gen-4 video generation, image generation, and text-to-speech |\\n\\n### New LLM API Endpoints\\n\\n| Endpoint | Method | Description | Documentation |\\n| -------- | ------ | ----------- | ------------- |\\n| `/v1/vector_stores/{vector_store_id}/files` | POST | Create vector store file | [Docs](../../docs/vector_store_files) |\\n| `/v1/vector_stores/{vector_store_id}/files` | GET | List vector store files | [Docs](../../docs/vector_store_files) |\\n| `/v1/vector_stores/{vector_store_id}/files/{file_id}` | GET | Retrieve vector store file | [Docs](../../docs/vector_store_files) |\\n| `/v1/vector_stores/{vector_store_id}/files/{file_id}/content` | GET | Retrieve file content | [Docs](../../docs/vector_store_files) |\\n| `/v1/vector_stores/{vector_store_id}/files/{file_id}` | DELETE | Delete vector store file | [Docs](../../docs/vector_store_files) |\\n| `/v1/vector_stores/{vector_store_id}` | DELETE | Delete vector store | [Docs](../../docs/vector_store_files) |\\n\\n---\\n\\n## New Models / Updated Models\\n\\n#### New Model Support\\n\\n| Provider | Model | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Features |\\n| -------- | ----- | -------------- | ------------------- | -------------------- | -------- |\\n| OpenAI | `gpt-5.1` | 272K | $1.25 | $10.00 | Reasoning, vision, PDF input, responses API |\\n| OpenAI | `gpt-5.1-2025-11-13` | 272K | $1.25 | $10.00 | Reasoning, vision, PDF input, responses API |\\n| OpenAI | `gpt-5.1-chat-latest` | 128K | $1.25 | $10.00 | Reasoning, vision, PDF input |\\n| OpenAI | `gpt-5.1-codex` | 272K | $1.25 | $10.00 | Responses API, reasoning, vision |\\n| OpenAI | `gpt-5.1-codex-mini` | 272K | $0.25 | $2.00 | Responses API, reasoning, vision |\\n| Moonshot | `moonshot/kimi-k2-thinking` | 262K | $0.60 | $2.50 | Function calling, web search, reasoning |\\n| Mistral | `mistral/magistral-medium-2509` | 40K | $2.00 | $5.00 | Reasoning, function calling |\\n| Vertex AI | `vertex_ai/moonshotai/kimi-k2-thinking-maas` | 256K | $0.60 | $2.50 | Function calling, web search |\\n| OpenRouter | `openrouter/deepseek/deepseek-v3.2-exp` | 164K | $0.20 | $0.40 | Function calling, prompt caching |\\n| OpenRouter | `openrouter/minimax/minimax-m2` | 205K | $0.26 | $1.02 | Function calling, reasoning |\\n| OpenRouter | `openrouter/z-ai/glm-4.6` | 203K | $0.40 | $1.75 | Function calling, reasoning |\\n| OpenRouter | `openrouter/z-ai/glm-4.6:exacto` | 203K | $0.45 | $1.90 | Function calling, reasoning |\\n| Voyage | `voyage/voyage-3.5` | 32K | $0.06 | - | Embeddings |\\n| Voyage | `voyage/voyage-3.5-lite` | 32K | $0.02 | - | Embeddings |\\n\\n#### Video Generation Models\\n\\n| Provider | Model | Cost Per Second | Resolutions | Features |\\n| -------- | ----- | --------------- | ----------- | -------- |\\n| RunwayML | `runwayml/gen4_turbo` | $0.05 | 1280x720, 720x1280 | Text + image to video |\\n| RunwayML | `runwayml/gen4_aleph` | $0.15 | 1280x720, 720x1280 | Text + image to video |\\n| RunwayML | `runwayml/gen3a_turbo` | $0.05 | 1280x720, 720x1280 | Text + image to video |\\n\\n#### Image Generation Models\\n\\n| Provider | Model | Cost Per Image | Resolutions | Features |\\n| -------- | ----- | -------------- | ----------- | -------- |\\n| RunwayML | `runwayml/gen4_image` | $0.05 | 1280x720, 1920x1080 | Text + image to image |\\n| RunwayML | `runwayml/gen4_image_turbo` | $0.02 | 1280x720, 1920x1080 | Text + image to image |\\n| Fal.ai | `fal_ai/fal-ai/flux-pro/v1.1` | $0.04/image | - | Image generation |\\n| Fal.ai | `fal_ai/fal-ai/flux/schnell` | $0.003/image | - | Fast image generation |\\n| Fal.ai | `fal_ai/fal-ai/bytedance/seedream/v3/text-to-image` | $0.03/image | - | Image generation |\\n| Fal.ai | `fal_ai/fal-ai/bytedance/dreamina/v3.1/text-to-image` | $0.03/image | - | Image generation |\\n| Fal.ai | `fal_ai/fal-ai/ideogram/v3` | $0.06/image | - | Image generation |\\n| Fal.ai | `fal_ai/fal-ai/imagen4/preview/fast` | $0.02/image | - | Fast image generation |\\n| Fal.ai | `fal_ai/fal-ai/imagen4/preview/ultra` | $0.06/image | - | High-quality image generation |\\n\\n#### Audio Models\\n\\n| Provider | Model | Cost | Features |\\n| -------- | ----- | ---- | -------- |\\n| RunwayML | `runwayml/eleven_multilingual_v2` | $0.0003/char | Text-to-speech |\\n\\n#### Features\\n\\n- **[OpenAI](../../docs/providers/openai)**\\n    - Add GPT-5.1 family support with reasoning capabilities - [PR #16598](https://github.com/BerriAI/litellm/pull/16598)\\n    - Add support for `reasoning_effort=\'none\'` for GPT-5.1 - [PR #16658](https://github.com/BerriAI/litellm/pull/16658)\\n    - Add `verbosity` parameter support for GPT-5 family models - [PR #16660](https://github.com/BerriAI/litellm/pull/16660)\\n    - Fix forward OpenAI organization for image generation - [PR #16607](https://github.com/BerriAI/litellm/pull/16607)\\n\\n- **[Gemini (Google AI Studio + Vertex AI)](../../docs/providers/gemini)**\\n    - Add support for `reasoning_effort=\'none\'` for Gemini models - [PR #16548](https://github.com/BerriAI/litellm/pull/16548)\\n    - Add all Gemini image models support in image generation - [PR #16526](https://github.com/BerriAI/litellm/pull/16526)\\n    - Add Gemini image edit support - [PR #16430](https://github.com/BerriAI/litellm/pull/16430)\\n    - Fix preserve non-ASCII characters in function call arguments - [PR #16550](https://github.com/BerriAI/litellm/pull/16550)\\n    - Fix Gemini conversation format issue with MCP auto-execution - [PR #16592](https://github.com/BerriAI/litellm/pull/16592)\\n\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Add support for filtering knowledge base queries - [PR #16543](https://github.com/BerriAI/litellm/pull/16543)\\n    - Ensure correct `aws_region` is used when provided dynamically for embeddings - [PR #16547](https://github.com/BerriAI/litellm/pull/16547)\\n    - Add support for custom KMS encryption keys in Bedrock Batch operations - [PR #16662](https://github.com/BerriAI/litellm/pull/16662)\\n    - Add bearer token authentication support for AgentCore - [PR #16556](https://github.com/BerriAI/litellm/pull/16556)\\n    - Fix AgentCore SSE stream iterator to async for proper streaming support - [PR #16293](https://github.com/BerriAI/litellm/pull/16293)\\n\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Add context management param support - [PR #16528](https://github.com/BerriAI/litellm/pull/16528)\\n    - Fix preserve `$defs` for Anthropic tools input schema - [PR #16648](https://github.com/BerriAI/litellm/pull/16648)\\n    - Fix support Anthropic tool_use and tool_result in token counter - [PR #16351](https://github.com/BerriAI/litellm/pull/16351)\\n\\n- **[Vertex AI](../../docs/providers/vertex_ai)**\\n    - Add Vertex Kimi-K2-Thinking support - [PR #16671](https://github.com/BerriAI/litellm/pull/16671)\\n    - Add `vertex_credentials` support to `litellm.rerank()` - [PR #16479](https://github.com/BerriAI/litellm/pull/16479)\\n\\n- **[Mistral](../../docs/providers/mistral)**\\n    - Fix Magistral streaming to emit reasoning chunks - [PR #16434](https://github.com/BerriAI/litellm/pull/16434)\\n\\n- **[Moonshot (Kimi)](../../docs/providers/moonshot)**\\n    - Add Kimi K2 thinking model support - [PR #16445](https://github.com/BerriAI/litellm/pull/16445)\\n\\n- **[SambaNova](../../docs/providers/sambanova)**\\n    - Fix SambaNova API rejecting requests when message content is passed as a list format - [PR #16612](https://github.com/BerriAI/litellm/pull/16612)\\n\\n- **[VLLM](../../docs/providers/vllm)**\\n    - Fix use vllm passthrough config for hosted vllm provider instead of raising error - [PR #16537](https://github.com/BerriAI/litellm/pull/16537)\\n    - Add headers to VLLM Passthrough requests with success event logging - [PR #16532](https://github.com/BerriAI/litellm/pull/16532)\\n\\n- **[Azure](../../docs/providers/azure)**\\n    - Fix improve Azure auth parameter handling for None values - [PR #14436](https://github.com/BerriAI/litellm/pull/14436)\\n\\n- **[Groq](../../docs/providers/groq)**\\n    - Fix parse failed chunks for Groq - [PR #16595](https://github.com/BerriAI/litellm/pull/16595)\\n\\n- **[Voyage](../../docs/providers/voyage)**\\n    - Add Voyage 3.5 and 3.5-lite embeddings pricing and doc update - [PR #16641](https://github.com/BerriAI/litellm/pull/16641)\\n\\n- **[Fal.ai](../../docs/image_generation)**\\n    - Add fal-ai/flux/schnell support - [PR #16580](https://github.com/BerriAI/litellm/pull/16580)\\n    - Add all Imagen4 variants of fal ai in model map - [PR #16579](https://github.com/BerriAI/litellm/pull/16579)\\n\\n### Bug Fixes\\n\\n- **General**\\n    - Fix sanitize null token usage in OpenAI-compatible responses - [PR #16493](https://github.com/BerriAI/litellm/pull/16493)\\n    - Fix apply provided timeout value to ClientTimeout.total - [PR #16395](https://github.com/BerriAI/litellm/pull/16395)\\n    - Fix raising wrong 429 error on wrong exception - [PR #16482](https://github.com/BerriAI/litellm/pull/16482)\\n    - Add new models, delete repeat models, update pricing - [PR #16491](https://github.com/BerriAI/litellm/pull/16491)\\n    - Update model logging format for custom LLM provider - [PR #16485](https://github.com/BerriAI/litellm/pull/16485)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### New Endpoints\\n\\n- **[GET /providers](../../docs/proxy/management_endpoints)**\\n    - Add GET list of providers endpoint - [PR #16432](https://github.com/BerriAI/litellm/pull/16432)\\n\\n#### Features\\n\\n- **[Video Generation API](../../docs/video_generation)**\\n    - Allow internal users to access video generation routes - [PR #16472](https://github.com/BerriAI/litellm/pull/16472)\\n\\n- **[Vector Stores API](../../docs/vector_stores)**\\n    - Vector store files stable release with complete CRUD operations - [PR #16643](https://github.com/BerriAI/litellm/pull/16643)\\n      - `POST /v1/vector_stores/{vector_store_id}/files` - Create vector store file\\n      - `GET /v1/vector_stores/{vector_store_id}/files` - List vector store files\\n      - `GET /v1/vector_stores/{vector_store_id}/files/{file_id}` - Retrieve vector store file\\n      - `GET /v1/vector_stores/{vector_store_id}/files/{file_id}/content` - Retrieve file content\\n      - `DELETE /v1/vector_stores/{vector_store_id}/files/{file_id}` - Delete vector store file\\n      - `DELETE /v1/vector_stores/{vector_store_id}` - Delete vector store\\n    - Ensure users can access `search_results` for both stream + non-stream response - [PR #16459](https://github.com/BerriAI/litellm/pull/16459)\\n\\n#### Bugs\\n\\n- **[Video Generation API](../../docs/video_generation)**\\n    - Fix use GET for `/v1/videos/{video_id}/content` - [PR #16672](https://github.com/BerriAI/litellm/pull/16672)\\n\\n- **General**\\n    - Fix remove generic exception handling - [PR #16599](https://github.com/BerriAI/litellm/pull/16599)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n\\n- **Proxy CLI Auth**\\n    - Fix remove strict master_key check in add_deployment - [PR #16453](https://github.com/BerriAI/litellm/pull/16453)\\n\\n- **Virtual Keys**\\n    - UI - Add Tags To Edit Key Flow - [PR #16500](https://github.com/BerriAI/litellm/pull/16500)\\n    - UI - Test Key Page show models based on selected endpoint - [PR #16452](https://github.com/BerriAI/litellm/pull/16452)\\n    - UI - Expose user_alias in view and update path - [PR #16669](https://github.com/BerriAI/litellm/pull/16669)\\n\\n- **Models + Endpoints**\\n    - UI - Add LiteLLM Params to Edit Model - [PR #16496](https://github.com/BerriAI/litellm/pull/16496)\\n    - UI - Add Model use backend data - [PR #16664](https://github.com/BerriAI/litellm/pull/16664)\\n    - UI - Remove Description Field from LLM Credentials - [PR #16608](https://github.com/BerriAI/litellm/pull/16608)\\n    - UI - Add RunwayML on Admin UI supported models/providers - [PR #16606](https://github.com/BerriAI/litellm/pull/16606)\\n    - Infra - Migrate Add Model Fields to Backend - [PR #16620](https://github.com/BerriAI/litellm/pull/16620)\\n    - Add API Endpoint for creating model access group - [PR #16663](https://github.com/BerriAI/litellm/pull/16663)\\n\\n- **Teams**\\n    - UI - Invite User Searchable Team Select - [PR #16454](https://github.com/BerriAI/litellm/pull/16454)\\n    - Fix use user budget instead of key budget when creating new team - [PR #16074](https://github.com/BerriAI/litellm/pull/16074)\\n\\n- **Budgets**\\n    - UI - Move Budgets out of Experimental - [PR #16544](https://github.com/BerriAI/litellm/pull/16544)\\n\\n- **Guardrails**\\n    - UI - Config Guardrails should not be deletable from table - [PR #16540](https://github.com/BerriAI/litellm/pull/16540)\\n    - Fix remove enterprise restriction from guardrails list endpoint - [PR #15333](https://github.com/BerriAI/litellm/pull/15333)\\n\\n- **Callbacks**\\n    - UI - New Callbacks table - [PR #16512](https://github.com/BerriAI/litellm/pull/16512)\\n    - Fix delete callbacks failing - [PR #16473](https://github.com/BerriAI/litellm/pull/16473)\\n\\n- **Usage & Analytics**\\n    - UI - Improve Usage Indicator - [PR #16504](https://github.com/BerriAI/litellm/pull/16504)\\n    - UI - Model Info Page Health Check - [PR #16416](https://github.com/BerriAI/litellm/pull/16416)\\n    - Infra - Show Deprecation Warning for Model Analytics Tab - [PR #16417](https://github.com/BerriAI/litellm/pull/16417)\\n    - Fix Litellm tags usage add request_id - [PR #16111](https://github.com/BerriAI/litellm/pull/16111)\\n\\n- **Health Check**\\n    - Add Langfuse OTEL and SQS to Health Check - [PR #16514](https://github.com/BerriAI/litellm/pull/16514)\\n\\n- **General UI**\\n    - UI - Normalize table action columns appearance - [PR #16657](https://github.com/BerriAI/litellm/pull/16657)\\n    - UI - Button Styles and Sizing in Settings Pages - [PR #16600](https://github.com/BerriAI/litellm/pull/16600)\\n    - UI - SSO Modal Cosmetic Changes - [PR #16554](https://github.com/BerriAI/litellm/pull/16554)\\n    - Fix UI logos loading with SERVER_ROOT_PATH - [PR #16618](https://github.com/BerriAI/litellm/pull/16618)\\n    - Fix remove misleading \'Custom\' option mention from OpenAI endpoint tooltips - [PR #16622](https://github.com/BerriAI/litellm/pull/16622)\\n\\n- **SSO**\\n    - Ensure `role` from SSO provider is used when a user is inserted onto LiteLLM - [PR #16794](https://github.com/BerriAI/litellm/pull/16794)\\n\\n#### Bugs\\n\\n- **Management Endpoints**\\n    - Fix inconsistent error responses in customer management endpoints - [PR #16450](https://github.com/BerriAI/litellm/pull/16450)\\n    - Fix correct date range filtering in /spend/logs endpoint - [PR #16443](https://github.com/BerriAI/litellm/pull/16443)\\n    - Fix /spend/logs/ui Access Control - [PR #16446](https://github.com/BerriAI/litellm/pull/16446)\\n    - Add pagination for /spend/logs/session/ui endpoint - [PR #16603](https://github.com/BerriAI/litellm/pull/16603)\\n    - Fix LiteLLM Usage shows key_hash - [PR #16471](https://github.com/BerriAI/litellm/pull/16471)\\n    - Fix app_roles missing from jwt payload - [PR #16448](https://github.com/BerriAI/litellm/pull/16448)\\n\\n---\\n\\n## Logging / Guardrail / Prompt Management Integrations\\n\\n\\n#### New Integration\\n\\n- **\ud83c\udd95 [Zscaler AI Guard](../../docs/proxy/guardrails/zscaler_ai_guard)**\\n    - Add Zscaler AI Guard hook for security policy enforcement - [PR #15691](https://github.com/BerriAI/litellm/pull/15691)\\n\\n#### Logging\\n\\n- **[Langfuse](../../docs/proxy/logging#langfuse)**\\n    - Fix handle null usage values to prevent validation errors - [PR #16396](https://github.com/BerriAI/litellm/pull/16396)\\n\\n- **[CloudZero](../../docs/proxy/logging)**\\n    - Fix updated spend would not be sent to CloudZero - [PR #16201](https://github.com/BerriAI/litellm/pull/16201)\\n\\n#### Guardrails\\n\\n- **[IBM Detector](../../docs/proxy/guardrails)**\\n    - Ensure detector-id is passed as header to IBM detector server - [PR #16649](https://github.com/BerriAI/litellm/pull/16649)\\n\\n#### Prompt Management\\n\\n- **[Custom Prompt Management](../../docs/proxy/prompt_management)**\\n    - Add SDK focused examples for custom prompt management - [PR #16441](https://github.com/BerriAI/litellm/pull/16441)\\n\\n---\\n\\n## Spend Tracking, Budgets and Rate Limiting\\n\\n- **End User Budgets**\\n    - Allow pointing max_end_user budget to an id, so the default ID applies to all end users - [PR #16456](https://github.com/BerriAI/litellm/pull/16456)\\n\\n---\\n\\n## MCP Gateway\\n\\n- **Configuration**\\n    - Add dynamic OAuth2 metadata discovery for MCP servers - [PR #16676](https://github.com/BerriAI/litellm/pull/16676)\\n    - Fix allow tool call even when server name prefix is missing - [PR #16425](https://github.com/BerriAI/litellm/pull/16425)\\n    - Fix exclude unauthorized MCP servers from allowed server list - [PR #16551](https://github.com/BerriAI/litellm/pull/16551)\\n    - Fix unable to delete MCP server from permission settings - [PR #16407](https://github.com/BerriAI/litellm/pull/16407)\\n    - Fix avoid crashing when MCP server record lacks credentials - [PR #16601](https://github.com/BerriAI/litellm/pull/16601)\\n\\n---\\n\\n## Agents\\n\\n- **[Agent Registration (A2A Spec)](../../docs/agents)**\\n    - Support agent registration + discovery following Agent-to-Agent specification - [PR #16615](https://github.com/BerriAI/litellm/pull/16615)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n- **Embeddings Performance**\\n    - Use router\'s O(1) lookup and shared sessions for embeddings - [PR #16344](https://github.com/BerriAI/litellm/pull/16344)\\n\\n- **Router Reliability**\\n    - Support default fallbacks for unknown models - [PR #16419](https://github.com/BerriAI/litellm/pull/16419)\\n\\n- **Callback Management**\\n    - Add atexit handlers to flush callbacks for async completions - [PR #16487](https://github.com/BerriAI/litellm/pull/16487)\\n\\n---\\n\\n## General Proxy Improvements\\n\\n- **Configuration Management**\\n    - Fix update model_cost_map_url to use environment variable - [PR #16429](https://github.com/BerriAI/litellm/pull/16429)\\n\\n---\\n\\n## Documentation Updates\\n\\n- **Provider Documentation**\\n    - Fix streaming example in README - [PR #16461](https://github.com/BerriAI/litellm/pull/16461)\\n    - Update broken Slack invite links to support page - [PR #16546](https://github.com/BerriAI/litellm/pull/16546)\\n    - Fix code block indentation for fallbacks page - [PR #16542](https://github.com/BerriAI/litellm/pull/16542)\\n    - Documentation code example corrections - [PR #16502](https://github.com/BerriAI/litellm/pull/16502)\\n    - Document `reasoning_effort` summary field options - [PR #16549](https://github.com/BerriAI/litellm/pull/16549)\\n\\n- **API Documentation**\\n    - Add docs on APIs for model access management - [PR #16673](https://github.com/BerriAI/litellm/pull/16673)\\n    - Add docs for showing how to auto reload new pricing data - [PR #16675](https://github.com/BerriAI/litellm/pull/16675)\\n    - LiteLLM Quick start - show how model resolution works - [PR #16602](https://github.com/BerriAI/litellm/pull/16602)\\n    - Add docs for tracking callback failure - [PR #16474](https://github.com/BerriAI/litellm/pull/16474)\\n\\n- **General Documentation**\\n    - Fix container api link in release page - [PR #16440](https://github.com/BerriAI/litellm/pull/16440)\\n    - Add softgen to projects that are using litellm - [PR #16423](https://github.com/BerriAI/litellm/pull/16423)\\n\\n---\\n\\n## New Contributors\\n\\n* @artplan1 made their first contribution in [PR #16423](https://github.com/BerriAI/litellm/pull/16423)\\n* @JehandadK made their first contribution in [PR #16472](https://github.com/BerriAI/litellm/pull/16472)\\n* @vmiscenko made their first contribution in [PR #16453](https://github.com/BerriAI/litellm/pull/16453)\\n* @mcowger made their first contribution in [PR #16429](https://github.com/BerriAI/litellm/pull/16429)\\n* @yellowsubmarine372 made their first contribution in [PR #16395](https://github.com/BerriAI/litellm/pull/16395)\\n* @Hebruwu made their first contribution in [PR #16201](https://github.com/BerriAI/litellm/pull/16201)\\n* @jwang-gif made their first contribution in [PR #15691](https://github.com/BerriAI/litellm/pull/15691)\\n* @AnthonyMonaco made their first contribution in [PR #16502](https://github.com/BerriAI/litellm/pull/16502)\\n* @andrewm4894 made their first contribution in [PR #16487](https://github.com/BerriAI/litellm/pull/16487)\\n* @f14-bertolotti made their first contribution in [PR #16485](https://github.com/BerriAI/litellm/pull/16485)\\n* @busla made their first contribution in [PR #16293](https://github.com/BerriAI/litellm/pull/16293)\\n* @MightyGoldenOctopus made their first contribution in [PR #16537](https://github.com/BerriAI/litellm/pull/16537)\\n* @ultmaster made their first contribution in [PR #14436](https://github.com/BerriAI/litellm/pull/14436)\\n* @bchrobot made their first contribution in [PR #16542](https://github.com/BerriAI/litellm/pull/16542)\\n* @sep-grindr made their first contribution in [PR #16622](https://github.com/BerriAI/litellm/pull/16622)\\n* @pnookala-godaddy made their first contribution in [PR #16607](https://github.com/BerriAI/litellm/pull/16607)\\n* @dtunikov made their first contribution in [PR #16592](https://github.com/BerriAI/litellm/pull/16592)\\n* @lukapecnik made their first contribution in [PR #16648](https://github.com/BerriAI/litellm/pull/16648)\\n* @jyeros made their first contribution in [PR #16618](https://github.com/BerriAI/litellm/pull/16618)\\n\\n---\\n\\n## Full Changelog\\n\\n**[View complete changelog on GitHub](https://github.com/BerriAI/litellm/compare/v1.79.3.rc.1...v1.80.0.rc.1)**\\n\\n---"},{"id":"v1-79-3","metadata":{"permalink":"/release_notes/v1-79-3","source":"@site/release_notes/v1.79.3-stable/index.md","title":"v1.79.3-stable - Built-in Guardrails on AI Gateway","description":"Deploy this version","date":"2025-11-08T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.79.3-stable - Built-in Guardrails on AI Gateway","slug":"v1-79-3","date":"2025-11-08T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.80.0-stable - Introducing Agent Hub: Register, Publish, and Share Agents","permalink":"/release_notes/v1-80-0"},"nextItem":{"title":"v1.79.1-stable - Guardrail Playground","permalink":"/release_notes/v1-79-1"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.79.3-stable\\n```\\n\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.79.3.rc.1\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights\\n\\n- **LiteLLM Custom Guardrail** - Built-in guardrail with UI configuration support\\n- **Performance Improvements** - `/responses` API 19\xd7 Lower Median Latency\\n- **Veo3 Video Generation (Vertex AI + Google AI Studio)** - Use OpenAI Video API to generate videos with Vertex AI and Google AI Studio Veo3 models\\n\\n---\\n\\n### Built-in Guardrails on AI Gateway\\n\\n<Image \\n  img={require(\'../../img/release_notes/built_in_guard.png\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n\\n<br/>\\n\\nThis release introduces built-in guardrails for LiteLLM AI Gateway, allowing you to enforce protections without depending on an external guardrail API.\\n\\n- **Blocking Keywords** - Block known sensitive keywords like \\"litellm\\", \\"python\\", etc.\\n- **Pattern Detection** - Block known sensitive patterns like emails, Social Security Numbers, API keys, etc.\\n- **Custom Regex Patterns** - Define custom regex patterns for your specific use case.\\n\\n\\nGet started with the built-in guardrails on AI Gateway [here](https://docs.litellm.ai/docs/proxy/guardrails/litellm_content_filter).\\n\\n---\\n\\n### Performance \u2013 `/responses` 19\xd7 Lower Median Latency\\n\\nThis update significantly improves `/responses` latency by integrating our internal network management for connection handling, eliminating per-request setup overhead.\\n\\n#### Results\\n\\n| Metric | Before | After | Improvement |\\n|--------|--------|-------|-------------|\\n| Median latency | 3,600 ms | **190 ms** | **\u221295% (~19\xd7 faster)** |\\n| p95 latency | 4,300 ms | **280 ms** | \u221293% |\\n| p99 latency | 4,600 ms | **590 ms** | \u221287% |\\n| Average latency | 3,571 ms | **208 ms** | \u221294% |\\n| RPS | 231 | **1,059** | +358% |\\n\\n#### Test Setup\\n\\n| Category | Specification |\\n|----------|---------------|\\n| **Load Testing** | Locust: 1,000 concurrent users, 500 ramp-up |\\n| **System** | 4 vCPUs, 8 GB RAM, 4 workers, 4 instances |\\n| **Database** | PostgreSQL (Redis unused) |\\n| **Configuration** | [config.yaml](https://gist.github.com/AlexsanderHamir/550791675fd752befcac6a9e44024652) |\\n| **Load Script** | [no_cache_hits.py](https://gist.github.com/AlexsanderHamir/99d673bf74cdd81fd39f59fa9048f2e8) |\\n\\n---\\n\\n## New Models / Updated Models\\n\\n#### New Model Support\\n\\n| Provider | Model | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Features |\\n| -------- | ----- | -------------- | ------------------- | -------------------- | -------- |\\n| Azure | `azure/gpt-5-pro` | 272K | $15.00 | $120.00 | Responses API, reasoning, vision, PDF input |\\n| Azure | `azure/gpt-image-1-mini` | - | - | - | Image generation - per pixel pricing |\\n| Azure | `azure/container` | - | - | - | Container API - $0.03/session |\\n| OpenAI | `openai/container` | - | - | - | Container API - $0.03/session |\\n| Cohere | `cohere/embed-v4.0` | 128K | $0.12 | - | Embeddings with image input support |\\n| Gemini | `gemini/gemini-live-2.5-flash-preview-native-audio-09-2025` | 1M | $0.30 | $2.00 | Native audio, vision, web search |\\n| Vertex AI | `vertex_ai/minimaxai/minimax-m2-maas` | 196K | $0.30 | $1.20 | Function calling, tool choice |\\n| NVIDIA | `nvidia/nemotron-nano-9b-v2` | - | - | - | Chat completions |\\n\\n#### OCR Models\\n\\n| Provider | Model | Cost Per Page | Features |\\n| -------- | ----- | ------------- | -------- |\\n| Azure AI | `azure_ai/doc-intelligence/prebuilt-read` | $0.0015 | Document reading |\\n| Azure AI | `azure_ai/doc-intelligence/prebuilt-layout` | $0.01 | Layout analysis |\\n| Azure AI | `azure_ai/doc-intelligence/prebuilt-document` | $0.01 | Document processing |\\n| Vertex AI | `vertex_ai/mistral-ocr-2505` | $0.0005 | OCR processing |\\n\\n#### Search Models\\n\\n| Provider | Model | Pricing | Features |\\n| -------- | ----- | ------- | -------- |\\n| Firecrawl | `firecrawl/search` | Tiered: $0.00166-$0.0166/query | 10-100 results per query |\\n| SearXNG | `searxng/search` | Free | Open-source metasearch |\\n\\n#### Features\\n\\n- **[Azure](../../docs/providers/azure)**\\n    - Add Azure GPT-5-Pro Responses API support with reasoning capabilities - [PR #16235](https://github.com/BerriAI/litellm/pull/16235)\\n    - Add gpt-image-1-mini pricing for Azure with quality tiers (low/medium/high) - [PR #16182](https://github.com/BerriAI/litellm/pull/16182)\\n    - Add support for returning Azure Content Policy error information when exceptions from Azure OpenAI occur - [PR #16231](https://github.com/BerriAI/litellm/pull/16231)\\n    - Fix Azure GPT-5 incorrectly routed to O-series config (temperature parameter unsupported) - [PR #16246](https://github.com/BerriAI/litellm/pull/16246)\\n    - Fix Azure doesn\'t accept extra body param - [PR #16116](https://github.com/BerriAI/litellm/pull/16116)\\n    - Fix Azure DALL-E-3 health check content policy violation by using safe default prompt - [PR #16329](https://github.com/BerriAI/litellm/pull/16329)\\n\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Fix empty assistant message handling in AWS Bedrock Converse API to prevent 400 Bad Request errors - [PR #15850](https://github.com/BerriAI/litellm/pull/15850)\\n    - Fix: Filter AWS authentication params from Bedrock InvokeModel request body - [PR #16315](https://github.com/BerriAI/litellm/pull/16315)\\n    - Fix Bedrock proxy adding name to file content, breaks when cache_control in use - [PR #16275](https://github.com/BerriAI/litellm/pull/16275)\\n    - Fix global.anthropic.claude-haiku-4-5-20251001-v1:0 supports_reasoning flag and update pricing - [PR #16263](https://github.com/BerriAI/litellm/pull/16263)\\n\\n- **[Gemini (Google AI Studio + Vertex AI)](../../docs/providers/gemini)**\\n    - Add gemini live audio model cost in model map - [PR #16183](https://github.com/BerriAI/litellm/pull/16183)\\n    - Fix translation problem with Gemini parallel tool calls - [PR #16194](https://github.com/BerriAI/litellm/pull/16194)\\n    - Fix: Send Gemini API key via x-goog-api-key header with custom api_base - [PR #16085](https://github.com/BerriAI/litellm/pull/16085)\\n    - Fix image_config.aspect_ratio not working for gemini-2.5-flash-image - [PR #15999](https://github.com/BerriAI/litellm/pull/15999)\\n    - Fix Gemini minimal reasoning env overrides disabling thoughts - [PR #16347](https://github.com/BerriAI/litellm/pull/16347)\\n    - Fix cache_read_input_token_cost for gemini-2.5-flash - [PR #16354](https://github.com/BerriAI/litellm/pull/16354)\\n\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Fix Anthropic token counting for VertexAI - [PR #16171](https://github.com/BerriAI/litellm/pull/16171)\\n    - Fix anthropic-adapter: properly translate Anthropic image format to OpenAI - [PR #16202](https://github.com/BerriAI/litellm/pull/16202)\\n    - Enable automated prompt caching message format for Claude on Databricks - [PR #16200](https://github.com/BerriAI/litellm/pull/16200)\\n    - Add support for Anthropic Memory Tool - [PR #16115](https://github.com/BerriAI/litellm/pull/16115)\\n    - Propagate cache creation/read token costs for model info to fix Anthropic long context cost calculations - [PR #16376](https://github.com/BerriAI/litellm/pull/16376)\\n\\n- **[Vertex AI](../../docs/providers/vertex_ai)**\\n    - Add Vertex MiniMAX m2 model support - [PR #16373](https://github.com/BerriAI/litellm/pull/16373)\\n    - Correctly map 429 Resource Exhausted to RateLimitError - [PR #16363](https://github.com/BerriAI/litellm/pull/16363)\\n    - Add `vertex_credentials` support to `litellm.rerank()` for Vertex AI - [PR #16266](https://github.com/BerriAI/litellm/pull/16266)\\n\\n- **[Databricks](../../docs/providers/databricks)**\\n    - Fix databricks streaming - [PR #16368](https://github.com/BerriAI/litellm/pull/16368)\\n\\n- **[Deepgram](../../docs/providers/deepgram)**\\n    - Return the diarized transcript when it\'s required in the request - [PR #16133](https://github.com/BerriAI/litellm/pull/16133)\\n\\n- **[Fireworks](../../docs/providers/fireworks_ai)**\\n    - Update Fireworks audio endpoints to new `api.fireworks.ai` domains - [PR #16346](https://github.com/BerriAI/litellm/pull/16346)\\n\\n- **[Cohere](../../docs/providers/cohere)**\\n    - Add cohere embed-v4.0 model support - [PR #16358](https://github.com/BerriAI/litellm/pull/16358)\\n\\n- **[Watsonx](../../docs/providers/watsonx)**\\n    - Support `reasoning_effort` for watsonx chat models - [PR #16261](https://github.com/BerriAI/litellm/pull/16261)\\n\\n- **[OpenAI](../../docs/providers/openai)**\\n    - Remove automatic summary from reasoning_effort transformation - [PR #16210](https://github.com/BerriAI/litellm/pull/16210)\\n\\n- **[XAI](../../docs/providers/xai)**\\n    - Remove Grok 4 Models Reasoning Effort Parameter - [PR #16265](https://github.com/BerriAI/litellm/pull/16265)\\n\\n- **[Hosted VLLM](../../docs/providers/vllm)**\\n    - Fix HostedVLLMRerankConfig will not be used - [PR #16352](https://github.com/BerriAI/litellm/pull/16352)\\n\\n#### New Provider Support\\n\\n- **[Bedrock Agentcore](../../docs/providers/bedrock)**\\n    - Add Bedrock Agentcore as a provider on LiteLLM Python SDK and LiteLLM AI Gateway - [PR #16252](https://github.com/BerriAI/litellm/pull/16252)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n\\n- **[OCR API](../../docs/ocr)**\\n    - Add VertexAI OCR provider support + cost tracking - [PR #16216](https://github.com/BerriAI/litellm/pull/16216)\\n    - Add Azure AI Doc Intelligence OCR support - [PR #16219](https://github.com/BerriAI/litellm/pull/16219)\\n\\n- **[Search API](../../docs/search)**\\n    - Add firecrawl search API support with tiered pricing - [PR #16257](https://github.com/BerriAI/litellm/pull/16257)\\n    - Add searxng search API provider - [PR #16259](https://github.com/BerriAI/litellm/pull/16259)\\n\\n- **[Responses API](../../docs/response_api)**\\n    - Support responses API streaming in langfuse otel - [PR #16153](https://github.com/BerriAI/litellm/pull/16153)\\n    - Pass extra_body parameters to provider in Responses API requests - [PR #16320](https://github.com/BerriAI/litellm/pull/16320)\\n\\n- **[Container API](../../docs/container_api)**\\n    - Add E2E Container API Support - [PR #16136](https://github.com/BerriAI/litellm/pull/16136)\\n    - Update container documentation to be similar to others - [PR #16327](https://github.com/BerriAI/litellm/pull/16327)\\n\\n- **[Video Generation API](../../docs/video_generation)**\\n    - Add Vertex and Gemini Videos API with Cost Tracking + UI support - [PR #16323](https://github.com/BerriAI/litellm/pull/16323)\\n    - Add `custom_llm_provider` support for video endpoints (non-generation) - [PR #16121](https://github.com/BerriAI/litellm/pull/16121)\\n\\n- **[Audio API](../../docs/audio)**\\n    - Add gpt-4o-transcribe cost tracking - [PR #16412](https://github.com/BerriAI/litellm/pull/16412)\\n\\n- **[Vector Stores](../../docs/vector_stores)**\\n    - Milvus - search vector store support + support multi-part form data on passthrough - [PR #16035](https://github.com/BerriAI/litellm/pull/16035)\\n    - Azure AI Vector Stores - support \\"virtual\\" indexes + create vector store on passthrough API - [PR #16160](https://github.com/BerriAI/litellm/pull/16160)\\n    - Milvus - Passthrough API support - adds create + read vector store support via passthrough API\'s - [PR #16170](https://github.com/BerriAI/litellm/pull/16170)\\n\\n- **[Embeddings API](../../docs/embedding/supported_embedding)**\\n    - Use valid CallTypes enum value in embeddings endpoint - [PR #16328](https://github.com/BerriAI/litellm/pull/16328)\\n\\n- **[Rerank API](../../docs/rerank)**\\n    - Generalize tiered pricing in generic cost calculator - [PR #16150](https://github.com/BerriAI/litellm/pull/16150)\\n\\n#### Bugs\\n\\n- **General**\\n    - Fix index field not populated in streaming mode with n>1 and tool calls - [PR #15962](https://github.com/BerriAI/litellm/pull/15962)\\n    - Pass aws_region_name in litellm_params - [PR #16321](https://github.com/BerriAI/litellm/pull/16321)\\n    - Add `retry-after` header support for errors `502`, `503`, `504` - [PR #16288](https://github.com/BerriAI/litellm/pull/16288)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n\\n- **Virtual Keys**\\n    - UI - Delete Team Member with friction - [PR #16167](https://github.com/BerriAI/litellm/pull/16167)\\n    - UI - Litellm test key audio support - [PR #16251](https://github.com/BerriAI/litellm/pull/16251)\\n    - UI - Test Key Page Revert Model To Single Select - [PR #16390](https://github.com/BerriAI/litellm/pull/16390)\\n\\n- **Models + Endpoints**\\n    - UI - Add Model Existing Credentials Improvement - [PR #16166](https://github.com/BerriAI/litellm/pull/16166)\\n    - UI - Add Azure AD Token field and Azure API Key optional - [PR #16331](https://github.com/BerriAI/litellm/pull/16331)\\n    - UI - Fixed Label for vLLM in Model Create Flow - [PR #16285](https://github.com/BerriAI/litellm/pull/16285)\\n    - UI - Include Model Access Group Models on Team Models Table - [PR #16298](https://github.com/BerriAI/litellm/pull/16298)\\n    - Fix /model_group/info Returning Entire Model List for SSO Users - [PR #16296](https://github.com/BerriAI/litellm/pull/16296)\\n    - Litellm non root docker Model Hub Table fix - [PR #16282](https://github.com/BerriAI/litellm/pull/16282)\\n\\n- **Guardrails**\\n    - UI - Fix regression where Guardrail Entity Could not be selected and entity was not displayed - [PR #16165](https://github.com/BerriAI/litellm/pull/16165)\\n    - UI - Guardrail Info Page Show PII Config - [PR #16164](https://github.com/BerriAI/litellm/pull/16164)\\n    - Change guardrail_information to list type - [PR #16127](https://github.com/BerriAI/litellm/pull/16127)\\n    - UI - LiteLLM Guardrail - ensure you can see UI Friendly name for PII Patterns - [PR #16382](https://github.com/BerriAI/litellm/pull/16382)\\n    - UI - Guardrails - LiteLLM Content Filter, Allow Viewing/Editing Content Filter Settings - [PR #16383](https://github.com/BerriAI/litellm/pull/16383)\\n    - UI - Guardrails - allow updating guardrails through UI. Ensure litellm_params actually get updated in memory - [PR #16384](https://github.com/BerriAI/litellm/pull/16384)\\n\\n- **SSO Settings**\\n    - Support dot notation on ui sso - [PR #16135](https://github.com/BerriAI/litellm/pull/16135)\\n    - UI - Prevent trailing slash in sso proxy base url input - [PR #16244](https://github.com/BerriAI/litellm/pull/16244)\\n    - UI - SSO Proxy Base URL input validation and remove normalizing / - [PR #16332](https://github.com/BerriAI/litellm/pull/16332)\\n    - UI - Surface SSO Create errors on create flow - [PR #16369](https://github.com/BerriAI/litellm/pull/16369)\\n\\n- **Usage & Analytics**\\n    - UI - Tag Usage Top Model Table View and Label Fix - [PR #16249](https://github.com/BerriAI/litellm/pull/16249)\\n    - UI - Litellm usage date picker - [PR #16264](https://github.com/BerriAI/litellm/pull/16264)\\n\\n- **Cache Settings**\\n    - UI - Cache Settings Redis Add Semantic Cache Settings - [PR #16398](https://github.com/BerriAI/litellm/pull/16398)\\n\\n#### Bugs\\n\\n- **General**\\n    - UI - Remove encoding_format in request for embedding models - [PR #16367](https://github.com/BerriAI/litellm/pull/16367)\\n    - UI - Revert Changes for Test Key Multiple Model Select - [PR #16372](https://github.com/BerriAI/litellm/pull/16372)\\n    - UI - Various Small Issues - [PR #16406](https://github.com/BerriAI/litellm/pull/16406)\\n\\n---\\n\\n## AI Integrations\\n\\n### Logging\\n\\n- **[Langfuse](../../docs/proxy/logging#langfuse)**\\n    - Fix langfuse input tokens logic for cached tokens - [PR #16203](https://github.com/BerriAI/litellm/pull/16203)\\n\\n- **[Opik](../../docs/proxy/logging#opik)**\\n    - Fix the bug with not incorrect attachment to existing trace & refactor - [PR #15529](https://github.com/BerriAI/litellm/pull/15529)\\n\\n- **[S3](../../docs/proxy/logging#s3)**\\n    - S3 logger, add support for ssl_verify when using minio logger - [PR #16211](https://github.com/BerriAI/litellm/pull/16211)\\n    - Strip base64 in s3 - [PR #16157](https://github.com/BerriAI/litellm/pull/16157)\\n    - Add allowing Key based prefix to s3 path - [PR #16237](https://github.com/BerriAI/litellm/pull/16237)\\n    - Add Prometheus metric to track callback logging failures in S3 - [PR #16209](https://github.com/BerriAI/litellm/pull/16209)\\n\\n- **[OpenTelemetry](../../docs/proxy/logging#opentelemetry)**\\n    - OTEL - Log Cost Breakdown on OTEL Logger - [PR #16334](https://github.com/BerriAI/litellm/pull/16334)\\n\\n- **[DataDog](../../docs/proxy/logging#datadog)**\\n    - Add DD Agent Host support for `datadog` callback - [PR #16379](https://github.com/BerriAI/litellm/pull/16379)\\n\\n### Guardrails\\n\\n- **[Noma](../../docs/proxy/guardrails)**\\n    - Revert Noma Apply Guardrail implementation - [PR #16214](https://github.com/BerriAI/litellm/pull/16214)\\n    - Litellm noma guardrail support images - [PR #16199](https://github.com/BerriAI/litellm/pull/16199)\\n\\n- **[PANW Prisma AIRS](../../docs/proxy/guardrails)**\\n    - PANW prisma airs guardrail deduplication and enhanced session tracking - [PR #16273](https://github.com/BerriAI/litellm/pull/16273)\\n\\n- **[LiteLLM Custom Guardrail](../../docs/proxy/guardrails)**\\n    - Add LiteLLM Gateway built in guardrail - [PR #16338](https://github.com/BerriAI/litellm/pull/16338)\\n    - UI - Allow configuring LiteLLM Custom Guardrail - [PR #16339](https://github.com/BerriAI/litellm/pull/16339)\\n    - Bug Fix: Content Filter Guard - [PR #16414](https://github.com/BerriAI/litellm/pull/16414)\\n\\n### Secret Managers\\n\\n- **[CyberArk](../../docs/secret_managers)**\\n    - Add CyberArk Secrets Manager Integration - [PR #16278](https://github.com/BerriAI/litellm/pull/16278)\\n    - Cyber Ark - Add Key Rotations support - [PR #16289](https://github.com/BerriAI/litellm/pull/16289)\\n\\n- **[HashiCorp Vault](../../docs/secret_managers)**\\n    - Add configurable mount name and path prefix for HashiCorp Vault - [PR #16253](https://github.com/BerriAI/litellm/pull/16253)\\n    - Secret Manager - Hashicorp, add auth via approle - [PR #16374](https://github.com/BerriAI/litellm/pull/16374)\\n\\n- **[AWS Secrets Manager](../../docs/secret_managers)**\\n    - Add tags and descriptions support to aws secrets manager - [PR #16224](https://github.com/BerriAI/litellm/pull/16224)\\n\\n- **[Custom Secret Manager](../../docs/secret_managers)**\\n    - Add Custom Secret Manager - Allow users to define and write a custom secret manager - [PR #16297](https://github.com/BerriAI/litellm/pull/16297)\\n\\n- **General**\\n    - Email Notifications - Ensure Users get Key Rotated Email - [PR #16292](https://github.com/BerriAI/litellm/pull/16292)\\n    - Fix verify ssl on sts boto3 - [PR #16313](https://github.com/BerriAI/litellm/pull/16313)\\n\\n---\\n\\n## Spend Tracking, Budgets and Rate Limiting\\n\\n- **Cost Tracking**\\n    - Fix OpenAI Responses API streaming tests usage field names and cost calculation - [PR #16236](https://github.com/BerriAI/litellm/pull/16236)\\n\\n---\\n\\n## MCP Gateway\\n\\n- **Configuration**\\n    - Configure static mcp header - [PR #16179](https://github.com/BerriAI/litellm/pull/16179)\\n    - Persist mcp credentials in db - [PR #16308](https://github.com/BerriAI/litellm/pull/16308)\\n\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n- **Memory Leak Fixes**\\n    - Resolve memory accumulation caused by Pydantic 2.11+ deprecation warnings - [PR #16110](https://github.com/BerriAI/litellm/pull/16110)\\n\\n- **Session Management**\\n    - Add shared_session support to responses API - [PR #16260](https://github.com/BerriAI/litellm/pull/16260)\\n\\n- **Error Handling**\\n    - Gracefully handle connection closed errors during streaming - [PR #16294](https://github.com/BerriAI/litellm/pull/16294)\\n    - Handle None values in daily spend sort key - [PR #16245](https://github.com/BerriAI/litellm/pull/16245)\\n\\n- **Configuration**\\n    - Remove minimum validation for cache control injection index - [PR #16149](https://github.com/BerriAI/litellm/pull/16149)\\n    - Improve clearing logic - only remove unvisited endpoints - [PR #16400](https://github.com/BerriAI/litellm/pull/16400)\\n\\n- **Redis**\\n    - Handle float redis_version from AWS ElastiCache Valkey - [PR #16207](https://github.com/BerriAI/litellm/pull/16207)\\n\\n- **Hooks**\\n    - Add parallel execution handling in during_call_hook - [PR #16279](https://github.com/BerriAI/litellm/pull/16279)\\n\\n- **Infrastructure**\\n    - Install runtime node for prisma - [PR #16410](https://github.com/BerriAI/litellm/pull/16410)\\n\\n\\n\\n---\\n\\n## Documentation Updates\\n\\n- **Provider Documentation**\\n    - Docs - v1.79.1 - [PR #16163](https://github.com/BerriAI/litellm/pull/16163)\\n    - Fix broken link on model_management.md - [PR #16217](https://github.com/BerriAI/litellm/pull/16217)\\n    - Fix image generation response format - use \'images\' array instead of \'image\' object - [PR #16378](https://github.com/BerriAI/litellm/pull/16378)\\n\\n- **General Documentation**\\n    - Add minimum resource requirement for production - [PR #16146](https://github.com/BerriAI/litellm/pull/16146)\\n    - Add benchmark comparison with other AI gateways - [PR #16248](https://github.com/BerriAI/litellm/pull/16248)\\n    - LiteLLM content filter guard documentation - [PR #16413](https://github.com/BerriAI/litellm/pull/16413)\\n    - Fix typo of the word orginal - [PR #16255](https://github.com/BerriAI/litellm/pull/16255)\\n\\n- **Security**\\n    - Remove tornado test files (including test.key), fixes Python 3.13 security issues - [PR #16342](https://github.com/BerriAI/litellm/pull/16342)\\n\\n---\\n\\n## New Contributors\\n\\n* @steve-gore-snapdocs made their first contribution in [PR #16149](https://github.com/BerriAI/litellm/pull/16149)\\n* @timbmg made their first contribution in [PR #16120](https://github.com/BerriAI/litellm/pull/16120)\\n* @Nivg made their first contribution in [PR #16202](https://github.com/BerriAI/litellm/pull/16202)\\n* @pablobgar made their first contribution in [PR #16194](https://github.com/BerriAI/litellm/pull/16194)\\n* @AlanPonnachan made their first contribution in [PR #16150](https://github.com/BerriAI/litellm/pull/16150)\\n* @Chesars made their first contribution in [PR #16236](https://github.com/BerriAI/litellm/pull/16236)\\n* @bowenliang123 made their first contribution in [PR #16255](https://github.com/BerriAI/litellm/pull/16255)\\n* @dean-zavad made their first contribution in [PR #16199](https://github.com/BerriAI/litellm/pull/16199)\\n* @alexkuzmik made their first contribution in [PR #15529](https://github.com/BerriAI/litellm/pull/15529)\\n* @Granine made their first contribution in [PR #16281](https://github.com/BerriAI/litellm/pull/16281)\\n* @Oodapow made their first contribution in [PR #16279](https://github.com/BerriAI/litellm/pull/16279)\\n* @jgoodyear made their first contribution in [PR #16275](https://github.com/BerriAI/litellm/pull/16275)\\n* @Qanpi made their first contribution in [PR #16321](https://github.com/BerriAI/litellm/pull/16321)\\n* @ShimonMimoun made their first contribution in [PR #16313](https://github.com/BerriAI/litellm/pull/16313)\\n* @andriykislitsyn made their first contribution in [PR #16288](https://github.com/BerriAI/litellm/pull/16288)\\n* @reckless-huang made their first contribution in [PR #16263](https://github.com/BerriAI/litellm/pull/16263)\\n* @chenmoneygithub made their first contribution in [PR #16368](https://github.com/BerriAI/litellm/pull/16368)\\n* @stembe-digitalex made their first contribution in [PR #16354](https://github.com/BerriAI/litellm/pull/16354)\\n* @jfcherng made their first contribution in [PR #16352](https://github.com/BerriAI/litellm/pull/16352)\\n* @xingyaoww made their first contribution in [PR #16246](https://github.com/BerriAI/litellm/pull/16246)\\n* @emerzon made their first contribution in [PR #16373](https://github.com/BerriAI/litellm/pull/16373)\\n* @wwwillchen made their first contribution in [PR #16376](https://github.com/BerriAI/litellm/pull/16376)\\n* @fabriciojoc made their first contribution in [PR #16203](https://github.com/BerriAI/litellm/pull/16203)\\n* @jroberts2600 made their first contribution in [PR #16273](https://github.com/BerriAI/litellm/pull/16273)\\n\\n---\\n\\n## Full Changelog\\n\\n**[View complete changelog on GitHub](https://github.com/BerriAI/litellm/compare/v1.79.1-nightly...v1.79.2.rc.1)**"},{"id":"v1-79-1","metadata":{"permalink":"/release_notes/v1-79-1","source":"@site/release_notes/v1.79.1-stable/index.md","title":"v1.79.1-stable - Guardrail Playground","description":"Deploy this version","date":"2025-11-01T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.79.1-stable - Guardrail Playground","slug":"v1-79-1","date":"2025-11-01T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.79.3-stable - Built-in Guardrails on AI Gateway","permalink":"/release_notes/v1-79-3"},"nextItem":{"title":"v1.79.0-stable - Search APIs","permalink":"/release_notes/v1-79-0"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.79.1-stable\\n```\\n\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.80.0\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights\\n\\n- **Container API Support** - End-to-end OpenAI Container API support with proxy integration, logging, and cost tracking\\n- **FAL AI Image Generation** - Native support for FAL AI image generation models with cost tracking\\n- **UI Enhancements** - Guardrail Playground, Cache Settings, Tag Routing, SSO Settings\\n- **Batch API Rate Limiting** - Input-based rate limits support for Batch API requests\\n- **Vector Store Expansion** - Milvus vector store support and Azure AI virtual indexes\\n- **Memory Leak Fixes** - Resolved issues accounting for 90% of memory leaks on Python SDK & AI Gateway\\n\\n---\\n\\n## Dependency Upgrades\\n\\n- **Dependencies**\\n    - Build(deps): bump starlette from 0.47.2 to 0.49.1 - [PR #16027](https://github.com/BerriAI/litellm/pull/16027)\\n    - Build(deps): bump fastapi from 0.116.1 to 0.120.1 - [PR #16054](https://github.com/BerriAI/litellm/pull/16054)\\n    - Build(deps): bump hono from 4.9.7 to 4.10.3 in /litellm-js/spend-logs - [PR #15915](https://github.com/BerriAI/litellm/pull/15915)\\n\\n\\n## New Models / Updated Models\\n\\n#### New Model Support\\n\\n| Provider | Model | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Features |\\n| -------- | ----- | -------------- | ------------------- | -------------------- | -------- |\\n| Mistral | `mistral/codestral-embed` | 8K | $0.15 | - | Embeddings |\\n| Mistral | `mistral/codestral-embed-2505` | 8K | $0.15 | - | Embeddings |\\n| Gemini | `gemini/gemini-embedding-001` | 2K | $0.15 | - | Embeddings |\\n| FAL AI | `fal_ai/fal-ai/flux-pro/v1.1-ultra` | - | - | - | Image generation - $0.0398/image |\\n| FAL AI | `fal_ai/fal-ai/imagen4/preview` | - | - | - | Image generation - $0.0398/image |\\n| FAL AI | `fal_ai/fal-ai/recraft/v3/text-to-image` | - | - | - | Image generation - $0.0398/image |\\n| FAL AI | `fal_ai/fal-ai/stable-diffusion-v35-medium` | - | - | - | Image generation - $0.0398/image |\\n| FAL AI | `fal_ai/bria/text-to-image/3.2` | - | - | - | Image generation - $0.0398/image |\\n| OpenAI | `openai/sora-2-pro` | - | - | - | Video generation - $0.30/video/second |\\n\\n#### Features\\n\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Extended Claude 3-7 Sonnet deprecation date from 2026-02-01 to 2026-02-19 - [PR #15976](https://github.com/BerriAI/litellm/pull/15976)\\n    - Extended Claude Opus 4-0 deprecation date from 2025-03-01 to 2026-05-01 - [PR #15976](https://github.com/BerriAI/litellm/pull/15976)\\n    - Removed Claude Haiku 3-5 deprecation date (previously 2025-03-01) - [PR #15976](https://github.com/BerriAI/litellm/pull/15976)\\n    - Added Claude Opus 4-1, Claude Opus 4-0 20250513, Claude Sonnet 4 20250514 deprecation dates - [PR #15976](https://github.com/BerriAI/litellm/pull/15976)\\n    - Added web search support for Claude Opus 4-1 - [PR #15976](https://github.com/BerriAI/litellm/pull/15976)\\n\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Fix empty assistant message handling in AWS Bedrock Converse API to prevent 400 Bad Request errors - [PR #15850](https://github.com/BerriAI/litellm/pull/15850)\\n    - Allow using ARNs when generating images via Bedrock - [PR #15789](https://github.com/BerriAI/litellm/pull/15789)\\n    - Add per model group header forwarding for Bedrock Invoke API - [PR #16042](https://github.com/BerriAI/litellm/pull/16042)\\n    - Preserve Bedrock inference profile IDs in health checks - [PR #15947](https://github.com/BerriAI/litellm/pull/15947)\\n    - Added fallback logic for detecting file content-type when S3 returns generic type - When using Bedrock with S3-hosted files, if the S3 object\'s Content-Type is not correctly set (e.g., binary/octet-stream instead of image/png), Bedrock can now handle it correctly - [PR #15635](https://github.com/BerriAI/litellm/pull/15635)\\n\\n- **[Azure](../../docs/providers/azure)**\\n    - Add deprecation dates for Azure OpenAI models (gpt-4o-2024-08-06, gpt-4o-2024-11-20, gpt-4.1 series, o3-2025-04-16, text-embedding-3-small) - [PR #15976](https://github.com/BerriAI/litellm/pull/15976)\\n    - Fix Azure OpenAI ContextWindowExceededError mapping from Azure errors - [PR #15981](https://github.com/BerriAI/litellm/pull/15981)\\n    - Add handling for `v1` under Azure API versions - [PR #15984](https://github.com/BerriAI/litellm/pull/15984)\\n    - Fix azure doesn\'t accept extra body param - [PR #16116](https://github.com/BerriAI/litellm/pull/16116)\\n\\n- **[OpenAI](../../docs/providers/openai)**\\n    - Add deprecation dates for gpt-3.5-turbo-1106, gpt-4-0125-preview, gpt-4-1106-preview, o1-mini-2024-09-12 - [PR #15976](https://github.com/BerriAI/litellm/pull/15976)\\n    - Add extended Sora-2 modality support (text + image inputs) - [PR #15976](https://github.com/BerriAI/litellm/pull/15976)\\n    - Updated OpenAI Sora-2-Pro pricing to $0.30/video/second - [PR #15976](https://github.com/BerriAI/litellm/pull/15976)\\n\\n- **[OpenRouter](../../docs/providers/openrouter)**\\n    - Add Claude Haiku 4.5 pricing for OpenRouter - [PR #15909](https://github.com/BerriAI/litellm/pull/15909)\\n    - Add base_url config with environment variables documentation - [PR #15946](https://github.com/BerriAI/litellm/pull/15946)\\n\\n- **[Mistral](../../docs/providers/mistral)**\\n    - Add codestral-embed-2505 embedding model - [PR #16071](https://github.com/BerriAI/litellm/pull/16071)\\n\\n- **[Gemini (Google AI Studio + Vertex AI)](../../docs/providers/gemini)**\\n    - Fix gemini request mutation for tool use - [PR #16002](https://github.com/BerriAI/litellm/pull/16002)\\n    - Add gemini-embedding-001 pricing entry for Google GenAI API - [PR #16078](https://github.com/BerriAI/litellm/pull/16078)\\n    - Changes to fix frequency_penalty and presence_penalty issue for gemini-2.5-pro model - [PR #16041](https://github.com/BerriAI/litellm/pull/16041)\\n\\n- **[DeepInfra](../../docs/providers/deepinfra)**\\n    - Add vision support for Qwen/Qwen3-chat-32b model - [PR #15976](https://github.com/BerriAI/litellm/pull/15976)\\n\\n- **[Vercel AI Gateway](../../docs/providers/vercel_ai_gateway)**\\n    - Fix vercel_ai_gateway entry for glm-4.6 (moved from vercel_ai_gateway/glm-4.6 to vercel_ai_gateway/zai/glm-4.6) - [PR #16084](https://github.com/BerriAI/litellm/pull/16084)\\n\\n- **[Fireworks](../../docs/providers/fireworks_ai)**\\n    - Don\'t add \\"accounts/fireworks/models\\" prefix for Fireworks Provider - [PR #15938](https://github.com/BerriAI/litellm/pull/15938)\\n\\n- **[Cohere](../../docs/providers/cohere)**\\n    - Add OpenAI-compatible annotations support for Cohere v2 citations - [PR #16038](https://github.com/BerriAI/litellm/pull/16038)\\n\\n- **[Deepgram](../../docs/providers/deepgram)**\\n    - Handle Deepgram detected language when available - [PR #16093](https://github.com/BerriAI/litellm/pull/16093)\\n\\n### Bug Fixes\\n\\n- **[Xai](../../docs/providers/xai)**\\n    - Add Xai websearch cost tracking - [PR #16001](https://github.com/BerriAI/litellm/pull/16001)\\n\\n#### New Provider Support\\n\\n- **[FAL AI](../../docs/image_generation)**\\n    - Add FAL AI Image Generation support - [PR #16067](https://github.com/BerriAI/litellm/pull/16067)\\n\\n- **[OCI (Oracle Cloud Infrastructure)](../../docs/providers/oci)**\\n    - Add OCI Signer Authentication support - [PR #16064](https://github.com/BerriAI/litellm/pull/16064)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n\\n- **[Container API](../../docs/containers)**\\n    - Add end-to-end OpenAI Container API support to LiteLLM SDK - [PR #16136](https://github.com/BerriAI/litellm/pull/16136)\\n    - Add proxy support for container APIs - [PR #16049](https://github.com/BerriAI/litellm/pull/16049)\\n    - Add logging support for Container API - [PR #16049](https://github.com/BerriAI/litellm/pull/16049)\\n    - Add cost tracking support for containers with documentation - [PR #16117](https://github.com/BerriAI/litellm/pull/16117)\\n\\n- **[Responses API](../../docs/response_api)**\\n    - Respect `LiteLLM-Disable-Message-Redaction` header for Responses API - [PR #15966](https://github.com/BerriAI/litellm/pull/15966)\\n    - Add /openai routes for responses API (Azure OpenAI SDK Compatibility) - [PR #15988](https://github.com/BerriAI/litellm/pull/15988)\\n    - Redact reasoning summaries in ResponsesAPI output when message logging is disabled - [PR #15965](https://github.com/BerriAI/litellm/pull/15965)\\n    - Support text.format parameter in Responses API for providers without native ResponsesAPIConfig - [PR #16023](https://github.com/BerriAI/litellm/pull/16023)\\n    - Add LLM provider response headers to Responses API - [PR #16091](https://github.com/BerriAI/litellm/pull/16091)\\n\\n- **[Video Generation API](../../docs/video_generation)**\\n    - Add `custom_llm_provider` support for video endpoints (non-generation) - [PR #16121](https://github.com/BerriAI/litellm/pull/16121)\\n    - Fix documentation for videos - [PR #15937](https://github.com/BerriAI/litellm/pull/15937)\\n    - Add OpenAI client usage documentation for videos and fix navigation visibility - [PR #15996](https://github.com/BerriAI/litellm/pull/15996)\\n\\n- **[Moderations API](../../docs/moderations)**\\n    - Moderations endpoint now respects `api_base` configuration parameter - [PR #16087](https://github.com/BerriAI/litellm/pull/16087)\\n\\n- **[Vector Stores](../../docs/vector_stores)**\\n    - Milvus - search vector store support - [PR #16035](https://github.com/BerriAI/litellm/pull/16035)\\n    - Azure AI Vector Stores - support \\"virtual\\" indexes + create vector store on passthrough API - [PR #16160](https://github.com/BerriAI/litellm/pull/16160)\\n\\n- **[Passthrough Endpoints](../../docs/pass_through/vertex_ai)**\\n    - Support multi-part form data on passthrough - [PR #16035](https://github.com/BerriAI/litellm/pull/16035)\\n\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n\\n- **Virtual Keys**\\n    - Validation for Proxy Base URL in SSO Settings - [PR #16082](https://github.com/BerriAI/litellm/pull/16082)\\n    - Test Key UI Embeddings support - [PR #16065](https://github.com/BerriAI/litellm/pull/16065)\\n    - Add Key Type Select in Key Settings - [PR #16034](https://github.com/BerriAI/litellm/pull/16034)\\n    - Key Already Exist Error Notification - [PR #15993](https://github.com/BerriAI/litellm/pull/15993)\\n\\n- **Models + Endpoints**\\n    - Changed API Base from Select to Input in New LLM Credentials - [PR #15987](https://github.com/BerriAI/litellm/pull/15987)\\n    - Remove limit from admin UI numerical input - [PR #15991](https://github.com/BerriAI/litellm/pull/15991)\\n    - Config Models should not be editable - [PR #16020](https://github.com/BerriAI/litellm/pull/16020)\\n    - Add tags in model creation - [PR #16138](https://github.com/BerriAI/litellm/pull/16138)\\n    - Add Tags to update model - [PR #16140](https://github.com/BerriAI/litellm/pull/16140)\\n\\n- **Guardrails**\\n    - Add Apply Guardrail Testing Playground - [PR #16030](https://github.com/BerriAI/litellm/pull/16030)\\n    - Config Guardrails should not be editable and guardrail info fix - [PR #16142](https://github.com/BerriAI/litellm/pull/16142)\\n\\n- **Cache Settings**\\n    - Allow setting cache settings on UI - [PR #16143](https://github.com/BerriAI/litellm/pull/16143)\\n\\n- **Routing**\\n    - Allow setting all routing strategies, tag filtering on UI - [PR #16139](https://github.com/BerriAI/litellm/pull/16139)\\n\\n- **Admin Settings**\\n    - Add license metadata to health/readiness endpoint - [PR #15997](https://github.com/BerriAI/litellm/pull/15997)\\n    - Litellm Backend SSO Changes - [PR #16029](https://github.com/BerriAI/litellm/pull/16029)\\n\\n\\n\\n---\\n\\n## Logging / Guardrail / Prompt Management Integrations\\n\\n#### Features\\n\\n- **[OpenTelemetry](../../docs/proxy/logging#opentelemetry)**\\n    - Enable OpenTelemetry context propagation by external tracers - [PR #15940](https://github.com/BerriAI/litellm/pull/15940)\\n    - Ensure error information is logged on OTEL - [PR #15978](https://github.com/BerriAI/litellm/pull/15978)\\n\\n- **[Langfuse](../../docs/proxy/logging#langfuse)**\\n    - Fix duplicate trace in langfuse_otel - [PR #15931](https://github.com/BerriAI/litellm/pull/15931)\\n    - Support tool usage messages with Langfuse OTEL integration - [PR #15932](https://github.com/BerriAI/litellm/pull/15932)\\n\\n- **[DataDog](../../docs/proxy/logging#datadog)**\\n    - Ensure key\'s metadata + guardrail is logged on DD - [PR #15980](https://github.com/BerriAI/litellm/pull/15980)\\n\\n- **[Opik](../../docs/proxy/logging#opik)**\\n    - Enhance requester metadata retrieval from API key auth - [PR #15897](https://github.com/BerriAI/litellm/pull/15897)\\n    - User auth key metadata Documentation - [PR #16004](https://github.com/BerriAI/litellm/pull/16004)\\n\\n- **[SQS](../../docs/proxy/logging#sqs)**\\n    - Add Base64 handling for SQS Logger - [PR #16028](https://github.com/BerriAI/litellm/pull/16028)\\n\\n- **General**\\n    - Fix: User API key and team id and user id missing from custom callback is not misfiring - [PR #15982](https://github.com/BerriAI/litellm/pull/15982)\\n\\n#### Guardrails\\n\\n- **[IBM Guardrails](../../docs/proxy/guardrails)**\\n    - Update IBM Guardrails to correctly use SSL Verify argument - [PR #15975](https://github.com/BerriAI/litellm/pull/15975)\\n    - Add additional detail to ibm_guardrails.md documentation - [PR #15971](https://github.com/BerriAI/litellm/pull/15971)\\n\\n- **[Model Armor](../../docs/proxy/guardrails)**\\n    - Support during_call for model armor guardrails - [PR #15970](https://github.com/BerriAI/litellm/pull/15970)\\n\\n- **[Lasso Security](../../docs/proxy/guardrails)**\\n    - Upgrade to Lasso API v3 and fix ULID generation - [PR #15941](https://github.com/BerriAI/litellm/pull/15941)\\n\\n- **[PANW Prisma AIRS](../../docs/proxy/guardrails)**\\n    - Add per-request profile overrides to PANW Prisma AIRS - [PR #16069](https://github.com/BerriAI/litellm/pull/16069)\\n\\n- **[Grayswan](../../docs/proxy/guardrails)**\\n    - Improve Grayswan guardrail documentation - [PR #15875](https://github.com/BerriAI/litellm/pull/15875)\\n\\n- **[Pillar AI](../../docs/proxy/guardrails)**\\n    - Graceful degradation for pillar service when using litellm - [PR #15857](https://github.com/BerriAI/litellm/pull/15857)\\n\\n- **General**\\n    - Ensure Key Guardrails are applied - [PR #16025](https://github.com/BerriAI/litellm/pull/16025)\\n\\n#### Prompt Management\\n\\n- **[GitLab](../../docs/prompt_management)**\\n    - Add GitlabPromptCache and enable subfolder access - [PR #15712](https://github.com/BerriAI/litellm/pull/15712)\\n\\n---\\n\\n## Spend Tracking, Budgets and Rate Limiting\\n\\n- **Cost Tracking**\\n    - Fix spend tracking for OCR/aOCR requests (log `pages_processed` + recognize `OCRResponse`) - [PR #16070](https://github.com/BerriAI/litellm/pull/16070)\\n\\n- **Rate Limiting**\\n    - Add support for Batch API Rate limiting - PR1 adds support for input based rate limits - [PR #16075](https://github.com/BerriAI/litellm/pull/16075)\\n    - Handle multiple rate limit types per descriptor and prevent IndexError - [PR #16039](https://github.com/BerriAI/litellm/pull/16039)\\n\\n---\\n\\n## MCP Gateway\\n\\n- **OAuth**\\n    - Add support for dynamic client registration - [PR #15921](https://github.com/BerriAI/litellm/pull/15921)\\n    - Respect X-Forwarded- headers in OAuth endpoints - [PR #16036](https://github.com/BerriAI/litellm/pull/16036)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n- **Memory Leak Fixes**\\n    - Fix: prevent httpx DeprecationWarning memory leak in AsyncHTTPHandler - [PR #16024](https://github.com/BerriAI/litellm/pull/16024)\\n    - Fix: resolve memory accumulation caused by Pydantic 2.11+ deprecation warnings - [PR #16110](https://github.com/BerriAI/litellm/pull/16110)\\n    - Fix(apscheduler): prevent memory leaks from jitter and frequent job intervals - [PR #15846](https://github.com/BerriAI/litellm/pull/15846)\\n\\n- **Configuration**\\n    - Remove minimum validation for cache control injection index - [PR #16149](https://github.com/BerriAI/litellm/pull/16149)\\n    - Fix prompt_caching.md: wrong prompt_tokens definition - [PR #16044](https://github.com/BerriAI/litellm/pull/16044)\\n\\n\\n---\\n\\n## Documentation Updates\\n\\n- **Provider Documentation**\\n    - Use custom-llm-provider header in examples - [PR #16055](https://github.com/BerriAI/litellm/pull/16055)\\n    - Litellm docs readme fixes - [PR #16107](https://github.com/BerriAI/litellm/pull/16107)\\n    - Readme fixes add supported providers - [PR #16109](https://github.com/BerriAI/litellm/pull/16109)\\n\\n- **Model References**\\n    - Add supports vision field to qwen-vl models in model_prices_and_context_window.json - [PR #16106](https://github.com/BerriAI/litellm/pull/16106)\\n\\n- **General Documentation**\\n    - 1-79-0 docs - [PR #15936](https://github.com/BerriAI/litellm/pull/15936)\\n    - Add minimum resource requirement for production - [PR #16146](https://github.com/BerriAI/litellm/pull/16146)\\n\\n---\\n\\n## New Contributors\\n\\n* @RobGeada made their first contribution in [PR #15975](https://github.com/BerriAI/litellm/pull/15975)\\n* @shanto12 made their first contribution in [PR #15946](https://github.com/BerriAI/litellm/pull/15946)\\n* @dima-hx430 made their first contribution in [PR #15976](https://github.com/BerriAI/litellm/pull/15976)\\n* @m-misiura made their first contribution in [PR #15971](https://github.com/BerriAI/litellm/pull/15971)\\n* @ylgibby made their first contribution in [PR #15947](https://github.com/BerriAI/litellm/pull/15947)\\n* @Somtom made their first contribution in [PR #15909](https://github.com/BerriAI/litellm/pull/15909)\\n* @rodolfo-nobrega made their first contribution in [PR #16023](https://github.com/BerriAI/litellm/pull/16023)\\n* @bernata made their first contribution in [PR #15997](https://github.com/BerriAI/litellm/pull/15997)\\n* @AlbertDeFusco made their first contribution in [PR #15881](https://github.com/BerriAI/litellm/pull/15881)\\n* @komarovd95 made their first contribution in [PR #15789](https://github.com/BerriAI/litellm/pull/15789)\\n* @langpingxue made their first contribution in [PR #15635](https://github.com/BerriAI/litellm/pull/15635)\\n* @OrionCodeDev made their first contribution in [PR #16070](https://github.com/BerriAI/litellm/pull/16070)\\n* @sbinnee made their first contribution in [PR #16078](https://github.com/BerriAI/litellm/pull/16078)\\n* @JetoPistola made their first contribution in [PR #16106](https://github.com/BerriAI/litellm/pull/16106)\\n* @gvioss made their first contribution in [PR #16093](https://github.com/BerriAI/litellm/pull/16093)\\n* @pale-aura made their first contribution in [PR #16084](https://github.com/BerriAI/litellm/pull/16084)\\n* @tanvithakur94 made their first contribution in [PR #16041](https://github.com/BerriAI/litellm/pull/16041)\\n* @li-boxuan made their first contribution in [PR #16044](https://github.com/BerriAI/litellm/pull/16044)\\n* @1stprinciple made their first contribution in [PR #15938](https://github.com/BerriAI/litellm/pull/15938)\\n* @raghav-stripe made their first contribution in [PR #16137](https://github.com/BerriAI/litellm/pull/16137)\\n* @steve-gore-snapdocs made their first contribution in [PR #16149](https://github.com/BerriAI/litellm/pull/16149)\\n\\n---\\n\\n## Full Changelog\\n\\n**[View complete changelog on GitHub](https://github.com/BerriAI/litellm/compare/v1.79.0-stable...v1.80.0-stable)**"},{"id":"v1-79-0","metadata":{"permalink":"/release_notes/v1-79-0","source":"@site/release_notes/v1.79.0-stable/index.md","title":"v1.79.0-stable - Search APIs","description":"Deploy this version","date":"2025-10-26T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.79.0-stable - Search APIs","slug":"v1-79-0","date":"2025-10-26T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.79.1-stable - Guardrail Playground","permalink":"/release_notes/v1-79-1"},"nextItem":{"title":"v1.78.5-stable - Native OCR Support","permalink":"/release_notes/v1-78-5"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.79.0-stable\\n```\\n\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.79.0\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Major Changes\\n\\n- **Cohere models will now be routed to Cohere v2 API by default** - [PR #15722](https://github.com/BerriAI/litellm/pull/15722)\\n\\n---\\n\\n## Key Highlights\\n\\n- **Search APIs** - Native `/v1/search` endpoint with support for Perplexity, Tavily, Parallel AI, Exa AI, DataforSEO, and Google PSE with cost tracking\\n- **Vector Stores** - Vertex AI Search API integration as vector store through LiteLLM with passthrough endpoint support\\n- **Guardrails Expansion** - Apply guardrails across Responses API, Image Gen, Text completions, Audio transcriptions, Audio Speech, Rerank, and Anthropic Messages API via unified `apply_guardrails` function\\n- **New Guardrail Providers** - Gray Swan, Dynamo AI, IBM Guardrails, Lasso Security v3, and Bedrock Guardrail apply_guardrail endpoint support\\n- **Video Generation API** - Native support for OpenAI Sora-2 and Azure Sora-2 (Pro, Pro-High-Res) with cost tracking and logging support\\n- **Azure AI Speech (TTS)** - Native Azure AI Speech integration with cost tracking for standard and HD voices\\n\\n---\\n\\n## New Models / Updated Models\\n\\n#### New Model Support\\n\\n| Provider | Model | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Features |\\n| -------- | ----- | -------------- | ------------------- | -------------------- | -------- |\\n| Bedrock | `anthropic.claude-3-7-sonnet-20240620-v1:0` | 200K | $3.60 | $18.00 | Chat, reasoning, vision, function calling, prompt caching, computer use |\\n| Bedrock GovCloud | `us-gov-west-1/anthropic.claude-3-7-sonnet-20250219-v1:0` | 200K | $3.60 | $18.00 | Chat, reasoning, vision, function calling, prompt caching, computer use |\\n| Vertex AI | `mistral-medium-3` | 128K | $0.40 | $2.00 | Chat, function calling, tool choice |\\n| Vertex AI | `codestral-2` | 128K | $0.30 | $0.90 | Chat, function calling, tool choice |\\n| Bedrock | `amazon.titan-image-generator-v1` | - | - | - | Image generation - $0.008/image, $0.01/premium image |\\n| Bedrock | `amazon.titan-image-generator-v2` | - | - | - | Image generation - $0.008/image, $0.01/premium image |\\n| OpenAI | `sora-2` | - | - | - | Video generation - $0.10/video/second |\\n| Azure | `sora-2` | - | - | - | Video generation - $0.10/video/second |\\n| Azure | `sora-2-pro` | - | - | - | Video generation - $0.30/video/second |\\n| Azure | `sora-2-pro-high-res` | - | - | - | Video generation - $0.50/video/second |\\n\\n#### Features\\n\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Fix cache_control incorrectly applied to all content items instead of last item only - [PR #15699](https://github.com/BerriAI/litellm/pull/15699)\\n    - Forward anthropic-beta headers to Bedrock, VertexAI - [PR #15700](https://github.com/BerriAI/litellm/pull/15700)\\n    - Change max_tokens value to match max_output_tokens for claude sonnet - [PR #15715](https://github.com/BerriAI/litellm/pull/15715)\\n\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Add AWS us-gov-west-1 Claude 3.7 Sonnet costs - [PR #15775](https://github.com/BerriAI/litellm/pull/15775)\\n    - Fix the date for sonnet 3.7 in govcloud - [PR #15800](https://github.com/BerriAI/litellm/pull/15800)\\n    - Use proper bedrock model name in health check - [PR #15808](https://github.com/BerriAI/litellm/pull/15808)\\n    - Support for embeddings_by_type Response Format in Bedrock Cohere Embed v1 - [PR #15707](https://github.com/BerriAI/litellm/pull/15707)\\n    - Add titan image generations with cost tracking - [PR #15916](https://github.com/BerriAI/litellm/pull/15916)\\n\\n- **[Gemini](../../docs/providers/gemini)**\\n    - Add imageConfig parameter for gemini-2.5-flash-image - [PR #15530](https://github.com/BerriAI/litellm/pull/15530)\\n    - Replace deprecated gemini-1.5-pro-preview-0514 - [PR #15852](https://github.com/BerriAI/litellm/pull/15852)\\n    - Update vertex ai gemini costs - [PR #15911](https://github.com/BerriAI/litellm/pull/15911)\\n\\n- **[Ollama](../../docs/providers/ollama)**\\n    - Set \'think\' to False when reasoning effort is minimal/none/disable - [PR #15763](https://github.com/BerriAI/litellm/pull/15763)\\n    - Handle parsing ollama chunk error - [PR #15717](https://github.com/BerriAI/litellm/pull/15717)\\n\\n- **[Vertex AI](../../docs/providers/vertex)**\\n    - Add mistral medium 3 and Codestral 2 on vertex - [PR #15887](https://github.com/BerriAI/litellm/pull/15887)\\n\\n- **[Databricks](../../docs/providers/databricks)**\\n    - Allow prompt caching to be used for Anthropic Claude on Databricks - [PR #15801](https://github.com/BerriAI/litellm/pull/15801)\\n\\n- **[Azure](../../docs/providers/azure)**\\n    - Add Azure AVA TTS integration - [PR #15749](https://github.com/BerriAI/litellm/pull/15749)\\n    - Add Azure AVA (Speech AI) Cost Tracking - [PR #15754](https://github.com/BerriAI/litellm/pull/15754)\\n    - Azure AI Speech - Ensure `voice` is mapped from request body to SSML body, allow sending `role` and `style` - [PR #15810](https://github.com/BerriAI/litellm/pull/15810)\\n    - Add Azure support for video generation functionality (Sora-2) - [PR #15901](https://github.com/BerriAI/litellm/pull/15901)\\n\\n- **[OpenAI](../../docs/providers/openai)**\\n    - OpenAI videos refactoring - [PR #15900](https://github.com/BerriAI/litellm/pull/15900)\\n\\n- **General**\\n    - Read from custom-llm-provider header - [PR #15528](https://github.com/BerriAI/litellm/pull/15528)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n\\n- **[Responses API](../../docs/response_api)**\\n    - Add gpt 4.1 pricing for response endpoint - [PR #15593](https://github.com/BerriAI/litellm/pull/15593)\\n    - Fix Incorrect status value in responses api with gemini - [PR #15753](https://github.com/BerriAI/litellm/pull/15753)\\n    - Simplify reasoning item handling for gpt-5-codex - [PR #15815](https://github.com/BerriAI/litellm/pull/15815)\\n    - ErrorEvent ValidationError when OpenAI Responses API returns nested error structure - [PR #15804](https://github.com/BerriAI/litellm/pull/15804)\\n    - Fix reasoning item ID auto-generation causing encrypted content verification errors - [PR #15782](https://github.com/BerriAI/litellm/pull/15782)\\n    - Support tags in metadata - [PR #15867](https://github.com/BerriAI/litellm/pull/15867)\\n    - Security: prevent User A from retrieving User B\'s response, if response.id is leaked - [PR #15757](https://github.com/BerriAI/litellm/pull/15757)\\n\\n- **[Batch API](../../docs/batch_api)**\\n    - Add pre and post call for list batches - [PR #15673](https://github.com/BerriAI/litellm/pull/15673)\\n    - Add function responsible to call precall - [PR #15636](https://github.com/BerriAI/litellm/pull/15636)\\n    - Fix \\"User default_user_id does not have access to the object\\" when object not in db - [PR #15873](https://github.com/BerriAI/litellm/pull/15873)\\n\\n- **[OCR API](../../docs/ocr)**\\n    - Add Azure AI - OCR to docs - [PR #15768](https://github.com/BerriAI/litellm/pull/15768)\\n    - Add mode + Health check support for OCR models - [PR #15767](https://github.com/BerriAI/litellm/pull/15767)\\n\\n- **[Search API](../../docs/search_api)**\\n    - Add def search() APIs for Web Search - Perplexity API - [PR #15769](https://github.com/BerriAI/litellm/pull/15769)\\n    - Add Tavily Search API - [PR #15770](https://github.com/BerriAI/litellm/pull/15770)\\n    - Add Parallel AI - Search API - [PR #15772](https://github.com/BerriAI/litellm/pull/15772)\\n    - Add EXA AI Search API to LiteLLM - [PR #15774](https://github.com/BerriAI/litellm/pull/15774)\\n    - Add /search endpoint on LiteLLM Gateway - [PR #15780](https://github.com/BerriAI/litellm/pull/15780)\\n    - Add DataforSEO Search API - [PR #15817](https://github.com/BerriAI/litellm/pull/15817)\\n    - Add Google PSE Search Provider - [PR #15816](https://github.com/BerriAI/litellm/pull/15816)\\n    - Add cost tracking for Search API requests - Google PSE, Tavily, Parallel AI, Exa AI - [PR #15821](https://github.com/BerriAI/litellm/pull/15821)\\n    - Backend: Allow storing configured Search APIs in DB - [PR #15862](https://github.com/BerriAI/litellm/pull/15862)\\n    - Exa Search API - ensure request params are sent to Exa AI - [PR #15855](https://github.com/BerriAI/litellm/pull/15855)\\n\\n- **[Vector Stores](../../docs/vector_stores)**\\n    - Support Vertex AI Search API as vector store through LiteLLM - [PR #15781](https://github.com/BerriAI/litellm/pull/15781)\\n    - Azure AI - Search Vector Stores - [PR #15873](https://github.com/BerriAI/litellm/pull/15873)\\n    - VertexAI Search Vector Store - Passthrough endpoint support + Vector store search Cost tracking support - [PR #15824](https://github.com/BerriAI/litellm/pull/15824)\\n    - Don\'t raise error if managed object is not found - [PR #15873](https://github.com/BerriAI/litellm/pull/15873)\\n    - Show config.yaml vector stores on UI - [PR #15873](https://github.com/BerriAI/litellm/pull/15873)\\n    - Cost tracking for search spend - [PR #15859](https://github.com/BerriAI/litellm/pull/15859)\\n\\n- **[Images API](../../docs/image_generation)**\\n    - Pass user-defined headers and extra_headers to image-edit calls - [PR #15811](https://github.com/BerriAI/litellm/pull/15811)\\n\\n- **[Video Generation API](../../docs/video_generation)**\\n    - Add Azure support for video generation functionality (Sora-2, Sora-2-Pro, Sora-2-Pro-High-Res) - [PR #15901](https://github.com/BerriAI/litellm/pull/15901)\\n    - OpenAI video generation refactoring (Sora-2) - [PR #15900](https://github.com/BerriAI/litellm/pull/15900)\\n\\n- **[Bedrock /invoke](../../docs/bedrock_invoke)**\\n    - Fix: Hooks broken on /bedrock passthrough due to missing metadata - [PR #15849](https://github.com/BerriAI/litellm/pull/15849)\\n\\n- **[Realtime API](../../docs/realtime_api)**\\n    - Fix: OpenAI Realtime API integration fails due to websockets.exceptions.PayloadTooBig error - [PR #15751](https://github.com/BerriAI/litellm/pull/15751)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n\\n- **Passthrough**\\n    - Set auth on passthrough endpoints, on the UI - [PR #15778](https://github.com/BerriAI/litellm/pull/15778)\\n    - Fix pass-through endpoint budget enforcement bug - [PR #15805](https://github.com/BerriAI/litellm/pull/15805)\\n\\n- **Organizations**\\n    - Allow org admins to create teams on UI - [PR #15924](https://github.com/BerriAI/litellm/pull/15924)\\n\\n- **Search Tools**\\n    - UI - Search Tools, allow adding search tools on UI + testing search - [PR #15871](https://github.com/BerriAI/litellm/pull/15871)\\n    - UI - Add logos for search providers - [PR #15872](https://github.com/BerriAI/litellm/pull/15872)\\n\\n- **General**\\n    - Fix routing for custom server root path - [PR #15701](https://github.com/BerriAI/litellm/pull/15701)\\n\\n---\\n\\n## Logging / Guardrail / Prompt Management Integrations\\n\\n#### Features\\n\\n- **[OpenTelemetry](../../docs/proxy/logging#opentelemetry)**\\n    - Fix OpenTelemetry Logging functionality - [PR #15645](https://github.com/BerriAI/litellm/pull/15645)\\n    - Fix issue where headers were not being split correctly - [PR #15916](https://github.com/BerriAI/litellm/pull/15916)\\n\\n- **[Sentry](../../docs/proxy/logging#sentry)**\\n    - Add SENTRY_ENVIRONMENT configuration for Sentry integration - [PR #15760](https://github.com/BerriAI/litellm/pull/15760)\\n\\n- **[Helicone](../../docs/proxy/logging#helicone)**\\n    - Fix JSON serialization error in Helicone logging by removing OpenTelemetry span from metadata - [PR #15728](https://github.com/BerriAI/litellm/pull/15728)\\n\\n- **[MLFlow](../../docs/proxy/logging#mlflow)**\\n    - Fix MLFlow tags - split request_tags into (key, val) if request_tag has colon - [PR #15914](https://github.com/BerriAI/litellm/pull/15914)\\n\\n- **General**\\n    - Rename configured_cold_storage_logger to cold_storage_custom_logger - [PR #15798](https://github.com/BerriAI/litellm/pull/15798)\\n\\n#### Guardrails\\n\\n- **[Gray Swan](../../docs/proxy/guardrails)**\\n    - Add GraySwan Guardrails support - [PR #15756](https://github.com/BerriAI/litellm/pull/15756)\\n    - Rename GraySwan to Gray Swan - [PR #15771](https://github.com/BerriAI/litellm/pull/15771)\\n\\n- **[Dynamo AI](../../docs/proxy/guardrails)**\\n    - New Guardrail - Dynamo AI Guardrail - [PR #15920](https://github.com/BerriAI/litellm/pull/15920)\\n\\n- **[IBM Guardrails](../../docs/proxy/guardrails)**\\n    - IBM Guardrails integration - [PR #15924](https://github.com/BerriAI/litellm/pull/15924)\\n\\n- **[Lasso Security](../../docs/proxy/guardrails)**\\n    - Add v3 API Support - [PR #12452](https://github.com/BerriAI/litellm/pull/12452)\\n    - Fixed lasso import config, redis cluster hash tags for test keys - [PR #15917](https://github.com/BerriAI/litellm/pull/15917)\\n\\n- **[Bedrock Guardrails](../../docs/proxy/guardrails)**\\n    - Implement Bedrock Guardrail apply_guardrail endpoint support - [PR #15892](https://github.com/BerriAI/litellm/pull/15892)\\n\\n- **General**\\n    - Guardrails - Responses API, Image Gen, Text completions, Audio transcriptions, Audio Speech, Rerank, Anthropic Messages API support via the unified `apply_guardrails` function - [PR #15706](https://github.com/BerriAI/litellm/pull/15706)\\n\\n---\\n\\n## Spend Tracking, Budgets and Rate Limiting\\n\\n- **Rate Limiting**\\n    - Support absolute RPM/TPM in priority_reservation - [PR #15813](https://github.com/BerriAI/litellm/pull/15813)\\n    - Org level tpm/rpm limits + Team tpm/rpm validation when assigned to org - [PR #15549](https://github.com/BerriAI/litellm/pull/15549)\\n\\n---\\n\\n## MCP Gateway\\n\\n- **OAuth**\\n    - Auth Header Fix for MCP Tool Call - [PR #15736](https://github.com/BerriAI/litellm/pull/15736)\\n    - Add response_type + PKCE parameters to OAuth authorization endpoint - [PR #15720](https://github.com/BerriAI/litellm/pull/15720)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n- **Database**\\n    - Minimize the occurrence of deadlocks - [PR #15281](https://github.com/BerriAI/litellm/pull/15281)\\n\\n- **Redis**\\n    - Apply max_connections configuration to Redis async client - [PR #15797](https://github.com/BerriAI/litellm/pull/15797)\\n\\n- **Caching**\\n    - Add documentation for `enable_caching_on_provider_specific_optional_params` setting - [PR #15885](https://github.com/BerriAI/litellm/pull/15885)\\n\\n---\\n\\n## Documentation Updates\\n\\n- **Provider Documentation**\\n    - Update worker recommendation - [PR #15702](https://github.com/BerriAI/litellm/pull/15702)\\n    - Fix the wrong request body in json mode doc - [PR #15729](https://github.com/BerriAI/litellm/pull/15729)\\n    - Add details in docs - [PR #15721](https://github.com/BerriAI/litellm/pull/15721)\\n    - Add responses api on openai docs - [PR #15866](https://github.com/BerriAI/litellm/pull/15866)\\n    - Add OpenAI responses api - [PR #15868](https://github.com/BerriAI/litellm/pull/15868)\\n\\n---\\n\\n## New Contributors\\n\\n* @tlecomte made their first contribution in [PR #15528](https://github.com/BerriAI/litellm/pull/15528)\\n* @tomhaynes made their first contribution in [PR #15645](https://github.com/BerriAI/litellm/pull/15645)\\n* @talalryz made their first contribution in [PR #15720](https://github.com/BerriAI/litellm/pull/15720)\\n* @1vinodsingh1 made their first contribution in [PR #15736](https://github.com/BerriAI/litellm/pull/15736)\\n* @nuernber made their first contribution in [PR #15775](https://github.com/BerriAI/litellm/pull/15775)\\n* @Thomas-Mildner made their first contribution in [PR #15760](https://github.com/BerriAI/litellm/pull/15760)\\n* @javiergarciapleo made their first contribution in [PR #15721](https://github.com/BerriAI/litellm/pull/15721)\\n* @lshgdut made their first contribution in [PR #15717](https://github.com/BerriAI/litellm/pull/15717)\\n* @kk-wangjifeng made their first contribution in [PR #15530](https://github.com/BerriAI/litellm/pull/15530)\\n* @anthonyivn2 made their first contribution in [PR #15801](https://github.com/BerriAI/litellm/pull/15801)\\n* @romanglo made their first contribution in [PR #15707](https://github.com/BerriAI/litellm/pull/15707)\\n* @mythral made their first contribution in [PR #15859](https://github.com/BerriAI/litellm/pull/15859)\\n* @mubashirosmani made their first contribution in [PR #15866](https://github.com/BerriAI/litellm/pull/15866)\\n* @CAFxX made their first contribution in [PR #15281](https://github.com/BerriAI/litellm/pull/15281)\\n* @reflection made their first contribution in [PR #15914](https://github.com/BerriAI/litellm/pull/15914)\\n* @shadielfares made their first contribution in [PR #15917](https://github.com/BerriAI/litellm/pull/15917)\\n\\n---\\n\\n## PR Count Summary\\n\\n### 10/26/2025\\n* New Models / Updated Models: 20\\n* LLM API Endpoints: 29\\n* Management Endpoints / UI: 5\\n* Logging / Guardrail / Prompt Management Integrations: 10\\n* Spend Tracking, Budgets and Rate Limiting: 2\\n* MCP Gateway: 2\\n* Performance / Loadbalancing / Reliability improvements: 3\\n* Documentation Updates: 5\\n\\n---\\n\\n## Full Changelog\\n\\n**[View complete changelog on GitHub](https://github.com/BerriAI/litellm/compare/v1.78.5-stable...v1.79.0-stable)**"},{"id":"v1-78-5","metadata":{"permalink":"/release_notes/v1-78-5","source":"@site/release_notes/v1.78.5-stable/index.md","title":"v1.78.5-stable - Native OCR Support","description":"Deploy this version","date":"2025-10-18T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.78.5-stable - Native OCR Support","slug":"v1-78-5","date":"2025-10-18T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.79.0-stable - Search APIs","permalink":"/release_notes/v1-79-0"},"nextItem":{"title":"v1.78.0-stable - MCP Gateway: Control Tool Access by Team, Key","permalink":"/release_notes/v1-78-0"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.78.5-stable\\n```\\n\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.78.5\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights\\n\\n- **Native OCR Endpoints** - Native `/v1/ocr` endpoint support with cost tracking for Mistral OCR and Azure AI OCR\\n- **Global Vendor Discounts** - Specify global vendor discount percentages for accurate cost tracking and reporting\\n- **Team Spending Reports** - Team admins can now export detailed spending reports for their teams\\n- **Claude Haiku 4.5** - Day 0 support for Claude Haiku 4.5 across Bedrock, Vertex AI, and OpenRouter with 200K context window\\n- **GPT-5-Codex** - Support for GPT-5-Codex via Responses API on OpenAI and Azure\\n- **Performance Improvements** - Major router optimizations: O(1) model lookups, 10-100x faster shallow copy, 30-40% faster timing calls, and O(n) to O(1) hash generation\\n\\n---\\n\\n## New Models / Updated Models\\n\\n#### New Model Support\\n\\n| Provider | Model | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Features |\\n| -------- | ----- | -------------- | ------------------- | -------------------- | -------- |\\n| Anthropic | `claude-haiku-4-5` | 200K | $1.00 | $5.00 | Chat, reasoning, vision, function calling, prompt caching, computer use |\\n| Anthropic | `claude-haiku-4-5-20251001` | 200K | $1.00 | $5.00 | Chat, reasoning, vision, function calling, prompt caching, computer use |\\n| Bedrock | `anthropic.claude-haiku-4-5-20251001-v1:0` | 200K | $1.00 | $5.00 | Chat, reasoning, vision, function calling, prompt caching |\\n| Bedrock | `global.anthropic.claude-haiku-4-5-20251001-v1:0` | 200K | $1.00 | $5.00 | Chat, reasoning, vision, function calling, prompt caching |\\n| Bedrock | `jp.anthropic.claude-haiku-4-5-20251001-v1:0` | 200K | $1.10 | $5.50 | Chat, reasoning, vision, function calling, prompt caching (JP Cross-Region) |\\n| Bedrock | `us.anthropic.claude-haiku-4-5-20251001-v1:0` | 200K | $1.10 | $5.50 | Chat, reasoning, vision, function calling, prompt caching (US region) |\\n| Bedrock | `eu.anthropic.claude-haiku-4-5-20251001-v1:0` | 200K | $1.10 | $5.50 | Chat, reasoning, vision, function calling, prompt caching (EU region) |\\n| Bedrock | `apac.anthropic.claude-haiku-4-5-20251001-v1:0` | 200K | $1.10 | $5.50 | Chat, reasoning, vision, function calling, prompt caching (APAC region) |\\n| Bedrock | `au.anthropic.claude-haiku-4-5-20251001-v1:0` | 200K | $1.10 | $5.50 | Chat, reasoning, vision, function calling, prompt caching (AU region) |\\n| Vertex AI | `vertex_ai/claude-haiku-4-5@20251001` | 200K | $1.00 | $5.00 | Chat, reasoning, vision, function calling, prompt caching |\\n| OpenAI | `gpt-5` | 272K | $1.25 | $10.00 | Chat, responses API, reasoning, vision, function calling, prompt caching |\\n| OpenAI | `gpt-5-codex` | 272K | $1.25 | $10.00 | Responses API mode |\\n| Azure | `azure/gpt-5-codex` | 272K | $1.25 | $10.00 | Responses API mode |\\n| Gemini | `gemini-2.5-flash-image` | 32K | $0.30 | $2.50 | Image generation (GA - Nano Banana) - $0.039/image |\\n| ZhipuAI | `glm-4.6` | - | - | - | Chat completions |\\n\\n#### Features\\n\\n- **[OpenAI](../../docs/providers/openai)**\\n    - GPT-5 return reasoning content via /chat/completions + GPT-5-Codex working on Claude Code - [PR #15441](https://github.com/BerriAI/litellm/pull/15441)\\n\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Reduce claude-4-sonnet max_output_tokens to 64k - [PR #15409](https://github.com/BerriAI/litellm/pull/15409)\\n    - Added claude-haiku-4.5 - [PR #15579](https://github.com/BerriAI/litellm/pull/15579)\\n    - Add support for thinking blocks and redacted thinking blocks in Anthropic v1/messages API - [PR #15501](https://github.com/BerriAI/litellm/pull/15501)\\n\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Add anthropic.claude-haiku-4-5-20251001-v1:0 on Bedrock, VertexAI - [PR #15581](https://github.com/BerriAI/litellm/pull/15581)\\n    - Add Claude Haiku 4.5 support for Bedrock global and US regions - [PR #15650](https://github.com/BerriAI/litellm/pull/15650)\\n    - Add Claude Haiku 4.5 support for Bedrock Other regions - [PR #15653](https://github.com/BerriAI/litellm/pull/15653)\\n    - Add JP Cross-Region Inference jp.anthropic.claude-haiku-4-5-20251001 - [PR #15598](https://github.com/BerriAI/litellm/pull/15598)\\n    - Fix: bedrock-pricing-geo-inregion-cross-region / add Global Cross-Region Inference - [PR #15685](https://github.com/BerriAI/litellm/pull/15685)\\n    - Fix: Support us-gov prefix for AWS GovCloud Bedrock models - [PR #15626](https://github.com/BerriAI/litellm/pull/15626)\\n    - Fix GPT-OSS in Bedrock now supports streaming. Revert fake streaming - [PR #15668](https://github.com/BerriAI/litellm/pull/15668)\\n\\n- **[Gemini](../../docs/providers/gemini)**\\n    - Feat(pricing): Add Gemini 2.5 Flash Image (Nano Banana) in GA - [PR #15557](https://github.com/BerriAI/litellm/pull/15557)\\n    - Fix: Gemini 2.5 Flash Image should not have supports_web_search=true - [PR #15642](https://github.com/BerriAI/litellm/pull/15642)\\n    - Remove penalty params as supported params for gemini preview model - [PR #15503](https://github.com/BerriAI/litellm/pull/15503)\\n\\n- **[Ollama](../../docs/providers/ollama)**\\n    - Fix(ollama/chat): correctly map reasoning_effort to think in requests - [PR #15465](https://github.com/BerriAI/litellm/pull/15465)\\n\\n- **[OpenRouter](../../docs/providers/openrouter)**\\n    - Add anthropic/claude-sonnet-4.5 to OpenRouter cost map - [PR #15472](https://github.com/BerriAI/litellm/pull/15472)\\n    - Prompt caching for anthropic models with OpenRouter - [PR #15535](https://github.com/BerriAI/litellm/pull/15535)\\n    - Get completion cost directly from OpenRouter - [PR #15448](https://github.com/BerriAI/litellm/pull/15448)\\n    - Fix OpenRouter Claude Opus 4 model naming - [PR #15495](https://github.com/BerriAI/litellm/pull/15495)\\n\\n- **[CometAPI](../../docs/providers/comet)**\\n    - Fix(cometapi): improve CometAPI provider support (embeddings, image generation, docs) - [PR #15591](https://github.com/BerriAI/litellm/pull/15591)\\n\\n- **[Lemonade](../../docs/providers/lemonade)**\\n    - Adding new models to the lemonade provider - [PR #15554](https://github.com/BerriAI/litellm/pull/15554)\\n\\n- **[Watson X](../../docs/providers/watsonx)**\\n    - Fix (pricing): Fix pricing for watsonx model family for various models - [PR #15670](https://github.com/BerriAI/litellm/pull/15670)\\n\\n- **[Vercel AI Gateway](../../docs/providers/vercel_ai_gateway)**\\n    - Add glm-4.6 model to pricing configuration - [PR #15679](https://github.com/BerriAI/litellm/pull/15679)\\n\\n- **[Vertex AI](../../docs/providers/vertex)**\\n    - Add Vertex AI Discovery Engine Rerank Support - [PR #15532](https://github.com/BerriAI/litellm/pull/15532)\\n\\n### Bug Fixes\\n\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Fix: Pricing for Claude Sonnet 4.5 in US regions is 10x too high - [PR #15374](https://github.com/BerriAI/litellm/pull/15374)\\n\\n- **[OpenRouter](../../docs/providers/openrouter)**\\n    - Change gpt-5-codex support in model_price json - [PR #15540](https://github.com/BerriAI/litellm/pull/15540)\\n\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Fix filtering headers for signature calcs - [PR #15590](https://github.com/BerriAI/litellm/pull/15590)\\n\\n- **General**\\n    - Add native reasoning and streaming support flag for gpt-5-codex - [PR #15569](https://github.com/BerriAI/litellm/pull/15569)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n\\n- **[Responses API](../../docs/response_api)**\\n    - Responses API - enable calling anthropic/gemini models in Responses API streaming in openai ruby sdk + DB - sanity check pending migrations before startup - [PR #15432](https://github.com/BerriAI/litellm/pull/15432)\\n    - Add support for responses mode in health check - [PR #15658](https://github.com/BerriAI/litellm/pull/15658)\\n\\n- **[OCR API](../../docs/ocr)**\\n    - Feat: Add native litellm.ocr() functions - [PR #15567](https://github.com/BerriAI/litellm/pull/15567)\\n    - Feat: Add /ocr route on LiteLLM AI Gateway - Adds support for native Mistral OCR calling - [PR #15571](https://github.com/BerriAI/litellm/pull/15571)\\n    - Feat: Add Azure AI Mistral OCR Integration - [PR #15572](https://github.com/BerriAI/litellm/pull/15572)\\n    - Feat: Native /ocr endpoint support - [PR #15573](https://github.com/BerriAI/litellm/pull/15573)\\n    - Feat: Add Cost Tracking for /ocr endpoints - [PR #15678](https://github.com/BerriAI/litellm/pull/15678)\\n\\n- **[/generateContent](../../docs/providers/gemini)**\\n    - Fix: GEMINI - CLI - add google_routes to llm_api_routes - [PR #15500](https://github.com/BerriAI/litellm/pull/15500)\\n    - Fix Pydantic validation error for citationMetadata.citationSources in Google GenAI responses - [PR #15592](https://github.com/BerriAI/litellm/pull/15592)\\n\\n- **[Images API](../../docs/image_generation)**\\n    - Fix: Dall-e-2 for Image Edits API - [PR #15604](https://github.com/BerriAI/litellm/pull/15604)\\n\\n- **[Bedrock Passthrough](../../docs/pass_through/bedrock)**\\n    - Feat: Allow calling /invoke, /converse routes through AI Gateway + models on config.yaml - [PR #15618](https://github.com/BerriAI/litellm/pull/15618)\\n\\n#### Bugs\\n\\n- **General**\\n    - Fix: Convert object to a correct type - [PR #15634](https://github.com/BerriAI/litellm/pull/15634)\\n    - Bug Fix: Tags as metadata dicts were raising exceptions - [PR #15625](https://github.com/BerriAI/litellm/pull/15625)\\n    - Add type hint to function_to_dict and fix typo - [PR #15580](https://github.com/BerriAI/litellm/pull/15580)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n\\n- **Virtual Keys**\\n    - Docs: Key Rotations - [PR #15455](https://github.com/BerriAI/litellm/pull/15455)\\n    - Fix: UI - Key Max Budget Removal Error Fix - [PR #15672](https://github.com/BerriAI/litellm/pull/15672)\\n    - litellm_Key Settings Max Budget Removal Error Fix - [PR #15669](https://github.com/BerriAI/litellm/pull/15669)\\n\\n- **Teams**\\n    - Feat: Allow Team Admins to export a report of the team spending - [PR #15542](https://github.com/BerriAI/litellm/pull/15542)\\n\\n- **Passthrough**\\n    - Feat: Passthrough - allow admin to give access to specific passthrough endpoints - [PR #15401](https://github.com/BerriAI/litellm/pull/15401)\\n\\n- **SCIM v2**\\n    - Feat(scim_v2.py): if group.id doesn\'t exist, use external id + Passthrough - ensure updates and deletions persist across instances - [PR #15276](https://github.com/BerriAI/litellm/pull/15276)\\n\\n- **SSO**\\n    - Feat: UI SSO - Add PKCE for OKTA SSO - [PR #15608](https://github.com/BerriAI/litellm/pull/15608)\\n    - Fix: Separate OAuth M2M authentication from UI SSO + Handle Introspection endpoint for Oauth2 - [PR #15667](https://github.com/BerriAI/litellm/pull/15667)\\n    - Fix/entraid app roles jwt claim clean - [PR #15583](https://github.com/BerriAI/litellm/pull/15583)\\n\\n---\\n\\n## Logging / Guardrail / Prompt Management Integrations\\n\\n#### Guardrails\\n\\n- **General**\\n    - Fix apply_guardrail endpoint returning raw string instead of ApplyGuardrailResponse - [PR #15436](https://github.com/BerriAI/litellm/pull/15436)\\n    - Fix: Ensure guardrail memory sync after database updates - [PR #15633](https://github.com/BerriAI/litellm/pull/15633)\\n    - Feat: add guardrail for image generation - [PR #15619](https://github.com/BerriAI/litellm/pull/15619)\\n    - Feat: Add Guardrails for /v1/messages and /v1/responses API - [PR #15686](https://github.com/BerriAI/litellm/pull/15686)\\n\\n- **[Pillar Security](../../docs/proxy/guardrails)**\\n    - Feature: update pillar security integration to support no persistence mode in litellm proxy - [PR #15599](https://github.com/BerriAI/litellm/pull/15599)\\n\\n#### Prompt Management\\n\\n- **General**\\n    - Small fix code snippet custom_prompt_management.md - [PR #15544](https://github.com/BerriAI/litellm/pull/15544)\\n\\n---\\n\\n## Spend Tracking, Budgets and Rate Limiting\\n\\n- **Cost Tracking**\\n    - Feat: Cost Tracking - specify a global vendor discount for costs - [PR #15546](https://github.com/BerriAI/litellm/pull/15546)\\n    - Feat: UI - Allow setting Provider Discounts on UI - [PR #15550](https://github.com/BerriAI/litellm/pull/15550)\\n\\n- **Budgets**\\n    - Fix: improve budget clarity - [PR #15682](https://github.com/BerriAI/litellm/pull/15682)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n- **Router Optimizations**\\n    - Perf(router): use shallow copy instead of deepcopy for model aliases - 10-100x faster than deepcopy on nested dict structures - [PR #15576](https://github.com/BerriAI/litellm/pull/15576)\\n    - Perf(router): optimize string concatenation in hash generation - Improves time complexity from O(n\xb2) to O(n) - [PR #15575](https://github.com/BerriAI/litellm/pull/15575)\\n    - Perf(router): optimize model lookups with O(1) data structures - Replace O(n) scans with index map lookups - [PR #15578](https://github.com/BerriAI/litellm/pull/15578)\\n    - Perf(router): optimize model lookups with O(1) index maps - Use model_id_to_deployment_index_map and model_name_to_deployment_indices for instant lookups - [PR #15574](https://github.com/BerriAI/litellm/pull/15574)\\n    - Perf(router): optimize timing functions in completion hot path - Use time.perf_counter() for duration measurements and time.monotonic() for timeout calculations, providing 30-40% faster timing calls - [PR #15617](https://github.com/BerriAI/litellm/pull/15617)\\n\\n- **SSL/TLS Performance**\\n    - Feat(ssl): add configurable ECDH curve for TLS performance - Configure via ssl_ecdh_curve setting to disable PQC on OpenSSL 3.x for better performance - [PR #15617](https://github.com/BerriAI/litellm/pull/15617)\\n\\n- **Token Counter**\\n    - Fix(token-counter): extract model_info from deployment for custom_tokenizer - [PR #15680](https://github.com/BerriAI/litellm/pull/15680)\\n\\n- **Performance Metrics**\\n    - Add: perf summary - [PR #15458](https://github.com/BerriAI/litellm/pull/15458)\\n\\n- **CI/CD**\\n    - Fix: CI/CD - Missing env key & Linter type error - [PR #15606](https://github.com/BerriAI/litellm/pull/15606)\\n\\n---\\n\\n## Documentation Updates\\n\\n- **Provider Documentation**\\n    - Litellm docs 10 11 2025 - [PR #15457](https://github.com/BerriAI/litellm/pull/15457)\\n    - Docs: add ecs deployment guide - [PR #15468](https://github.com/BerriAI/litellm/pull/15468)\\n    - Docs: Update benchmark results - [PR #15461](https://github.com/BerriAI/litellm/pull/15461)\\n    - Fix: add missing context to benchmark docs - [PR #15688](https://github.com/BerriAI/litellm/pull/15688)\\n\\n- **General**\\n    - Fixed a few typos - [PR #15267](https://github.com/BerriAI/litellm/pull/15267)\\n\\n---\\n\\n## New Contributors\\n\\n* @jlan-nl made their first contribution in [PR #15374](https://github.com/BerriAI/litellm/pull/15374)\\n* @ImadSaddik made their first contribution in [PR #15267](https://github.com/BerriAI/litellm/pull/15267)\\n* @huangyafei made their first contribution in [PR #15472](https://github.com/BerriAI/litellm/pull/15472)\\n* @mubashir1osmani made their first contribution in [PR #15468](https://github.com/BerriAI/litellm/pull/15468)\\n* @kowyo made their first contribution in [PR #15465](https://github.com/BerriAI/litellm/pull/15465)\\n* @dhruvyad made their first contribution in [PR #15448](https://github.com/BerriAI/litellm/pull/15448)\\n* @davizucon made their first contribution in [PR #15544](https://github.com/BerriAI/litellm/pull/15544)\\n* @FelipeRodriguesGare made their first contribution in [PR #15540](https://github.com/BerriAI/litellm/pull/15540)\\n* @ndrsfel made their first contribution in [PR #15557](https://github.com/BerriAI/litellm/pull/15557)\\n* @shinharaguchi made their first contribution in [PR #15598](https://github.com/BerriAI/litellm/pull/15598)\\n* @TensorNull made their first contribution in [PR #15591](https://github.com/BerriAI/litellm/pull/15591)\\n* @TeddyAmkie made their first contribution in [PR #15583](https://github.com/BerriAI/litellm/pull/15583)\\n* @aniketmaurya made their first contribution in [PR #15580](https://github.com/BerriAI/litellm/pull/15580)\\n* @eddierichter-amd made their first contribution in [PR #15554](https://github.com/BerriAI/litellm/pull/15554)\\n* @konekohana made their first contribution in [PR #15535](https://github.com/BerriAI/litellm/pull/15535)\\n* @Classic298 made their first contribution in [PR #15495](https://github.com/BerriAI/litellm/pull/15495)\\n* @afogel made their first contribution in [PR #15599](https://github.com/BerriAI/litellm/pull/15599)\\n* @orolega made their first contribution in [PR #15633](https://github.com/BerriAI/litellm/pull/15633)\\n* @LucasSugi made their first contribution in [PR #15634](https://github.com/BerriAI/litellm/pull/15634)\\n* @uc4w6c made their first contribution in [PR #15619](https://github.com/BerriAI/litellm/pull/15619)\\n* @Sameerlite made their first contribution in [PR #15658](https://github.com/BerriAI/litellm/pull/15658)\\n* @yuneng-jiang made their first contribution in [PR #15672](https://github.com/BerriAI/litellm/pull/15672)\\n* @Nikro made their first contribution in [PR #15680](https://github.com/BerriAI/litellm/pull/15680)\\n\\n---\\n\\n## Full Changelog\\n\\n**[View complete changelog on GitHub](https://github.com/BerriAI/litellm/compare/v1.78.0-stable...v1.78.4-stable)**"},{"id":"v1-78-0","metadata":{"permalink":"/release_notes/v1-78-0","source":"@site/release_notes/v1.78.0-stable/index.md","title":"v1.78.0-stable - MCP Gateway: Control Tool Access by Team, Key","description":"Deploy this version","date":"2025-10-11T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.78.0-stable - MCP Gateway: Control Tool Access by Team, Key","slug":"v1-78-0","date":"2025-10-11T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.78.5-stable - Native OCR Support","permalink":"/release_notes/v1-78-5"},"nextItem":{"title":"v1.77.7-stable - 2.9x Lower Median Latency","permalink":"/release_notes/v1-77-7"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.78.0-stable\\n```\\n\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.78.0.post1\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights\\n\\n- **MCP Gateway - Control Tool Access by Team, Key** - Control MCP tool access by team/key. \\n- **Performance Improvements** - 70% Lower p99 Latency\\n- **GPT-5 Pro & GPT-Image-1-Mini** - Day 0 support for OpenAI\'s GPT-5 Pro (400K context) and gpt-image-1-mini image generation\\n- **EnkryptAI Guardrails** - New guardrail integration for content moderation\\n- **Tag-Based Budgets** - Support for setting budgets based on request tags\\n\\n---\\n\\n### MCP Gateway - Control Tool Access by Team, Key\\n\\n<Image \\n  img={require(\'../../img/release_notes/tool_control.png\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n\\n<br/>\\n\\nProxy admins can now control MCP tool access by team or key. This makes it easy to grant different teams selective access to tools from the same MCP server.\\n\\nFor example, you can now give your Engineering team access to `list_repositories`, `create_issue`, and `search_code` tools, while Sales only gets `search_code` and `close_issue` tools. \\n\\nThis makes it easier for Proxy Admins to govern MCP Tool Access.\\n\\n[Get Started](../../docs/mcp_control#set-allowed-tools-for-a-key-team-or-organization)\\n\\n---\\n\\n## Performance - 70% Lower p99 Latency\\n\\n<Image img={require(\'../../img/release_notes/1_78_0_perf.png\')}  style={{ width: \'800px\', height: \'auto\' }} />\\n\\n<br/>\\n\\nThis release cuts p99 latency by 70% on LiteLLM AI Gateway, making it even better for low-latency use cases.\\n\\nThese gains come from two key enhancements:\\n\\n**Reliable Sessions**\\n\\nAdded support for shared sessions with aiohttp. The shared_session parameter is now consistently used across all calls, enabling connection pooling.\\n\\n**Faster Routing**\\n\\nA new `model_name_to_deployment_indices` hash map replaces O(n) list scans in `_get_all_deployments()` with O(1) hash lookups, boosting routing performance and scalability.\\n\\nAs a result, performance improved across all latency percentiles:\\n\\n- **Median latency:** 110 ms \u2192 **100 ms** (\u22129.1%)\\n- **p95 latency:** 440 ms \u2192 **150 ms** (\u221265.9%)\\n- **p99 latency:** 810 ms \u2192 **240 ms** (\u221270.4%)\\n- **Average latency:** 310 ms \u2192 **111.73 ms** (\u221264.0%)\\n\\n### **Test Setup**\\n\\n**Locust**\\n\\n- **Concurrent users:**\xa01,000\\n- **Ramp-up:**\xa0500\\n\\n**System Specs**\\n\\n- **Database was used**\\n- **CPU:**\xa04 vCPUs\\n- **Memory:**\xa08 GB RAM\\n- **LiteLLM Workers:**\xa04\\n- **Instances**: 4\\n\\n**Configuration (config.yaml)**\\n\\nView the complete configuration:\xa0[gist.github.com/AlexsanderHamir/config.yaml](https://gist.github.com/AlexsanderHamir/53f7d554a5d2afcf2c4edb5b6be68ff4)\\n\\n**Load Script (no_cache_hits.py)**\\n\\nView the complete load testing script:\xa0[gist.github.com/AlexsanderHamir/no_cache_hits.py](https://gist.github.com/AlexsanderHamir/42c33d7a4dc7a57f56a78b560dee3a42)\\n\\n---\\n\\n## New Models / Updated Models\\n\\n#### New Model Support\\n\\n| Provider | Model | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Features |\\n| -------- | ----- | -------------- | ------------------- | -------------------- | -------- |\\n| OpenAI | `gpt-5-pro` | 400K | $15.00 | $120.00 | Responses API, reasoning, vision, function calling, prompt caching, web search |\\n| OpenAI | `gpt-5-pro-2025-10-06` | 400K | $15.00 | $120.00 | Responses API, reasoning, vision, function calling, prompt caching, web search |\\n| OpenAI | `gpt-image-1-mini` | - | $2.00/img | - | Image generation and editing |\\n| OpenAI | `gpt-realtime-mini` | 128K | $0.60 | $2.40 | Realtime audio, function calling |\\n| Azure AI | `azure_ai/Phi-4-mini-reasoning` | 131K | $0.08 | $0.32 | Function calling |\\n| Azure AI | `azure_ai/Phi-4-reasoning` | 32K | $0.125 | $0.50 | Function calling, reasoning |\\n| Azure AI | `azure_ai/MAI-DS-R1` | 128K | $1.35 | $5.40 | Reasoning, function calling |\\n| Bedrock | `au.anthropic.claude-sonnet-4-5-20250929-v1:0` | 200K | $3.30 | $16.50 | Chat, reasoning, vision, function calling, prompt caching |\\n| Bedrock | `global.anthropic.claude-sonnet-4-5-20250929-v1:0` | 200K | $3.00 | $15.00 | Chat, reasoning, vision, function calling, prompt caching |\\n| Bedrock | `global.anthropic.claude-sonnet-4-20250514-v1:0` | 1M | $3.00 | $15.00 | Chat, reasoning, vision, function calling, prompt caching |\\n| Bedrock | `cohere.embed-v4:0` | 128K | $0.12 | - | Embeddings, image input support |\\n| OCI | `oci/cohere.command-latest` | 128K | $1.56 | $1.56 | Function calling |\\n| OCI | `oci/cohere.command-a-03-2025` | 256K | $1.56 | $1.56 | Function calling |\\n| OCI | `oci/cohere.command-plus-latest` | 128K | $1.56 | $1.56 | Function calling |\\n| Together AI | `together_ai/moonshotai/Kimi-K2-Instruct-0905` | 262K | $1.00 | $3.00 | Function calling |\\n| Together AI | `together_ai/Qwen/Qwen3-Next-80B-A3B-Instruct` | 262K | $0.15 | $1.50 | Function calling |\\n| Together AI | `together_ai/Qwen/Qwen3-Next-80B-A3B-Thinking` | 262K | $0.15 | $1.50 | Function calling |\\n| Vertex AI | MedGemma models | Varies | Varies | Varies | Medical-focused Gemma models on custom endpoints |\\n| Watson X | 27 new foundation models | Varies | Varies | Varies | Granite, Llama, Mistral families |\\n\\n#### Features\\n\\n- **[OpenAI](../../docs/providers/openai)**\\n    - Add GPT-5 Pro model configuration and documentation - [PR #15258](https://github.com/BerriAI/litellm/pull/15258)\\n    - Add stop parameter to non-supported params for GPT-5 - [PR #15244](https://github.com/BerriAI/litellm/pull/15244)\\n    - Day 0 Support, Add gpt-image-1-mini - [PR #15259](https://github.com/BerriAI/litellm/pull/15259)\\n    - Add gpt-realtime-mini support - [PR #15283](https://github.com/BerriAI/litellm/pull/15283)\\n    - Add gpt-5-pro-2025-10-06 to model costs - [PR #15344](https://github.com/BerriAI/litellm/pull/15344)\\n    - Minimal fix: gpt5 models should not go on cooldown when called with temperature!=1 - [PR #15330](https://github.com/BerriAI/litellm/pull/15330)\\n\\n- **[Snowflake Cortex](../../docs/providers/snowflake)**\\n    - Add function calling support for Snowflake Cortex REST API - [PR #15221](https://github.com/BerriAI/litellm/pull/15221)\\n\\n- **[Gemini](../../docs/providers/gemini)**\\n    - Fix header forwarding for Gemini/Vertex AI providers in proxy mode - [PR #15231](https://github.com/BerriAI/litellm/pull/15231)\\n\\n- **[Azure](../../docs/providers/azure)**\\n    - Removed stop param from unsupported azure models - [PR #15229](https://github.com/BerriAI/litellm/pull/15229)\\n    - Fix(azure/responses): remove invalid status param from azure call - [PR #15253](https://github.com/BerriAI/litellm/pull/15253)\\n    - Add new Azure AI models with pricing details - [PR #15387](https://github.com/BerriAI/litellm/pull/15387)\\n    - AzureAD Default credentials - select credential type based on environment - [PR #14470](https://github.com/BerriAI/litellm/pull/14470)\\n\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Add Global Cross-Region Inference - [PR #15210](https://github.com/BerriAI/litellm/pull/15210)\\n    - Add Cohere Embed v4 support for AWS Bedrock - [PR #15298](https://github.com/BerriAI/litellm/pull/15298)\\n    - Fix(bedrock): include cacheWriteInputTokens in prompt_tokens calculation - [PR #15292](https://github.com/BerriAI/litellm/pull/15292)\\n    - Add Bedrock AU Cross-Region Inference for Claude Sonnet 4.5 - [PR #15402](https://github.com/BerriAI/litellm/pull/15402)\\n    - Converse \u2192 /v1/messages streaming doesn\'t handle parallel tool calls with Claude models - [PR #15315](https://github.com/BerriAI/litellm/pull/15315)\\n\\n- **[Vertex AI](../../docs/providers/vertex)**\\n    - Implement Context Caching for Vertex AI provider - [PR #15226](https://github.com/BerriAI/litellm/pull/15226)\\n    - Support for Vertex AI Gemma Models on Custom Endpoints - [PR #15397](https://github.com/BerriAI/litellm/pull/15397)\\n    - VertexAI - gemma model family support (custom endpoints) - [PR #15419](https://github.com/BerriAI/litellm/pull/15419)\\n    - VertexAI Gemma model family streaming support + Added MedGemma - [PR #15427](https://github.com/BerriAI/litellm/pull/15427)\\n\\n- **[OCI](../../docs/providers/oci)**\\n    - Add OCI Cohere support with tool calling and streaming capabilities - [PR #15365](https://github.com/BerriAI/litellm/pull/15365)\\n\\n- **[Watson X](../../docs/providers/watsonx)**\\n    - Add Watson X foundation model definitions to model_prices_and_context_window.json - [PR #15219](https://github.com/BerriAI/litellm/pull/15219)\\n    - Watsonx - Apply correct prompt templates for openai/gpt-oss model family - [PR #15341](https://github.com/BerriAI/litellm/pull/15341)\\n\\n- **[OpenRouter](../../docs/providers/openrouter)**\\n    - Fix - (openrouter): move cache_control to content blocks for claude/gemini - [PR #15345](https://github.com/BerriAI/litellm/pull/15345)\\n    - Fix - OpenRouter cache_control to only apply to last content block - [PR #15395](https://github.com/BerriAI/litellm/pull/15395)\\n\\n- **[Together AI](../../docs/providers/togetherai)**\\n    - Add new together models - [PR #15383](https://github.com/BerriAI/litellm/pull/15383)\\n\\n### Bug Fixes\\n\\n- **General**\\n    - Bug fix: gpt-5-chat-latest has incorrect max_input_tokens value - [PR #15116](https://github.com/BerriAI/litellm/pull/15116)\\n    - Fix reasoning response ID - [PR #15265](https://github.com/BerriAI/litellm/pull/15265)\\n    - Fix issue with parsing assistant messages - [PR #15320](https://github.com/BerriAI/litellm/pull/15320)\\n    - Fix litellm_param based costing - [PR #15336](https://github.com/BerriAI/litellm/pull/15336)\\n    - Fix lint errors - [PR #15406](https://github.com/BerriAI/litellm/pull/15406)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n\\n- **[Responses API](../../docs/response_api)**\\n    - Added streaming support for response api streaming image generation - [PR #15269](https://github.com/BerriAI/litellm/pull/15269)\\n    - Add native Responses API support for litellm_proxy provider - [PR #15347](https://github.com/BerriAI/litellm/pull/15347)\\n    - Temporarily relax ResponsesAPIResponse parsing to support custom backends (e.g., vLLM) - [PR #15362](https://github.com/BerriAI/litellm/pull/15362)\\n\\n- **[Files API](../../docs/files_api)**\\n    - Feat(files): add @client decorator to file operations - [PR #15339](https://github.com/BerriAI/litellm/pull/15339)\\n\\n- **[/generateContent](../../docs/providers/gemini)**\\n    - Fix gemini cli by actually streaming the response - [PR #15264](https://github.com/BerriAI/litellm/pull/15264)\\n\\n- **[Azure Passthrough](../../docs/pass_through/azure)**\\n    - Azure - passthrough support with router models - [PR #15240](https://github.com/BerriAI/litellm/pull/15240)\\n\\n#### Bugs\\n\\n- **General**\\n    - Fix x-litellm-cache-key header not being returned on cache hit - [PR #15348](https://github.com/BerriAI/litellm/pull/15348)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n\\n- **Proxy CLI Auth**\\n    - Proxy CLI - dont store existing key in the URL, store it in the state param - [PR #15290](https://github.com/BerriAI/litellm/pull/15290)\\n\\n- **Models + Endpoints**\\n    - Make PATCH `/model/{model_id}/update` handle `team_id` consistently with POST `/model/new` - [PR #15297](https://github.com/BerriAI/litellm/pull/15297)\\n    - Feature: adds Infinity as a provider in the UI - [PR #15285](https://github.com/BerriAI/litellm/pull/15285)\\n    - Fix: model + endpoints page crash when config file contains router_settings.model_group_alias - [PR #15308](https://github.com/BerriAI/litellm/pull/15308)\\n    - Models & Endpoints Initial Refactor - [PR #15435](https://github.com/BerriAI/litellm/pull/15435)\\n    - Litellm UI API Reference page updates - [PR #15438](https://github.com/BerriAI/litellm/pull/15438)\\n\\n- **Teams**\\n    - Teams page: new column \\"Your Role\\" on the teams table - [PR #15384](https://github.com/BerriAI/litellm/pull/15384)\\n    - LiteLLM Dashboard Teams UI refactor - [PR #15418](https://github.com/BerriAI/litellm/pull/15418)\\n\\n- **UI Infrastructure**\\n    - Added prettier to autoformat frontend - [PR #15215](https://github.com/BerriAI/litellm/pull/15215)\\n    - Adds turbopack to the npm run dev command in UI to build faster during development - [PR #15250](https://github.com/BerriAI/litellm/pull/15250)\\n    - (perf) fix: Replaces bloated key list calls with lean key aliases endpoint - [PR #15252](https://github.com/BerriAI/litellm/pull/15252)\\n    - Potentially fixes a UI spasm issue with an expired cookie - [PR #15309](https://github.com/BerriAI/litellm/pull/15309)\\n    - LiteLLM UI Refactor Infrastructure - [PR #15236](https://github.com/BerriAI/litellm/pull/15236)\\n    - Enforces removal of unused imports from UI - [PR #15416](https://github.com/BerriAI/litellm/pull/15416)\\n    - Fix: usage page >> Model Activity >> spend per day graph: y-axis clipping on large spend values - [PR #15389](https://github.com/BerriAI/litellm/pull/15389)\\n    - Updates guardrail provider logos - [PR #15421](https://github.com/BerriAI/litellm/pull/15421)\\n\\n- **Admin Settings**\\n    - Fix: Router settings do not update despite success message - [PR #15249](https://github.com/BerriAI/litellm/pull/15249)\\n    - Fix: Prevents DB from accidentally overriding config file values if they are empty in DB - [PR #15340](https://github.com/BerriAI/litellm/pull/15340)\\n\\n- **SSO**\\n    - SSO - support EntraID app roles - [PR #15351](https://github.com/BerriAI/litellm/pull/15351)\\n\\n---\\n\\n## Logging / Guardrail / Prompt Management Integrations\\n\\n#### Features\\n\\n- **[PostHog](../../docs/observability/posthog)**\\n    - Feat: posthog per request api key - [PR #15379](https://github.com/BerriAI/litellm/pull/15379)\\n\\n#### Guardrails\\n\\n- **[EnkryptAI](../../docs/proxy/guardrails)**\\n    - Add EnkryptAI Guardrails on LiteLLM - [PR #15390](https://github.com/BerriAI/litellm/pull/15390)\\n\\n---\\n\\n## Spend Tracking, Budgets and Rate Limiting\\n\\n- **Tag Management**\\n    - Tag Management - Add support for setting tag based budgets - [PR #15433](https://github.com/BerriAI/litellm/pull/15433)\\n\\n- **Dynamic Rate Limiter v3**\\n    - QA/Fixes - Dynamic Rate Limiter v3 - final QA - [PR #15311](https://github.com/BerriAI/litellm/pull/15311)\\n    - Fix dynamic Rate limiter v3 - inserting litellm_model_saturation - [PR #15394](https://github.com/BerriAI/litellm/pull/15394)\\n\\n- **Shared Health Check**\\n    - Implement Shared Health Check State Across Pods - [PR #15380](https://github.com/BerriAI/litellm/pull/15380)\\n\\n---\\n\\n## MCP Gateway\\n\\n- **Tool Control**\\n    - MCP Gateway - UI - Select allowed tools for Key, Teams - [PR #15241](https://github.com/BerriAI/litellm/pull/15241)\\n    - MCP Gateway - Backend - Allow storing allowed tools by team/key - [PR #15243](https://github.com/BerriAI/litellm/pull/15243)\\n    - MCP Gateway - Fine-grained Database Object Storage Control - [PR #15255](https://github.com/BerriAI/litellm/pull/15255)\\n    - MCP Gateway - Litellm mcp fixes team control - [PR #15304](https://github.com/BerriAI/litellm/pull/15304)\\n    - MCP Gateway - QA/Fixes - Ensure Team/Key level enforcement works for MCPs - [PR #15305](https://github.com/BerriAI/litellm/pull/15305)\\n    - Feature: Include server_name in /v1/mcp/server/health endpoint response - [PR #15431](https://github.com/BerriAI/litellm/pull/15431)\\n\\n- **OpenAPI Integration**\\n    - MCP - support converting OpenAPI specs to MCP servers - [PR #15343](https://github.com/BerriAI/litellm/pull/15343)\\n    - MCP - specify allowed params per tool - [PR #15346](https://github.com/BerriAI/litellm/pull/15346)\\n\\n- **Configuration**\\n    - MCP - support setting CA_BUNDLE_PATH - [PR #15253](https://github.com/BerriAI/litellm/pull/15253)\\n    - Fix: Ensure MCP client stays open during tool call - [PR #15391](https://github.com/BerriAI/litellm/pull/15391)\\n    - Remove hardcoded \\"public\\" schema in migration.sql - [PR #15363](https://github.com/BerriAI/litellm/pull/15363)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n- **Router Optimizations**\\n    - Fix - Router: add model_name index for O(1) deployment lookups - [PR #15113](https://github.com/BerriAI/litellm/pull/15113)\\n    - Refactor Utils: extract inner function from client - [PR #15234](https://github.com/BerriAI/litellm/pull/15234)\\n    - Fix Networking: remove limitations - [PR #15302](https://github.com/BerriAI/litellm/pull/15302)\\n\\n- **Session Management**\\n    - Fix - Sessions not being shared - [PR #15388](https://github.com/BerriAI/litellm/pull/15388)\\n    - Fix: remove panic from hot path - [PR #15396](https://github.com/BerriAI/litellm/pull/15396)\\n    - Fix - shared session parsing and usage issue - [PR #15440](https://github.com/BerriAI/litellm/pull/15440)\\n    - Fix: handle closed aiohttp sessions - [PR #15442](https://github.com/BerriAI/litellm/pull/15442)\\n    - Fix: prevent session leaks when recreating aiohttp sessions - [PR #15443](https://github.com/BerriAI/litellm/pull/15443)\\n\\n- **SSL/TLS Performance**\\n    - Perf: optimize SSL/TLS handshake performance with prioritized cipher - [PR #15398](https://github.com/BerriAI/litellm/pull/15398)\\n\\n- **Dependencies**\\n    - Upgrades tenacity version to 8.5.0 - [PR #15303](https://github.com/BerriAI/litellm/pull/15303)\\n\\n- **Data Masking**\\n    - Fix - SensitiveDataMasker converts lists to string - [PR #15420](https://github.com/BerriAI/litellm/pull/15420)\\n\\n---\\n\\n\\n## General AI Gateway Improvements\\n\\n#### Security\\n\\n- **General**\\n    - Fix: redact AWS credentials when redact_user_api_key_info enabled - [PR #15321](https://github.com/BerriAI/litellm/pull/15321)\\n\\n---\\n\\n## Documentation Updates\\n\\n- **Provider Documentation**\\n    - Update doc: perf update - [PR #15211](https://github.com/BerriAI/litellm/pull/15211)\\n    - Add W&B Inference documentation - [PR #15278](https://github.com/BerriAI/litellm/pull/15278)\\n\\n- **Deployment**\\n    - Deletion of docker-compose buggy comment that cause `config.yaml` based startup fail - [PR #15425](https://github.com/BerriAI/litellm/pull/15425)\\n\\n---\\n\\n## New Contributors\\n\\n* @Gal-bloch made their first contribution in [PR #15219](https://github.com/BerriAI/litellm/pull/15219)\\n* @lcfyi made their first contribution in [PR #15315](https://github.com/BerriAI/litellm/pull/15315)\\n* @ashengstd made their first contribution in [PR #15362](https://github.com/BerriAI/litellm/pull/15362)\\n* @vkolehmainen made their first contribution in [PR #15363](https://github.com/BerriAI/litellm/pull/15363)\\n* @jlan-nl made their first contribution in [PR #15330](https://github.com/BerriAI/litellm/pull/15330)\\n* @BCook98 made their first contribution in [PR #15402](https://github.com/BerriAI/litellm/pull/15402)\\n* @PabloGmz96 made their first contribution in [PR #15425](https://github.com/BerriAI/litellm/pull/15425)\\n\\n---\\n\\n## **[Full Changelog](https://github.com/BerriAI/litellm/compare/v1.77.7.rc.1...v1.78.0.rc.1)**"},{"id":"v1-77-7","metadata":{"permalink":"/release_notes/v1-77-7","source":"@site/release_notes/v1.77.7-stable/index.md","title":"v1.77.7-stable - 2.9x Lower Median Latency","description":"Deploy this version","date":"2025-10-04T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.77.7-stable - 2.9x Lower Median Latency","slug":"v1-77-7","date":"2025-10-04T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.78.0-stable - MCP Gateway: Control Tool Access by Team, Key","permalink":"/release_notes/v1-78-0"},"nextItem":{"title":"v1.77.5-stable - MCP OAuth 2.0 Support","permalink":"/release_notes/v1-77-5"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.77.7.rc.1\\n```\\n\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.77.7.rc.1\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights\\n\\n- **Dynamic Rate Limiter v3** - Automatically maximizes throughput when capacity is available (< 80% saturation) by allowing lower-priority requests to use unused capacity, then switches to fair priority-based allocation under high load (\u2265 80%) to prevent blocking\\n- **Major Performance Improvements** - 2.9x lower median latency at 1,000 concurrent users.\\n- **Claude Sonnet 4.5** - Support for Anthropic\'s new Claude Sonnet 4.5 model family with 200K+ context and tiered pricing\\n- **MCP Gateway Enhancements** - Fine-grained tool control, server permissions, and forwardable headers\\n- **AMD Lemonade & Nvidia NIM** - New provider support for AMD Lemonade and Nvidia NIM Rerank\\n- **GitLab Prompt Management** - GitLab-based prompt management integration\\n\\n### Performance - 2.9x Lower Median Latency\\n\\n<Image img={require(\'../../img/release_notes/perf_77_7.png\')}  style={{ width: \'800px\', height: \'auto\' }} />\\n\\n<br/>\\n\\nThis update removes LiteLLM router inefficiencies, reducing complexity from O(M\xd7N) to O(1). Previously, it built a new array and ran repeated checks like data[\\"model\\"] in llm_router.get_model_ids(). Now, a direct ID-to-deployment map eliminates redundant allocations and scans.\\n\\nAs a result, performance improved across all latency percentiles:\\n\\n- **Median latency:** 320 ms \u2192 **110 ms** (\u221265.6%)\\n- **p95 latency:** 850 ms \u2192 **440 ms** (\u221248.2%)\\n- **p99 latency:** 1,400 ms \u2192 **810 ms** (\u221242.1%)\\n- **Average latency:** 864 ms \u2192 **310 ms** (\u221264%)\\n\\n\\n#### Test Setup\\n\\n**Locust**\\n\\n- **Concurrent users:** 1,000\\n- **Ramp-up:** 500\\n\\n**System Specs**\\n\\n- **CPU:** 4 vCPUs\\n- **Memory:** 8 GB RAM\\n- **LiteLLM Workers:** 4\\n- **Instances**: 4\\n\\n**Configuration (config.yaml)**\\n\\nView the complete configuration: [gist.github.com/AlexsanderHamir/config.yaml](https://gist.github.com/AlexsanderHamir/53f7d554a5d2afcf2c4edb5b6be68ff4)\\n\\n**Load Script (no_cache_hits.py)**\\n\\nView the complete load testing script: [gist.github.com/AlexsanderHamir/no_cache_hits.py](https://gist.github.com/AlexsanderHamir/42c33d7a4dc7a57f56a78b560dee3a42)\\n\\n### MCP OAuth 2.0 Support\\n\\n<Image img={require(\'../../img/mcp_updates.jpg\')} style={{ width: \'800px\', height: \'auto\' }} />\\n\\n<br/>\\n\\nThis release adds support for OAuth 2.0 Client Credentials for MCP servers. This is great for **Internal Dev Tools** use-cases, as it enables your users to call MCP servers, with their own credentials. E.g. Allowing your developers to call the Github MCP, with their own credentials.\\n\\n[Set it up today on Claude Code](../../docs/tutorials/claude_responses_api#connecting-mcp-servers)\\n\\n### Scheduled Key Rotations\\n\\n<Image img={require(\'../../img/release_notes/schedule_key_rotations.png\')}  style={{ width: \'800px\', height: \'auto\' }} />\\n\\n<br/>\\n\\nThis release brings support for scheduling virtual key rotations on LiteLLM AI Gateway. \\n \\nFrom this release you can enforce Virtual Keys to rotate on a schedule of your choice e.g every 15 days/30 days/60 days etc.\\n \\nThis is great for Proxy Admins who need to enforce security policies for production workloads. \\n\\n[Get Started](../../docs/proxy/virtual_keys#scheduled-key-rotations)\\n\\n\\n## New Models / Updated Models\\n\\n#### New Model Support\\n\\n| Provider | Model | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Features |\\n| -------- | ----- | -------------- | ------------------- | -------------------- | -------- |\\n| Anthropic | `claude-sonnet-4-5` | 200K | $3.00 | $15.00 | Chat, reasoning, vision, function calling, prompt caching |\\n| Anthropic | `claude-sonnet-4-5-20250929` | 200K | $3.00 | $15.00 | Chat, reasoning, vision, function calling, prompt caching |\\n| Bedrock | `eu.anthropic.claude-sonnet-4-5-20250929-v1:0` | 200K | $3.00 | $15.00 | Chat, reasoning, vision, function calling, prompt caching |\\n| Azure AI | `azure_ai/grok-4` | 131K | $5.50 | $27.50 | Chat, reasoning, function calling, web search |\\n| Azure AI | `azure_ai/grok-4-fast-reasoning` | 131K | $0.43 | $1.73 | Chat, reasoning, function calling, web search |\\n| Azure AI | `azure_ai/grok-4-fast-non-reasoning` | 131K | $0.43 | $1.73 | Chat, function calling, web search |\\n| Azure AI | `azure_ai/grok-code-fast-1` | 131K | $3.50 | $17.50 | Chat, function calling, web search |\\n| Groq | `groq/moonshotai/kimi-k2-instruct-0905` | Context varies | Pricing varies | Pricing varies | Chat, function calling |\\n| Ollama | Ollama Cloud models | Varies | Free | Free | Self-hosted models via Ollama Cloud |\\n\\n#### Features\\n\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Add new claude-sonnet-4-5 model family with tiered pricing above 200K tokens - [PR #15041](https://github.com/BerriAI/litellm/pull/15041)\\n    - Add anthropic/claude-sonnet-4-5 to model price json with prompt caching support - [PR #15049](https://github.com/BerriAI/litellm/pull/15049)\\n    - Add 200K prices for Sonnet 4.5 - [PR #15140](https://github.com/BerriAI/litellm/pull/15140)\\n    - Add cost tracking for /v1/messages in streaming response - [PR #15102](https://github.com/BerriAI/litellm/pull/15102)\\n    - Add /v1/messages/count_tokens to Anthropic routes for non-admin user access - [PR #15034](https://github.com/BerriAI/litellm/pull/15034)\\n- **[Gemini](../../docs/providers/gemini)**\\n    - Ignore type param for gemini tools - [PR #15022](https://github.com/BerriAI/litellm/pull/15022)\\n- **[Vertex AI](../../docs/providers/vertex)**\\n    - Add LiteLLM Overhead metric for VertexAI - [PR #15040](https://github.com/BerriAI/litellm/pull/15040)\\n    - Support googlemap grounding in vertex ai - [PR #15179](https://github.com/BerriAI/litellm/pull/15179)\\n- **[Azure](../../docs/providers/azure)**\\n    - Add azure_ai grok-4 model family - [PR #15137](https://github.com/BerriAI/litellm/pull/15137)\\n    - Use the `extra_query` parameter for GET requests in Azure Batch - [PR #14997](https://github.com/BerriAI/litellm/pull/14997)\\n    - Use extra_query for download results (Batch API) - [PR #15025](https://github.com/BerriAI/litellm/pull/15025)\\n    - Add support for Azure AD token-based authorization - [PR #14813](https://github.com/BerriAI/litellm/pull/14813)\\n- **[Ollama](../../docs/providers/ollama)**\\n    - Add ollama cloud models - [PR #15008](https://github.com/BerriAI/litellm/pull/15008)\\n- **[Groq](../../docs/providers/groq)**\\n    - Add groq/moonshotai/kimi-k2-instruct-0905 - [PR #15079](https://github.com/BerriAI/litellm/pull/15079)\\n- **[OpenAI](../../docs/providers/openai)**\\n    - Add support for GPT 5 codex models - [PR #14841](https://github.com/BerriAI/litellm/pull/14841)\\n- **[DeepInfra](../../docs/providers/deepinfra)**\\n    - Update DeepInfra model data refresh with latest pricing - [PR #14939](https://github.com/BerriAI/litellm/pull/14939)\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Add JP Cross-Region Inference - [PR #15188](https://github.com/BerriAI/litellm/pull/15188)\\n    - Add \\"eu.anthropic.claude-sonnet-4-5-20250929-v1:0\\" - [PR #15181](https://github.com/BerriAI/litellm/pull/15181)\\n    - Add twelvelabs bedrock Async Invoke Support - [PR #14871](https://github.com/BerriAI/litellm/pull/14871)\\n- **[Nvidia NIM](../../docs/providers/nvidia_nim)**\\n    - Add Nvidia NIM Rerank Support - [PR #15152](https://github.com/BerriAI/litellm/pull/15152)\\n\\n### Bug Fixes\\n\\n- **[VLLM](../../docs/providers/vllm)**\\n    - Fix response_format bug in hosted vllm audio_transcription - [PR #15010](https://github.com/BerriAI/litellm/pull/15010)\\n    - Fix passthrough of atranscription into kwargs going to upstream provider - [PR #15005](https://github.com/BerriAI/litellm/pull/15005)\\n- **[OCI](../../docs/providers/oci)**\\n    - Fix OCI Generative AI Integration when using Proxy - [PR #15072](https://github.com/BerriAI/litellm/pull/15072)\\n- **General**\\n    - Fix: Authorization header to use correct \\"Bearer\\" capitalization - [PR #14764](https://github.com/BerriAI/litellm/pull/14764)\\n    - Bug fix: gpt-5-chat-latest has incorrect max_input_tokens value - [PR #15116](https://github.com/BerriAI/litellm/pull/15116)\\n    - Update request handling for original exceptions - [PR #15013](https://github.com/BerriAI/litellm/pull/15013)\\n\\n#### New Provider Support\\n\\n- **[AMD Lemonade](../../docs/providers/lemonade)**\\n    - Add AMD Lemonade provider support - [PR #14840](https://github.com/BerriAI/litellm/pull/14840)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n\\n- **[Responses API](../../docs/response_api)**\\n    - Return Cost for Responses API Streaming requests - [PR #15053](https://github.com/BerriAI/litellm/pull/15053)\\n\\n- **[/generateContent](../../docs/providers/gemini)**\\n    - Add full support for native Gemini API translation - [PR #15029](https://github.com/BerriAI/litellm/pull/15029)\\n\\n- **Passthrough Gemini Routes**\\n    - Add Gemini generateContent passthrough cost tracking - [PR #15014](https://github.com/BerriAI/litellm/pull/15014)\\n    - Add streamGenerateContent cost tracking in passthrough - [PR #15199](https://github.com/BerriAI/litellm/pull/15199)\\n\\n- **Passthrough Vertex AI Routes**\\n    - Add cost tracking for Vertex AI Passthrough `/predict` endpoint - [PR #15019](https://github.com/BerriAI/litellm/pull/15019)\\n    - Add cost tracking for Vertex AI Live API WebSocket Passthrough - [PR #14956](https://github.com/BerriAI/litellm/pull/14956)\\n\\n- **General**\\n    - Preserve Whitespace Characters in Model Response Streams - [PR #15160](https://github.com/BerriAI/litellm/pull/15160)\\n    - Add provider name to payload specification - [PR #15130](https://github.com/BerriAI/litellm/pull/15130)\\n    - Ensure query params are forwarded from origin url to downstream request - [PR #15087](https://github.com/BerriAI/litellm/pull/15087)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n\\n- **Virtual Keys**\\n    - Ensure LLM_API_KEYs can access pass through routes - [PR #15115](https://github.com/BerriAI/litellm/pull/15115)\\n    - Support \'guaranteed_throughput\' when setting limits on keys belonging to a team - [PR #15120](https://github.com/BerriAI/litellm/pull/15120)\\n    \\n- **Models + Endpoints**\\n    - Ensure OCI secret fields not shared on /models and /v1/models endpoints - [PR #15085](https://github.com/BerriAI/litellm/pull/15085)\\n    - Add snowflake on UI - [PR #15083](https://github.com/BerriAI/litellm/pull/15083)\\n    - Make UI theme settings publicly accessible for custom branding - [PR #15074](https://github.com/BerriAI/litellm/pull/15074)\\n    \\n- **Admin Settings**\\n    - Ensure OTEL settings are saved in DB after set on UI - [PR #15118](https://github.com/BerriAI/litellm/pull/15118)\\n    - Top api key tags - [PR #15151](https://github.com/BerriAI/litellm/pull/15151), [PR #15156](https://github.com/BerriAI/litellm/pull/15156)\\n\\n- **MCP**\\n    - show health status of MCP servers - [PR #15185](https://github.com/BerriAI/litellm/pull/15185)\\n    - allow setting extra headers on the UI - [PR #15185](https://github.com/BerriAI/litellm/pull/15185)\\n    - allow editing allowed tools on the UI - [PR #15185](https://github.com/BerriAI/litellm/pull/15185)\\n\\n### Bug Fixes\\n\\n- **Virtual Keys**\\n    - (security) prevent user key from updating other user keys - [PR #15201](https://github.com/BerriAI/litellm/pull/15201)\\n    - (security) don\'t return all keys with blank key alias on /v2/key/info - [PR #15201](https://github.com/BerriAI/litellm/pull/15201)\\n    - Fix Session Token Cookie Infinite Logout Loop - [PR #15146](https://github.com/BerriAI/litellm/pull/15146)\\n\\n- **Models + Endpoints**\\n    - Make UI theme settings publicly accessible for custom branding - [PR #15074](https://github.com/BerriAI/litellm/pull/15074)\\n\\n- **Teams**\\n    - fix failed copy to clipboard for http ui - [PR #15195](https://github.com/BerriAI/litellm/pull/15195)\\n\\n- **Logs**\\n    - fix logs page render logs on filter lookup - [PR #15195](https://github.com/BerriAI/litellm/pull/15195)\\n    - fix lookup list of end users (migrate to more efficient /customers/list lookup) - [PR #15195](https://github.com/BerriAI/litellm/pull/15195)\\n\\n- **Test key**\\n    - update selected model on key change - [PR #15197](https://github.com/BerriAI/litellm/pull/15197)\\n\\n- **Dashboard**\\n    - Fix LiteLLM model name fallback in dashboard overview - [PR #14998](https://github.com/BerriAI/litellm/pull/14998)\\n\\n\\n---\\n\\n## Logging / Guardrail / Prompt Management Integrations\\n\\n#### Features\\n\\n- **[OpenTelemetry](../../docs/observability/otel)**\\n    - Use generation_name for span naming in logging method - [PR #14799](https://github.com/BerriAI/litellm/pull/14799)\\n- **[Langfuse](../../docs/proxy/logging#langfuse)**\\n    - Handle non-serializable objects in Langfuse logging - [PR #15148](https://github.com/BerriAI/litellm/pull/15148)\\n    - Set usage_details.total in langfuse integration - [PR #15015](https://github.com/BerriAI/litellm/pull/15015)\\n- **[Prometheus](../../docs/proxy/prometheus)**\\n    - support custom metadata labels on key/team - [PR #15094](https://github.com/BerriAI/litellm/pull/15094)\\n\\n\\n#### Guardrails\\n\\n- **[Javelin](../../docs/proxy/guardrails)**\\n    - Add Javelin standalone guardrails integration for LiteLLM Proxy - [PR #14983](https://github.com/BerriAI/litellm/pull/14983)\\n    - Add logging for important status fields in guardrails - [PR #15090](https://github.com/BerriAI/litellm/pull/15090)\\n    - Don\'t run post_call guardrail if no text returned from Bedrock - [PR #15106](https://github.com/BerriAI/litellm/pull/15106)\\n\\n#### Prompt Management\\n\\n- **[GitLab](../../docs/proxy/prompt_management)**\\n    - GitLab based Prompt manager - [PR #14988](https://github.com/BerriAI/litellm/pull/14988)\\n\\n---\\n\\n## Spend Tracking, Budgets and Rate Limiting\\n\\n- **Cost Tracking** \\n    - Proxy: end user cost tracking in the responses API - [PR #15124](https://github.com/BerriAI/litellm/pull/15124)\\n- **Parallel Request Limiter v3** \\n    - Use well known redis cluster hashing algorithm - [PR #15052](https://github.com/BerriAI/litellm/pull/15052)\\n    - Fixes to dynamic rate limiter v3 - add saturation detection - [PR #15119](https://github.com/BerriAI/litellm/pull/15119)\\n    - Dynamic Rate Limiter v3 - fixes for detecting saturation + fixes for post saturation behavior - [PR #15192](https://github.com/BerriAI/litellm/pull/15192)\\n- **Teams** \\n    - Add model specific tpm/rpm limits to teams on LiteLLM - [PR #15044](https://github.com/BerriAI/litellm/pull/15044)\\n\\n---\\n\\n## MCP Gateway\\n\\n- **Server Configuration** \\n    - Specify forwardable headers, specify allowed/disallowed tools for MCP servers - [PR #15002](https://github.com/BerriAI/litellm/pull/15002)\\n    - Enforce server permissions on call tools - [PR #15044](https://github.com/BerriAI/litellm/pull/15044)\\n    - MCP Gateway Fine-grained Tools Addition - [PR #15153](https://github.com/BerriAI/litellm/pull/15153)\\n- **Bug Fixes** \\n    - Remove servername prefix mcp tools tests - [PR #14986](https://github.com/BerriAI/litellm/pull/14986)\\n    - Resolve regression with duplicate Mcp-Protocol-Version header - [PR #15050](https://github.com/BerriAI/litellm/pull/15050)\\n    - Fix test_mcp_server.py - [PR #15183](https://github.com/BerriAI/litellm/pull/15183)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n- **Router Optimizations**\\n    - **+62.5% P99 Latency Improvement** - Remove router inefficiencies (from O(M*N) to O(1)) - [PR #15046](https://github.com/BerriAI/litellm/pull/15046)\\n    - Remove hasattr checks in Router - [PR #15082](https://github.com/BerriAI/litellm/pull/15082)\\n    - Remove Double Lookups - [PR #15084](https://github.com/BerriAI/litellm/pull/15084)\\n    - Optimize _filter_cooldown_deployments from O(n\xd7m + k\xd7n) to O(n) - [PR #15091](https://github.com/BerriAI/litellm/pull/15091)\\n    - Optimize unhealthy deployment filtering in retry path (O(n*m) \u2192 O(n+m)) - [PR #15110](https://github.com/BerriAI/litellm/pull/15110)\\n- **Cache Optimizations**\\n    - Reduce complexity of InMemoryCache.evict_cache from O(n*log(n)) to O(log(n)) - [PR #15000](https://github.com/BerriAI/litellm/pull/15000)\\n    - Avoiding expensive operations when cache isn\'t available - [PR #15182](https://github.com/BerriAI/litellm/pull/15182)\\n- **Worker Management**\\n    - Add proxy CLI option to recycle workers after N requests - [PR #15007](https://github.com/BerriAI/litellm/pull/15007)\\n- **Metrics & Monitoring**\\n    - LiteLLM Overhead metric tracking - Add support for tracking litellm overhead on cache hits - [PR #15045](https://github.com/BerriAI/litellm/pull/15045)\\n\\n---\\n\\n## Documentation Updates\\n\\n- **Provider Documentation** \\n    - Update litellm docs from latest release - [PR #15004](https://github.com/BerriAI/litellm/pull/15004)\\n    - Add missing api_key parameter - [PR #15058](https://github.com/BerriAI/litellm/pull/15058)\\n- **General Documentation** \\n    - Use docker compose instead of docker-compose - [PR #15024](https://github.com/BerriAI/litellm/pull/15024)\\n    - Add railtracks to projects that are using litellm - [PR #15144](https://github.com/BerriAI/litellm/pull/15144)\\n    - Perf: Last week improvement - [PR #15193](https://github.com/BerriAI/litellm/pull/15193)\\n    - Sync models GitHub documentation with Loom video and cross-reference - [PR #15191](https://github.com/BerriAI/litellm/pull/15191)\\n\\n---\\n\\n## Security Fixes\\n\\n- **JWT Token Security** - Don\'t log JWT SSO token on .info() log - [PR #15145](https://github.com/BerriAI/litellm/pull/15145)\\n\\n---\\n\\n## New Contributors\\n\\n* @herve-ves made their first contribution in [PR #14998](https://github.com/BerriAI/litellm/pull/14998)\\n* @wenxi-onyx made their first contribution in [PR #15008](https://github.com/BerriAI/litellm/pull/15008)\\n* @jpetrucciani made their first contribution in [PR #15005](https://github.com/BerriAI/litellm/pull/15005)\\n* @abhijitjavelin made their first contribution in [PR #14983](https://github.com/BerriAI/litellm/pull/14983)\\n* @ZeroClover made their first contribution in [PR #15039](https://github.com/BerriAI/litellm/pull/15039)\\n* @cedarm made their first contribution in [PR #15043](https://github.com/BerriAI/litellm/pull/15043)\\n* @Isydmr made their first contribution in [PR #15025](https://github.com/BerriAI/litellm/pull/15025)\\n* @serializer made their first contribution in [PR #15013](https://github.com/BerriAI/litellm/pull/15013)\\n* @eddierichter-amd made their first contribution in [PR #14840](https://github.com/BerriAI/litellm/pull/14840)\\n* @malags made their first contribution in [PR #15000](https://github.com/BerriAI/litellm/pull/15000)\\n* @henryhwang made their first contribution in [PR #15029](https://github.com/BerriAI/litellm/pull/15029)\\n* @plafleur made their first contribution in [PR #15111](https://github.com/BerriAI/litellm/pull/15111)\\n* @tyler-liner made their first contribution in [PR #14799](https://github.com/BerriAI/litellm/pull/14799)\\n* @Amir-R25 made their first contribution in [PR #15144](https://github.com/BerriAI/litellm/pull/15144)\\n* @georg-wolflein made their first contribution in [PR #15124](https://github.com/BerriAI/litellm/pull/15124)\\n* @niharm made their first contribution in [PR #15140](https://github.com/BerriAI/litellm/pull/15140)\\n* @anthony-liner made their first contribution in [PR #15015](https://github.com/BerriAI/litellm/pull/15015)\\n* @rishiganesh2002 made their first contribution in [PR #15153](https://github.com/BerriAI/litellm/pull/15153)\\n* @danielaskdd made their first contribution in [PR #15160](https://github.com/BerriAI/litellm/pull/15160)\\n* @JVenberg made their first contribution in [PR #15146](https://github.com/BerriAI/litellm/pull/15146)\\n* @speglich made their first contribution in [PR #15072](https://github.com/BerriAI/litellm/pull/15072)\\n* @daily-kim made their first contribution in [PR #14764](https://github.com/BerriAI/litellm/pull/14764)\\n\\n---\\n\\n## **[Full Changelog](https://github.com/BerriAI/litellm/compare/v1.77.5.rc.4...v1.77.7.rc.1)**"},{"id":"v1-77-5","metadata":{"permalink":"/release_notes/v1-77-5","source":"@site/release_notes/v1.77.5-stable/index.md","title":"v1.77.5-stable - MCP OAuth 2.0 Support","description":"Deploy this version","date":"2025-09-29T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.77.5-stable - MCP OAuth 2.0 Support","slug":"v1-77-5","date":"2025-09-29T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.77.7-stable - 2.9x Lower Median Latency","permalink":"/release_notes/v1-77-7"},"nextItem":{"title":"v1.77.3-stable - Priority Based Rate Limiting","permalink":"/release_notes/v1-77-3"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.77.5-stable\\n```\\n\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.77.5\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights\\n\\n- **MCP OAuth 2.0 Support** - Enhanced authentication for Model Context Protocol integrations\\n- **Scheduled Key Rotations** - Automated key rotation capabilities for enhanced security\\n- **New Gemini 2.5 Flash & Flash-lite Models** - Latest September 2025 preview models with improved pricing and features\\n- **Performance Improvements** - 54% RPS improvement\\n\\n---\\n\\n### Performance Improvements - 54% RPS Improvement\\n\\n<Image img={require(\'../../img/release_notes/perf_77_5.png\')}  style={{ width: \'800px\', height: \'auto\' }} />\\n\\n<br/>\\n\\nThis release brings a 54% RPS improvement (1,040 \u2192 1,602 RPS, aggregated) per instance. \\n\\nThe improvement comes from fixing O(n\xb2) inefficiencies in the LiteLLM Router, primarily caused by repeated use of `in` statements inside loops over large arrays. \\n\\nTests were run with a database-only setup (no cache hits).\\n\\n#### Test Setup\\n\\nAll benchmarks were executed using Locust with 1,000 concurrent users and a ramp-up of 500. The environment was configured to stress the routing layer and eliminate caching as a variable.\\n\\n**System Specs**\\n\\n- **CPU:** 8 vCPUs\\n- **Memory:** 32 GB RAM\\n\\n**Configuration (config.yaml)**\\n\\nView the complete configuration: [gist.github.com/AlexsanderHamir/config.yaml](https://gist.github.com/AlexsanderHamir/53f7d554a5d2afcf2c4edb5b6be68ff4)\\n\\n**Load Script (no_cache_hits.py)**\\n\\nView the complete load testing script: [gist.github.com/AlexsanderHamir/no_cache_hits.py](https://gist.github.com/AlexsanderHamir/42c33d7a4dc7a57f56a78b560dee3a42)\\n\\n---\\n\\n\\n## New Models / Updated Models\\n\\n#### New Model Support\\n\\n| Provider | Model | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Features |\\n| -------- | ----- | -------------- | ------------------- | -------------------- | -------- |\\n| Gemini | `gemini-2.5-flash-preview-09-2025` | 1M | $0.30 | $2.50 | Chat, reasoning, vision, audio |\\n| Gemini | `gemini-2.5-flash-lite-preview-09-2025` | 1M | $0.10 | $0.40 | Chat, reasoning, vision, audio |\\n| Gemini | `gemini-flash-latest` | 1M | $0.30 | $2.50 | Chat, reasoning, vision, audio |\\n| Gemini | `gemini-flash-lite-latest` | 1M | $0.10 | $0.40 | Chat, reasoning, vision, audio |\\n| DeepSeek | `deepseek-chat` | 131K | $0.60 | $1.70 | Chat, function calling, caching |\\n| DeepSeek | `deepseek-reasoner` | 131K | $0.60 | $1.70 | Chat, reasoning |\\n| Bedrock | `deepseek.v3-v1:0` | 164K | $0.58 | $1.68 | Chat, reasoning, function calling |\\n| Azure | `azure/gpt-5-codex` | 272K | $1.25 | $10.00 | Responses API, reasoning, vision |\\n| OpenAI | `gpt-5-codex` | 272K | $1.25 | $10.00 | Responses API, reasoning, vision |\\n| SambaNova | `sambanova/DeepSeek-V3.1` | 33K | $3.00 | $4.50 | Chat, reasoning, function calling |\\n| SambaNova | `sambanova/gpt-oss-120b` | 131K | $3.00 | $4.50 | Chat, reasoning, function calling |\\n| Bedrock | `qwen.qwen3-coder-480b-a35b-v1:0` | 262K | $0.22 | $1.80 | Chat, reasoning, function calling |\\n| Bedrock | `qwen.qwen3-235b-a22b-2507-v1:0` | 262K | $0.22 | $0.88 | Chat, reasoning, function calling |\\n| Bedrock | `qwen.qwen3-coder-30b-a3b-v1:0` | 262K | $0.15 | $0.60 | Chat, reasoning, function calling |\\n| Bedrock | `qwen.qwen3-32b-v1:0` | 131K | $0.15 | $0.60 | Chat, reasoning, function calling |\\n| Vertex AI | `vertex_ai/qwen/qwen3-next-80b-a3b-instruct-maas` | 262K | $0.15 | $1.20 | Chat, function calling |\\n| Vertex AI | `vertex_ai/qwen/qwen3-next-80b-a3b-thinking-maas` | 262K | $0.15 | $1.20 | Chat, function calling |\\n| Vertex AI | `vertex_ai/deepseek-ai/deepseek-v3.1-maas` | 164K | $1.35 | $5.40 | Chat, reasoning, function calling |\\n| OpenRouter | `openrouter/x-ai/grok-4-fast:free` | 2M | $0.00 | $0.00 | Chat, reasoning, function calling |\\n| XAI | `xai/grok-4-fast-reasoning` | 2M | $0.20 | $0.50 | Chat, reasoning, function calling |\\n| XAI | `xai/grok-4-fast-non-reasoning` | 2M | $0.20 | $0.50 | Chat, function calling |\\n\\n#### Features\\n\\n- **[Gemini](../../docs/providers/gemini)**\\n    - Added Gemini 2.5 Flash and Flash-lite preview models (September 2025 release) with improved pricing - [PR #14948](https://github.com/BerriAI/litellm/pull/14948)\\n    - Added new Anthropic web fetch tool support - [PR #14951](https://github.com/BerriAI/litellm/pull/14951)\\n- **[XAI](../../docs/providers/xai)**\\n    - Add xai/grok-4-fast models - [PR #14833](https://github.com/BerriAI/litellm/pull/14833)\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Updated Claude Sonnet 4 configs to reflect million-token context window pricing - [PR #14639](https://github.com/BerriAI/litellm/pull/14639)\\n    - Added supported text field to anthropic citation response - [PR #14164](https://github.com/BerriAI/litellm/pull/14164)\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Added support for Qwen models family & Deepseek 3.1 to Amazon Bedrock - [PR #14845](https://github.com/BerriAI/litellm/pull/14845)\\n    - Support requestMetadata in Bedrock Converse API - [PR #14570](https://github.com/BerriAI/litellm/pull/14570)\\n- **[Vertex AI](../../docs/providers/vertex)**\\n    - Added vertex_ai/qwen models and azure/gpt-5-codex - [PR #14844](https://github.com/BerriAI/litellm/pull/14844)\\n    - Update vertex ai qwen model pricing - [PR #14828](https://github.com/BerriAI/litellm/pull/14828)\\n    - Vertex AI Context Caching: use Vertex ai API v1 instead of v1beta1 and accept \'cachedContent\' param - [PR #14831](https://github.com/BerriAI/litellm/pull/14831)\\n- **[SambaNova](../../docs/providers/sambanova)**\\n    - Add sambanova deepseek v3.1 and gpt-oss-120b - [PR #14866](https://github.com/BerriAI/litellm/pull/14866)\\n- **[OpenAI](../../docs/providers/openai)**\\n    - Fix inconsistent token configs for gpt-5 models - [PR #14942](https://github.com/BerriAI/litellm/pull/14942)\\n    - GPT-3.5-Turbo price updated - [PR #14858](https://github.com/BerriAI/litellm/pull/14858)\\n- **[OpenRouter](../../docs/providers/openrouter)**\\n    - Add gpt-5 and gpt-5-codex to OpenRouter cost map - [PR #14879](https://github.com/BerriAI/litellm/pull/14879)\\n- **[VLLM](../../docs/providers/vllm)**\\n    - Fix vllm passthrough - [PR #14778](https://github.com/BerriAI/litellm/pull/14778)\\n- **[Flux](../../docs/image_generation)**\\n    - Support flux image edit - [PR #14790](https://github.com/BerriAI/litellm/pull/14790)\\n\\n### Bug Fixes\\n\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Fix: Support claude code auth via subscription (anthropic) - [PR #14821](https://github.com/BerriAI/litellm/pull/14821)\\n    - Fix Anthropic streaming IDs - [PR #14965](https://github.com/BerriAI/litellm/pull/14965)\\n    - Revert incorrect changes to sonnet-4 max output tokens - [PR #14933](https://github.com/BerriAI/litellm/pull/14933)\\n- **[OpenAI](../../docs/providers/openai)**\\n    - Fix a bug where openai image edit silently ignores multiple images - [PR #14893](https://github.com/BerriAI/litellm/pull/14893)\\n- **[VLLM](../../docs/providers/vllm)**\\n    - Fix: vLLM provider\'s rerank endpoint from /v1/rerank to /rerank - [PR #14938](https://github.com/BerriAI/litellm/pull/14938)\\n\\n#### New Provider Support\\n\\n- **[W&B Inference](../../docs/providers/wandb)**\\n    - Add W&B Inference to LiteLLM - [PR #14416](https://github.com/BerriAI/litellm/pull/14416)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n\\n- **General**\\n    - Add SDK support for additional headers - [PR #14761](https://github.com/BerriAI/litellm/pull/14761)\\n    - Add shared_session parameter for aiohttp ClientSession reuse - [PR #14721](https://github.com/BerriAI/litellm/pull/14721)\\n\\n#### Bugs\\n\\n- **General**\\n    - Fix: Streaming tool call index assignment for multiple tool calls - [PR #14587](https://github.com/BerriAI/litellm/pull/14587)\\n    - Fix load credentials in token counter proxy - [PR #14808](https://github.com/BerriAI/litellm/pull/14808)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n\\n- **Proxy CLI Auth** \\n    - Allow re-using cli auth token - [PR #14780](https://github.com/BerriAI/litellm/pull/14780)\\n    - Create a python method to login using litellm proxy - [PR #14782](https://github.com/BerriAI/litellm/pull/14782)\\n    - Fixes for LiteLLM Proxy CLI to Auth to Gateway - [PR #14836](https://github.com/BerriAI/litellm/pull/14836)\\n    \\n**Virtual Keys**    \\n    - Initial support for scheduled key rotations - [PR #14877](https://github.com/BerriAI/litellm/pull/14877)\\n    - Allow scheduling key rotations when creating virtual keys - [PR #14960](https://github.com/BerriAI/litellm/pull/14960)\\n\\n**Models + Endpoints**\\n    - Fix: added Oracle to provider\'s list - [PR #14835](https://github.com/BerriAI/litellm/pull/14835)\\n\\n\\n#### Bugs\\n\\n- **SSO** - Fix: SSO \\"Clear\\" button writes empty values instead of removing SSO config - [PR #14826](https://github.com/BerriAI/litellm/pull/14826)\\n- **Admin Settings** - Remove useful links from admin settings - [PR #14918](https://github.com/BerriAI/litellm/pull/14918)\\n- **Management Routes** - Add /user/list to management routes - [PR #14868](https://github.com/BerriAI/litellm/pull/14868)\\n---\\n\\n## Logging / Guardrail / Prompt Management Integrations\\n\\n#### Features\\n\\n- **[DataDog](../../docs/proxy/logging#datadog)**\\n    - Logging - `datadog` callback Log message content w/o sending to datadog - [PR #14909](https://github.com/BerriAI/litellm/pull/14909)\\n- **[Langfuse](../../docs/proxy/logging#langfuse)**\\n    - Adding langfuse usage details for cached tokens - [PR #10955](https://github.com/BerriAI/litellm/pull/10955)\\n- **[Opik](../../docs/proxy/logging#opik)**\\n    - Improve opik integration code - [PR #14888](https://github.com/BerriAI/litellm/pull/14888)\\n- **[SQS](../../docs/proxy/logging#sqs)**\\n    - Error logging support for SQS Logger - [PR #14974](https://github.com/BerriAI/litellm/pull/14974)\\n\\n#### Guardrails\\n\\n- **LakeraAI v2 Guardrail** - Ensure exception is raised correctly - [PR #14867](https://github.com/BerriAI/litellm/pull/14867)\\n- **Presidio Guardrail** - Support custom entity types in Presidio guardrail with Union[PiiEntityType, str] - [PR #14899](https://github.com/BerriAI/litellm/pull/14899)\\n- **Noma Guardrail** - Add noma guardrail provider to ui - [PR #14415](https://github.com/BerriAI/litellm/pull/14415)\\n\\n#### Prompt Management\\n\\n- **BitBucket Integration** - Add BitBucket Integration for Prompt Management - [PR #14882](https://github.com/BerriAI/litellm/pull/14882)\\n\\n---\\n\\n## Spend Tracking, Budgets and Rate Limiting\\n\\n- **Service Tier Pricing** - Add service_tier based pricing support for openai (BOTH Service & Priority Support) - [PR #14796](https://github.com/BerriAI/litellm/pull/14796)\\n- **Cost Tracking** - Show input, output, tool call cost breakdown in StandardLoggingPayload - [PR #14921](https://github.com/BerriAI/litellm/pull/14921)\\n- **Parallel Request Limiter v3** \\n    - Ensure Lua scripts can execute on redis cluster - [PR #14968](https://github.com/BerriAI/litellm/pull/14968)\\n    - Fix: get metadata info from both metadata and litellm_metadata fields - [PR #14783](https://github.com/BerriAI/litellm/pull/14783)\\n- **Priority Reservation** - Fix: Priority Reservation: keys without priority metadata receive higher priority than keys with explicit priority configurations - [PR #14832](https://github.com/BerriAI/litellm/pull/14832)\\n\\n---\\n\\n## MCP Gateway\\n\\n- **MCP Configuration** - Enable custom fields in mcp_info configuration - [PR #14794](https://github.com/BerriAI/litellm/pull/14794)\\n- **MCP Tools** - Remove server_name prefix from list_tools - [PR #14720](https://github.com/BerriAI/litellm/pull/14720)\\n- **OAuth Flow** - Initial commit for v2 oauth flow - [PR #14964](https://github.com/BerriAI/litellm/pull/14964)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n- **Memory Leak Fix** - Fix InMemoryCache unbounded growth when TTLs are set - [PR #14869](https://github.com/BerriAI/litellm/pull/14869)\\n- **Cache Performance** - Fix: cache root cause - [PR #14827](https://github.com/BerriAI/litellm/pull/14827)\\n- **Concurrency Fix** - Fix concurrency/scaling when many Python threads do streaming using *sync* completions - [PR #14816](https://github.com/BerriAI/litellm/pull/14816)\\n- **Performance Optimization** - Fix: reduce get_deployment cost to O(1) - [PR #14967](https://github.com/BerriAI/litellm/pull/14967)\\n- **Performance Optimization** - Fix: remove slow string operation - [PR #14955](https://github.com/BerriAI/litellm/pull/14955)\\n- **DB Connection Management** - Fix: DB connection state retries - [PR #14925](https://github.com/BerriAI/litellm/pull/14925)\\n\\n\\n\\n---\\n\\n## Documentation Updates\\n\\n- **Provider Documentation** - Fix docs for provider_specific_params.md - [PR #14787](https://github.com/BerriAI/litellm/pull/14787)\\n- **Model References** - Update model references from gemini-pro to gemini-2.5-pro - [PR #14775](https://github.com/BerriAI/litellm/pull/14775)\\n- **Letta Guide** - Add Letta Guide documentation - [PR #14798](https://github.com/BerriAI/litellm/pull/14798)\\n- **README** - Make the README document clearer - [PR #14860](https://github.com/BerriAI/litellm/pull/14860)\\n- **Session Management** - Update docs for session management availability - [PR #14914](https://github.com/BerriAI/litellm/pull/14914)\\n- **Cost Documentation** - Add documentation for additional cost-related keys in custom pricing - [PR #14949](https://github.com/BerriAI/litellm/pull/14949)\\n- **Azure Passthrough** - Add azure passthrough documentation - [PR #14958](https://github.com/BerriAI/litellm/pull/14958)\\n- **General Documentation** - Doc updates sept 2025 - [PR #14769](https://github.com/BerriAI/litellm/pull/14769)\\n    - Clarified bridging between endpoints and mode in docs.\\n    - Added Vertex AI Gemini API configuration as an alternative in relevant guides.\\n    Linked AWS authentication info in the Bedrock guardrails documentation.\\n    - Added Cancel Response API usage with code snippets\\n    - Clarified that SSO (Single Sign-On) is free for up to 5 users:\\n    - Alphabetized sidebar, leaving quick start / intros at top of categories\\n    - Documented max_connections under cache_params.\\n    - Clarified IAM AssumeRole Policy requirements.\\n    - Added transform utilities example to Getting Started (showing request transformation).\\n    - Added references to models.litellm.ai as the full models list in various docs.\\n    - Added a code snippet for async_post_call_success_hook.\\n    - Removed broken links to callbacks management guide. - Reformatted and linked cookbooks + other relevant docs\\n- **Documentation Corrections** - Corrected docs updates sept 2025 - [PR #14916](https://github.com/BerriAI/litellm/pull/14916)\\n\\n---\\n\\n## New Contributors\\n\\n* @uzaxirr made their first contribution in [PR #14761](https://github.com/BerriAI/litellm/pull/14761)\\n* @xprilion made their first contribution in [PR #14416](https://github.com/BerriAI/litellm/pull/14416)\\n* @CH-GAGANRAJ made their first contribution in [PR #14779](https://github.com/BerriAI/litellm/pull/14779)\\n* @otaviofbrito made their first contribution in [PR #14778](https://github.com/BerriAI/litellm/pull/14778)\\n* @danielmklein made their first contribution in [PR #14639](https://github.com/BerriAI/litellm/pull/14639)\\n* @Jetemple made their first contribution in [PR #14826](https://github.com/BerriAI/litellm/pull/14826)\\n* @akshoop made their first contribution in [PR #14818](https://github.com/BerriAI/litellm/pull/14818)\\n* @hazyone made their first contribution in [PR #14821](https://github.com/BerriAI/litellm/pull/14821)\\n* @leventov made their first contribution in [PR #14816](https://github.com/BerriAI/litellm/pull/14816)\\n* @fabriciojoc made their first contribution in [PR #10955](https://github.com/BerriAI/litellm/pull/10955)\\n* @onlylonly made their first contribution in [PR #14845](https://github.com/BerriAI/litellm/pull/14845)\\n* @Copilot made their first contribution in [PR #14869](https://github.com/BerriAI/litellm/pull/14869)\\n* @arsh72 made their first contribution in [PR #14899](https://github.com/BerriAI/litellm/pull/14899)\\n* @berri-teddy made their first contribution in [PR #14914](https://github.com/BerriAI/litellm/pull/14914)\\n* @vpbill made their first contribution in [PR #14415](https://github.com/BerriAI/litellm/pull/14415)\\n* @kgritesh made their first contribution in [PR #14893](https://github.com/BerriAI/litellm/pull/14893)\\n* @oytunkutrup1 made their first contribution in [PR #14858](https://github.com/BerriAI/litellm/pull/14858)\\n* @nherment made their first contribution in [PR #14933](https://github.com/BerriAI/litellm/pull/14933)\\n* @deepanshululla made their first contribution in [PR #14974](https://github.com/BerriAI/litellm/pull/14974)\\n* @TeddyAmkie made their first contribution in [PR #14758](https://github.com/BerriAI/litellm/pull/14758)\\n* @SmartManoj made their first contribution in [PR #14775](https://github.com/BerriAI/litellm/pull/14775)\\n* @uc4w6c made their first contribution in [PR #14720](https://github.com/BerriAI/litellm/pull/14720)\\n* @luizrennocosta made their first contribution in [PR #14783](https://github.com/BerriAI/litellm/pull/14783)\\n* @AlexsanderHamir made their first contribution in [PR #14827](https://github.com/BerriAI/litellm/pull/14827)\\n* @dharamendrak made their first contribution in [PR #14721](https://github.com/BerriAI/litellm/pull/14721)\\n* @TomeHirata made their first contribution in [PR #14164](https://github.com/BerriAI/litellm/pull/14164)\\n* @mrFranklin made their first contribution in [PR #14860](https://github.com/BerriAI/litellm/pull/14860)\\n* @luisfucros made their first contribution in [PR #14866](https://github.com/BerriAI/litellm/pull/14866)\\n* @huangyafei made their first contribution in [PR #14879](https://github.com/BerriAI/litellm/pull/14879)\\n* @thiswillbeyourgithub made their first contribution in [PR #14949](https://github.com/BerriAI/litellm/pull/14949)\\n* @Maximgitman made their first contribution in [PR #14965](https://github.com/BerriAI/litellm/pull/14965)\\n* @subnet-dev made their first contribution in [PR #14938](https://github.com/BerriAI/litellm/pull/14938)\\n* @22mSqRi made their first contribution in [PR #14972](https://github.com/BerriAI/litellm/pull/14972)\\n\\n---\\n\\n## **[Full Changelog](https://github.com/BerriAI/litellm/compare/v1.77.3.rc.1...v1.77.5.rc.1)**"},{"id":"v1-77-3","metadata":{"permalink":"/release_notes/v1-77-3","source":"@site/release_notes/v1.77.3-stable/index.md","title":"v1.77.3-stable - Priority Based Rate Limiting","description":"Deploy this version","date":"2025-09-21T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.77.3-stable - Priority Based Rate Limiting","slug":"v1-77-3","date":"2025-09-21T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.77.5-stable - MCP OAuth 2.0 Support","permalink":"/release_notes/v1-77-5"},"nextItem":{"title":"v1.77.2-stable - Bedrock Batches API","permalink":"/release_notes/v1-77-2"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.77.3-stable\\n```\\n\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.77.3\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights\\n\\n- **+550 RPS Performance Improvements** - Optimizations in request handling and object initialization.\\n- **Priority Quota Reservation** - Proxy admins can now reserve TPM/RPM capacity for specific keys.\\n\\n## Priority Quota Reservation\\n\\nThis release adds support for priority quota reservation. This allows Proxy Admins to reserve specific percentages of model capacity for different use cases. \\n \\nThis is great for use cases where you want to ensure your realtime use cases must always get priority responses and background development jobs can take longer. \\n\\n<Image img={require(\'../../img/release_notes/quota.png\')}  style={{ width: \'800px\', height: \'auto\' }} />\\n\\n<br/>\\n\\nThis release adds support for priority quota reservation. This allows **Proxy Admins** to reserve TPM/RPM capacity for keys based on metadata priority levels, ensuring critical production workloads get guaranteed access regardless of development traffic volume.\\n\\nGet started [here](../../docs/proxy/dynamic_rate_limit#priority-quota-reservation)\\n\\n## +550 RPS Performance Improvements\\n\\n<Image img={require(\'../../img/release_notes/perf_imp.png\')}  style={{ width: \'800px\', height: \'auto\' }} />\\n\\n<br/>\\n\\nThis release delivers significant RPS improvements through targeted optimizations. \\n \\nWe\'ve achieved a +500 RPS boost by fixing cache type inconsistencies that were causing frequent cache misses, plus an additional +50 RPS by removing unnecessary coroutine checks from the hot path. \\n\\n\\n## New Models / Updated Models\\n\\n#### New Model Support\\n\\n| Provider | Model | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Features |\\n| -------- | ----- | -------------- | ------------------- | -------------------- | -------- |\\n| SambaNova | `sambanova/deepseek-v3.1` | 128K | $0.90 | $0.90 | Chat completions |\\n| SambaNova | `sambanova/gpt-oss-120b` | 128K | $0.72 | $0.72 | Chat completions |\\n| OVHCloud | Various models | Varies | Contact provider | Contact provider | Chat completions |\\n| CompactifAI | Various models | Varies | Contact provider | Contact provider | Chat completions |\\n| TwelveLabs | `twelvelabs/marengo-embed-2.7` | 32K | $0.12 | $0.00 | Embeddings |\\n\\n#### Features\\n\\n- **[OVHCloud AI Endpoints](../../docs/providers/ovhcloud)**\\n    - New provider support with comprehensive model catalog - [PR #14494](https://github.com/BerriAI/litellm/pull/14494)\\n- **[CompactifAI](../../docs/providers/compactifai)**\\n    - New provider integration - [PR #14532](https://github.com/BerriAI/litellm/pull/14532)\\n- **[SambaNova](../../docs/providers/sambanova)**\\n    - Added DeepSeek v3.1 and GPT-OSS-120B models - [PR #14500](https://github.com/BerriAI/litellm/pull/14500)\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Cross-region inference profile cost calculation - [PR #14566](https://github.com/BerriAI/litellm/pull/14566)\\n    - AWS external ID parameter support for authentication - [PR #14582](https://github.com/BerriAI/litellm/pull/14582)\\n    - CountTokens API implementation - [PR #14557](https://github.com/BerriAI/litellm/pull/14557)\\n    - Titan V2 encoding_format parameter support - [PR #14687](https://github.com/BerriAI/litellm/pull/14687)\\n    - Nova Canvas image generation inference profiles - [PR #14578](https://github.com/BerriAI/litellm/pull/14578)\\n    - Bedrock Batches API - batch processing support with file upload and request transformation - [PR #14618](https://github.com/BerriAI/litellm/pull/14618)\\n    - Bedrock Twelve Labs embedding provider support - [PR #14697](https://github.com/BerriAI/litellm/pull/14697)\\n- **[Vertex AI](../../docs/providers/vertex)**\\n    - Gemini labels field provider-aware filtering - [PR #14563](https://github.com/BerriAI/litellm/pull/14563)\\n    - Gemini Batch API support - [PR #14733](https://github.com/BerriAI/litellm/pull/14733)\\n- **[Volcengine](../../docs/providers/volcengine)**\\n    - Fixed thinking parameters when disabled - [PR #14569](https://github.com/BerriAI/litellm/pull/14569)\\n- **[Cohere](../../docs/providers/cohere)**\\n    - Handle Generate API deprecation, default to chat endpoints - [PR #14676](https://github.com/BerriAI/litellm/pull/14676)\\n- **[TwelveLabs](../../docs/providers/twelvelabs)**\\n    - Added Marengo Embed 2.7 embedding support - [PR #14674](https://github.com/BerriAI/litellm/pull/14674)\\n\\n### Bug Fixes\\n\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Empty arguments handling in tool call invocation - [PR #14583](https://github.com/BerriAI/litellm/pull/14583)\\n- **[Vertex AI](../../docs/providers/vertex)**\\n    - Avoid deepcopy crash with non-pickleables in Gemini/Vertex - [PR #14418](https://github.com/BerriAI/litellm/pull/14418)\\n- **[XAI](../../docs/providers/xai)**\\n    - Fix unsupported stop parameter for grok-code models - [PR #14565](https://github.com/BerriAI/litellm/pull/14565)\\n- **[Gemini](../../docs/providers/gemini)**\\n    - Updated error message for Gemini API - [PR #14589](https://github.com/BerriAI/litellm/pull/14589)\\n    - Fixed 2.5 Flash Image Preview model routing - [PR #14715](https://github.com/BerriAI/litellm/pull/14715)\\n    - API key passing for token counting endpoints - [PR #14744](https://github.com/BerriAI/litellm/pull/14744)\\n\\n#### New Provider Support\\n\\n- **[OVHCloud AI Endpoints](../../docs/providers/ovhcloud)**\\n    - Complete provider integration with model catalog and authentication - [PR #14494](https://github.com/BerriAI/litellm/pull/14494)\\n- **[CompactifAI](../../docs/providers/compactifai)**\\n    - New provider support with documentation - [PR #14532](https://github.com/BerriAI/litellm/pull/14532)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n\\n- **[/responses](../../docs/response_api)**\\n    - Added cancel endpoint support for non-admin users - [PR #14594](https://github.com/BerriAI/litellm/pull/14594)\\n    - Improved response session handling and cold storage configuration with s3 - [PR #14534](https://github.com/BerriAI/litellm/pull/14534)\\n    - Added OpenAI & Azure /responses/cancel endpoint support - [PR #14561](https://github.com/BerriAI/litellm/pull/14561)\\n- **General**\\n    - Enhanced rate limit error messages with details - [PR #14736](https://github.com/BerriAI/litellm/pull/14736)\\n    - Middle-truncation for spend log payloads - [PR #14637](https://github.com/BerriAI/litellm/pull/14637)\\n\\n#### Bugs\\n\\n- **[/chat/completions](../../docs/completion/input)**\\n    - Fixed completion chat ID handling - [PR #14548](https://github.com/BerriAI/litellm/pull/14548)\\n    - Prevent AttributeError for _get_tags_from_request_kwargs - [PR #14735](https://github.com/BerriAI/litellm/pull/14735)\\n- **[/responses](../../docs/response_api)**\\n    - Fixed cost calculation - [PR #14675](https://github.com/BerriAI/litellm/pull/14675)\\n- **General**\\n    - Rate limiter AttributeError fix - [PR #14609](https://github.com/BerriAI/litellm/pull/14609)\\n\\n---\\n\\n## Spend Tracking, Budgets and Rate Limiting\\n\\n- **Responses API Cost Calculation** fix - [PR #14675](https://github.com/BerriAI/litellm/pull/14675)\\n- **Anthropic Cache Token Pricing** - Separate 1-hour vs 5-minute cache creation costs - [PR #14620](https://github.com/BerriAI/litellm/pull/14620), [PR #14652](https://github.com/BerriAI/litellm/pull/14652)\\n- **Indochina Time Timezone** support for budget resets - [PR #14666](https://github.com/BerriAI/litellm/pull/14666)\\n- **Soft Budget Alert Cache Issues** - Resolved soft budget alert cache issues - [PR #14491](https://github.com/BerriAI/litellm/pull/14491)\\n- **Dynamic Rate Limiter v3** - Priority routing improvements - [PR #14734](https://github.com/BerriAI/litellm/pull/14734)\\n- **Enhanced Rate Limit Errors** - More detailed error messages - [PR #14736](https://github.com/BerriAI/litellm/pull/14736)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n\\n- **Team Member Service Account Keys** - Allow team members to view keys they create - [PR #14619](https://github.com/BerriAI/litellm/pull/14619)\\n- **Default Budget for JWT Teams** - Auto-assign budgets to generated teams - [PR #14514](https://github.com/BerriAI/litellm/pull/14514)\\n- **SSO Access Control Groups** - Enhanced token info endpoint integration - [PR #14738](https://github.com/BerriAI/litellm/pull/14738)\\n- **Health Test Connect Protection** - Restrict access based on model creation permissions - [PR #14650](https://github.com/BerriAI/litellm/pull/14650)\\n- **Amazon Bedrock Guardrail Info View** - Enhanced logging visualization - [PR #14696](https://github.com/BerriAI/litellm/pull/14696)\\n\\n#### Bug Fixes\\n\\n- **SCIM v2** - Fix group PUSH and PUT operations for non-existent members - [PR #14581](https://github.com/BerriAI/litellm/pull/14581)\\n- **Guardrail View/Edit/Delete** behavior fixes - [PR #14622](https://github.com/BerriAI/litellm/pull/14622)\\n- **In-Memory Guardrail** update failures - [PR #14653](https://github.com/BerriAI/litellm/pull/14653)\\n\\n---\\n\\n## Logging / Guardrail Integrations\\n\\n#### Features\\n\\n- **[DataDog](../../docs/proxy/logging#datadog)**\\n    - Enhanced spend tracking metrics - [PR #14555](https://github.com/BerriAI/litellm/pull/14555)\\n    - Stream support with is_streamed_request parameter - [PR #14673](https://github.com/BerriAI/litellm/pull/14673)\\n    - Fixed tool calls metadata passing - [PR #14531](https://github.com/BerriAI/litellm/pull/14531)\\n- **[Langfuse](../../docs/proxy/logging#langfuse)**\\n    - Added logging support for Responses API - [PR #14597](https://github.com/BerriAI/litellm/pull/14597)\\n- **[Langsmith](../../docs/proxy/logging#langsmith)**\\n    - Langsmith Sampling Rate - Key/Team-level tracing configuration - [PR #14740](https://github.com/BerriAI/litellm/pull/14740)\\n- **[Prometheus](../../docs/proxy/logging#prometheus)**\\n    - Multi-worker support improvements - [PR #14530](https://github.com/BerriAI/litellm/pull/14530)\\n    - User email labels in monitoring - [PR #14520](https://github.com/BerriAI/litellm/pull/14520)\\n- **[Opik](../../docs/proxy/logging#opik)**\\n    - Fixed timezone issue - [PR #14708](https://github.com/BerriAI/litellm/pull/14708)\\n\\n### Bug Fixes\\n\\n- **[S3](../../docs/proxy/logging#s3-buckets)**\\n    - Fixed 404 error when using s3_endpoint_url - [PR #14559](https://github.com/BerriAI/litellm/pull/14559)\\n\\n#### Guardrails\\n\\n- **Tool Permission Guardrail** - Fine-grained tool access control - [PR #14519](https://github.com/BerriAI/litellm/pull/14519)\\n- **Bedrock Guardrails** - Selective guarding support with runtime endpoint configuration - [PR #14575](https://github.com/BerriAI/litellm/pull/14575), [PR #14650](https://github.com/BerriAI/litellm/pull/14650)\\n- **Default Last Message** in guardrails - [PR #14640](https://github.com/BerriAI/litellm/pull/14640)\\n- **AWS exceptions handling despite 200 response** - [PR #14658](https://github.com/BerriAI/litellm/pull/14658)\\n#### New Integration\\n\\n- **[PostHog](../../docs/observability/posthog)** - Complete observability integration for LiteLLM usage tracking and analytics - [PR #14610](https://github.com/BerriAI/litellm/pull/14610)\\n\\n---\\n\\n\\n## MCP Gateway\\n\\n- **MCP Server Alias Parsing** - Multi-part URL path support - [PR #14558](https://github.com/BerriAI/litellm/pull/14558)\\n- **MCP Filter Recomputation** - After server deletion - [PR #14542](https://github.com/BerriAI/litellm/pull/14542)\\n- **MCP Gateway Tools List** improvements - [PR #14695](https://github.com/BerriAI/litellm/pull/14695)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n- **+500 RPS Performance Boost** when sending the `user` field - [PR #14616](https://github.com/BerriAI/litellm/pull/14616)\\n- **+50 RPS** by removing iscoroutine from hot path - [PR #14649](https://github.com/BerriAI/litellm/pull/14649)\\n- **7% reduction** in __init__ overhead - [PR #14689](https://github.com/BerriAI/litellm/pull/14689)\\n- **Generic Object Pool** implementation for better resource management - [PR #14702](https://github.com/BerriAI/litellm/pull/14702)\\n\\n---\\n\\n## General Proxy Improvements\\n\\n- **Middle-Truncation** for spend log payloads - [PR #14637](https://github.com/BerriAI/litellm/pull/14637)\\n\\n#### Security\\n\\n- **Security Update** - Bump aiohttp==3.12.14, fix CVE-2025-53643 - [PR #14638](https://github.com/BerriAI/litellm/pull/14638)\\n\\n---\\n\\n## New Contributors\\n\\n* @luisfucros made their first contribution in [PR #14500](https://github.com/BerriAI/litellm/pull/14500)\\n* @hanakannzashi made their first contribution in [PR #14548](https://github.com/BerriAI/litellm/pull/14548)\\n* @eliasto made their first contribution in [PR #14494](https://github.com/BerriAI/litellm/pull/14494)\\n* @Rasmusafj made their first contribution in [PR #14491](https://github.com/BerriAI/litellm/pull/14491)\\n* @LingXuanYin made their first contribution in [PR #14569](https://github.com/BerriAI/litellm/pull/14569)\\n* @ronaldpereira made their first contribution in [PR #14613](https://github.com/BerriAI/litellm/pull/14613)\\n* @hula-la made their first contribution in [PR #14534](https://github.com/BerriAI/litellm/pull/14534)\\n* @carlos-marchal-ph made their first contribution in [PR #14610](https://github.com/BerriAI/litellm/pull/14610)\\n* @akraines made their first contribution in [PR #14637](https://github.com/BerriAI/litellm/pull/14637)\\n* @mrFranklin made their first contribution in [PR #14708](https://github.com/BerriAI/litellm/pull/14708)\\n* @tcx4c70 made their first contribution in [PR #14675](https://github.com/BerriAI/litellm/pull/14675)\\n* @michaeltansg made their first contribution in [PR #14666](https://github.com/BerriAI/litellm/pull/14666)\\n* @tosi29 made their first contribution in [PR #14725](https://github.com/BerriAI/litellm/pull/14725)\\n* @gmdfalk made their first contribution in [PR #14735](https://github.com/BerriAI/litellm/pull/14735)\\n* @FelipeRodriguesGare made their first contribution in [PR #14733](https://github.com/BerriAI/litellm/pull/14733)\\n* @mritunjaysharma394 made their first contribution in [PR #14678](https://github.com/BerriAI/litellm/pull/14678)\\n\\n---\\n\\n## **[Full Changelog](https://github.com/BerriAI/litellm/compare/v1.77.2.rc.1...v1.77.3.rc.1)**"},{"id":"v1-77-2","metadata":{"permalink":"/release_notes/v1-77-2","source":"@site/release_notes/v1.77.2-stable/index.md","title":"v1.77.2-stable - Bedrock Batches API","description":"Deploy this version","date":"2025-09-13T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.77.2-stable - Bedrock Batches API","slug":"v1-77-2","date":"2025-09-13T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.77.3-stable - Priority Based Rate Limiting","permalink":"/release_notes/v1-77-3"},"nextItem":{"title":"v1.76.3-stable - Performance, Video Generation & CloudZero Integration","permalink":"/release_notes/v1-76-3"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:main-v1.77.2-stable\\n```\\n\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.77.2.post1\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights\\n\\n- **Bedrock Batches API** - Support for creating Batch Inference Jobs on Bedrock using LiteLLM\'s unified batch API (OpenAI compatible)\\n- **Qwen API Tiered Pricing** - Cost tracking support for Dashscope (Qwen) models with multiple pricing tiers\\n\\n## New Models / Updated Models\\n\\n#### New Model Support\\n\\n| Provider    | Model                           | Context Window | Pricing ($/1M tokens) | Features |\\n| ----------- | ------------------------------- | -------------- | --------------------- | -------- |\\n| DeepInfra   | `deepinfra/deepseek-ai/DeepSeek-R1` | 164K | **Input:** $0.70<br/>**Output:** $2.40 | Chat completions, tool calling |\\n| Heroku      | `heroku/claude-4-sonnet`        | 8K | Contact provider for pricing | Function calling, tool choice |\\n| Heroku      | `heroku/claude-3-7-sonnet`      | 8K | Contact provider for pricing | Function calling, tool choice |\\n| Heroku      | `heroku/claude-3-5-sonnet-latest` | 8K | Contact provider for pricing | Function calling, tool choice |\\n| Heroku      | `heroku/claude-3-5-haiku`       | 4K | Contact provider for pricing | Function calling, tool choice |\\n| Dashscope   | `dashscope/qwen-plus-latest`    | 1M | **Tiered Pricing:**<br/>\u2022 0-256K tokens: $0.40 / $1.20<br/>\u2022 256K-1M tokens: $1.20 / $3.60 | Function calling, reasoning |\\n| Dashscope   | `dashscope/qwen3-max-preview`   | 262K | **Tiered Pricing:**<br/>\u2022 0-32K tokens: $1.20 / $6.00<br/>\u2022 32K-128K tokens: $2.40 / $12.00<br/>\u2022 128K-252K tokens: $3.00 / $15.00 | Function calling, reasoning |\\n| Dashscope   | `dashscope/qwen-flash`          | 1M | **Tiered Pricing:**<br/>\u2022 0-256K tokens: $0.05 / $0.40<br/>\u2022 256K-1M tokens: $0.25 / $2.00 | Function calling, reasoning |\\n| Dashscope   | `dashscope/qwen3-coder-plus`    | 1M | **Tiered Pricing:**<br/>\u2022 0-32K tokens: $1.00 / $5.00<br/>\u2022 32K-128K tokens: $1.80 / $9.00<br/>\u2022 128K-256K tokens: $3.00 / $15.00<br/>\u2022 256K-1M tokens: $6.00 / $60.00 | Function calling, reasoning, caching |\\n| Dashscope   | `dashscope/qwen3-coder-flash`   | 1M | **Tiered Pricing:**<br/>\u2022 0-32K tokens: $0.30 / $1.50<br/>\u2022 32K-128K tokens: $0.50 / $2.50<br/>\u2022 128K-256K tokens: $0.80 / $4.00<br/>\u2022 256K-1M tokens: $1.60 / $9.60 | Function calling, reasoning, caching |\\n\\n---\\n\\n#### Features\\n\\n- **[Bedrock](../../docs/providers/bedrock_batches)**\\n    - Bedrock Batches API - batch processing support with file upload and request transformation - [PR #14518](https://github.com/BerriAI/litellm/pull/14518), [PR #14522](https://github.com/BerriAI/litellm/pull/14522)\\n- **[VLLM](../../docs/providers/vllm)**\\n    - Added transcription endpoint support - [PR #14523](https://github.com/BerriAI/litellm/pull/14523)\\n- **[Ollama](../../docs/providers/ollama)**\\n    - `ollama_chat/` - images, thinking, and content as list handling - [PR #14523](https://github.com/BerriAI/litellm/pull/14523)\\n- **General**\\n    - New debug flag for detailed request/response logging [PR #14482](https://github.com/BerriAI/litellm/pull/14482)\\n\\n#### Bug Fixes\\n\\n- **[Azure OpenAI](../../docs/providers/azure)**\\n    - Fixed extra_body injection causing payload rejection in image generation - [PR #14475](https://github.com/BerriAI/litellm/pull/14475)\\n- **[LM Studio](../../docs/providers/lm-studio)**\\n    - Resolved illegal Bearer header value issue - [PR #14512](https://github.com/BerriAI/litellm/pull/14512)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Bug Fixes\\n\\n- **[/messages](../../docs/anthropic_unified)**\\n    - Don\'t send content block after message w/ finish reason + usage block - [PR #14477](https://github.com/BerriAI/litellm/pull/14477)\\n- **[/generateContent](../../docs/generateContent)**\\n    - Gemini CLI Integration - Fixed token count errors - [PR #14451](https://github.com/BerriAI/litellm/pull/14451), [PR #14417](https://github.com/BerriAI/litellm/pull/14417)\\n\\n---\\n\\n## Spend Tracking, Budgets and Rate Limiting\\n\\n#### Features\\n\\n- **[Qwen API Tiered Pricing](../../docs/providers/dashscope)** - Added comprehensive tiered cost tracking for Dashscope/Qwen models - [PR #14471](https://github.com/BerriAI/litellm/pull/14471), [PR #14479](https://github.com/BerriAI/litellm/pull/14479)\\n\\n#### Bug Fixes\\n\\n- **Provider Budgets** - Fixed provider budget calculations - [PR #14459](https://github.com/BerriAI/litellm/pull/14459)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n\\n- **User Headers Mapping** - New X-LiteLLM Users mapping feature for enhanced user tracking - [PR #14485](https://github.com/BerriAI/litellm/pull/14485)\\n- **Key Unblocking** - Support for hashed tokens in `/key/unblock` endpoint - [PR #14477](https://github.com/BerriAI/litellm/pull/14477)\\n- **Model Group Header Forwarding** - Enhanced wildcard model support with documentation - [PR #14528](https://github.com/BerriAI/litellm/pull/14528)\\n\\n#### Bug Fixes\\n\\n- **Log Tab Key Alias** - Fixed filtering inaccuracies for failed logs - [PR #14469](https://github.com/BerriAI/litellm/pull/14469), [PR #14529](https://github.com/BerriAI/litellm/pull/14529)\\n\\n---\\n\\n## Logging / Guardrail Integrations\\n\\n#### Features\\n\\n- **Noma Integration** - Added non-blocking monitor mode with anonymize input support - [PR #14401](https://github.com/BerriAI/litellm/pull/14401)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n#### Performance\\n- Removed dynamic creation of static values - [PR #14538](https://github.com/BerriAI/litellm/pull/14538)\\n- Using `_PROXY_MaxParallelRequestsHandler_v3` by default for optimal throughput - [PR #14450](https://github.com/BerriAI/litellm/pull/14450)\\n- Improved execution context propagation into logging tasks - [PR #14455](https://github.com/BerriAI/litellm/pull/14455)\\n\\n---\\n\\n\\n\\n## New Contributors\\n* @Sameerlite made their first contribution in [PR #14460](https://github.com/BerriAI/litellm/pull/14460)\\n* @holzman made their first contribution in [PR #14459](https://github.com/BerriAI/litellm/pull/14459)\\n* @sashank5644 made their first contribution in [PR #14469](https://github.com/BerriAI/litellm/pull/14469)\\n* @TomAlon made their first contribution in [PR #14401](https://github.com/BerriAI/litellm/pull/14401)\\n* @AlexsanderHamir made their first contribution in [PR #14538](https://github.com/BerriAI/litellm/pull/14538)\\n\\n---\\n\\n## **[Full Changelog](https://github.com/BerriAI/litellm/compare/v1.77.1.dev.2...v1.77.2.dev)**"},{"id":"v1-76-3","metadata":{"permalink":"/release_notes/v1-76-3","source":"@site/release_notes/v1.76.3-stable/index.md","title":"v1.76.3-stable - Performance, Video Generation & CloudZero Integration","description":"This release has a known issue where startup is leading to Out of Memory errors when deploying on Kubernetes. We recommend waiting before upgrading to this version.","date":"2025-09-06T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.76.3-stable - Performance, Video Generation & CloudZero Integration","slug":"v1-76-3","date":"2025-09-06T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.77.2-stable - Bedrock Batches API","permalink":"/release_notes/v1-77-2"},"nextItem":{"title":"v1.76.1-stable - Gemini 2.5 Flash Image","permalink":"/release_notes/v1-76-1"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n:::warning\\n\\nThis release has a known issue where startup is leading to Out of Memory errors when deploying on Kubernetes. We recommend waiting before upgrading to this version.\\n\\n:::\\n\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.76.3\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.76.3\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights\\n\\n- **Major Performance Improvements** +400 RPS when using correct amount of workers + CPU cores combination\\n- **Video Generation Support** - Added Google AI Studio  and Vertex AI Veo Video Generation through LiteLLM Pass through routes\\n- **CloudZero Integration** - New cost tracking integration for exporting LiteLLM Usage and Spend data to CloudZero. \\n\\n## Major Changes \\n- **Performance Optimization**: LiteLLM Proxy now achieves +400 RPS when using correct amount of CPU cores - [PR #14153](https://github.com/BerriAI/litellm/pull/14153), [PR #14242](https://github.com/BerriAI/litellm/pull/14242)\\n  \\n  By default, LiteLLM will now use `num_workers = os.cpu_count()` to achieve optimal performance. \\n  \\n  **Override Options:**\\n  \\n  Set environment variable:\\n  ```bash\\n  DEFAULT_NUM_WORKERS_LITELLM_PROXY=1\\n  ```\\n  \\n  Or start LiteLLM Proxy with:\\n  ```bash\\n  litellm --num_workers 1\\n  ```\\n\\n- **Security Fix**: Fixed memory_usage_in_mem_cache cache endpoint vulnerability - [PR #14229](https://github.com/BerriAI/litellm/pull/14229)\\n\\n---\\n\\n## Performance Improvements\\n\\nThis release includes significant performance optimizations. On our internal benchmarks we saw 1 instance get +400 RPS when using correct amount of  workers + CPU cores combination.\\n\\n- **+400 RPS Performance Boost** - LiteLLM Proxy now uses correct amount of CPU cores for optimal performance - [PR #14153](https://github.com/BerriAI/litellm/pull/14153)\\n- **Default CPU Workers** - Changed DEFAULT_NUM_WORKERS_LITELLM_PROXY default to number of CPUs - [PR #14242](https://github.com/BerriAI/litellm/pull/14242)\\n\\n\\n---\\n\\n## New Models / Updated Models\\n\\n#### New Model Support\\n\\n| Provider    | Model                                  | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Features |\\n| ----------- | -------------------------------------- | -------------- | ------------------- | -------------------- | -------- |\\n| OpenRouter | `openrouter/openai/gpt-4.1` | 1M | $2.00 | $8.00 | Chat completions with vision |\\n| OpenRouter | `openrouter/openai/gpt-4.1-mini` | 1M | $0.40 | $1.60 | Efficient chat completions |\\n| OpenRouter | `openrouter/openai/gpt-4.1-nano` | 1M | $0.10 | $0.40 | Ultra-efficient chat |\\n| Vertex AI | `vertex_ai/openai/gpt-oss-20b-maas` | 131K | $0.075 | $0.30 | Reasoning support |\\n| Vertex AI | `vertex_ai/openai/gpt-oss-120b-maas` | 131K | $0.15 | $0.60 | Advanced reasoning |\\n| Gemini | `gemini/veo-3.0-generate-preview` | 1K | - | $0.75/sec | Video generation |\\n| Gemini | `gemini/veo-3.0-fast-generate-preview` | 1K | - | $0.40/sec | Fast video generation |\\n| Gemini | `gemini/veo-2.0-generate-001` | 1K | - | $0.35/sec | Video generation |\\n| Volcengine | `doubao-embedding-large` | 4K | Free | Free | 2048-dim embeddings |\\n| Together AI | `together_ai/deepseek-ai/DeepSeek-V3.1` | 128K | $0.60 | $1.70 | Reasoning support |\\n\\n#### Features\\n\\n- **[Google Gemini](../../docs/providers/gemini)**\\n    - Added \'thoughtSignature\' support via \'thinking_blocks\' - [PR #14122](https://github.com/BerriAI/litellm/pull/14122)\\n    - Added support for reasoning_effort=\'minimal\' for Gemini models - [PR #14262](https://github.com/BerriAI/litellm/pull/14262)\\n- **[OpenRouter](../../docs/providers/openrouter)**\\n    - Added GPT-4.1 model family - [PR #14101](https://github.com/BerriAI/litellm/pull/14101)\\n- **[Groq](../../docs/providers/groq)**\\n    - Added support for reasoning_effort parameter - [PR #14207](https://github.com/BerriAI/litellm/pull/14207)\\n- **[X.AI](../../docs/providers/xai)**\\n    - Fixed XAI cost calculation - [PR #14127](https://github.com/BerriAI/litellm/pull/14127)\\n- **[Vertex AI](../../docs/providers/vertex)**\\n    - Added support for GPT-OSS models on Vertex AI - [PR #14184](https://github.com/BerriAI/litellm/pull/14184)\\n    - Added additionalProperties to Vertex AI Schema definition - [PR #14252](https://github.com/BerriAI/litellm/pull/14252)\\n- **[VLLM](../../docs/providers/vllm)**\\n    - Handle output parsing responses API output - [PR #14121](https://github.com/BerriAI/litellm/pull/14121)\\n- **[Ollama](../../docs/providers/ollama)**\\n    - Added unified \'thinking\' param support via `reasoning_content` - [PR #14121](https://github.com/BerriAI/litellm/pull/14121)\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Added supported text field to anthropic citation response - [PR #14126](https://github.com/BerriAI/litellm/pull/14126)\\n- **[OCI Provider](../../docs/providers/oci)**\\n    - Handle assistant messages with both content and tool_calls - [PR #14171](https://github.com/BerriAI/litellm/pull/14171)\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Fixed structure output - [PR #14130](https://github.com/BerriAI/litellm/pull/14130)\\n    - Added initial support for Bedrock Batches API - [PR #14190](https://github.com/BerriAI/litellm/pull/14190)\\n- **[Databricks](../../docs/providers/databricks)**\\n    - Added support for anthropic citation API in Databricks - [PR #14077](https://github.com/BerriAI/litellm/pull/14077)\\n\\n### Bug Fixes\\n- **[Google Gemini (Google AI Studio + Vertex AI)](../../docs/providers/gemini)**\\n    - Fixed Gemini 2.5 Pro schema validation with OpenAI-style type arrays in tools - [PR #14154](https://github.com/BerriAI/litellm/pull/14154)\\n    - Fixed Gemini Tool Calling empty enum property - [PR #14155](https://github.com/BerriAI/litellm/pull/14155)\\n\\n#### New Provider Support\\n\\n- **[Volcengine](../../docs/providers/volcengine)**\\n    - Added Volcengine embedding module with handler and transformation logic - [PR #14028](https://github.com/BerriAI/litellm/pull/14028)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n\\n- **[Images API](../../docs/image_generation)**\\n    - Added pass through image generation and image editing on OpenAI - [PR #14292](https://github.com/BerriAI/litellm/pull/14292)\\n    - Support extra_body parameter for image generation - [PR #14211](https://github.com/BerriAI/litellm/pull/14211)\\n- **[Responses API](../../docs/response_api)**\\n    - Fixed response API for reasoning item in input for litellm proxy - [PR #14200](https://github.com/BerriAI/litellm/pull/14200)\\n    - Added structured output for SDK - [PR #14206](https://github.com/BerriAI/litellm/pull/14206)\\n- **[Bedrock Passthrough](../../docs/pass_through/bedrock)**\\n    - Support AWS_BEDROCK_RUNTIME_ENDPOINT on bedrock passthrough - [PR #14156](https://github.com/BerriAI/litellm/pull/14156)\\n- **[Google AI Studio Passthrough](../../docs/pass_through/google_ai_studio)**\\n    - Allow using Veo Video Generation through LiteLLM Pass through routes - [PR #14228](https://github.com/BerriAI/litellm/pull/14228)\\n- **General**\\n    - Added support for safety_identifier parameter in chat.completions.create - [PR #14174](https://github.com/BerriAI/litellm/pull/14174)\\n    - Fixed misclassified 500 error on invalid image_url in /chat/completions request - [PR #14149](https://github.com/BerriAI/litellm/pull/14149)\\n    - Fixed token count error for Gemini CLI - [PR #14133](https://github.com/BerriAI/litellm/pull/14133)\\n\\n#### Bugs\\n\\n- **General**\\n    - Remove \\"/\\" or \\":\\" from model name when being used as h11 header name - [PR #14191](https://github.com/BerriAI/litellm/pull/14191)\\n    - Bug fix for openai.gpt-oss when using reasoning_effort parameter - [PR #14300](https://github.com/BerriAI/litellm/pull/14300)\\n\\n---\\n\\n## Spend Tracking, Budgets and Rate Limiting\\n\\n### Features\\n    - Added header support for spend_logs_metadata - [PR #14186](https://github.com/BerriAI/litellm/pull/14186)\\n    - Litellm passthrough cost tracking for chat completion - [PR #14256](https://github.com/BerriAI/litellm/pull/14256)\\n\\n### Bug Fixes\\n    - Fixed TPM Rate Limit Bug - [PR #14237](https://github.com/BerriAI/litellm/pull/14237)\\n    - Fixed Key Budget not resets at expectable times - [PR #14241](https://github.com/BerriAI/litellm/pull/14241)\\n\\n\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n\\n- **UI Improvements**\\n    - Logs page screen size fixed - [PR #14135](https://github.com/BerriAI/litellm/pull/14135)\\n    - Create Organization Tooltip added on Success - [PR #14132](https://github.com/BerriAI/litellm/pull/14132)\\n    - Back to Keys should say Back to Logs - [PR #14134](https://github.com/BerriAI/litellm/pull/14134)\\n    - Add client side pagination on All Models table - [PR #14136](https://github.com/BerriAI/litellm/pull/14136)\\n    - Model Filters UI improvement - [PR #14131](https://github.com/BerriAI/litellm/pull/14131)\\n    - Remove table filter on user info page - [PR #14169](https://github.com/BerriAI/litellm/pull/14169)\\n    - Team name badge added on the User Details - [PR #14003](https://github.com/BerriAI/litellm/pull/14003)\\n    - Fix: Log page parameter passing error - [PR #14193](https://github.com/BerriAI/litellm/pull/14193)\\n- **Authentication & Authorization**\\n    - Support for ES256/ES384/ES512 and EdDSA JWT verification - [PR #14118](https://github.com/BerriAI/litellm/pull/14118)\\n    - Ensure `team_id` is a required field for generating service account keys - [PR #14270](https://github.com/BerriAI/litellm/pull/14270)\\n\\n#### Bugs\\n\\n- **General**\\n    - Validate store model in db setting - [PR #14269](https://github.com/BerriAI/litellm/pull/14269)\\n\\n---\\n\\n## Logging / Guardrail Integrations\\n\\n#### Features\\n\\n- **[Datadog](../../docs/proxy/logging#datadog)**\\n    - Ensure `apm_id` is set on DD LLM Observability traces - [PR #14272](https://github.com/BerriAI/litellm/pull/14272)\\n- **[Braintrust](../../docs/proxy/logging#braintrust)**\\n    - Fix logging when OTEL is enabled - [PR #14122](https://github.com/BerriAI/litellm/pull/14122)\\n- **[OTEL](../../docs/proxy/logging#otel)**\\n    - Optional Metrics and Logs following semantic conventions - [PR #14179](https://github.com/BerriAI/litellm/pull/14179)\\n- **[Slack Alerting](../../docs/proxy/alerting)**\\n    - Added alert type to alert message to slack for easier handling - [PR #14176](https://github.com/BerriAI/litellm/pull/14176)\\n\\n#### Guardrails\\n    - Added guardrail to the Anthropic API endpoint - [PR #14107](https://github.com/BerriAI/litellm/pull/14107)\\n\\n#### New Integration\\n\\n- **[CloudZero](../../docs/proxy/cost_tracking)**\\n    - LiteLLM x CloudZero Integration for Cost Tracking - [PR #14296](https://github.com/BerriAI/litellm/pull/14296)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n#### Features\\n\\n- **Performance**\\n    - LiteLLM Proxy: +400 RPS when using correct amount of CPU cores - [PR #14153](https://github.com/BerriAI/litellm/pull/14153)\\n    - Allow using `x-litellm-stream-timeout` header for stream timeout in requests - [PR #14147](https://github.com/BerriAI/litellm/pull/14147)\\n    - Change DEFAULT_NUM_WORKERS_LITELLM_PROXY default to number CPUs - [PR #14242](https://github.com/BerriAI/litellm/pull/14242)\\n- **Monitoring**\\n    - Added Prometheus missing metrics - [PR #14139](https://github.com/BerriAI/litellm/pull/14139)\\n- **Timeout**\\n    - **Stream Timeout Control** - Allow using `x-litellm-stream-timeout` header for stream timeout in requests - [PR #14147](https://github.com/BerriAI/litellm/pull/14147)\\n- **Routing**\\n    - Fixed x-litellm-tags not routing with Responses API - [PR #14289](https://github.com/BerriAI/litellm/pull/14289)\\n\\n#### Bugs\\n\\n- **Security**\\n    - Fixed memory_usage_in_mem_cache cache endpoint vulnerability - [PR #14229](https://github.com/BerriAI/litellm/pull/14229)\\n\\n---\\n\\n## General Proxy Improvements\\n\\n#### Features\\n\\n- **SCIM Support**\\n    - Added better SCIM debugging - [PR #14221](https://github.com/BerriAI/litellm/pull/14221)\\n    - Bug fixes for handling SCIM Group Memberships - [PR #14226](https://github.com/BerriAI/litellm/pull/14226)\\n- **Kubernetes**\\n    - Added optional PodDisruptionBudget for litellm proxy - [PR #14093](https://github.com/BerriAI/litellm/pull/14093)\\n- **Error Handling**\\n    - Add model to azure error message - [PR #14294](https://github.com/BerriAI/litellm/pull/14294)\\n\\n---\\n\\n## New Contributors\\n* @iabhi4 made their first contribution in [PR #14093](https://github.com/BerriAI/litellm/pull/14093)\\n* @zainhas made their first contribution in [PR #14087](https://github.com/BerriAI/litellm/pull/14087)\\n* @LifeDJIK made their first contribution in [PR #14146](https://github.com/BerriAI/litellm/pull/14146)\\n* @retanoj made their first contribution in [PR #14133](https://github.com/BerriAI/litellm/pull/14133)\\n* @zhxlp made their first contribution in [PR #14193](https://github.com/BerriAI/litellm/pull/14193)\\n* @kayoch1n made their first contribution in [PR #14191](https://github.com/BerriAI/litellm/pull/14191)\\n* @kutsushitaneko made their first contribution in [PR #14171](https://github.com/BerriAI/litellm/pull/14171)\\n* @mjmendo made their first contribution in [PR #14176](https://github.com/BerriAI/litellm/pull/14176)\\n* @HarshavardhanK made their first contribution in [PR #14213](https://github.com/BerriAI/litellm/pull/14213)\\n* @eycjur made their first contribution in [PR #14207](https://github.com/BerriAI/litellm/pull/14207)\\n* @22mSqRi made their first contribution in [PR #14241](https://github.com/BerriAI/litellm/pull/14241)\\n* @onlylhf made their first contribution in [PR #14028](https://github.com/BerriAI/litellm/pull/14028)\\n* @btpemercier made their first contribution in [PR #11319](https://github.com/BerriAI/litellm/pull/11319)\\n* @tremlin made their first contribution in [PR #14287](https://github.com/BerriAI/litellm/pull/14287)\\n* @TobiMayr made their first contribution in [PR #14262](https://github.com/BerriAI/litellm/pull/14262)\\n* @Eitan1112 made their first contribution in [PR #14252](https://github.com/BerriAI/litellm/pull/14252)\\n\\n---\\n\\n## **[Full Changelog](https://github.com/BerriAI/litellm/compare/v1.76.1-nightly...v1.76.3-nightly)**"},{"id":"v1-76-1","metadata":{"permalink":"/release_notes/v1-76-1","source":"@site/release_notes/v1.76.1-stable/index.md","title":"v1.76.1-stable - Gemini 2.5 Flash Image","description":"Deploy this version","date":"2025-08-30T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.76.1-stable - Gemini 2.5 Flash Image","slug":"v1-76-1","date":"2025-08-30T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.76.3-stable - Performance, Video Generation & CloudZero Integration","permalink":"/release_notes/v1-76-3"},"nextItem":{"title":"v1.76.0-stable - RPS Improvements","permalink":"/release_notes/v1-76-0"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.76.1\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.76.1\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights\\n\\n- **Major Performance Improvements** - 6.5x faster LiteLLM Python SDK completion with fastuuid integration.\\n- **New Model Support** - Gemini 2.5 Flash Image Preview, Grok Code Fast, and GPT Realtime models\\n- **Enhanced Provider Support** - DeepSeek-v3.1 pricing on Fireworks AI, Vercel AI Gateway, and improved Anthropic/GitHub Copilot integration\\n- **MCP Improvements** - Better connection testing and SSE MCP tools bug fixes\\n\\n## Major Changes \\n- Added support for using Gemini 2.5 Flash Image Preview with /chat/completions. **\ud83d\udea8 Warning** If you were using `gemini-2.0-flash-exp-image-generation` please follow this migration guide.\\n  [Gemini Image Generation Migration Guide](../../docs/extras/gemini_img_migration)\\n---\\n\\n## Performance Improvements\\n\\nThis release includes significant performance optimizations:\\n\\n- **6.5x faster LiteLLM Python SDK Completion** - Major performance boost for completion operations - [PR #13990](https://github.com/BerriAI/litellm/pull/13990)\\n- **fastuuid Integration** - 2.1x faster UUID generation with +80 RPS improvement for /chat/completions and other LLM endpoints - [PR #13992](https://github.com/BerriAI/litellm/pull/13992), [PR #14016](https://github.com/BerriAI/litellm/pull/14016)\\n- **Optimized Request Logging** - Don\'t print request params by default for +50 RPS improvement - [PR #14015](https://github.com/BerriAI/litellm/pull/14015)\\n- **Cache Performance** - 21% speedup in InMemoryCache.evict_cache and 45% speedup in `_is_debugging_on` function - [PR #14012](https://github.com/BerriAI/litellm/pull/14012), [PR #13988](https://github.com/BerriAI/litellm/pull/13988)\\n\\n---\\n\\n## New Models / Updated Models\\n\\n#### New Model Support\\n\\n| Provider    | Model                                  | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Features |\\n| ----------- | -------------------------------------- | -------------- | ------------------- | -------------------- | -------- |\\n| Google | `gemini-2.5-flash-image-preview` | 1M | $0.30 | $2.50 | Chat completions + image generation ($0.039/image) |\\n| X.AI | `xai/grok-code-fast` | 256K | $0.20 | $1.50 | Code generation |\\n| OpenAI | `gpt-realtime` | 32K | $4.00 | $16.00 | Real-time conversation + audio |\\n| Vercel AI Gateway | `vercel_ai_gateway/openai/o3` | 200K | $2.00 | $8.00 | Advanced reasoning |\\n| Vercel AI Gateway | `vercel_ai_gateway/openai/o3-mini` | 200K | $1.10 | $4.40 | Efficient reasoning |\\n| Vercel AI Gateway | `vercel_ai_gateway/openai/o4-mini` | 200K | $1.10 | $4.40 | Latest mini model |\\n| DeepInfra | `deepinfra/zai-org/GLM-4.5` | 131K | $0.55 | $2.00 | Chat completions |\\n| Perplexity | `perplexity/codellama-34b-instruct` | 16K | $0.35 | $1.40 | Code generation |\\n| Fireworks AI | `fireworks_ai/accounts/fireworks/models/deepseek-v3p1` | 128K | $0.56 | $1.68 | Chat completions |\\n\\n**Additional Models Added:** Various other Vercel AI Gateway models were added too. See [models.litellm.ai](https://models.litellm.ai) for the full list.\\n\\n#### Features\\n\\n- **[Google Gemini](../../docs/providers/gemini)**\\n    - Added support for `gemini-2.5-flash-image-preview` with image return capability - [PR #13979](https://github.com/BerriAI/litellm/pull/13979), [PR #13983](https://github.com/BerriAI/litellm/pull/13983)\\n    - Support for requests with only system prompt - [PR #14010](https://github.com/BerriAI/litellm/pull/14010)\\n    - Fixed invalid model name error for Gemini Imagen models - [PR #13991](https://github.com/BerriAI/litellm/pull/13991)\\n- **[X.AI](../../docs/providers/xai)**\\n    - Added `xai/grok-code-fast` model family support - [PR #14054](https://github.com/BerriAI/litellm/pull/14054)\\n    - Fixed frequency_penalty parameter for grok-4 models - [PR #14078](https://github.com/BerriAI/litellm/pull/14078)\\n- **[OpenAI](../../docs/providers/openai)**\\n    - Added support for gpt-realtime models - [PR #14082](https://github.com/BerriAI/litellm/pull/14082)\\n    - Support for reasoning and reasoning_effort parameters by default - [PR #12865](https://github.com/BerriAI/litellm/pull/12865)\\n- **[Fireworks AI](../../docs/providers/fireworks_ai)**\\n    - Added DeepSeek-v3.1 pricing - [PR #13958](https://github.com/BerriAI/litellm/pull/13958)\\n- **[DeepInfra](../../docs/providers/deepinfra)**\\n    - Fixed reasoning_effort setting for DeepSeek-V3.1 - [PR #14053](https://github.com/BerriAI/litellm/pull/14053)\\n- **[GitHub Copilot](../../docs/providers/github_copilot)**\\n    - Added support for thinking and reasoning_effort parameters - [PR #13691](https://github.com/BerriAI/litellm/pull/13691)\\n    - Added image headers support - [PR #13955](https://github.com/BerriAI/litellm/pull/13955)\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Support for custom Anthropic-compatible API endpoints - [PR #13945](https://github.com/BerriAI/litellm/pull/13945)\\n    - Fixed /messages fallback from Anthropic API to Bedrock API - [PR #13946](https://github.com/BerriAI/litellm/pull/13946)\\n- **[Nebius](../../docs/providers/nebius)**\\n    - Expanded provider models and normalized model IDs - [PR #13965](https://github.com/BerriAI/litellm/pull/13965)\\n- **[Vertex AI](../../docs/providers/vertex)**\\n    - Fixed Vertex Mistral streaming issues - [PR #13952](https://github.com/BerriAI/litellm/pull/13952)\\n    - Fixed anyOf corner cases for Gemini tool calls - [PR #12797](https://github.com/BerriAI/litellm/pull/12797)\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Fixed structure output issues - [PR #14005](https://github.com/BerriAI/litellm/pull/14005)\\n- **[OpenRouter](../../docs/providers/openrouter)**\\n    - Added GPT-5 family models pricing - [PR #13536](https://github.com/BerriAI/litellm/pull/13536)\\n\\n#### New Provider Support\\n\\n- **[Vercel AI Gateway](../../docs/providers/vercel_ai_gateway)**\\n    - New provider support added - [PR #13144](https://github.com/BerriAI/litellm/pull/13144)\\n- **[DataRobot](../../docs/providers/datarobot)**\\n    - Added provider documentation - [PR #14038](https://github.com/BerriAI/litellm/pull/14038), [PR #14074](https://github.com/BerriAI/litellm/pull/14074)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n\\n- **[Images API](../../docs/image_generation)**\\n    - Support for multiple images in OpenAI images/edits endpoint - [PR #13916](https://github.com/BerriAI/litellm/pull/13916)\\n    - Allow using dynamic `api_key` for image generation requests - [PR #14007](https://github.com/BerriAI/litellm/pull/14007)\\n- **[Responses API](../../docs/response_api)**\\n    - Fixed `/responses` endpoint ignoring extra_headers in GitHub Copilot - [PR #13775](https://github.com/BerriAI/litellm/pull/13775)\\n    - Added support for new web_search tool - [PR #14083](https://github.com/BerriAI/litellm/pull/14083)\\n- **[Azure Passthrough](../../docs/providers/azure/azure)**\\n    - Fixed Azure Passthrough request with streaming - [PR #13831](https://github.com/BerriAI/litellm/pull/13831)\\n\\n#### Bugs\\n\\n- **General**\\n    - Fixed handling of None metadata in batch requests - [PR #13996](https://github.com/BerriAI/litellm/pull/13996)\\n    - Fixed token_counter with special token input - [PR #13374](https://github.com/BerriAI/litellm/pull/13374)\\n    - Removed incorrect web search support for azure/gpt-4.1 family - [PR #13566](https://github.com/BerriAI/litellm/pull/13566)\\n\\n---\\n\\n## [MCP Gateway](../../docs/mcp)\\n\\n#### Features\\n\\n- **SSE MCP Tools**\\n    - Bug fix for adding SSE MCP tools - improved connection testing when adding MCPs - [PR #14048](https://github.com/BerriAI/litellm/pull/14048)\\n\\n[Read More](../../docs/mcp)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n\\n- **Team Management**\\n    - Allow setting Team Member RPM/TPM limits when creating a team - [PR #13943](https://github.com/BerriAI/litellm/pull/13943)\\n- **UI Improvements**\\n    - Fixed Next.js Security Vulnerabilities in UI Dashboard - [PR #14084](https://github.com/BerriAI/litellm/pull/14084)\\n    - Fixed collapsible navbar design - [PR #14075](https://github.com/BerriAI/litellm/pull/14075)\\n\\n#### Bugs\\n\\n- **Authentication**\\n    - Fixed Virtual keys with llm_api type causing Internal Server Error for /anthropic/* and other LLM passthrough routes - [PR #14046](https://github.com/BerriAI/litellm/pull/14046)\\n\\n---\\n\\n## Logging / Guardrail Integrations\\n\\n#### Features\\n\\n- **[Langfuse OTEL](../../docs/proxy/logging#langfuse)**\\n    - Allow using LANGFUSE_OTEL_HOST for configuring host - [PR #14013](https://github.com/BerriAI/litellm/pull/14013)\\n- **[Braintrust](../../docs/proxy/logging#braintrust)**\\n    - Added span name metadata feature - [PR #13573](https://github.com/BerriAI/litellm/pull/13573)\\n    - Fixed tests to reference moved attributes in `braintrust_logging` module - [PR #13978](https://github.com/BerriAI/litellm/pull/13978)\\n- **[OpenMeter](../../docs/proxy/logging#openmeter)**\\n    - Set user from token user_id for OpenMeter integration - [PR #13152](https://github.com/BerriAI/litellm/pull/13152)\\n\\n#### New Guardrail Support\\n\\n- **[Noma Security](../../docs/proxy/guardrails)**\\n    - Added Noma Security guardrail support - [PR #13572](https://github.com/BerriAI/litellm/pull/13572)\\n- **[Pangea](../../docs/proxy/guardrails)**\\n    - Updated Pangea Guardrail to support new AIDR endpoint - [PR #13160](https://github.com/BerriAI/litellm/pull/13160)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n#### Features\\n\\n- **Caching**\\n    - Verify if cache entry has expired prior to serving it to client - [PR #13933](https://github.com/BerriAI/litellm/pull/13933)\\n    - Fixed error saving latency as timedelta on Redis - [PR #14040](https://github.com/BerriAI/litellm/pull/14040)\\n- **Router**\\n    - Refactored router to choose weights by \'weight\', \'rpm\', \'tpm\' in one loop for simple_shuffle - [PR #13562](https://github.com/BerriAI/litellm/pull/13562)\\n- **Logging**\\n    - Fixed LoggingWorker graceful shutdown to prevent CancelledError warnings - [PR #14050](https://github.com/BerriAI/litellm/pull/14050)\\n    - Enhanced logging for containers to log on files both with usual format and json format - [PR #13394](https://github.com/BerriAI/litellm/pull/13394)\\n\\n#### Bugs\\n\\n- **Dependencies**\\n    - Bumped `orjson` version to \\"3.11.2\\" - [PR #13969](https://github.com/BerriAI/litellm/pull/13969)\\n\\n---\\n\\n## General Proxy Improvements\\n\\n#### Features\\n\\n- **AWS**\\n    - Add support for AWS assume_role with a session token - [PR #13919](https://github.com/BerriAI/litellm/pull/13919)\\n- **OCI Provider**\\n    - Added oci_key_file as an optional_parameter - [PR #14036](https://github.com/BerriAI/litellm/pull/14036)\\n- **Configuration**\\n    - Allow configuration to set threshold before request entry in spend log gets truncated - [PR #14042](https://github.com/BerriAI/litellm/pull/14042)\\n    - Enhanced proxy_config configuration: add support for existing configmap in Helm charts - [PR #14041](https://github.com/BerriAI/litellm/pull/14041)\\n- **Docker**\\n    - Added back supervisor to non-root image - [PR #13922](https://github.com/BerriAI/litellm/pull/13922)\\n\\n\\n---\\n\\n## New Contributors\\n* @ArthurRenault made their first contribution in [PR #13922](https://github.com/BerriAI/litellm/pull/13922)\\n* @stevenmanton made their first contribution in [PR #13919](https://github.com/BerriAI/litellm/pull/13919)\\n* @uc4w6c made their first contribution in [PR #13914](https://github.com/BerriAI/litellm/pull/13914)\\n* @nielsbosma made their first contribution in [PR #13573](https://github.com/BerriAI/litellm/pull/13573)\\n* @Yuki-Imajuku made their first contribution in [PR #13567](https://github.com/BerriAI/litellm/pull/13567)\\n* @codeflash-ai[bot] made their first contribution in [PR #13988](https://github.com/BerriAI/litellm/pull/13988)\\n* @ColeFrench made their first contribution in [PR #13978](https://github.com/BerriAI/litellm/pull/13978)\\n* @dttran-glo made their first contribution in [PR #13969](https://github.com/BerriAI/litellm/pull/13969)\\n* @manascb1344 made their first contribution in [PR #13965](https://github.com/BerriAI/litellm/pull/13965)\\n* @DorZion made their first contribution in [PR #13572](https://github.com/BerriAI/litellm/pull/13572)\\n* @edwardsamuel made their first contribution in [PR #13536](https://github.com/BerriAI/litellm/pull/13536)\\n* @blahgeek made their first contribution in [PR #13374](https://github.com/BerriAI/litellm/pull/13374)\\n* @Deviad made their first contribution in [PR #13394](https://github.com/BerriAI/litellm/pull/13394)\\n* @XSAM made their first contribution in [PR #13775](https://github.com/BerriAI/litellm/pull/13775)\\n* @KRRT7 made their first contribution in [PR #14012](https://github.com/BerriAI/litellm/pull/14012)\\n* @ikaadil made their first contribution in [PR #13991](https://github.com/BerriAI/litellm/pull/13991)\\n* @timelfrink made their first contribution in [PR #13691](https://github.com/BerriAI/litellm/pull/13691)\\n* @qidu made their first contribution in [PR #13562](https://github.com/BerriAI/litellm/pull/13562)\\n* @nagyv made their first contribution in [PR #13243](https://github.com/BerriAI/litellm/pull/13243)\\n* @xywei made their first contribution in [PR #12885](https://github.com/BerriAI/litellm/pull/12885)\\n* @ericgtkb made their first contribution in [PR #12797](https://github.com/BerriAI/litellm/pull/12797)\\n* @NoWall57 made their first contribution in [PR #13945](https://github.com/BerriAI/litellm/pull/13945)\\n* @lmwang9527 made their first contribution in [PR #14050](https://github.com/BerriAI/litellm/pull/14050)\\n* @WilsonSunBritten made their first contribution in [PR #14042](https://github.com/BerriAI/litellm/pull/14042)\\n* @Const-antine made their first contribution in [PR #14041](https://github.com/BerriAI/litellm/pull/14041)\\n* @dmvieira made their first contribution in [PR #14040](https://github.com/BerriAI/litellm/pull/14040)\\n* @gotsysdba made their first contribution in [PR #14036](https://github.com/BerriAI/litellm/pull/14036)\\n* @moshemorad made their first contribution in [PR #14005](https://github.com/BerriAI/litellm/pull/14005)\\n* @joshualipman123 made their first contribution in [PR #13144](https://github.com/BerriAI/litellm/pull/13144)\\n\\n---\\n\\n## **[Full Changelog](https://github.com/BerriAI/litellm/compare/v1.76.0-nightly...v1.76.1)**"},{"id":"v1-76-0","metadata":{"permalink":"/release_notes/v1-76-0","source":"@site/release_notes/v1.76.0-stable/index.md","title":"v1.76.0-stable - RPS Improvements","description":"LiteLLM is hiring a Founding Backend Engineer, in San Francisco.","date":"2025-08-23T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.76.0-stable - RPS Improvements","slug":"v1-76-0","date":"2025-08-23T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.76.1-stable - Gemini 2.5 Flash Image","permalink":"/release_notes/v1-76-1"},"nextItem":{"title":"v1.75.8-stable - Team Member Rate Limits","permalink":"/release_notes/v1-75-8"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n:::info\\n\\nLiteLLM is hiring a **Founding Backend Engineer**, in San Francisco. \\n\\n[Apply here](https://www.ycombinator.com/companies/litellm/jobs/6uvoBp3-founding-backend-engineer) if you\'re interested!\\n:::\\n\\n\\n\\n\\n\\n## Deploy this version\\n\\n:::info\\n\\nThis release is not live yet. \\n:::\\n\\n\\n---\\n\\n## New Models / Updated Models\\n\\n#### Bugs\\n- **[OpenAI](../../docs/providers/openai)**\\n    - Gpt-5 chat: clarify does not support function calling [PR #13612](https://github.com/BerriAI/litellm/pull/13612), s/o \xa0@[superpoussin22](https://github.com/superpoussin22)\\n- **[VertexAI](../../docs/providers/vertex)**\\n    - fix vertexai batch file format by\xa0@[thiagosalvatore](https://github.com/thiagosalvatore)\xa0in\xa0[PR #13576](https://github.com/BerriAI/litellm/pull/13576)\\n- **[LiteLLM Proxy](../../docs/providers/litellm_proxy)**\\n    - Add support for calling image_edits + image_generations via SDK to Proxy - [PR #13735](https://github.com/BerriAI/litellm/pull/13735)\\n- **[OpenRouter](../../docs/providers/openrouter)**\\n    - Fix max_output_tokens value for anthropic Claude 4 - [PR #13526](https://github.com/BerriAI/litellm/pull/13526)\\n- **[Gemini](../../docs/providers/gemini)**\\n    - Fix prompt caching cost calculation - [PR #13742](https://github.com/BerriAI/litellm/pull/13742)\\n- **[Azure](../../docs/providers/azure)**\\n    - Support `../openai/v1/respones` api base - [PR #13526](https://github.com/BerriAI/litellm/pull/13526)\\n    - Fix azure/gpt-5-chat max_input_tokens - [PR #13660](https://github.com/BerriAI/litellm/pull/13660)\\n- **[Groq](../../docs/providers/groq)**\\n    - streaming ASCII encoding issue - [PR #13675](https://github.com/BerriAI/litellm/pull/13675)\\n- **[Baseten](../../docs/providers/baseten)**\\n    - Refactored integration to use new openai-compatible endpoints - [PR #13783](https://github.com/BerriAI/litellm/pull/13783)\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - fix application inference profile for pass-through endpoints for bedrock - [PR #13881](https://github.com/BerriAI/litellm/pull/13881)\\n- **[DataRobot](../../docs/providers/datarobot)**\\n    - Updated URL handling for DataRobot provider URL - [PR #13880](https://github.com/BerriAI/litellm/pull/13880)\\n\\n#### Features\\n- **[Together AI](../../docs/providers/together)**\\n    - Added Qwen3, Deepseek R1 0528 Throughput, GLM 4.5 and GPT-OSS models cost tracking - [PR #13637](https://github.com/BerriAI/litellm/pull/13637), s/o \xa0@[Tasmay-Tibrewal](https://github.com/Tasmay-Tibrewal)\\n- **[Fireworks AI](../../docs/providers/fireworks_ai)**\\n    - add fireworks_ai/accounts/fireworks/models/deepseek-v3-0324 - [PR #13821](https://github.com/BerriAI/litellm/pull/13821)\\n- **[VertexAI](../../docs/providers/vertex)**\\n    - Add VertexAI qwen API Service - [PR #13828](https://github.com/BerriAI/litellm/pull/13828)\\n    - Add new VertexAI image models\xa0vertex_ai/imagen-4.0-generate-001,\xa0vertex_ai/imagen-4.0-ultra-generate-001,\xa0vertex_ai/imagen-4.0-fast-generate-001\xa0 - [PR #13874](https://github.com/BerriAI/litellm/pull/13874)\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Add long context support w/ cost tracking - [PR #13759](https://github.com/BerriAI/litellm/pull/13759)\\n- **[DeepInfra](../../docs/providers/deepinfra)**\\n    - Add rerank endpoint support for deepinfra - [PR #13820](https://github.com/BerriAI/litellm/pull/13820)\\n    - Add new models for cost tracking - [PR #13883](https://github.com/BerriAI/litellm/pull/13883), s/o \xa0@[Toy-97](https://github.com/Toy-97)\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Add tool prompt caching on async calls - [PR #13803](https://github.com/BerriAI/litellm/pull/13803), s/o \xa0@[UlookEE](https://github.com/UlookEE)\\n    - role chaining and session name with webauthentication for aws bedrock - [PR #13753](https://github.com/BerriAI/litellm/pull/13753), s/o @[RichardoC](https://github.com/RichardoC)\\n- **[Ollama](../../docs/providers/ollama)**\\n    - Handle Ollama null response when using tool calling with non-tool trained models - [PR #13902](https://github.com/BerriAI/litellm/pull/13902)\\n- **[OpenRouter](../../docs/providers/openrouter)**\\n    - Add deepseek/deepseek-chat-v3.1 support - [PR #13897](https://github.com/BerriAI/litellm/pull/13897)\\n- **[Mistral](../../docs/providers/mistral)**\\n    - Add support for calling mistral files via chat completions - [PR #13866](https://github.com/BerriAI/litellm/pull/13866), s/o \xa0@[jinskjoy](https://github.com/jinskjoy)\\n    - Handle empty assistant content - [PR #13671](https://github.com/BerriAI/litellm/pull/13671)\\n    - Support new \u2018thinking\u2019 response block - [PR #13671](https://github.com/BerriAI/litellm/pull/13671)\\n- **[Databricks](../../docs/providers/databricks)**\\n    - remove deprecated dbrx models (dbrx-instruct, llama 3.1) - [PR #13843](https://github.com/BerriAI/litellm/pull/13843)\\n- **[AI/ML API](../../docs/providers/ai_ml_api)**\\n    - Image gen api support - [PR #13893](https://github.com/BerriAI/litellm/pull/13893)\\n\\n\\n## LLM API Endpoints\\n#### Bugs\\n- **[Responses API](../../docs/response_api)**\\n    - add default api version for openai responses api calls - [PR #13526](https://github.com/BerriAI/litellm/pull/13526)\\n    - support\xa0allowed_openai_params - [PR #13671](https://github.com/BerriAI/litellm/pull/13671)\\n\\n\\n## MCP Gateway\\n#### Bugs\\n- fix StreamableHTTPSessionManager .run() error - [PR #13666](https://github.com/BerriAI/litellm/pull/13666)\\n\\n## Vector Stores \\n#### Bugs\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Using LiteLLM Managed Credentials for Query - [PR #13787](https://github.com/BerriAI/litellm/pull/13787)\\n\\n## Management Endpoints / UI\\n#### Bugs\\n- **[Passthrough](../../docs/pass_through/intro)**\\n    - Fix query passthrough deletion - [PR #13622](https://github.com/BerriAI/litellm/pull/13622)\\n\\n#### Features\\n- **Models**\\n    - Add Search Functionality for Public Model Names in Model Dashboard - [PR #13687](https://github.com/BerriAI/litellm/pull/13687)\\n    - Auto-Add `azure/` to deployment Name in UI - [PR #13685](https://github.com/BerriAI/litellm/pull/13685)\\n    - Models page row UI restructure - [PR #13771](https://github.com/BerriAI/litellm/pull/13771)\\n- **Notifications**\\n    - Add new notifications toast UI everywhere - [PR #13813](https://github.com/BerriAI/litellm/pull/13813)\\n- **Keys**\\n    - Fix key edit settings after regenerating a key - [PR #13815](https://github.com/BerriAI/litellm/pull/13815)\\n    - Require team_id when creating service account keys - [PR #13873](https://github.com/BerriAI/litellm/pull/13873)\\n    - Filter - show all options on filter option click - [PR #13858](https://github.com/BerriAI/litellm/pull/13858)\\n- **Usage**\\n    - Fix \u2018Cannot read properties of undefined\u2019 exception on user agent activity tab - [PR #13892](https://github.com/BerriAI/litellm/pull/13892)\\n- **SSO**\\n    - Free SSO usage for up to 5 users - [PR #13843](https://github.com/BerriAI/litellm/pull/13843)\\n\\n## Logging / Guardrail Integrations\\n#### Bugs\\n- **[Bedrock Guardrails](../../docs/proxy/guardrails/bedrock)**\\n    - Add bedrock api key support - [PR #13835](https://github.com/BerriAI/litellm/pull/13835)\\n#### Features\\n- **[Datadog LLM Observability](../../docs/integrations/datadog)**\\n    - Add support for Failure Logging\xa0[PR #13726](https://github.com/BerriAI/litellm/pull/13726)\\n    - Add time to first token, litellm overhead, guardrail overhead latency metrics - [PR #13734](https://github.com/BerriAI/litellm/pull/13734)\\n    - Add support for tracing guardrail input/output - [PR #13767](https://github.com/BerriAI/litellm/pull/13767)\\n- **[Langfuse OTEL](../../docs/integrations/langfuse)**\\n    - Allow using Key/Team Based Logging - [PR #13791](https://github.com/BerriAI/litellm/pull/13791)\\n- **[AIM](../../docs/integrations/aim)**\\n    - Migrate to new firewall API - [PR #13748](https://github.com/BerriAI/litellm/pull/13748)\\n- **[OTEL](../../docs/observability/opentelemetry_integration)**\\n    - Add OTEL tracing for actual LLM API call - [PR #13836](https://github.com/BerriAI/litellm/pull/13836)\\n- **[MLFlow](../../docs/observability/mlflow_integration)**\\n    - Include predicted output in MLflow tracing - [PR #13795](https://github.com/BerriAI/litellm/pull/13795), s/o\xa0@TomeHirata\xa0 \\n\\n\\n## Performance / Loadbalancing / Reliability improvements\\n#### Bugs\\n- **[Cooldowns](../../docs/routing#how-cooldowns-work)**\\n    - don\'t return raw Azure Exceptions to client (can contain prompt leakage) - [PR #13529](https://github.com/BerriAI/litellm/pull/13529)\\n- **[Auto-router](../../docs/proxy/auto_routing)**\\n    - Ensures the relevant dependencies for auto router existing on LiteLLM Docker - [PR #13788](https://github.com/BerriAI/litellm/pull/13788)\\n- **Model Alias**\\n    - Fix calling key with access to model alias - [PR #13830](https://github.com/BerriAI/litellm/pull/13830)\\n\\n#### Features\\n- **[S3 Caching](../../docs/proxy/caching)**\\n    - Use namespace as prefix for s3 cache - [PR #13704](https://github.com/BerriAI/litellm/pull/13704)\\n    - Async S3 Caching support (4x RPS improvement) - [PR #13852](https://github.com/BerriAI/litellm/pull/13852), s/o @[michal-otmianowski](https://github.com/michal-otmianowski)\\n- **Model Group header forwarding**\\n    - reuse same logic as global header forwarding - [PR #13741](https://github.com/BerriAI/litellm/pull/13741)\\n    - add support for hosted_vllm on UI - [PR #13885](https://github.com/BerriAI/litellm/pull/13885)\\n- **Performance**\\n    - Improve LiteLLM Python SDK RPS by +200 RPS (braintrust import + aiohttp transport fixes) - [PR #13839](https://github.com/BerriAI/litellm/pull/13839)\\n    - Use O(1) Set lookups for model routing - [PR #13879](https://github.com/BerriAI/litellm/pull/13879)\\n    - Reduce Significant CPU overhead from litellm_logging.py - [PR #13895](https://github.com/BerriAI/litellm/pull/13895)\\n    - Improvements for Async Success Handler (Logging Callbacks) - Approx +130 RPS - [PR #13905](https://github.com/BerriAI/litellm/pull/13905)\\n\\n\\n## General Proxy Improvements\\n#### Bugs\\n\\n- **SDK**\\n    - Fix litellm compatibility with newest release of openAI (>v1.100.0) - [PR #13728](https://github.com/BerriAI/litellm/pull/13728)\\n- **Helm**\\n    - Add possibility to configure resources for migrations-job - [PR #13617](https://github.com/BerriAI/litellm/pull/13617)\\n    - Ensure Helm chart auto generated master keys follow sk-xxxx format - [PR #13871](https://github.com/BerriAI/litellm/pull/13871)\\n    - Enhance database configuration: add support for optional endpointKey - [PR #13763](https://github.com/BerriAI/litellm/pull/13763)\\n- **Rate Limits**\\n    - fixing descriptor/response size mismatch on parallel_request_limiter_v3 - [PR #13863](https://github.com/BerriAI/litellm/pull/13863), s/o \xa0@[luizrennocosta](https://github.com/luizrennocosta)\\n- **Non-root**\\n    - fix permission access on prisma migrate in non-root image - [PR #13848](https://github.com/BerriAI/litellm/pull/13848), s/o @[Ithanil](https://github.com/Ithanil)"},{"id":"v1-75-8","metadata":{"permalink":"/release_notes/v1-75-8","source":"@site/release_notes/v1.75.8/index.md","title":"v1.75.8-stable - Team Member Rate Limits","description":"Deploy this version","date":"2025-08-16T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.75.8-stable - Team Member Rate Limits","slug":"v1-75-8","date":"2025-08-16T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.76.0-stable - RPS Improvements","permalink":"/release_notes/v1-76-0"},"nextItem":{"title":"v1.75.5-stable - Redis latency improvements","permalink":"/release_notes/v1-75-5"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.75.8-stable\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.75.8\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights\\n\\n- **Team Member Rate Limits** - Individual rate limiting for team members with JWT authentication support.\\n- **Performance Improvements** - New experimental HTTP handler flag for 100+ RPS improvement on OpenAI calls.\\n- **GPT-5 Model Family Support** - Full support for OpenAI\'s GPT-5 models with `reasoning_effort` parameter and Azure OpenAI integration.\\n- **Azure AI Flux Image Generation** - Support for Azure AI\'s Flux image generation models.\\n\\n---\\n\\n## Team Member Rate Limits\\n\\n<Image \\n  img={require(\'../../img/release_notes/team_member_rate_limits.png\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n<p style={{textAlign: \'left\', color: \'#666\'}}>\\n  LiteLLM MCP Architecture: Use MCP tools with all LiteLLM supported models\\n</p>\\n\\n\\nThis release adds support for setting rate limits on individual members (including machine users) within a team. Teams can now give each agent its own rate limits\u2014so that heavy-traffic agents don\u2019t impact other agents or human users. \\n\\nAgents can authenticate with LiteLLM using JWT and the same team role as human users, while still enforcing per-agent rate limits.\\n\\n\\n## New Models / Updated Models\\n\\n#### New Model Support\\n\\n| Provider    | Model                                  | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Features |\\n| ----------- | -------------------------------------- | -------------- | ------------------- | -------------------- | -------- |\\n| Azure AI | `azure_ai/FLUX-1.1-pro` | - | - | $40/image | Image generation |\\n| Azure AI | `azure_ai/FLUX.1-Kontext-pro` | - | - | $40/image | Image generation |\\n| Vertex AI | `vertex_ai/deepseek-ai/deepseek-r1-0528-maas` | 65k | $1.35 | $5.4 | Chat completions + reasoning |\\n| OpenRouter | `openrouter/deepseek/deepseek-chat-v3-0324` | 65k | $0.14 | $0.28 | Chat completions |\\n\\n\\n#### Features\\n\\n- **[OpenAI](../../docs/providers/openai)**\\n    - Added `reasoning_effort` parameter support for GPT-5 model family - [PR #13475](https://github.com/BerriAI/litellm/pull/13475), [Get Started](../../docs/providers/openai#openai-chat-completion-models)\\n    - Support for `reasoning` parameter in Responses API - [PR #13475](https://github.com/BerriAI/litellm/pull/13475), [Get Started](../../docs/response_api)\\n- **[Azure OpenAI](../../docs/providers/azure/azure)**\\n    - GPT-5 support with max_tokens and `reasoning` parameter - [PR #13510](https://github.com/BerriAI/litellm/pull/13510), [Get Started](../../docs/providers/azure/azure#gpt-5-models)\\n- **[AWS Bedrock](../../docs/providers/bedrock)**\\n    - Streaming support for bedrock gpt-oss model family - [PR #13346](https://github.com/BerriAI/litellm/pull/13346), [Get Started](../../docs/providers/bedrock#openai-gpt-oss)\\n    - `/messages` endpoint compatibility with `bedrock/converse/<model>` - [PR #13627](https://github.com/BerriAI/litellm/pull/13627)\\n    - Cache point support for assistant and tool messages - [PR #13640](https://github.com/BerriAI/litellm/pull/13640)\\n- **[Azure AI](../../docs/providers/azure)**\\n    - New Azure AI Flux Image Generation provider - [PR #13592](https://github.com/BerriAI/litellm/pull/13592), [Get Started](../../docs/providers/azure_ai_img)\\n    - Fixed Content-Type header for image generation - [PR #13584](https://github.com/BerriAI/litellm/pull/13584)\\n- **[CometAPI](../../docs/providers/comet)**\\n    - New provider support with chat completions and streaming - [PR #13458](https://github.com/BerriAI/litellm/pull/13458)\\n- **[SambaNova](../../docs/providers/sambanova)**\\n    - Added embedding model support - [PR #13308](https://github.com/BerriAI/litellm/pull/13308), [Get Started](../../docs/providers/sambanova#sambanova---embeddings)\\n- **[Vertex AI](../../docs/providers/vertex)**\\n    - Added `/countTokens` endpoint support for Gemini CLI integration - [PR #13545](https://github.com/BerriAI/litellm/pull/13545)\\n    - Token counter support for VertexAI models - [PR #13558](https://github.com/BerriAI/litellm/pull/13558)\\n- **[hosted_vllm](../../docs/providers/vllm)**\\n    - Added `reasoning_effort` parameter support - [PR #13620](https://github.com/BerriAI/litellm/pull/13620), [Get Started](../../docs/providers/vllm#reasoning-effort)\\n\\n#### Bugs\\n\\n- **[OCI](../../docs/providers/oci)**\\n    - Fixed streaming issues - [PR #13437](https://github.com/BerriAI/litellm/pull/13437)\\n- **[Ollama](../../docs/providers/ollama)**\\n    - Fixed GPT-OSS streaming with \'thinking\' field - [PR #13375](https://github.com/BerriAI/litellm/pull/13375)\\n- **[VolcEngine](../../docs/providers/volcengine)**\\n    - Fixed thinking disabled parameter handling - [PR #13598](https://github.com/BerriAI/litellm/pull/13598)\\n- **[Streaming](../../docs/completion/stream)**\\n    - Consistent \'finish_reason\' chunk indexing - [PR #13560](https://github.com/BerriAI/litellm/pull/13560)\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n\\n- **[/messages](../../docs/anthropic/messages)**\\n    - Tool use arguments properly returned for non-anthropic models - [PR #13638](https://github.com/BerriAI/litellm/pull/13638)\\n\\n#### Bugs\\n\\n- **[Real-time API](../../docs/realtime)**\\n    - Fixed endpoint for no intent scenarios - [PR #13476](https://github.com/BerriAI/litellm/pull/13476)\\n- **[Responses API](../../docs/response_api)**\\n    - Fixed `stream=True` + `background=True` with Responses API - [PR #13654](https://github.com/BerriAI/litellm/pull/13654)\\n\\n---\\n\\n## [MCP Gateway](../../docs/mcp)\\n\\n#### Features\\n\\n- **Access Control & Configuration**\\n    - Enhanced MCPServerManager with access groups and description support - [PR #13549](https://github.com/BerriAI/litellm/pull/13549)\\n\\n#### Bugs\\n\\n- **Authentication**\\n    - Fixed MCP gateway key authentication - [PR #13630](https://github.com/BerriAI/litellm/pull/13630)\\n\\n[Read More](../../docs/mcp)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n\\n- **Team Management**\\n    - Team Member Rate Limits implementation - [PR #13601](https://github.com/BerriAI/litellm/pull/13601)\\n    - JWT authentication support for team member rate limits - [PR #13601](https://github.com/BerriAI/litellm/pull/13601)\\n    - Show team member TPM/RPM limits in UI - [PR #13662](https://github.com/BerriAI/litellm/pull/13662)\\n    - Allow editing team member RPM/TPM limits - [PR #13669](https://github.com/BerriAI/litellm/pull/13669)\\n    - Allow unsetting TPM and RPM in Teams Settings - [PR #13430](https://github.com/BerriAI/litellm/pull/13430)\\n    - Team Member Permissions Page access column changes - [PR #13145](https://github.com/BerriAI/litellm/pull/13145)\\n- **Key Management**\\n    - Display errors from backend on the UI Keys page - [PR #13435](https://github.com/BerriAI/litellm/pull/13435)\\n    - Added confirmation modal before deleting keys - [PR #13655](https://github.com/BerriAI/litellm/pull/13655)\\n    - Support for `user` parameter in LiteLLM SDK to Proxy communication - [PR #13555](https://github.com/BerriAI/litellm/pull/13555)\\n- **UI Improvements**\\n    - Fixed internal users table overflow - [PR #12736](https://github.com/BerriAI/litellm/pull/12736)\\n    - Enhanced chart readability with short-form notation for large numbers - [PR #12370](https://github.com/BerriAI/litellm/pull/12370)\\n    - Fixed image overflow in LiteLLM model display - [PR #13639](https://github.com/BerriAI/litellm/pull/13639)\\n    - Removed ambiguous network response errors - [PR #13582](https://github.com/BerriAI/litellm/pull/13582)\\n- **Credentials**\\n    - Added CredentialDeleteModal component and integration with CredentialsPanel - [PR #13550](https://github.com/BerriAI/litellm/pull/13550)\\n- **Admin & Permissions**\\n    - Allow routes for admin viewer - [PR #13588](https://github.com/BerriAI/litellm/pull/13588)\\n\\n#### Bugs\\n\\n- **SCIM Integration**\\n    - Fixed SCIM Team Memberships metadata handling - [PR #13553](https://github.com/BerriAI/litellm/pull/13553)\\n- **Authentication**\\n    - Fixed incorrect key info endpoint - [PR #13633](https://github.com/BerriAI/litellm/pull/13633)\\n\\n---\\n\\n## Logging / Guardrail Integrations\\n\\n#### Features\\n\\n- **[Langfuse OTEL](../../docs/proxy/logging#langfuse)**\\n    - Added key/team logging for Langfuse OTEL Logger - [PR #13512](https://github.com/BerriAI/litellm/pull/13512)\\n    - Fixed LangfuseOtelSpanAttributes constants to match expected values - [PR #13659](https://github.com/BerriAI/litellm/pull/13659)\\n- **[MLflow](../../docs/proxy/logging#mlflow)**\\n    - Updated MLflow logger usage span attributes - [PR #13561](https://github.com/BerriAI/litellm/pull/13561)\\n\\n#### Bugs\\n\\n- **Security**\\n    - Hide sensitive data in `/model/info` - azure entra client_secret - [PR #13577](https://github.com/BerriAI/litellm/pull/13577)\\n    - Fixed trivy/secrets false positives - [PR #13631](https://github.com/BerriAI/litellm/pull/13631)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n#### Features\\n\\n- **HTTP Performance**\\n    - New \'EXPERIMENTAL_OPENAI_BASE_LLM_HTTP_HANDLER\' flag for +100 RPS improvement on OpenAI calls - [PR #13625](https://github.com/BerriAI/litellm/pull/13625)\\n- **Database Monitoring**\\n    - Added DB metrics to Prometheus - [PR #13626](https://github.com/BerriAI/litellm/pull/13626)\\n- **Error Handling**\\n    - Added safe divide by 0 protection to prevent crashes - [PR #13624](https://github.com/BerriAI/litellm/pull/13624)\\n\\n#### Bugs\\n\\n- **Dependencies**\\n    - Updated boto3 to 1.36.0 and aioboto3 to 13.4.0 - [PR #13665](https://github.com/BerriAI/litellm/pull/13665)\\n\\n---\\n\\n## General Proxy Improvements\\n\\n#### Features\\n\\n- **Database**\\n    - Removed redundant `use_prisma_migrate` flag - now default - [PR #13555](https://github.com/BerriAI/litellm/pull/13555)\\n- **LLM Translation**\\n    - Added model ID check - [PR #13507](https://github.com/BerriAI/litellm/pull/13507)\\n    - Refactored Anthropic configurations and added support for `anthropic_beta` headers - [PR #13590](https://github.com/BerriAI/litellm/pull/13590)\\n\\n\\n---\\n\\n## New Contributors\\n* @TensorNull made their first contribution in [PR #13458](https://github.com/BerriAI/litellm/pull/13458)\\n* @MajorD00m made their first contribution in [PR #13577](https://github.com/BerriAI/litellm/pull/13577)\\n* @VerunicaM made their first contribution in [PR #13584](https://github.com/BerriAI/litellm/pull/13584)\\n* @huangyafei made their first contribution in [PR #13607](https://github.com/BerriAI/litellm/pull/13607)\\n* @TomeHirata made their first contribution in [PR #13561](https://github.com/BerriAI/litellm/pull/13561)\\n* @willfinnigan made their first contribution in [PR #13659](https://github.com/BerriAI/litellm/pull/13659)\\n* @dcbark01 made their first contribution in [PR #13633](https://github.com/BerriAI/litellm/pull/13633)\\n* @javacruft made their first contribution in [PR #13631](https://github.com/BerriAI/litellm/pull/13631)\\n\\n---\\n\\n## **[Full Changelog](https://github.com/BerriAI/litellm/compare/v1.75.5-stable.rc-draft...v1.75.8-nightly)**"},{"id":"v1-75-5","metadata":{"permalink":"/release_notes/v1-75-5","source":"@site/release_notes/v1.75.5-stable/index.md","title":"v1.75.5-stable - Redis latency improvements","description":"Deploy this version","date":"2025-08-10T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.75.5-stable - Redis latency improvements","slug":"v1-75-5","date":"2025-08-10T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.75.8-stable - Team Member Rate Limits","permalink":"/release_notes/v1-75-8"},"nextItem":{"title":"v1.74.15-stable","permalink":"/release_notes/v1-74-15"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.75.5-stable\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.75.5.post2\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights\\n\\n- **Redis - Latency Improvements** - Reduces P99 latency by 50% with Redis enabled. \\n- **Responses API Session Management** - Support for managing responses API sessions with images.\\n- **Oracle Cloud Infrastructure** - New LLM provider for calling models on Oracle Cloud Infrastructure.\\n- **Digital Ocean\'s Gradient AI** - New LLM provider for calling models on Digital Ocean\'s Gradient AI platform.\\n\\n---\\n\\n### Risk of Upgrade\\n\\nIf you build the proxy from the pip package, you should hold off on upgrading. This version makes `prisma migrate deploy` our default for managing the DB. This is safer, as it doesn\'t reset the DB, but it requires a manual `prisma generate` step. \\n\\nUsers of our Docker image, are **not** affected by this change. \\n\\n---\\n\\n## Redis Latency Improvements\\n\\n<Image \\n  img={require(\'../../img/release_notes/faster_caching_calls.png\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n\\n<br/>\\n\\nThis release adds in-memory caching for Redis requests, enabling faster response times in high-traffic. Now, LiteLLM instances will check their in-memory cache for a cache hit, before checking Redis. This reduces caching-related latency from 100ms for LLM API calls to sub-1ms, on cache hits. \\n\\n---\\n\\n## Responses API Session Management w/ Images\\n\\n<Image \\n  img={require(\'../../img/release_notes/responses_api_session_mgt_images.jpg\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n\\n<br/>\\n\\nLiteLLM now supports session management for Responses API requests with images. This is great for use-cases like chatbots, that are using the Responses API to track the state of a conversation. LiteLLM session management works across **ALL** LLM API\'s (including Anthropic, Bedrock, OpenAI, etc). LiteLLM session management works by storing the request and response content in an s3 bucket, you can specify. \\n\\n---\\n\\n\\n## New Models / Updated Models\\n\\n#### New Model Support\\n\\n| Provider    | Model                                  | Context Window | Input ($/1M tokens) | Output ($/1M tokens) |\\n| ----------- | -------------------------------------- | -------------- | ------------------- | -------------------- | \\n| Bedrock | `bedrock/us.anthropic.claude-opus-4-1-20250805-v1:0` | 200k | $15 | $75 |\\n| Bedrock | `bedrock/openai.gpt-oss-20b-1:0` | 200k | 0.07 | 0.3 |\\n| Bedrock | `bedrock/openai.gpt-oss-120b-1:0` | 200k | 0.15 | 0.6 |\\n| Fireworks AI | `fireworks_ai/accounts/fireworks/models/glm-4p5` | 128k | 0.55 | 2.19 |\\n| Fireworks AI | `fireworks_ai/accounts/fireworks/models/glm-4p5-air` | 128k | 0.22 | 0.88 |\\n| Fireworks AI | `fireworks_ai/accounts/fireworks/models/gpt-oss-120b` | 131072 | 0.15 | 0.6 |\\n| Fireworks AI | `fireworks_ai/accounts/fireworks/models/gpt-oss-20b` | 131072 | 0.05 | 0.2 |\\n| Groq | `groq/openai/gpt-oss-20b` | 131072 | 0.1 | 0.5 |\\n| Groq | `groq/openai/gpt-oss-120b` | 131072 | 0.15 | 0.75 |\\n| OpenAI | `openai/gpt-5` | 400k | 1.25 | 10 | \\n| OpenAI | `openai/gpt-5-2025-08-07` | 400k | 1.25 | 10 | \\n| OpenAI | `openai/gpt-5-mini` | 400k | 0.25 | 2 |\\n| OpenAI | `openai/gpt-5-mini-2025-08-07` | 400k | 0.25 | 2 | \\n| OpenAI | `openai/gpt-5-nano` | 400k | 0.05 | 0.4 | \\n| OpenAI | `openai/gpt-5-nano-2025-08-07` | 400k | 0.05 | 0.4 | \\n| OpenAI | `openai/gpt-5-chat` | 400k | 1.25 | 10 | \\n| OpenAI | `openai/gpt-5-chat-latest` | 400k | 1.25 | 10 | \\n| Azure | `azure/gpt-5` | 400k | 1.25 | 10 | \\n| Azure | `azure/gpt-5-2025-08-07` | 400k | 1.25 | 10 | \\n| Azure | `azure/gpt-5-mini` | 400k | 0.25 | 2 | \\n| Azure | `azure/gpt-5-mini-2025-08-07` | 400k | 0.25 | 2 | \\n| Azure | `azure/gpt-5-nano-2025-08-07` | 400k | 0.05 | 0.4 | \\n| Azure | `azure/gpt-5-nano` | 400k | 0.05 | 0.4 | \\n| Azure | `azure/gpt-5-chat` | 400k | 1.25 | 10 | \\n| Azure | `azure/gpt-5-chat-latest` | 400k | 1.25 | 10 | \\n\\n#### Features\\n\\n- **[OCI](../../docs/providers/oci)**\\n    - New LLM provider - [PR #13206](https://github.com/BerriAI/litellm/pull/13206)\\n- **[JinaAI](../../docs/providers/jina_ai)**\\n    - support multimodal embedding models - [PR #13181](https://github.com/BerriAI/litellm/pull/13181)\\n- **GPT-5 ([OpenAI](../../docs/providers/openai)/[Azure](../../docs/providers/azure))**\\n    - Support drop_params for temperature - [PR #13390](https://github.com/BerriAI/litellm/pull/13390)\\n    - Map max_tokens to max_completion_tokens - [PR #13390](https://github.com/BerriAI/litellm/pull/13390)\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Add claude-opus-4-1 on model cost map - [PR #13384](https://github.com/BerriAI/litellm/pull/13384)\\n- **[OpenRouter](../../docs/providers/openrouter)**\\n    - Add gpt-oss to model cost map - [PR #13442](https://github.com/BerriAI/litellm/pull/13442)\\n- **[Cerebras](../../docs/providers/cerebras)**\\n    - Add gpt-oss to model cost map - [PR #13442](https://github.com/BerriAI/litellm/pull/13442)\\n- **[Azure](../../docs/providers/azure)**\\n    - Support drop params for \u2018temperature\u2019 on o-series models - [PR #13353](https://github.com/BerriAI/litellm/pull/13353)\\n- **[GradientAI](../../docs/providers/gradient_ai)**\\n    - New LLM Provider - [PR #12169](https://github.com/BerriAI/litellm/pull/12169)\\n\\n#### Bugs\\n\\n- **[OpenAI](../../docs/providers/openai)**\\n    - Add \u2018service_tier\u2019 and \u2018safety_identifier\u2019 as supported responses api params - [PR #13258](https://github.com/BerriAI/litellm/pull/13258)\\n    - Correct pricing for web search on 4o-mini - [PR #13269](https://github.com/BerriAI/litellm/pull/13269)\\n- **[Mistral](../../docs/providers/mistral)**\\n    - Handle $id and $schema fields when calling mistral - [PR #13389](https://github.com/BerriAI/litellm/pull/13389)\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n\\n- `/responses` \\n    - Responses API Session Handling w/ support for images - [PR #13347](https://github.com/BerriAI/litellm/pull/13347)\\n    - failed if input containing ResponseReasoningItem - [PR #13465](https://github.com/BerriAI/litellm/pull/13465)\\n    - Support custom tools - [PR #13418](https://github.com/BerriAI/litellm/pull/13418)\\n\\n#### Bugs\\n\\n- `/chat/completions` \\n    - Fix completion_token_details usage object missing \u2018text\u2019 tokens - [PR #13234](https://github.com/BerriAI/litellm/pull/13234)\\n    - (SDK) handle tool being a pydantic object - [PR #13274](https://github.com/BerriAI/litellm/pull/13274)\\n    - include cost in streaming usage object - [PR #13418](https://github.com/BerriAI/litellm/pull/13418)\\n    - Exclude none fields on\xa0/chat/completion - allows usage with n8n - [PR #13320](https://github.com/BerriAI/litellm/pull/13320)\\n- `/responses` \\n    - Transform function call in response for non-openai models (gemini/anthropic) - [PR #13260](https://github.com/BerriAI/litellm/pull/13260)\\n    - Fix unsupported operand error with model groups - [PR #13293](https://github.com/BerriAI/litellm/pull/13293)\\n    - Responses api session management for streaming responses - [PR #13396](https://github.com/BerriAI/litellm/pull/13396)\\n- `/v1/messages`\\n    - Added litellm claude code count tokens - [PR #13261](https://github.com/BerriAI/litellm/pull/13261)\\n- `/vector_stores`\\n    - Fix create/search vector store errors - [PR #13285](https://github.com/BerriAI/litellm/pull/13285)\\n---\\n\\n## [MCP Gateway](../../docs/mcp)\\n\\n#### Features\\n\\n- Add route check for internal users - [PR #13350](https://github.com/BerriAI/litellm/pull/13350)\\n- MCP Guardrails - docs - [PR #13392](https://github.com/BerriAI/litellm/pull/13392)\\n\\n\\n#### Bugs\\n\\n- Fix auth on UI for bearer token servers - [PR #13312](https://github.com/BerriAI/litellm/pull/13312)\\n- allow access group on mcp tool retrieval - [PR #13425](https://github.com/BerriAI/litellm/pull/13425)\\n\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n\\n- **Teams**\\n    - Add team deletion check for teams with keys - [PR #12953](https://github.com/BerriAI/litellm/pull/12953)\\n- **Models**\\n    - Add ability to set model alias per key/team - [PR #13276](https://github.com/BerriAI/litellm/pull/13276)\\n    - New button to reload model pricing from model cost map - [PR #13464](https://github.com/BerriAI/litellm/pull/13464), [PR #13470](https://github.com/BerriAI/litellm/pull/13470)\\n- **Keys**\\n    - Make \u2018team\u2019 field required when creating service account keys - [PR #13302](https://github.com/BerriAI/litellm/pull/13302)\\n    - Gray out key-based logging settings for non-enterprise users - prevents confusion on if \u2018logging\u2019 all up is supported - [PR #13431](https://github.com/BerriAI/litellm/pull/13431)\\n- **Navbar**\\n    - Add logo customization for LiteLLM admin UI - [PR #12958](https://github.com/BerriAI/litellm/pull/12958)\\n- **Logs**\\n    - Add token breakdowns on logs + session page - [PR #13357](https://github.com/BerriAI/litellm/pull/13357)\\n- **Usage**\\n    - Ensure Usage Page loads after the DB has large entries - [PR #13400](https://github.com/BerriAI/litellm/pull/13400)\\n- **Test Key Page**\\n    - allow uploading images for /chat/completions and /responses - [PR #13445](https://github.com/BerriAI/litellm/pull/13445)\\n- **MCP**\\n    - Add auth tokens to local storage auth - [PR #13473](https://github.com/BerriAI/litellm/pull/13473)\\n\\n#### Bugs\\n\\n- **Custom Root Path**\\n    - Fix login route when SSO is enabled - [PR #13267](https://github.com/BerriAI/litellm/pull/13267)\\n- **Customers/End-users**\\n    - Allow calling\xa0/v1/models\xa0when end user over budget - allows model listing to work on OpenWebUI when customer over budget - [PR #13320](https://github.com/BerriAI/litellm/pull/13320)\\n- **Teams**\\n    - Remove user - team membership, when user removed from team - [PR #13433](https://github.com/BerriAI/litellm/pull/13433)\\n- **Errors**\\n    - Bubble up network errors to user for Logging and Alerts page - [PR #13427](https://github.com/BerriAI/litellm/pull/13427)\\n- **Model Hub**\\n    - Show pricing for azure models, when base model is set - [PR #13418](https://github.com/BerriAI/litellm/pull/13418)\\n---\\n\\n## Logging / Guardrail Integrations\\n\\n#### Features\\n\\n- **Bedrock Guardrails**\\n    - Redacted sensitive information in bedrock guardrails error message - [PR #13356](https://github.com/BerriAI/litellm/pull/13356)\\n- **Standard Logging Payload**\\n    - Fix \u2018can\u2019t register atextexit\u2019 bug - [PR #13436](https://github.com/BerriAI/litellm/pull/13436)\\n\\n#### Bugs\\n\\n- **Braintrust**\\n    - Allow setting of braintrust callback base url - [PR #13368](https://github.com/BerriAI/litellm/pull/13368)\\n- **OTEL**\\n    - Track pre_call hook latency  - [PR #13362](https://github.com/BerriAI/litellm/pull/13362)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n#### Features\\n\\n- **Team-BYOK models**\\n    - Add wildcard model support - [PR #13278](https://github.com/BerriAI/litellm/pull/13278)\\n- **Caching**\\n    - GCP IAM auth support for caching - [PR #13275](https://github.com/BerriAI/litellm/pull/13275)\\n- **Latency**\\n    - reduce p99 latency w/ redis enabled by 50% - only updates model usage if tpm/rpm limits set - [PR #13362](https://github.com/BerriAI/litellm/pull/13362)\\n\\n---\\n\\n## General Proxy Improvements\\n\\n#### Features\\n\\n- **Models**\\n    - Support /v1/models/\\\\{model_id\\\\} retrieval - [PR #13268](https://github.com/BerriAI/litellm/pull/13268)\\n- **Multi-instance**\\n    - Ensure disable_llm_api_endpoints works - [PR #13278](https://github.com/BerriAI/litellm/pull/13278)\\n- **Logs**\\n    - Add apscheduler log suppress - [PR #13299](https://github.com/BerriAI/litellm/pull/13299)\\n- **Helm**\\n    - Add labels to migrations job template - [PR #13343](https://github.com/BerriAI/litellm/pull/13343) s/o [@unique-jakub](https://github.com/unique-jakub)\\n\\n#### Bugs\\n\\n- **Non-root image**\\n    - Fix non-root image for migration - [PR #13379](https://github.com/BerriAI/litellm/pull/13379)\\n- **Get Routes**\\n    - Load get routes when using fastapi-offline - [PR #13466](https://github.com/BerriAI/litellm/pull/13466)\\n- **Health checks**\\n    - Generate unique trace IDs for Langfuse health checks - [PR #13468](https://github.com/BerriAI/litellm/pull/13468)\\n- **Swagger**\\n    - Allow using Swagger for /chat/completions - [PR #13469](https://github.com/BerriAI/litellm/pull/13469)\\n- **Auth**\\n    - Fix JWTs access not working with model access groups - [PR #13474](https://github.com/BerriAI/litellm/pull/13474)\\n    \\n---\\n\\n## New Contributors\\n\\n* @bbartels made their first contribution in https://github.com/BerriAI/litellm/pull/13244\\n* @breno-aumo made their first contribution in https://github.com/BerriAI/litellm/pull/13206\\n* @pascalwhoop made their first contribution in https://github.com/BerriAI/litellm/pull/13122\\n* @ZPerling made their first contribution in https://github.com/BerriAI/litellm/pull/13045\\n* @zjx20 made their first contribution in https://github.com/BerriAI/litellm/pull/13181\\n* @edwarddamato made their first contribution in https://github.com/BerriAI/litellm/pull/13368\\n* @msannan2 made their first contribution in https://github.com/BerriAI/litellm/pull/12169\\n\\n\\n## **[Full Changelog](https://github.com/BerriAI/litellm/compare/v1.74.15-stable...v1.75.5-stable.rc-draft)**"},{"id":"v1-74-15","metadata":{"permalink":"/release_notes/v1-74-15","source":"@site/release_notes/v1.74.15-stable/index.md","title":"v1.74.15-stable","description":"Deploy this version","date":"2025-08-02T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.74.15-stable","slug":"v1-74-15","date":"2025-08-02T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.75.5-stable - Redis latency improvements","permalink":"/release_notes/v1-75-5"},"nextItem":{"title":"v1.74.9-stable - Auto-Router","permalink":"/release_notes/v1-74-9"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.74.15-stable\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.74.15.post2\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights\\n\\n- **User Agent Activity Tracking** - Track how much usage each coding tool gets.\\n- **Prompt Management** - Use Git-Ops style prompt management with prompt templates.\\n- **MCP Gateway: Guardrails** - Support for using Guardrails with MCP servers.\\n- **Google AI Studio Imagen4** - Support for using Imagen4 models on Google AI Studio.\\n\\n---\\n\\n## User Agent Activity Tracking\\n\\n<Image \\n  img={require(\'../../img/agent_1.png\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n\\n<br/>\\n\\nThis release brings support for tracking usage and costs for AI-powered coding tools like Claude Code, Roo Code, Gemini CLI through LiteLLM. You can now track LLM cost, total tokens used, and DAU/WAU/MAU for each coding tool.\\n\\nThis is great to central AI Platform teams looking to track how they are helping developer productivity. \\n\\n[Read More](https://docs.litellm.ai/docs/tutorials/cost_tracking_coding)\\n\\n---\\n\\n## Prompt Management\\n\\n<br/>\\n\\n\\n\\n[Read More](../../docs/proxy/prompt_management)\\n\\n---\\n\\n## New Models / Updated Models\\n\\n#### New Model Support\\n\\n| Provider    | Model                                  | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Cost per Image |\\n| ----------- | -------------------------------------- | -------------- | ------------------- | -------------------- | -------------- |\\n| OpenRouter | `openrouter/x-ai/grok-4` | 256k | $3 | $15 | N/A |\\n| Google AI Studio | `gemini/imagen-4.0-generate-001` | N/A | N/A | N/A | $0.04 |\\n| Google AI Studio | `gemini/imagen-4.0-ultra-generate-001` | N/A | N/A | N/A | $0.06 |\\n| Google AI Studio | `gemini/imagen-4.0-fast-generate-001` | N/A | N/A | N/A | $0.02 |\\n| Google AI Studio | `gemini/imagen-3.0-generate-002` | N/A | N/A | N/A | $0.04 |\\n| Google AI Studio | `gemini/imagen-3.0-generate-001` | N/A | N/A | N/A | $0.04 |\\n| Google AI Studio | `gemini/imagen-3.0-fast-generate-001` | N/A | N/A | N/A | $0.02 |\\n\\n#### Features\\n\\n- **[Google AI Studio](../../docs/providers/gemini)**\\n    - Added Google AI Studio Imagen4 model family support - [PR #13065](https://github.com/BerriAI/litellm/pull/13065), [Get Started](../../docs/providers/google_ai_studio/image_gen)\\n- **[Azure OpenAI](../../docs/providers/azure/azure)**\\n    - Azure `api_version=\\"preview\\"` support - [PR #13072](https://github.com/BerriAI/litellm/pull/13072), [Get Started](../../docs/providers/azure/azure#setting-api-version)\\n    - Password protected certificate files support - [PR #12995](https://github.com/BerriAI/litellm/pull/12995), [Get Started](../../docs/providers/azure/azure#authentication)\\n- **[AWS Bedrock](../../docs/providers/bedrock)**\\n    - Cost tracking via Anthropic `/v1/messages` - [PR #13072](https://github.com/BerriAI/litellm/pull/13072)\\n    - Computer use support - [PR #13150](https://github.com/BerriAI/litellm/pull/13150)\\n- **[OpenRouter](../../docs/providers/openrouter)**\\n    - Added Grok4 model support - [PR #13018](https://github.com/BerriAI/litellm/pull/13018)\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Auto Cache Control Injection - Improved cache_control_injection_points with negative index support - [PR #13187](https://github.com/BerriAI/litellm/pull/13187), [Get Started](../../docs/tutorials/prompt_caching)\\n    - Working mid-stream fallbacks with token usage tracking - [PR #13149](https://github.com/BerriAI/litellm/pull/13149), [PR #13170](https://github.com/BerriAI/litellm/pull/13170)\\n- **[Perplexity](../../docs/providers/perplexity)**\\n    - Citation annotations support - [PR #13225](https://github.com/BerriAI/litellm/pull/13225)\\n\\n#### Bugs\\n\\n- **[Gemini](../../docs/providers/gemini)**\\n    - Fix merge_reasoning_content_in_choices parameter issue - [PR #13066](https://github.com/BerriAI/litellm/pull/13066), [Get Started](../../docs/tutorials/openweb_ui#render-thinking-content-on-open-webui)\\n    - Added support for using `GOOGLE_API_KEY` environment variable for Google AI Studio - [PR #12507](https://github.com/BerriAI/litellm/pull/12507)\\n- **[vLLM/OpenAI-like](../../docs/providers/vllm)**\\n    - Fix missing extra_headers support for embeddings - [PR #13198](https://github.com/BerriAI/litellm/pull/13198)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Bugs\\n\\n- **[/generateContent](../../docs/generateContent)**\\n    - Support for query_params in generateContent routes for API Key setting - [PR #13100](https://github.com/BerriAI/litellm/pull/13100)\\n    - Ensure \\"x-goog-api-key\\" is used for auth to google ai studio when using /generateContent on LiteLLM - [PR #13098](https://github.com/BerriAI/litellm/pull/13098)\\n    - Ensure tool calling works as expected on generateContent - [PR #13189](https://github.com/BerriAI/litellm/pull/13189)\\n- **[/vertex_ai (Passthrough)](../../docs/pass_through/vertex_ai)**\\n    - Ensure multimodal embedding responses are logged properly - [PR #13050](https://github.com/BerriAI/litellm/pull/13050)\\n\\n---\\n\\n## [MCP Gateway](../../docs/mcp)\\n\\n#### Features\\n\\n- **Health Check Improvements**\\n    - Add health check endpoints for MCP servers - [PR #13106](https://github.com/BerriAI/litellm/pull/13106)\\n- **Guardrails Integration**\\n    - Add pre and during call hooks initialization - [PR #13067](https://github.com/BerriAI/litellm/pull/13067)\\n    - Move pre and during hooks to ProxyLogging - [PR #13109](https://github.com/BerriAI/litellm/pull/13109)\\n    - MCP pre and during guardrails implementation - [PR #13188](https://github.com/BerriAI/litellm/pull/13188)\\n- **Protocol & Header Support**\\n    - Add protocol headers support - [PR #13062](https://github.com/BerriAI/litellm/pull/13062)\\n- **URL & Namespacing**\\n    - Improve MCP server URL validation for internal/Kubernetes URLs - [PR #13099](https://github.com/BerriAI/litellm/pull/13099)\\n\\n\\n#### Bugs\\n\\n- **UI**\\n    - Fix scrolling issue with MCP tools - [PR #13015](https://github.com/BerriAI/litellm/pull/13015)\\n    - Fix MCP client list failure - [PR #13114](https://github.com/BerriAI/litellm/pull/13114)\\n\\n\\n[Read More](../../docs/mcp)\\n\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n\\n- **Usage Analytics**\\n    - New tab for user agent activity tracking - [PR #13146](https://github.com/BerriAI/litellm/pull/13146)\\n    - Daily usage per user analytics - [PR #13147](https://github.com/BerriAI/litellm/pull/13147)\\n    - Default usage chart date range set to last 7 days - [PR #12917](https://github.com/BerriAI/litellm/pull/12917)\\n    - New advanced date range picker component - [PR #13141](https://github.com/BerriAI/litellm/pull/13141), [PR #13221](https://github.com/BerriAI/litellm/pull/13221)\\n    - Show loader on usage cost charts after date selection - [PR #13113](https://github.com/BerriAI/litellm/pull/13113)\\n- **Models**\\n    - Added Voyage, Jinai, Deepinfra and VolcEngine providers on UI - [PR #13131](https://github.com/BerriAI/litellm/pull/13131)\\n    - Added Sagemaker on UI - [PR #13117](https://github.com/BerriAI/litellm/pull/13117)\\n    - Preserve model order in `/v1/models` and `/model_group/info` endpoints - [PR #13178](https://github.com/BerriAI/litellm/pull/13178)\\n\\n- **Key Management**\\n    - Properly parse JSON options for key generation in UI - [PR #12989](https://github.com/BerriAI/litellm/pull/12989)\\n- **Authentication**\\n    - **JWT Fields**  \\n        - Add dot notation support for all JWT fields - [PR #13013](https://github.com/BerriAI/litellm/pull/13013)\\n\\n#### Bugs\\n\\n- **Permissions**\\n    - Fix object permission for organizations - [PR #13142](https://github.com/BerriAI/litellm/pull/13142)\\n    - Fix list team v2 security check - [PR #13094](https://github.com/BerriAI/litellm/pull/13094)\\n- **Models**\\n    - Fix model reload on model update - [PR #13216](https://github.com/BerriAI/litellm/pull/13216)\\n- **Router Settings**\\n    - Fix displaying models for fallbacks in UI - [PR #13191](https://github.com/BerriAI/litellm/pull/13191)\\n    - Fix wildcard model name handling with custom values - [PR #13116](https://github.com/BerriAI/litellm/pull/13116)\\n    - Fix fallback delete functionality - [PR #12606](https://github.com/BerriAI/litellm/pull/12606)\\n\\n---\\n\\n## Logging / Guardrail Integrations\\n\\n#### Features\\n\\n- **[MLFlow](../../docs/proxy/logging#mlflow)**\\n    - Allow adding tags for MLFlow logging requests - [PR #13108](https://github.com/BerriAI/litellm/pull/13108)\\n- **[Langfuse OTEL](../../docs/proxy/logging#langfuse)**\\n    - Add comprehensive metadata support to Langfuse OpenTelemetry integration - [PR #12956](https://github.com/BerriAI/litellm/pull/12956)\\n- **[Datadog LLM Observability](../../docs/proxy/logging#datadog)**\\n    - Allow redacting message/response content for specific logging integrations - [PR #13158](https://github.com/BerriAI/litellm/pull/13158)\\n\\n#### Bugs\\n\\n- **API Key Logging**\\n    - Fix API Key being logged inappropriately - [PR #12978](https://github.com/BerriAI/litellm/pull/12978)\\n- **MCP Spend Tracking**\\n    - Set default value for MCP namespace tool name in spend table - [PR #12894](https://github.com/BerriAI/litellm/pull/12894)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n#### Features\\n\\n- **Background Health Checks**\\n    - Allow disabling background health checks for specific deployments - [PR #13186](https://github.com/BerriAI/litellm/pull/13186)\\n- **Database Connection Management**\\n    - Ensure stale Prisma clients disconnect DB connections properly - [PR #13140](https://github.com/BerriAI/litellm/pull/13140)\\n- **Jitter Improvements**\\n    - Fix jitter calculation (should be added not multiplied) - [PR #12901](https://github.com/BerriAI/litellm/pull/12901)\\n\\n#### Bugs\\n\\n- **Anthropic Streaming**\\n    - Always use choice index=0 for Anthropic streaming responses - [PR #12666](https://github.com/BerriAI/litellm/pull/12666)\\n- **Custom Auth**\\n    - Bubble up custom exceptions properly - [PR #13093](https://github.com/BerriAI/litellm/pull/13093)\\n- **OTEL with Managed Files**\\n    - Fix using managed files with OTEL integration - [PR #13171](https://github.com/BerriAI/litellm/pull/13171)\\n\\n---\\n\\n## General Proxy Improvements\\n\\n#### Features\\n\\n- **Database Migration**\\n    - Move to use_prisma_migrate by default - [PR #13117](https://github.com/BerriAI/litellm/pull/13117)\\n    - Resolve team-only models on auth checks - [PR #13117](https://github.com/BerriAI/litellm/pull/13117)\\n- **Infrastructure**\\n    - Loosened MCP Python version restrictions - [PR #13102](https://github.com/BerriAI/litellm/pull/13102)\\n    - Migrate build_and_test to CI/CD Postgres DB - [PR #13166](https://github.com/BerriAI/litellm/pull/13166)\\n- **Helm Charts**\\n    - Allow Helm hooks for migration jobs - [PR #13174](https://github.com/BerriAI/litellm/pull/13174)\\n    - Fix Helm migration job schema updates - [PR #12809](https://github.com/BerriAI/litellm/pull/12809)\\n\\n#### Bugs\\n\\n- **Docker**\\n    - Remove obsolete `version` attribute in docker-compose - [PR #13172](https://github.com/BerriAI/litellm/pull/13172)\\n    - Add openssl in runtime stage for non-root Dockerfile - [PR #13168](https://github.com/BerriAI/litellm/pull/13168)\\n- **Database Configuration**\\n    - Fix DB config through environment variables - [PR #13111](https://github.com/BerriAI/litellm/pull/13111)\\n- **Logging**\\n    - Suppress httpx logging - [PR #13217](https://github.com/BerriAI/litellm/pull/13217)\\n- **Token Counting**\\n    - Ignore unsupported keys like prefix in token counter - [PR #11954](https://github.com/BerriAI/litellm/pull/11954)\\n---\\n\\n## New Contributors\\n* @5731la made their first contribution in https://github.com/BerriAI/litellm/pull/12989\\n* @restato made their first contribution in https://github.com/BerriAI/litellm/pull/12980\\n* @strickvl made their first contribution in https://github.com/BerriAI/litellm/pull/12956\\n* @Ne0-1 made their first contribution in https://github.com/BerriAI/litellm/pull/12995\\n* @maxrabin made their first contribution in https://github.com/BerriAI/litellm/pull/13079\\n* @lvuna made their first contribution in https://github.com/BerriAI/litellm/pull/12894\\n* @Maximgitman made their first contribution in https://github.com/BerriAI/litellm/pull/12666\\n* @pathikrit made their first contribution in https://github.com/BerriAI/litellm/pull/12901\\n* @huetterma made their first contribution in https://github.com/BerriAI/litellm/pull/12809\\n* @betterthanbreakfast made their first contribution in https://github.com/BerriAI/litellm/pull/13029\\n* @phosae made their first contribution in https://github.com/BerriAI/litellm/pull/12606\\n* @sahusiddharth made their first contribution in https://github.com/BerriAI/litellm/pull/12507\\n* @Amit-kr26 made their first contribution in https://github.com/BerriAI/litellm/pull/11954\\n* @kowyo made their first contribution in https://github.com/BerriAI/litellm/pull/13172\\n* @AnandKhinvasara made their first contribution in https://github.com/BerriAI/litellm/pull/13187\\n* @unique-jakub made their first contribution in https://github.com/BerriAI/litellm/pull/13174\\n* @tyumentsev4 made their first contribution in https://github.com/BerriAI/litellm/pull/13134\\n* @aayush-malviya-acquia made their first contribution in https://github.com/BerriAI/litellm/pull/12978\\n* @kankute-sameer made their first contribution in https://github.com/BerriAI/litellm/pull/13225\\n* @AlexanderYastrebov made their first contribution in https://github.com/BerriAI/litellm/pull/13178\\n\\n## **[Full Changelog](https://github.com/BerriAI/litellm/compare/v1.74.9-stable...v1.74.15.rc)**"},{"id":"v1-74-9","metadata":{"permalink":"/release_notes/v1-74-9","source":"@site/release_notes/v1.74.9-stable/index.md","title":"v1.74.9-stable - Auto-Router","description":"Deploy this version","date":"2025-07-27T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.74.9-stable - Auto-Router","slug":"v1-74-9","date":"2025-07-27T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.74.15-stable","permalink":"/release_notes/v1-74-15"},"nextItem":{"title":"v1.74.7-stable","permalink":"/release_notes/v1-74-7"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.74.9-stable.patch.1\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.74.9.post2\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights\\n\\n- **Auto-Router** - Automatically route requests to specific models based on request content.\\n- **Model-level Guardrails** - Only run guardrails when specific models are used.\\n- **MCP Header Propagation** - Propagate headers from client to backend MCP.\\n- **New LLM Providers** - Added Bedrock inpainting support and Recraft API image generation  / image edits support.\\n\\n---\\n\\n## Auto-Router\\n\\n<Image img={require(\'../../img/release_notes/auto_router.png\')} />\\n\\n<br/>\\n\\nThis release introduces auto-routing to models based on request content. This means **Proxy Admins** can define a set of keywords that always routes to specific models when **users** opt in to using the auto-router.\\n\\nThis is great for internal use cases where you don\'t want **users** to think about which model to use - for example, use Claude models for coding vs GPT models for generating ad copy.\\n\\n\\n[Read More](../../docs/proxy/auto_routing)\\n\\n---\\n\\n## Model-level Guardrails\\n\\n<Image img={require(\'../../img/release_notes/model_level_guardrails.jpg\')} />\\n\\n<br/>\\n\\nThis release brings model-level guardrails support to your config.yaml + UI. This is great for cases when you have an on-prem and hosted model, and just want to run prevent sending PII to the hosted model.\\n\\n```yaml\\nmodel_list:\\n  - model_name: claude-sonnet-4\\n    litellm_params:\\n      model: anthropic/claude-sonnet-4-20250514\\n      api_key: os.environ/ANTHROPIC_API_KEY\\n      api_base: https://api.anthropic.com/v1\\n      guardrails: [\\"azure-text-moderation\\"] # \ud83d\udc48 KEY CHANGE\\n\\nguardrails:\\n  - guardrail_name: azure-text-moderation\\n    litellm_params:\\n      guardrail: azure/text_moderations\\n      mode: \\"post_call\\" \\n      api_key: os.environ/AZURE_GUARDRAIL_API_KEY\\n      api_base: os.environ/AZURE_GUARDRAIL_API_BASE \\n```\\n\\n\\n[Read More](../../docs/proxy/guardrails/quick_start#model-level-guardrails)\\n\\n---\\n## MCP Header Propagation\\n\\n<Image img={require(\'../../img/release_notes/mcp_header_propogation.png\')} />\\n\\n<br/>\\n\\nv1.74.9-stable allows you to propagate MCP server specific authentication headers via LiteLLM\\n\\n- Allowing users to specify which `header_name` is to be propagated to which `mcp_server` via headers\\n- Allows adding of different deployments of same MCP server type to use different authentication headers\\n\\n\\n[Read More](https://docs.litellm.ai/docs/mcp#new-server-specific-auth-headers-recommended)\\n\\n---\\n## New Models / Updated Models\\n\\n#### Pricing / Context Window Updates\\n\\n| Provider    | Model                                  | Context Window | Input ($/1M tokens) | Output ($/1M tokens) |\\n| ----------- | -------------------------------------- | -------------- | ------------------- | -------------------- |\\n| Fireworks AI | `fireworks/models/kimi-k2-instruct` | 131k | $0.6 | $2.5 | \\n| OpenRouter | `openrouter/qwen/qwen-vl-plus` | 8192 | $0.21 | $0.63 | \\n| OpenRouter | `openrouter/qwen/qwen3-coder` | 8192 | $1 | $5 | \\n| OpenRouter | `openrouter/bytedance/ui-tars-1.5-7b` | 128k | $0.10 | $0.20 | \\n| Groq | `groq/qwen/qwen3-32b` | 131k | $0.29 | $0.59 | \\n| VertexAI | `vertex_ai/meta/llama-3.1-8b-instruct-maas` | 128k | $0.00 | $0.00 | \\n| VertexAI | `vertex_ai/meta/llama-3.1-405b-instruct-maas` | 128k | $5 | $16 | \\n| VertexAI | `vertex_ai/meta/llama-3.2-90b-vision-instruct-maas` | 128k | $0.00 | $0.00 | \\n| Google AI Studio | `gemini/gemini-2.0-flash-live-001` | 1,048,576 | $0.35 | $1.5 | \\n| Google AI Studio | `gemini/gemini-2.5-flash-lite` | 1,048,576 | $0.1 | $0.4 | \\n| VertexAI | `vertex_ai/gemini-2.0-flash-lite-001` | 1,048,576 | $0.35 | $1.5 | \\n| OpenAI | `gpt-4o-realtime-preview-2025-06-03` | 128k | $5 | $20 |\\n\\n#### Features\\n\\n- **[Lambda AI](../../docs/providers/lambda_ai)**\\n    - New LLM API provider - [PR #12817](https://github.com/BerriAI/litellm/pull/12817)\\n- **[Github Copilot](../../docs/providers/github_copilot)**\\n    - Dynamic endpoint support - [PR #12827](https://github.com/BerriAI/litellm/pull/12827)\\n- **[Morph](../../docs/providers/morph)**\\n    - New LLM API provider - [PR #12821](https://github.com/BerriAI/litellm/pull/12821)\\n- **[Groq](../../docs/providers/groq)**\\n    - Remove deprecated groq/qwen-qwq-32b - [PR #12832](https://github.com/BerriAI/litellm/pull/12831)\\n- **[Recraft](../../docs/providers/recraft)**\\n    - New image generation API - [PR #12832](https://github.com/BerriAI/litellm/pull/12832)\\n    - New image edits api - [PR #12874](https://github.com/BerriAI/litellm/pull/12874)\\n- **[Azure OpenAI](../../docs/providers/azure/azure)**\\n    - Support DefaultAzureCredential without hard-coded environment variables - [PR #12841](https://github.com/BerriAI/litellm/pull/12841)\\n- **[Hyperbolic](../../docs/providers/hyperbolic)**\\n    - New LLM API provider - [PR #12826](https://github.com/BerriAI/litellm/pull/12826)\\n- **[OpenAI](../../docs/providers/openai)**\\n    - `/realtime` API - pass through intent query param - [PR #12838](https://github.com/BerriAI/litellm/pull/12838)\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Add inpainting support for Amazon Nova Canvas - [PR #12949](https://github.com/BerriAI/litellm/pull/12949) s/o @[SantoshDhaladhuli](https://github.com/SantoshDhaladhuli)\\n\\n#### Bugs\\n- **Gemini ([Google AI Studio](../../docs/providers/gemini) + [VertexAI](../../docs/providers/vertex))**\\n    - Fix leaking file descriptor error on sync calls - [PR #12824](https://github.com/BerriAI/litellm/pull/12824)\\n- **IBM Watsonx**\\n    - use correct parameter name for tool choice - [PR #9980](https://github.com/BerriAI/litellm/pull/9980)\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Only show \u2018reasoning_effort\u2019 for supported models - [PR #12847](https://github.com/BerriAI/litellm/pull/12847)\\n    - Handle $id and $schema in tool call requests (Anthropic API stopped accepting them) - [PR #12959](https://github.com/BerriAI/litellm/pull/12959)\\n- **[Openrouter](../../docs/providers/openrouter)**\\n    - filter out cache_control flag for non-anthropic models (allows usage with claude code) https://github.com/BerriAI/litellm/pull/12850\\n- **[Gemini](../../docs/providers/gemini)**\\n    - Shorten Gemini tool_call_id for Open AI compatibility - [PR #12941](https://github.com/BerriAI/litellm/pull/12941) s/o @[tonga54](https://github.com/tonga54)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n\\n- **[Passthrough endpoints](../../docs/pass_through/)**\\n    - Make key/user/team cost tracking OSS - [PR #12847](https://github.com/BerriAI/litellm/pull/12847)\\n- **[/v1/models](../../docs/providers/passthrough)**\\n    - Return fallback models as part of api response - [PR #12811](https://github.com/BerriAI/litellm/pull/12811) s/o @[murad-khafizov](https://github.com/murad-khafizov)\\n- **[/vector_stores](../../docs/providers/passthrough)**\\n    - Make permission management OSS - [PR #12990](https://github.com/BerriAI/litellm/pull/12990)\\n\\n#### Bugs\\n1. `/batches`\\n    1. Skip invalid batch during cost tracking check (prev. Would stop all checks) - [PR #12782](https://github.com/BerriAI/litellm/pull/12782)\\n2. `/chat/completions`\\n    1. Fix async retryer on\xa0.acompletion() - [PR #12886](https://github.com/BerriAI/litellm/pull/12886)\\n\\n---\\n\\n## [MCP Gateway](../../docs/mcp)\\n\\n#### Features\\n- **[Permission Management](../../docs/mcp#grouping-mcps-access-groups)**\\n    - Make permission management by key/team OSS - [PR #12988](https://github.com/BerriAI/litellm/pull/12988)\\n- **[MCP Alias](../../docs/mcp#mcp-aliases)**\\n    - Support mcp server aliases (useful for calling long mcp server names on Cursor) - [PR #12994](https://github.com/BerriAI/litellm/pull/12994)\\n- **Header Propagation**\\n    - Support propagating headers from client to backend MCP (useful for sending personal access tokens to backend MCP) - [PR #13003](https://github.com/BerriAI/litellm/pull/13003)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n- **Usage**\\n    - Support viewing usage by model group - [PR #12890](https://github.com/BerriAI/litellm/pull/12890)\\n- **Virtual Keys**\\n    - New `key_type` field on `/key/generate` - allows specifying if key can call LLM API vs. Management routes - [PR #12909](https://github.com/BerriAI/litellm/pull/12909)\\n- **Models**\\n    - Add \u2018auto router\u2019 on UI - [PR #12960](https://github.com/BerriAI/litellm/pull/12960)\\n    - Show global retry policy on UI - [PR #12969](https://github.com/BerriAI/litellm/pull/12969)\\n    - Add model-level guardrails on create + update - [PR #13006](https://github.com/BerriAI/litellm/pull/13006)\\n\\n#### Bugs\\n- **SSO**\\n    - Fix logout when SSO is enabled - [PR #12703](https://github.com/BerriAI/litellm/pull/12703)\\n    - Fix reset SSO when ui_access_mode is updated - [PR #13011](https://github.com/BerriAI/litellm/pull/13011)\\n- **Guardrails**\\n    - Show correct guardrails when editing a team - [PR #12823](https://github.com/BerriAI/litellm/pull/12823)\\n- **Virtual Keys**\\n    - Get updated token on regenerate key - [PR #12788](https://github.com/BerriAI/litellm/pull/12788)\\n    - Fix CVE with key injection - [PR #12840](https://github.com/BerriAI/litellm/pull/12840)\\n---\\n\\n## Logging / Guardrail Integrations\\n\\n#### Features\\n- **[Google Cloud Model Armor](../../docs/proxy/guardrails/model_armor)**\\n    - Document new guardrail - [PR #12492](https://github.com/BerriAI/litellm/pull/12492)\\n- **[Pillar Security](../../docs/proxy/guardrails/pillar_security)**\\n    - New LLM Guardrail - [PR #12791](https://github.com/BerriAI/litellm/pull/12791)\\n- **CloudZero**\\n    - Allow exporting spend to cloudzero - [PR #12908](https://github.com/BerriAI/litellm/pull/12908)\\n- **Model-level Guardrails**\\n    - Support model-level guardrails - [PR #12968](https://github.com/BerriAI/litellm/pull/12968)\\n\\n#### Bugs\\n- **[Prometheus](../../docs/proxy/prometheus)**\\n    - Fix `[tag]=false` when tag is set for tag-based metrics - [PR #12916](https://github.com/BerriAI/litellm/pull/12916)\\n- **[Guardrails AI](../../docs/proxy/guardrails/guardrails_ai)**\\n    - Use \u2018validatedOutput\u2019 to allow usage of \u201cfix\u201d guards - [PR #12891](https://github.com/BerriAI/litellm/pull/12891) s/o @[DmitriyAlergant](https://github.com/DmitriyAlergant)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n#### Features\\n- **[Auto-Router](../../docs/proxy/auto_routing)**\\n    - New auto-router powered by `semantic-router` - [PR #12955](https://github.com/BerriAI/litellm/pull/12955)\\n\\n#### Bugs\\n- **forward_clientside_headers**\\n    - Filter out `content-length` from headers (caused backend requests to hang) - [PR #12886](https://github.com/BerriAI/litellm/pull/12886/files)\\n- **Message Redaction**\\n    - Fix cannot pickle coroutine object error - [PR #13005](https://github.com/BerriAI/litellm/pull/13005)\\n---\\n\\n## General Proxy Improvements\\n\\n#### Features\\n- **Benchmarks**\\n    - Updated litellm proxy benchmarks (p50, p90, p99 overhead) - [PR #12842](https://github.com/BerriAI/litellm/pull/12842)\\n- **Request Headers**\\n    - Added new `x-litellm-num-retries` request header \\n- **Swagger**\\n    - Support local swagger on custom root paths - [PR #12911](https://github.com/BerriAI/litellm/pull/12911)\\n- **Health**\\n    - Track cost + add tags for health checks done by LiteLLM Proxy - [PR #12880](https://github.com/BerriAI/litellm/pull/12880)\\n#### Bugs\\n\\n- **Proxy Startup**\\n    - Fixes issue on startup where team member budget is None would block startup - [PR #12843](https://github.com/BerriAI/litellm/pull/12843)\\n- **Docker**\\n    - Move non-root docker to chain guard image (fewer vulnerabilities) - [PR #12707](https://github.com/BerriAI/litellm/pull/12707)\\n    - add\xa0azure-keyvault==4.2.0\xa0to Docker img - [PR #12873](https://github.com/BerriAI/litellm/pull/12873)\\n- **Separate Health App**\\n    - Pass through cmd args via supervisord (enables user config to still work via docker) - [PR #12871](https://github.com/BerriAI/litellm/pull/12871)\\n- **Swagger**\\n    - Bump DOMPurify version (fixes vulnerability) - [PR #12911](https://github.com/BerriAI/litellm/pull/12911)\\n    - Add back local swagger bundle (enables swagger to work in air gapped env.) - [PR #12911](https://github.com/BerriAI/litellm/pull/12911)\\n- **Request Headers**\\n    - Make \u2018user_header_name\u2019 field check case insensitive (fixes customer budget enforcement for OpenWebUi) - [PR #12950](https://github.com/BerriAI/litellm/pull/12950)\\n- **SpendLogs**\\n    - Fix issues writing to DB when\xa0custom_llm_provider\xa0is None - [PR #13001](https://github.com/BerriAI/litellm/pull/13001)\\n\\n---\\n\\n## New Contributors\\n* @magicalne made their first contribution in https://github.com/BerriAI/litellm/pull/12804\\n* @pavangudiwada made their first contribution in https://github.com/BerriAI/litellm/pull/12798\\n* @mdiloreto made their first contribution in https://github.com/BerriAI/litellm/pull/12707\\n* @murad-khafizov made their first contribution in https://github.com/BerriAI/litellm/pull/12811\\n* @eagle-p made their first contribution in https://github.com/BerriAI/litellm/pull/12791\\n* @apoorv-sharma made their first contribution in https://github.com/BerriAI/litellm/pull/12920\\n* @SantoshDhaladhuli made their first contribution in https://github.com/BerriAI/litellm/pull/12949\\n* @tonga54 made their first contribution in https://github.com/BerriAI/litellm/pull/12941\\n* @sings-to-bees-on-wednesdays made their first contribution in https://github.com/BerriAI/litellm/pull/12950\\n\\n## **[Full Changelog](https://github.com/BerriAI/litellm/compare/v1.74.7-stable...v1.74.9.rc-draft)**"},{"id":"v1-74-7","metadata":{"permalink":"/release_notes/v1-74-7","source":"@site/release_notes/v1.74.7/index.md","title":"v1.74.7-stable","description":"Deploy this version","date":"2025-07-19T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.74.7-stable","slug":"v1-74-7","date":"2025-07-19T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.74.9-stable - Auto-Router","permalink":"/release_notes/v1-74-9"},"nextItem":{"title":"v1.74.3-stable","permalink":"/release_notes/v1-74-3-stable"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.74.7-stable.patch.1\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.74.7.post2\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights \\n\\n\\n- **Vector Stores** - Support for Vertex RAG Engine, PG Vector, OpenAI & Azure OpenAI Vector Stores.\\n- **Bulk Editing Users** - Bulk editing users on the UI.\\n- **Health Check Improvements** - Prevent unnecessary pod restarts during high traffic.\\n- **New LLM Providers** - Added Moonshot AI and Vercel v0 provider support.\\n\\n---\\n\\n## Vector Stores API\\n\\n<Image img={require(\'../../img/release_notes/vector_stores.png\')} />\\n\\n\\nThis release introduces support for using VertexAI RAG Engine, PG Vector, Bedrock Knowledge Bases, and OpenAI Vector Stores with LiteLLM.\\n\\nThis is ideal for use cases requiring external knowledge sources with LLMs.\\n\\nThis brings the following benefits for LiteLLM users:\\n\\n**Proxy Admin Benefits:**\\n- Fine-grained access control: determine which Keys and Teams can access specific Vector Stores\\n- Complete usage tracking and monitoring across all vector store operations\\n\\n**Developer Benefits:**\\n- Simple, unified interface for querying vector stores and using them with LLM API requests\\n- Consistent API experience across all supported vector store providers \\n\\n\\n\\n[Get started](../../docs/completion/knowledgebase)\\n\\n\\n---\\n\\n## Bulk Editing Users\\n\\n<Image img={require(\'../../img/bulk_edit_graphic.png\')} />\\n\\nv1.74.7-stable introduces Bulk Editing Users on the UI. This is useful for:\\n- granting all existing users to a default team (useful for controlling access / tracking spend by team)\\n- controlling personal model access for existing users\\n\\n[Read more](https://docs.litellm.ai/docs/proxy/ui/bulk_edit_users)\\n\\n---\\n\\n## Health Check Server\\n\\n<Image alt=\\"Separate Health App Architecture\\" img={require(\'../../img/separate_health_app_architecture.png\')} style={{ borderRadius: \'8px\', marginBottom: \'1em\', maxWidth: \'100%\' }} />\\n\\nThis release brings reliability improvements that prevent unnecessary pod restarts during high traffic. Previously, when the main LiteLLM app was busy serving traffic, health endpoints would timeout even when pods were healthy. \\n \\nStarting with this release, you can run health endpoints on an isolated process with a dedicated port. This ensures liveness and readiness probes remain responsive even when the main LiteLLM app is under heavy load.\\n\\n[Read More](https://docs.litellm.ai/docs/proxy/prod#10-use-a-separate-health-check-app)\\n\\n\\n---\\n\\n## New Models / Updated Models\\n\\n#### Pricing / Context Window Updates\\n\\n| Provider    | Model                                  | Context Window | Input ($/1M tokens) | Output ($/1M tokens) |\\n| ----------- | -------------------------------------- | -------------- | ------------------- | -------------------- |\\n| Azure AI | `azure_ai/grok-3` | 131k | $3.30 | $16.50 |\\n| Azure AI | `azure_ai/global/grok-3` | 131k | $3.00 | $15.00 |\\n| Azure AI | `azure_ai/global/grok-3-mini` | 131k | $0.25 | $1.27 |\\n| Azure AI | `azure_ai/grok-3-mini` | 131k | $0.275 | $1.38 |\\n| Azure AI | `azure_ai/jais-30b-chat` | 8k | $3200 | $9710 |\\n| Groq | `groq/moonshotai-kimi-k2-instruct` | 131k | $1.00 | $3.00 |\\n| AI21 | `jamba-large-1.7` | 256k | $2.00 | $8.00 |\\n| AI21 | `jamba-mini-1.7` | 256k | $0.20 | $0.40 |\\n| Together.ai | `together_ai/moonshotai/Kimi-K2-Instruct` | 131k | $1.00 | $3.00 |\\n| v0 | `v0/v0-1.0-md` | 128k | $3.00 | $15.00 |\\n| v0 | `v0/v0-1.5-md` | 128k | $3.00 | $15.00 |\\n| v0 | `v0/v0-1.5-lg` | 512k | $15.00 | $75.00 |\\n| Moonshot | `moonshot/moonshot-v1-8k` | 8k | $0.20 | $2.00 |\\n| Moonshot | `moonshot/moonshot-v1-32k` | 32k | $1.00 | $3.00 |\\n| Moonshot | `moonshot/moonshot-v1-128k` | 131k | $2.00 | $5.00 |\\n| Moonshot | `moonshot/moonshot-v1-auto` | 131k | $2.00 | $5.00 |\\n| Moonshot | `moonshot/kimi-k2-0711-preview` | 131k | $0.60 | $2.50 |\\n| Moonshot | `moonshot/moonshot-v1-32k-0430` | 32k | $1.00 | $3.00 |\\n| Moonshot | `moonshot/moonshot-v1-128k-0430` | 131k | $2.00 | $5.00 |\\n| Moonshot | `moonshot/moonshot-v1-8k-0430` | 8k | $0.20 | $2.00 |\\n| Moonshot | `moonshot/kimi-latest` | 131k | $2.00 | $5.00 |\\n| Moonshot | `moonshot/kimi-latest-8k` | 8k | $0.20 | $2.00 |\\n| Moonshot | `moonshot/kimi-latest-32k` | 32k | $1.00 | $3.00 |\\n| Moonshot | `moonshot/kimi-latest-128k` | 131k | $2.00 | $5.00 |\\n| Moonshot | `moonshot/kimi-thinking-preview` | 131k | $30.00 | $30.00 |\\n| Moonshot | `moonshot/moonshot-v1-8k-vision-preview` | 8k | $0.20 | $2.00 |\\n| Moonshot | `moonshot/moonshot-v1-32k-vision-preview` | 32k | $1.00 | $3.00 |\\n| Moonshot | `moonshot/moonshot-v1-128k-vision-preview` | 131k | $2.00 | $5.00 |\\n\\n\\n#### Features\\n\\n- **[\ud83c\udd95 Moonshot API (Kimi)](../../docs/providers/moonshot)**\\n    - New LLM API integration for accessing Kimi models - [PR #12592](https://github.com/BerriAI/litellm/pull/12592), [Get Started](../../docs/providers/moonshot)\\n- **[\ud83c\udd95 v0 Provider](../../docs/providers/v0)**\\n    - New provider integration for v0.dev - [PR #12751](https://github.com/BerriAI/litellm/pull/12751), [Get Started](../../docs/providers/v0)\\n- **[OpenAI](../../docs/providers/openai)**\\n    - Use OpenAI DeepResearch models with `litellm.completion` (`/chat/completions`) - [PR #12627](https://github.com/BerriAI/litellm/pull/12627) **DOC NEEDED**\\n- **[Azure OpenAI](../../docs/providers/azure_openai)**\\n    - Use Azure OpenAI DeepResearch models with `litellm.completion` (`/chat/completions`) - [PR #12627](https://github.com/BerriAI/litellm/pull/12627) **DOC NEEDED**\\n    - Added `response_format` support for openai gpt-4.1 models - [PR #12745](https://github.com/BerriAI/litellm/pull/12745)\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Tool cache control support - [PR #12668](https://github.com/BerriAI/litellm/pull/12668)\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Claude 4 /invoke route support - [PR #12599](https://github.com/BerriAI/litellm/pull/12599), [Get Started](../../docs/providers/bedrock)\\n    - Application inference profile tool choice support - [PR #12599](https://github.com/BerriAI/litellm/pull/12599)\\n- **[Gemini](../../docs/providers/gemini)**\\n    - Custom TTL support for context caching - [PR #12541](https://github.com/BerriAI/litellm/pull/12541)\\n    - Fix implicit caching cost calculation for Gemini 2.x models - [PR #12585](https://github.com/BerriAI/litellm/pull/12585)\\n- **[VertexAI](../../docs/providers/vertex)**\\n    - Added Vertex AI RAG Engine support (use with OpenAI compatible `/vector_stores` API) - [PR #12752](https://github.com/BerriAI/litellm/pull/12595), [Get Started](../../docs/completion/knowledgebase)\\n- **[vLLM](../../docs/providers/vllm)**\\n    - Added support for using Rerank endpoints with vLLM - [PR #12738](https://github.com/BerriAI/litellm/pull/12738), [Get Started](../../docs/providers/vllm#rerank)\\n- **[AI21](../../docs/providers/ai21)**\\n    - Added ai21/jamba-1.7 model family pricing - [PR #12593](https://github.com/BerriAI/litellm/pull/12593), [Get Started](../../docs/providers/ai21)\\n- **[Together.ai](../../docs/providers/together_ai)**\\n    - [New Model] add together_ai/moonshotai/Kimi-K2-Instruct - [PR #12645](https://github.com/BerriAI/litellm/pull/12645), [Get Started](../../docs/providers/together_ai)\\n- **[Groq](../../docs/providers/groq)**\\n    - Add groq/moonshotai-kimi-k2-instruct model configuration - [PR #12648](https://github.com/BerriAI/litellm/pull/12648), [Get Started](../../docs/providers/groq)\\n- **[Github Copilot](../../docs/providers/github_copilot)**\\n    - Change System prompts to assistant prompts for GH Copilot - [PR #12742](https://github.com/BerriAI/litellm/pull/12742), [Get Started](../../docs/providers/github_copilot)\\n\\n\\n#### Bugs\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Fix streaming + response_format + tools bug - [PR #12463](https://github.com/BerriAI/litellm/pull/12463)\\n- **[XAI](../../docs/providers/xai)**\\n    - grok-4 does not support the `stop` param - [PR #12646](https://github.com/BerriAI/litellm/pull/12646)\\n- **[AWS](../../docs/providers/bedrock)**\\n    - Role chaining with web authentication for AWS Bedrock - [PR #12607](https://github.com/BerriAI/litellm/pull/12607)\\n- **[VertexAI](../../docs/providers/vertex)**\\n    - Add project_id to cached credentials - [PR #12661](https://github.com/BerriAI/litellm/pull/12661)\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Fix bedrock nova micro and nova lite context window info in [PR #12619](https://github.com/BerriAI/litellm/pull/12619)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n- **[/chat/completions](../../docs/completion/input)** \\n    - Include tool calls in output of trim_messages - [PR #11517](https://github.com/BerriAI/litellm/pull/11517)\\n- **[/v1/vector_stores](../../docs/vector_stores/search)**\\n    - New OpenAI-compatible vector store endpoints - [PR #12699](https://github.com/BerriAI/litellm/pull/12699), [Get Started](../../docs/vector_stores/search)\\n    - Vector store search endpoint - [PR #12749](https://github.com/BerriAI/litellm/pull/12749), [Get Started](../../docs/vector_stores/search)\\n    - Support for using PG Vector as a vector store - [PR #12667](https://github.com/BerriAI/litellm/pull/12667), [Get Started](../../docs/completion/knowledgebase)\\n- **[/streamGenerateContent](../../docs/generateContent)**\\n    - Non-gemini model support - [PR #12647](https://github.com/BerriAI/litellm/pull/12647)\\n\\n#### Bugs\\n- **[/vector_stores](../../docs/vector_stores/search)**\\n    - Knowledge Base Call returning error when passing as `tools` - [PR #12628](https://github.com/BerriAI/litellm/pull/12628)\\n\\n---\\n\\n## [MCP Gateway](../../docs/mcp)\\n\\n#### Features\\n- **[Access Groups](../../docs/mcp#grouping-mcps-access-groups)**\\n    - Allow MCP access groups to be added via litellm proxy config.yaml - [PR #12654](https://github.com/BerriAI/litellm/pull/12654)\\n    - List tools from access list for keys - [PR #12657](https://github.com/BerriAI/litellm/pull/12657)\\n- **[Namespacing](../../docs/mcp#mcp-namespacing)**\\n    - URL-based namespacing for better segregation - [PR #12658](https://github.com/BerriAI/litellm/pull/12658)\\n    - Make MCP_TOOL_PREFIX_SEPARATOR configurable from env - [PR #12603](https://github.com/BerriAI/litellm/pull/12603)\\n- **[Gateway Features](../../docs/mcp#mcp-gateway-features)**\\n    - Allow using MCPs with all LLM APIs (VertexAI, Gemini, Groq, etc.) when using /responses - [PR #12546](https://github.com/BerriAI/litellm/pull/12546)\\n\\n#### Bugs\\n    - Fix to update object permission on update/delete key/team - [PR #12701](https://github.com/BerriAI/litellm/pull/12701)\\n    - Include /mcp in list of available routes on proxy - [PR #12612](https://github.com/BerriAI/litellm/pull/12612)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n- **Keys**\\n    - Regenerate Key State Management improvements - [PR #12729](https://github.com/BerriAI/litellm/pull/12729)\\n- **Models**\\n    - Wildcard model filter support - [PR #12597](https://github.com/BerriAI/litellm/pull/12597)\\n    - Fixes for handling team only models on UI - [PR #12632](https://github.com/BerriAI/litellm/pull/12632)\\n- **Usage Page**\\n    - Fix Y-axis labels overlap on Spend per Tag chart - [PR #12754](https://github.com/BerriAI/litellm/pull/12754)\\n- **Teams**\\n    - Allow setting custom key duration + show key creation stats - [PR #12722](https://github.com/BerriAI/litellm/pull/12722)\\n    - Enable team admins to update member roles - [PR #12629](https://github.com/BerriAI/litellm/pull/12629)\\n- **Users**\\n    - New `/user/bulk_update` endpoint - [PR #12720](https://github.com/BerriAI/litellm/pull/12720)\\n- **Logs Page**\\n    - Add `end_user` filter on UI Logs Page - [PR #12663](https://github.com/BerriAI/litellm/pull/12663)\\n- **MCP Servers**\\n    - Copy MCP Server name functionality - [PR #12760](https://github.com/BerriAI/litellm/pull/12760)\\n- **Vector Stores**\\n    - UI support for clicking into Vector Stores - [PR #12741](https://github.com/BerriAI/litellm/pull/12741)\\n    - Allow adding Vertex RAG Engine, OpenAI, Azure through UI - [PR #12752](https://github.com/BerriAI/litellm/pull/12752)\\n- **General**\\n    - Add Copy-on-Click for all IDs (Key, Team, Organization, MCP Server) - [PR #12615](https://github.com/BerriAI/litellm/pull/12615)\\n- **[SCIM](../../docs/proxy/scim)**\\n    - Add GET /ServiceProviderConfig endpoint - [PR #12664](https://github.com/BerriAI/litellm/pull/12664)\\n\\n#### Bugs\\n- **Teams**\\n    - Ensure user id correctly added when creating new teams - [PR #12719](https://github.com/BerriAI/litellm/pull/12719)\\n    - Fixes for handling team-only models on UI - [PR #12632](https://github.com/BerriAI/litellm/pull/12632)\\n\\n---\\n\\n## Logging / Guardrail Integrations\\n\\n#### Features\\n- **[Google Cloud Model Armor](../../docs/proxy/guardrails/google_cloud_model_armor)**\\n    - New guardrails integration - [PR #12492](https://github.com/BerriAI/litellm/pull/12492)\\n- **[Bedrock Guardrails](../../docs/proxy/guardrails/bedrock)**\\n    - Allow disabling exception on \'BLOCKED\' action - [PR #12693](https://github.com/BerriAI/litellm/pull/12693)\\n- **[Guardrails AI](../../docs/proxy/guardrails/guardrails_ai)**\\n    - Support `llmOutput` based guardrails as pre-call hooks - [PR #12674](https://github.com/BerriAI/litellm/pull/12674)\\n- **[DataDog LLM Observability](../../docs/proxy/logging#datadog)**\\n    - Add support for tracking the correct span type based on LLM Endpoint used - [PR #12652](https://github.com/BerriAI/litellm/pull/12652)\\n- **[Custom Logging](../../docs/proxy/logging)**\\n    - Allow reading custom logger python scripts from S3 or GCS Bucket - [PR #12623](https://github.com/BerriAI/litellm/pull/12623)\\n\\n#### Bugs\\n- **[General Logging](../../docs/proxy/logging)**\\n    - StandardLoggingPayload on cache_hits should track custom llm provider - [PR #12652](https://github.com/BerriAI/litellm/pull/12652)\\n- **[S3 Buckets](../../docs/proxy/logging#s3-buckets)**\\n    - S3 v2 log uploader crashes when using with guardrails - [PR #12733](https://github.com/BerriAI/litellm/pull/12733)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n#### Features\\n- **Health Checks**\\n    - Separate health app for liveness probes - [PR #12669](https://github.com/BerriAI/litellm/pull/12669)\\n    - Health check app on separate port - [PR #12718](https://github.com/BerriAI/litellm/pull/12718)\\n- **Caching**\\n    - Add Azure Blob cache support - [PR #12587](https://github.com/BerriAI/litellm/pull/12587)\\n- **Router**\\n    - Handle ZeroDivisionError with zero completion tokens in lowest_latency strategy - [PR #12734](https://github.com/BerriAI/litellm/pull/12734)\\n\\n#### Bugs\\n- **Database**\\n    - Use upsert for managed object table to avoid UniqueViolationError - [PR #11795](https://github.com/BerriAI/litellm/pull/11795)\\n    - Refactor to support use_prisma_migrate for helm hook - [PR #12600](https://github.com/BerriAI/litellm/pull/12600)\\n- **Cache**\\n    - Fix: redis caching for embedding response models - [PR #12750](https://github.com/BerriAI/litellm/pull/12750)\\n\\n---\\n\\n## Helm Chart\\n\\n- DB Migration Hook: refactor to support use_prisma_migrate - for helm hook [PR](https://github.com/BerriAI/litellm/pull/12600)\\n- Add envVars and extraEnvVars support to Helm migrations job - [PR #12591](https://github.com/BerriAI/litellm/pull/12591)\\n\\n## General Proxy Improvements\\n\\n#### Features\\n- **Control Plane + Data Plane Architecture**\\n    - Control Plane + Data Plane support - [PR #12601](https://github.com/BerriAI/litellm/pull/12601)\\n- **Proxy CLI**\\n    - Add \\"keys import\\" command to CLI - [PR #12620](https://github.com/BerriAI/litellm/pull/12620)\\n- **Swagger Documentation**\\n    - Add swagger docs for LiteLLM /chat/completions, /embeddings, /responses - [PR #12618](https://github.com/BerriAI/litellm/pull/12618)\\n- **Dependencies**\\n    - Loosen rich version from ==13.7.1 to >=13.7.1 - [PR #12704](https://github.com/BerriAI/litellm/pull/12704)\\n\\n\\n#### Bugs\\n\\n- Verbose log is enabled by default fix - [PR #12596](https://github.com/BerriAI/litellm/pull/12596)\\n\\n- Add support for disabling callbacks in request body - [PR #12762](https://github.com/BerriAI/litellm/pull/12762)\\n- Handle circular references in spend tracking metadata JSON serialization - [PR #12643](https://github.com/BerriAI/litellm/pull/12643)\\n\\n---\\n\\n## New Contributors\\n* @AntonioKL made their first contribution in https://github.com/BerriAI/litellm/pull/12591\\n* @marcelodiaz558 made their first contribution in https://github.com/BerriAI/litellm/pull/12541\\n* @dmcaulay made their first contribution in https://github.com/BerriAI/litellm/pull/12463\\n* @demoray made their first contribution in https://github.com/BerriAI/litellm/pull/12587\\n* @staeiou made their first contribution in https://github.com/BerriAI/litellm/pull/12631\\n* @stefanc-ai2 made their first contribution in https://github.com/BerriAI/litellm/pull/12622\\n* @RichardoC made their first contribution in https://github.com/BerriAI/litellm/pull/12607\\n* @yeahyung made their first contribution in https://github.com/BerriAI/litellm/pull/11795\\n* @mnguyen96 made their first contribution in https://github.com/BerriAI/litellm/pull/12619\\n* @rgambee made their first contribution in https://github.com/BerriAI/litellm/pull/11517\\n* @jvanmelckebeke made their first contribution in https://github.com/BerriAI/litellm/pull/12725\\n* @jlaurendi made their first contribution in https://github.com/BerriAI/litellm/pull/12704\\n* @doublerr made their first contribution in https://github.com/BerriAI/litellm/pull/12661\\n\\n## **[Full Changelog](https://github.com/BerriAI/litellm/compare/v1.74.3-stable...v1.74.7-stable)**"},{"id":"v1-74-3-stable","metadata":{"permalink":"/release_notes/v1-74-3-stable","source":"@site/release_notes/v1.74.3-stable/index.md","title":"v1.74.3-stable","description":"Deploy this version","date":"2025-07-12T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.74.3-stable","slug":"v1-74-3-stable","date":"2025-07-12T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.74.7-stable","permalink":"/release_notes/v1-74-7"},"nextItem":{"title":"v1.74.0-stable","permalink":"/release_notes/v1-74-0-stable"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.74.3-stable\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.74.3.post1\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights \\n\\n- **MCP: Model Access Groups** - Add mcp servers to access groups, for easily managing access to users and teams.\\n- **MCP: Tool Cost Tracking** - Set prices for each MCP tool. \\n- **Model Hub v2** - New OSS Model Hub for telling developers what models are available on the proxy.\\n- **Bytez** - New LLM API Provider.\\n- **Dashscope API** - Call Alibaba\'s qwen models via new Dashscope API Provider.\\n\\n---\\n\\n## MCP Gateway: Model Access Groups\\n\\n<Image \\n  img={require(\'../../img/release_notes/mcp_access_groups.png\')}\\n  style={{width: \'80%\', display: \'block\', margin: \'0\'}}\\n/>\\n\\n<br/>\\n\\nv1.74.3-stable adds support for adding MCP servers to access groups, this makes it **easier for Proxy Admins** to manage access to MCP servers across users and teams.\\n\\nFor **developers**, this means you can now connect to multiple MCP servers by passing the access group name in the `x-mcp-servers` header.\\n\\nRead more [here](https://docs.litellm.ai/docs/mcp#grouping-mcps-access-groups)\\n\\n---\\n\\n## MCP Gateway: Tool Cost Tracking\\n\\n<Image \\n  img={require(\'../../img/release_notes/mcp_tool_cost_tracking.png\')}\\n  style={{width: \'80%\', display: \'block\', margin: \'0\'}}\\n/>\\n\\n<br/>\\n\\nThis release adds cost tracking for MCP tool calls. This is great for **Proxy Admins** giving MCP access to developers as you can now attribute MCP tool call costs to specific LiteLLM keys and teams.\\n\\nYou can set:\\n- **Uniform server cost**: Set a uniform cost for all tools from a server\\n- **Individual tool cost**: Define individual costs for specific tools (e.g., search_tool costs $10, get_weather costs $5).\\n- **Dynamic costs**: For use cases where you want to set costs based on the MCP\'s response, you can write a custom post mcp call hook to parse responses and set costs dynamically.\\n\\n[Get started](https://docs.litellm.ai/docs/mcp#mcp-cost-tracking)\\n\\n---\\n\\n## Model Hub v2\\n\\n<Image \\n  img={require(\'../../img/release_notes/model_hub_v2.png\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n\\n<br/>\\n\\nv1.74.3-stable introduces a new OSS Model Hub for telling developers what models are available on the proxy.\\n\\nThis is great for **Proxy Admins** as you can now tell developers what models are available on the proxy.\\n\\nThis improves on the previous model hub by enabling:\\n- The ability to show **Developers** models, even if they don\'t have a LiteLLM key.\\n- The ability for **Proxy Admins** to select specific models to be public on the model hub.\\n- Improved search and filtering capabilities:\\n    - search for models by partial name (e.g. `xai grok-4`)\\n    - filter by provider and feature (e.g. \'vision\' models)\\n    - sort by cost (e.g. cheapest vision model from OpenAI)\\n\\n[Get started](../../docs/proxy/model_hub)\\n\\n---\\n\\n\\n## New Models / Updated Models\\n\\n#### Pricing / Context Window Updates\\n\\n| Provider    | Model                                  | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Type |\\n| ----------- | -------------------------------------- | -------------- | ------------------- | -------------------- | ---- |\\n| Xai | `xai/grok-4` | 256k | $3.00 | $15.00 | New |\\n| Xai | `xai/grok-4-0709` | 256k | $3.00 | $15.00 | New |\\n| Xai | `xai/grok-4-latest` | 256k | $3.00 | $15.00 | New |\\n| Mistral | `mistral/devstral-small-2507` | 128k | $0.1 | $0.3 | New |\\n| Mistral | `mistral/devstral-medium-2507` | 128k | $0.4 | $2 | New |\\n| Azure OpenAI | `azure/o3-deep-research` | 200k | $10 | $40 | New |\\n\\n\\n#### Features\\n- **[Xinference](../../docs/providers/xinference)**\\n    - Image generation API support - [PR](https://github.com/BerriAI/litellm/pull/12439)\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - API Key Auth support for AWS Bedrock API - [PR](https://github.com/BerriAI/litellm/pull/12495)\\n- **[\ud83c\udd95 Dashscope](../../docs/providers/dashscope)**\\n    - New integration from Alibaba (enables qwen usage) - [PR](https://github.com/BerriAI/litellm/pull/12361)\\n- **[\ud83c\udd95 Bytez](../../docs/providers/bytez)**\\n    - New /chat/completion integration - [PR](https://github.com/BerriAI/litellm/pull/12121)\\n\\n#### Bugs\\n- **[Github Copilot](../../docs/providers/github_copilot)**\\n    - Fix API base url for Github Copilot - [PR](https://github.com/BerriAI/litellm/pull/12418)\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Ensure supported\xa0bedrock/converse/\xa0params =\xa0bedrock/\xa0params - [PR](https://github.com/BerriAI/litellm/pull/12466)\\n    - Fix cache token cost calculation - [PR](https://github.com/BerriAI/litellm/pull/12488)\\n- **[XAI](../../docs/providers/xai)**\\n    - ensure finish_reason includes tool calls when xai responses with tool calls - [PR](https://github.com/BerriAI/litellm/pull/12545)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n- **[/completions](../../docs/text_completion)**\\n    - Return \u2018reasoning_content\u2019 on streaming - [PR](https://github.com/BerriAI/litellm/pull/12377)\\n- **[/chat/completions](../../docs/completion/input)** \\n    - Add \'thinking blocks\' to stream chunk builder - [PR](https://github.com/BerriAI/litellm/pull/12395)\\n- **[/v1/messages](../../docs/anthropic_unified)**\\n    - Fallbacks support - [PR](https://github.com/BerriAI/litellm/pull/12440)\\n    - tool call handling for non-anthropic models (/v1/messages to /chat/completion bridge) - [PR](https://github.com/BerriAI/litellm/pull/12473)\\n\\n---\\n\\n## [MCP Gateway](../../docs/mcp)\\n\\n<Image \\n  img={require(\'../../img/release_notes/mcp_tool_cost_tracking.png\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n\\n#### Features\\n- **[Cost Tracking](../../docs/mcp#-mcp-cost-tracking)**\\n    - Add Cost Tracking - [PR](https://github.com/BerriAI/litellm/pull/12385)\\n    - Add usage tracking - [PR](https://github.com/BerriAI/litellm/pull/12397)\\n    - Add custom cost configuration for each MCP tool - [PR](https://github.com/BerriAI/litellm/pull/12499)\\n    - Add support for editing MCP cost per tool - [PR](https://github.com/BerriAI/litellm/pull/12501)\\n    - Allow using custom post call MCP hook for cost tracking - [PR](https://github.com/BerriAI/litellm/pull/12469)\\n- **[Auth](../../docs/mcp#using-your-mcp-with-client-side-credentials)**\\n    - Allow customizing what client side auth header to use - [PR](https://github.com/BerriAI/litellm/pull/12460)\\n    - Raises error when MCP server header is malformed in the request - [PR](https://github.com/BerriAI/litellm/pull/12494)\\n- **[MCP Server](../../docs/mcp#adding-your-mcp)**\\n    - Allow using stdio MCPs with LiteLLM (enables using Circle CI MCP w/ LiteLLM) - [PR](https://github.com/BerriAI/litellm/pull/12530), [Get Started](../../docs/mcp#adding-a-stdio-mcp-server)\\n\\n#### Bugs\\n- **General**\\n    - Fix task group is not initialized error - [PR](https://github.com/BerriAI/litellm/pull/12411) s/o [@juancarlosm](https://github.com/juancarlosm)\\n- **[MCP Server](../../docs/mcp#adding-your-mcp)**\\n    - Fix mcp tool separator to work with Claude code - [PR](https://github.com/BerriAI/litellm/pull/12430), [Get Started](../../docs/mcp#adding-your-mcp)\\n    - Add validation to mcp server name to not allow \\"-\\" (enables namespaces to work) - [PR](https://github.com/BerriAI/litellm/pull/12515)\\n\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n\\n<Image \\n  img={require(\'../../img/release_notes/model_hub_v2.png\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n\\n#### Features\\n- **Model Hub**\\n    - new model hub table view - [PR](https://github.com/BerriAI/litellm/pull/12468)\\n    - new\xa0/public/model_hub\xa0endpoint - [PR](https://github.com/BerriAI/litellm/pull/12468)\\n    - Make Model Hub OSS - [PR](https://github.com/BerriAI/litellm/pull/12553)\\n    - New \u2018make public\u2019 modal flow for showing proxy models on public model hub - [PR](https://github.com/BerriAI/litellm/pull/12555)\\n- **MCP**\\n    - support for internal users to use and manage MCP servers - [PR](https://github.com/BerriAI/litellm/pull/12458)\\n    - Adds UI support to add MCP access groups (similar to namespaces) - [PR](https://github.com/BerriAI/litellm/pull/12470)\\n    - MCP Tool Testing Playground - [PR](https://github.com/BerriAI/litellm/pull/12520)\\n    - Show cost config on root of MCP settings - [PR](https://github.com/BerriAI/litellm/pull/12526)\\n- **Test Key**\\n    - Stick sessions - [PR](https://github.com/BerriAI/litellm/pull/12365)\\n    - MCP Access Groups - allow mcp access groups - [PR](https://github.com/BerriAI/litellm/pull/12529)\\n- **Usage**\\n    - Truncate long labels and improve tooltip in Top API Keys chart - [PR](https://github.com/BerriAI/litellm/pull/12371)\\n    - Improve Chart Readability for Tag Usage - [PR](https://github.com/BerriAI/litellm/pull/12378)\\n- **Teams**\\n    - Prevent navigation reset after team member operations - [PR](https://github.com/BerriAI/litellm/pull/12424)\\n    - Team Members - reset budget, if duration set - [PR](https://github.com/BerriAI/litellm/pull/12534)\\n    - Use central team member budget when max_budget_in_team set on UI - [PR](https://github.com/BerriAI/litellm/pull/12533)\\n- **SSO**\\n    - Allow users to run a custom sso login handler - [PR](https://github.com/BerriAI/litellm/pull/12465)\\n- **Navbar**\\n    - improve user dropdown UI with premium badge and cleaner layout - [PR](https://github.com/BerriAI/litellm/pull/12502)\\n- **General**\\n    - Consistent layout for Create and Back buttons on all the pages - [PR](https://github.com/BerriAI/litellm/pull/12542)\\n    - Align Show Password with Checkbox - [PR](https://github.com/BerriAI/litellm/pull/12538)\\n    - Prevent writing default user setting updates to yaml (causes error in non-root env) - [PR](https://github.com/BerriAI/litellm/pull/12533)\\n\\n#### Bugs\\n- **Model Hub**\\n    - fix duplicates in\xa0/model_group/info\xa0- [PR](https://github.com/BerriAI/litellm/pull/12468)\\n- **MCP**\\n    - Fix UI not syncing MCP access groups properly with object permissions - [PR](https://github.com/BerriAI/litellm/pull/12523)\\n\\n---\\n\\n## Logging / Guardrail Integrations\\n\\n#### Features\\n- **[Langfuse](../../docs/observability/langfuse_integration)**\\n    - Version bump - [PR](https://github.com/BerriAI/litellm/pull/12376)\\n    - LANGFUSE_TRACING_ENVIRONMENT support - [PR](https://github.com/BerriAI/litellm/pull/12376)\\n- **[Bedrock Guardrails](../../docs/proxy/guardrails/bedrock)**\\n    - Raise Bedrock output text on \'BLOCKED\' actions from guardrail - [PR](https://github.com/BerriAI/litellm/pull/12435)\\n- **[OTEL](../../docs/observability/opentelemetry_integration)**\\n    - `OTEL_RESOURCE_ATTRIBUTES` support - [PR](https://github.com/BerriAI/litellm/pull/12468)\\n- **[Guardrails AI](../../docs/proxy/guardrails/guardrails_ai)**\\n    - pre-call + logging only guardrail (pii detection/competitor names) support - [PR](https://github.com/BerriAI/litellm/pull/12506)\\n- **[Guardrails](../../docs/proxy/guardrails/quick_start)**\\n    - [Enterprise] Support tag based mode for guardrails - [PR](https://github.com/BerriAI/litellm/pull/12508), [Get Started](../../docs/proxy/guardrails/quick_start#-tag-based-guardrail-modes)\\n- **[OpenAI Moderations API](../../docs/proxy/guardrails/openai_moderation)**\\n    - New guardrail integration - [PR](https://github.com/BerriAI/litellm/pull/12519)\\n- **[Prometheus](../../docs/proxy/prometheus)**\\n    - support tag based metrics (enables prometheus metrics for measuring roo-code/cline/claude code engagement) - [PR](https://github.com/BerriAI/litellm/pull/12534), [Get Started](../../docs/proxy/prometheus#custom-tags)\\n- **[Datadog LLM Observability](../../docs/observability/datadog)**\\n    - Added `total_cost` field to track costs in DataDog LLM observability metrics - [PR](https://github.com/BerriAI/litellm/pull/12467)\\n\\n#### Bugs\\n- **[Prometheus](../../docs/proxy/prometheus)**\\n    - Remove experimental `_by_tag` metrics (fixes cardinality issue) - [PR](https://github.com/BerriAI/litellm/pull/12395)\\n- **[Slack Alerting](../../docs/proxy/alerting)**\\n    - Fix slack alerting for outage and region outage alerts - [PR](https://github.com/BerriAI/litellm/pull/12464), [Get Started](../../docs/proxy/alerting#region-outage-alerting--enterprise-feature)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n#### Bugs\\n- **[Responses API Bridge](../../docs/response_api#calling-non-responses-api-endpoints-responses-to-chatcompletions-bridge)**\\n    - add image support for Responses API when falling back on Chat Completions - [PR](https://github.com/BerriAI/litellm/pull/12204) s/o [@ryan-castner](https://github.com/ryan-castner)\\n- **aiohttp**\\n    - Properly close aiohttp client sessions to prevent resource leaks - [PR](https://github.com/BerriAI/litellm/pull/12251)\\n- **Router**\\n    - don\'t add invalid deployment to router pattern match - [PR](https://github.com/BerriAI/litellm/pull/12459)\\n\\n\\n---\\n\\n## General Proxy Improvements\\n\\n#### Bugs\\n- **S3**\\n  - s3 config.yaml file - ensure yaml safe load is used - [PR](https://github.com/BerriAI/litellm/pull/12373)\\n- **Audit Logs**\\n  - Add audit logs for model updates - [PR](https://github.com/BerriAI/litellm/pull/12396)\\n- **Startup**\\n  - Multiple API Keys Created on Startup when max_budget is enabled - [PR](https://github.com/BerriAI/litellm/pull/12436)\\n- **Auth**\\n  - Resolve model group alias on Auth (if user has access to underlying model, allow alias request to work) - [PR](https://github.com/BerriAI/litellm/pull/12440)\\n- **config.yaml**\\n  - fix parsing environment_variables from config.yaml - [PR](https://github.com/BerriAI/litellm/pull/12482)\\n- **Security**\\n  - Log hashed jwt w/ prefix instead of actual value - [PR](https://github.com/BerriAI/litellm/pull/12524)\\n\\n#### Features\\n- **MCP**\\n    - Bump mcp version on docker img - [PR](https://github.com/BerriAI/litellm/pull/12362)\\n- **Request Headers**\\n    - Forward \u2018anthropic-beta\u2019 header when forward_client_headers_to_llm_api is true - [PR](https://github.com/BerriAI/litellm/pull/12462)\\n\\n---\\n\\n## New Contributors\\n* @kanaka made their first contribution in https://github.com/BerriAI/litellm/pull/12418\\n* @juancarlosm made their first contribution in https://github.com/BerriAI/litellm/pull/12411\\n* @DmitriyAlergant made their first contribution in https://github.com/BerriAI/litellm/pull/12356\\n* @Rayshard made their first contribution in https://github.com/BerriAI/litellm/pull/12487\\n* @minghao51 made their first contribution in https://github.com/BerriAI/litellm/pull/12361\\n* @jdietzsch91 made their first contribution in https://github.com/BerriAI/litellm/pull/12488\\n* @iwinux made their first contribution in https://github.com/BerriAI/litellm/pull/12473\\n* @andresC98 made their first contribution in https://github.com/BerriAI/litellm/pull/12413\\n* @EmaSuriano made their first contribution in https://github.com/BerriAI/litellm/pull/12509\\n* @strawgate made their first contribution in https://github.com/BerriAI/litellm/pull/12528\\n* @inf3rnus made their first contribution in https://github.com/BerriAI/litellm/pull/12121\\n\\n## **[Git Diff](https://github.com/BerriAI/litellm/compare/v1.74.0-stable...v1.74.3-stable)**"},{"id":"v1-74-0-stable","metadata":{"permalink":"/release_notes/v1-74-0-stable","source":"@site/release_notes/v1.74.0-stable/index.md","title":"v1.74.0-stable","description":"Deploy this version","date":"2025-07-05T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.74.0-stable","slug":"v1-74-0-stable","date":"2025-07-05T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.74.3-stable","permalink":"/release_notes/v1-74-3-stable"},"nextItem":{"title":"v1.73.6-stable","permalink":"/release_notes/v1-73-6-stable"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.74.0-stable\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.74.0.post2\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights \\n\\n- **MCP Gateway Namespace Servers** - Clients connecting to LiteLLM can now specify which MCP servers to use. \\n- **Key/Team Based Logging on UI** - Proxy Admins can configure team or key-based logging settings directly in the UI. \\n- **Azure Content Safety Guardrails** - Added support for prompt injection and text moderation with Azure Content Safety Guardrails. \\n- **VertexAI Deepseek Models** - Support for calling VertexAI Deepseek models with LiteLLM\'s/chat/completions or /responses API.\\n- **Github Copilot API** - You can now use Github Copilot as an LLM API provider.\\n\\n\\n### MCP Gateway: Namespaced MCP Servers\\n\\nThis release brings support for namespacing MCP Servers on LiteLLM MCP Gateway.  This means you can specify the `x-mcp-servers` header to specify which servers to list tools from. \\n \\nThis is useful when you want to point MCP clients to specific MCP Servers on LiteLLM. \\n\\n\\n#### Usage\\n\\n<Tabs>\\n<TabItem value=\\"openai\\" label=\\"OpenAI API\\">\\n\\n```bash title=\\"cURL Example with Server Segregation\\" showLineNumbers\\ncurl --location \'https://api.openai.com/v1/responses\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--header \\"Authorization: Bearer $OPENAI_API_KEY\\" \\\\\\n--data \'{\\n    \\"model\\": \\"gpt-4o\\",\\n    \\"tools\\": [\\n        {\\n            \\"type\\": \\"mcp\\",\\n            \\"server_label\\": \\"litellm\\",\\n            \\"server_url\\": \\"<your-litellm-proxy-base-url>/mcp\\",\\n            \\"require_approval\\": \\"never\\",\\n            \\"headers\\": {\\n                \\"x-litellm-api-key\\": \\"Bearer YOUR_LITELLM_API_KEY\\",\\n                \\"x-mcp-servers\\": \\"Zapier_Gmail\\"\\n            }\\n        }\\n    ],\\n    \\"input\\": \\"Run available tools\\",\\n    \\"tool_choice\\": \\"required\\"\\n}\'\\n```\\n\\nIn this example, the request will only have access to tools from the \\"Zapier_Gmail\\" MCP server.\\n\\n</TabItem>\\n\\n<TabItem value=\\"litellm\\" label=\\"LiteLLM Proxy\\">\\n\\n```bash title=\\"cURL Example with Server Segregation\\" showLineNumbers\\ncurl --location \'<your-litellm-proxy-base-url>/v1/responses\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--header \\"Authorization: Bearer $LITELLM_API_KEY\\" \\\\\\n--data \'{\\n    \\"model\\": \\"gpt-4o\\",\\n    \\"tools\\": [\\n        {\\n            \\"type\\": \\"mcp\\",\\n            \\"server_label\\": \\"litellm\\",\\n            \\"server_url\\": \\"<your-litellm-proxy-base-url>/mcp\\",\\n            \\"require_approval\\": \\"never\\",\\n            \\"headers\\": {\\n                \\"x-litellm-api-key\\": \\"Bearer YOUR_LITELLM_API_KEY\\",\\n                \\"x-mcp-servers\\": \\"Zapier_Gmail,Server2\\"\\n            }\\n        }\\n    ],\\n    \\"input\\": \\"Run available tools\\",\\n    \\"tool_choice\\": \\"required\\"\\n}\'\\n```\\n\\nThis configuration restricts the request to only use tools from the specified MCP servers.\\n\\n</TabItem>\\n\\n<TabItem value=\\"cursor\\" label=\\"Cursor IDE\\">\\n\\n```json title=\\"Cursor MCP Configuration with Server Segregation\\" showLineNumbers\\n{\\n  \\"mcpServers\\": {\\n    \\"LiteLLM\\": {\\n      \\"url\\": \\"<your-litellm-proxy-base-url>/mcp\\",\\n      \\"headers\\": {\\n        \\"x-litellm-api-key\\": \\"Bearer $LITELLM_API_KEY\\",\\n        \\"x-mcp-servers\\": \\"Zapier_Gmail,Server2\\"\\n      }\\n    }\\n  }\\n}\\n```\\n\\nThis configuration in Cursor IDE settings will limit tool access to only the specified MCP server.\\n\\n</TabItem>\\n</Tabs>\\n\\n### Team / Key Based Logging on UI\\n\\n<Image \\n  img={require(\'../../img/release_notes/team_key_logging.png\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n\\n<br />\\n\\nThis release brings support for Proxy Admins to configure Team/Key Based Logging Settings on the UI. This allows routing LLM request/response logs to different Langfuse/Arize projects based on the team or key.\\n\\nFor developers using LiteLLM, their logs are automatically routed to their specific Arize/Langfuse projects. On this release, we support the following integrations for key/team based logging:\\n\\n- `langfuse`\\n- `arize`\\n- `langsmith`\\n\\n### Azure Content Safety Guardrails\\n\\n<Image \\n  img={require(\'../../img/azure_content_safety_guardrails.jpg\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n\\n<br />\\n\\n\\nLiteLLM now supports **Azure Content Safety Guardrails** for Prompt Injection and Text Moderation. This is **great for internal chat-ui** use cases, as you can now create guardrails with detection for Azure\u2019s Harm Categories, specify custom severity thresholds and run them across 100+ LLMs for just that use-case (or across all your calls). \\n\\n[Get Started](../../docs/proxy/guardrails/azure_content_guardrail)\\n\\n\\n### Python SDK: 2.3 Second Faster Import Times\\n\\nThis release brings significant performance improvements to the Python SDK with 2.3 seconds faster import times. We\'ve refactored the initialization process to reduce startup overhead, making LiteLLM more efficient for applications that need quick initialization. This is a major improvement for applications that need to initialize LiteLLM quickly.\\n\\n\\n---\\n\\n## New Models / Updated Models\\n\\n#### Pricing / Context Window Updates\\n\\n| Provider    | Model                                  | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Type |\\n| ----------- | -------------------------------------- | -------------- | ------------------- | -------------------- | ---- |\\n| Watsonx | `watsonx/mistralai/mistral-large` | 131k | $3.00 | $10.00 | New |\\n| Azure AI | `azure_ai/cohere-rerank-v3.5` | 4k | $2.00/1k queries | - | New (Rerank) |\\n\\n\\n#### Features\\n- **[\ud83c\udd95 GitHub Copilot](../../docs/providers/github_copilot)** - Use GitHub Copilot API with LiteLLM - [PR](https://github.com/BerriAI/litellm/pull/12325), [Get Started](../../docs/providers/github_copilot)\\n- **[\ud83c\udd95 VertexAI DeepSeek](../../docs/providers/vertex)** - Add support for VertexAI DeepSeek models - [PR](https://github.com/BerriAI/litellm/pull/12312), [Get Started](../../docs/providers/vertex_partner#vertexai-deepseek)\\n- **[Azure AI](../../docs/providers/azure_ai)**\\n  - Add azure_ai cohere rerank v3.5 - [PR](https://github.com/BerriAI/litellm/pull/12283), [Get Started](../../docs/providers/azure_ai#rerank-endpoint)\\n- **[Vertex AI](../../docs/providers/vertex)**\\n  - Add size parameter support for image generation - [PR](https://github.com/BerriAI/litellm/pull/12292), [Get Started](../../docs/providers/vertex_image)\\n- **[Custom LLM](../../docs/providers/custom_llm_server)**\\n  - Pass through extra_ properties on \\"custom\\" llm provider - [PR](https://github.com/BerriAI/litellm/pull/12185)\\n\\n#### Bugs\\n- **[Mistral](../../docs/providers/mistral)**\\n  - Fix transform_response handling for empty string content - [PR](https://github.com/BerriAI/litellm/pull/12202)\\n  - Turn Mistral to use llm_http_handler - [PR](https://github.com/BerriAI/litellm/pull/12245)\\n- **[Gemini](../../docs/providers/gemini)**\\n  - Fix tool call sequence - [PR](https://github.com/BerriAI/litellm/pull/11999)\\n  - Fix custom api_base path preservation - [PR](https://github.com/BerriAI/litellm/pull/12215)\\n- **[Anthropic](../../docs/providers/anthropic)**\\n  - Fix user_id validation logic - [PR](https://github.com/BerriAI/litellm/pull/11432)\\n- **[Bedrock](../../docs/providers/bedrock)**\\n  - Support optional args for bedrock - [PR](https://github.com/BerriAI/litellm/pull/12287)\\n- **[Ollama](../../docs/providers/ollama)**\\n  - Fix default parameters for ollama-chat - [PR](https://github.com/BerriAI/litellm/pull/12201)\\n- **[VLLM](../../docs/providers/vllm)**\\n  - Add \'audio_url\' message type support - [PR](https://github.com/BerriAI/litellm/pull/12270)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n\\n- **[/batches](../../docs/batches)**\\n  - Support batch retrieve with target model Query Param - [PR](https://github.com/BerriAI/litellm/pull/12228)\\n  - Anthropic completion bridge improvements - [PR](https://github.com/BerriAI/litellm/pull/12228)\\n- **[/responses](../../docs/response_api)**\\n  - Azure responses api bridge improvements - [PR](https://github.com/BerriAI/litellm/pull/12224)\\n  - Fix responses api error handling - [PR](https://github.com/BerriAI/litellm/pull/12225)\\n- **[/mcp (MCP Gateway)](../../docs/mcp)**\\n  - Add MCP url masking on frontend - [PR](https://github.com/BerriAI/litellm/pull/12247)\\n  - Add MCP servers header to scope - [PR](https://github.com/BerriAI/litellm/pull/12266)\\n  - Litellm mcp tool prefix - [PR](https://github.com/BerriAI/litellm/pull/12289)\\n  - Segregate MCP tools on connections using headers - [PR](https://github.com/BerriAI/litellm/pull/12296)\\n  - Added changes to mcp url wrapping - [PR](https://github.com/BerriAI/litellm/pull/12207)\\n\\n\\n#### Bugs\\n- **[/v1/messages](../../docs/anthropic_unified)**\\n  - Remove hardcoded model name on streaming - [PR](https://github.com/BerriAI/litellm/pull/12131)\\n  - Support lowest latency routing - [PR](https://github.com/BerriAI/litellm/pull/12180)\\n  - Non-anthropic models token usage returned - [PR](https://github.com/BerriAI/litellm/pull/12184)\\n- **[/chat/completions](../../docs/providers/anthropic_unified)**\\n  - Support Cursor IDE tool_choice format `{\\"type\\": \\"auto\\"}` - [PR](https://github.com/BerriAI/litellm/pull/12168)\\n- **[/generateContent](../../docs/generate_content)**\\n  - Allow passing litellm_params - [PR](https://github.com/BerriAI/litellm/pull/12177)\\n  - Only pass supported params when using OpenAI models - [PR](https://github.com/BerriAI/litellm/pull/12297)\\n  - Fix using gemini-cli with Vertex Anthropic Models - [PR](https://github.com/BerriAI/litellm/pull/12246)\\n- **Streaming**\\n  - Fix Error code: 307 for LlamaAPI Streaming Chat - [PR](https://github.com/BerriAI/litellm/pull/11946)\\n  - Store finish reason even if is_finished - [PR](https://github.com/BerriAI/litellm/pull/12250)\\n\\n---\\n\\n## Spend Tracking / Budget Improvements\\n\\n#### Bugs\\n  - Fix allow strings in calculate cost - [PR](https://github.com/BerriAI/litellm/pull/12200)\\n  - VertexAI Anthropic streaming cost tracking with prompt caching fixes - [PR](https://github.com/BerriAI/litellm/pull/12188)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Bugs\\n- **Team Management**\\n  - Prevent team model reset on model add - [PR](https://github.com/BerriAI/litellm/pull/12144)\\n  - Return team-only models on /v2/model/info - [PR](https://github.com/BerriAI/litellm/pull/12144)\\n  - Render team member budget correctly - [PR](https://github.com/BerriAI/litellm/pull/12144)\\n- **UI Rendering**\\n  - Fix rendering ui on non-root images - [PR](https://github.com/BerriAI/litellm/pull/12226)\\n  - Correctly display \'Internal Viewer\' user role - [PR](https://github.com/BerriAI/litellm/pull/12284)\\n- **Configuration**\\n  - Handle empty config.yaml - [PR](https://github.com/BerriAI/litellm/pull/12189)\\n  - Fix gemini /models - replace models/ as expected - [PR](https://github.com/BerriAI/litellm/pull/12189)\\n\\n#### Features\\n- **Team Management**\\n  - Allow adding team specific logging callbacks - [PR](https://github.com/BerriAI/litellm/pull/12261)\\n  - Add Arize Team Based Logging - [PR](https://github.com/BerriAI/litellm/pull/12264)\\n  - Allow Viewing/Editing Team Based Callbacks - [PR](https://github.com/BerriAI/litellm/pull/12265)\\n- **UI Improvements**\\n  - Comma separated spend and budget display - [PR](https://github.com/BerriAI/litellm/pull/12317)\\n  - Add logos to callback list - [PR](https://github.com/BerriAI/litellm/pull/12244)\\n- **CLI**\\n  - Add litellm-proxy cli login for starting to use litellm proxy - [PR](https://github.com/BerriAI/litellm/pull/12216)\\n- **Email Templates**\\n  - Customizable Email template - Subject and Signature - [PR](https://github.com/BerriAI/litellm/pull/12218)\\n\\n---\\n\\n## Logging / Guardrail Integrations\\n\\n#### Features\\n- Guardrails \\n  - All guardrails are now supported on the UI - [PR](https://github.com/BerriAI/litellm/pull/12349)\\n- **[Azure Content Safety](../../docs/guardrails/azure_content_safety)**\\n  - Add Azure Content Safety Guardrails to LiteLLM proxy - [PR](https://github.com/BerriAI/litellm/pull/12268)\\n  - Add azure content safety guardrails to the UI - [PR](https://github.com/BerriAI/litellm/pull/12309)\\n- **[DeepEval](../../docs/observability/deepeval_integration)**\\n  - Fix DeepEval logging format for failure events - [PR](https://github.com/BerriAI/litellm/pull/12303)\\n- **[Arize](../../docs/proxy/logging#arize)**\\n  - Add Arize Team Based Logging - [PR](https://github.com/BerriAI/litellm/pull/12264)\\n- **[Langfuse](../../docs/proxy/logging#langfuse)**\\n  - Langfuse prompt_version support - [PR](https://github.com/BerriAI/litellm/pull/12301)\\n- **[Sentry Integration](../../docs/observability/sentry)**\\n  - Add sentry scrubbing - [PR](https://github.com/BerriAI/litellm/pull/12210)\\n- **[AWS SQS Logging](../../docs/proxy/logging#aws-sqs)**\\n  - New AWS SQS Logging Integration - [PR](https://github.com/BerriAI/litellm/pull/12176)\\n- **[S3 Logger](../../docs/proxy/logging#s3-buckets)**\\n  - Add failure logging support - [PR](https://github.com/BerriAI/litellm/pull/12299)\\n- **[Prometheus Metrics](../../docs/proxy/prometheus)**\\n  - Add better error validation for prometheus metrics and labels - [PR](https://github.com/BerriAI/litellm/pull/12182)\\n\\n#### Bugs\\n- **Security**\\n  - Ensure only LLM API route fails get logged on Langfuse - [PR](https://github.com/BerriAI/litellm/pull/12308)\\n- **OpenMeter**\\n  - Integration error handling fix - [PR](https://github.com/BerriAI/litellm/pull/12147)\\n- **Message Redaction**\\n  - Ensure message redaction works for responses API logging - [PR](https://github.com/BerriAI/litellm/pull/12291)\\n- **Bedrock Guardrails**\\n  - Fix bedrock guardrails post_call for streaming responses - [PR](https://github.com/BerriAI/litellm/pull/12252)\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n#### Features\\n- **Python SDK**\\n  - 2 second faster import times - [PR](https://github.com/BerriAI/litellm/pull/12135)\\n  - Reduce python sdk import time by .3s - [PR](https://github.com/BerriAI/litellm/pull/12140)\\n- **Error Handling**\\n  - Add error handling for MCP tools not found or invalid server - [PR](https://github.com/BerriAI/litellm/pull/12223)\\n- **SSL/TLS**\\n  - Fix SSL certificate error - [PR](https://github.com/BerriAI/litellm/pull/12327)\\n  - Fix custom ca bundle support in aiohttp transport - [PR](https://github.com/BerriAI/litellm/pull/12281)\\n\\n\\n---\\n\\n## General Proxy Improvements\\n\\n- **Startup**\\n  - Add new banner on startup - [PR](https://github.com/BerriAI/litellm/pull/12328)\\n- **Dependencies**\\n  - Update pydantic version - [PR](https://github.com/BerriAI/litellm/pull/12213)\\n\\n\\n---\\n\\n## New Contributors\\n* @wildcard made their first contribution in https://github.com/BerriAI/litellm/pull/12157\\n* @colesmcintosh made their first contribution in https://github.com/BerriAI/litellm/pull/12168\\n* @seyeong-han made their first contribution in https://github.com/BerriAI/litellm/pull/11946\\n* @dinggh made their first contribution in https://github.com/BerriAI/litellm/pull/12162\\n* @raz-alon made their first contribution in https://github.com/BerriAI/litellm/pull/11432\\n* @tofarr made their first contribution in https://github.com/BerriAI/litellm/pull/12200\\n* @szafranek made their first contribution in https://github.com/BerriAI/litellm/pull/12179\\n* @SamBoyd made their first contribution in https://github.com/BerriAI/litellm/pull/12147\\n* @lizzij made their first contribution in https://github.com/BerriAI/litellm/pull/12219\\n* @cipri-tom made their first contribution in https://github.com/BerriAI/litellm/pull/12201\\n* @zsimjee made their first contribution in https://github.com/BerriAI/litellm/pull/12185\\n* @jroberts2600 made their first contribution in https://github.com/BerriAI/litellm/pull/12175\\n* @njbrake made their first contribution in https://github.com/BerriAI/litellm/pull/12202\\n* @NANDINI-star made their first contribution in https://github.com/BerriAI/litellm/pull/12244\\n* @utsumi-fj made their first contribution in https://github.com/BerriAI/litellm/pull/12230\\n* @dcieslak19973 made their first contribution in https://github.com/BerriAI/litellm/pull/12283\\n* @hanouticelina made their first contribution in https://github.com/BerriAI/litellm/pull/12286\\n* @lowjiansheng made their first contribution in https://github.com/BerriAI/litellm/pull/11999\\n* @JoostvDoorn made their first contribution in https://github.com/BerriAI/litellm/pull/12281\\n* @takashiishida made their first contribution in https://github.com/BerriAI/litellm/pull/12239\\n\\n## **[Git Diff](https://github.com/BerriAI/litellm/compare/v1.73.6-stable...v1.74.0-stable)**"},{"id":"v1-73-6-stable","metadata":{"permalink":"/release_notes/v1-73-6-stable","source":"@site/release_notes/v1.73.6-stable/index.md","title":"v1.73.6-stable","description":"Deploy this version","date":"2025-06-28T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.73.6-stable","slug":"v1-73-6-stable","date":"2025-06-28T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.74.0-stable","permalink":"/release_notes/v1-74-0-stable"},"nextItem":{"title":"v1.73.0-stable - Set default team for new users","permalink":"/release_notes/v1-73-0-stable"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.73.6-stable.patch.1\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.73.6.post1\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n---\\n\\n## Key Highlights \\n\\n\\n### Claude on gemini-cli\\n\\n\\n<Image img={require(\'../../img/release_notes/gemini_cli.png\')} />\\n\\n<br/>\\n\\nThis release brings support for using gemini-cli with LiteLLM. \\n\\nYou can use claude-sonnet-4, gemini-2.5-flash (Vertex AI & Google AI Studio), gpt-4.1 and any LiteLLM supported model on gemini-cli.\\n\\nWhen you use gemini-cli with LiteLLM you get the following benefits:\\n\\n**Developer Benefits:**\\n- Universal Model Access: Use any LiteLLM supported model (Anthropic, OpenAI, Vertex AI, Bedrock, etc.) through the gemini-cli interface.\\n- Higher Rate Limits & Reliability: Load balance across multiple models and providers to avoid hitting individual provider limits, with fallbacks to ensure you get responses even if one provider fails.\\n\\n**Proxy Admin Benefits:**\\n- Centralized Management: Control access to all models through a single LiteLLM proxy instance without giving your developers API Keys to each provider.\\n- Budget Controls: Set spending limits and track costs across all gemini-cli usage.\\n\\n[Get Started](../../docs/tutorials/litellm_gemini_cli)\\n\\n<br/>\\n\\n### Batch API Cost Tracking\\n\\n<Image img={require(\'../../img/release_notes/batch_api_cost_tracking.jpg\')}/>\\n\\n<br/>\\n\\nv1.73.6 brings cost tracking for [LiteLLM Managed Batch API](../../docs/proxy/managed_batches) calls to LiteLLM. Previously, this was not being done for Batch API calls using LiteLLM Managed Files. Now, LiteLLM will store the status of each batch call in the DB and poll incomplete batch jobs in the background, emitting a spend log for cost tracking once the batch is complete.\\n\\nThere is no new flag / change needed on your end. Over the next few weeks we hope to extend this to cover batch cost tracking for the Anthropic passthrough as well. \\n\\n\\n[Get Started](../../docs/proxy/managed_batches)\\n\\n---\\n\\n## New Models / Updated Models\\n\\n### Pricing / Context Window Updates\\n\\n| Provider    | Model                                  | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Type |\\n| ----------- | -------------------------------------- | -------------- | ------------------- | -------------------- | ---- |\\n| Azure OpenAI | `azure/o3-pro` | 200k | $20.00 | $80.00 | New |\\n| OpenRouter | `openrouter/mistralai/mistral-small-3.2-24b-instruct` | 32k | $0.1 | $0.3 | New |\\n| OpenAI | `o3-deep-research` | 200k | $10.00 | $40.00 | New |\\n| OpenAI | `o3-deep-research-2025-06-26` | 200k | $10.00 | $40.00 | New |\\n| OpenAI | `o4-mini-deep-research` | 200k | $2.00 | $8.00 | New |\\n| OpenAI | `o4-mini-deep-research-2025-06-26` | 200k | $2.00 | $8.00 | New |\\n| Deepseek | `deepseek-r1` | 65k | $0.55 | $2.19 | New |\\n| Deepseek | `deepseek-v3` | 65k | $0.27 | $0.07 | New |\\n\\n\\n### Updated Models\\n#### Bugs\\n    - **[Sambanova](../../docs/providers/sambanova)**\\n        - Handle float timestamps - [PR](https://github.com/BerriAI/litellm/pull/11971) s/o [@neubig](https://github.com/neubig)\\n    - **[Azure](../../docs/providers/azure)**\\n        - support Azure Authentication method (azure ad token, api keys) on Responses API - [PR](https://github.com/BerriAI/litellm/pull/11941) s/o [@hsuyuming](https://github.com/hsuyuming)\\n        - Map \u2018image_url\u2019 str as nested dict - [PR](https://github.com/BerriAI/litellm/pull/12075) s/o [@davis-featherstone](https://github.com/davis-featherstone)\\n    - **[Watsonx](../../docs/providers/watsonx)**\\n        - Set \u2018model\u2019 field to None when model is part of a custom deployment - fixes error raised by WatsonX in those cases - [PR](https://github.com/BerriAI/litellm/pull/11854) s/o [@cbjuan](https://github.com/cbjuan)\\n    - **[Perplexity](../../docs/providers/perplexity)**\\n        - Support web_search_options - [PR](https://github.com/BerriAI/litellm/pull/11983)\\n        - Support citation token and search queries cost calculation - [PR](https://github.com/BerriAI/litellm/pull/11938)\\n    - **[Anthropic](../../docs/providers/anthropic)**\\n        - Null value in usage block handling - [PR](https://github.com/BerriAI/litellm/pull/12068)\\n    - **Gemini ([Google AI Studio](../../docs/providers/gemini) + [VertexAI](../../docs/providers/vertex))**\\n        - Only use accepted format values (enum and datetime) - else gemini raises errors - [PR](https://github.com/BerriAI/litellm/pull/11989) \\n        - Cache tools if passed alongside cached content (else gemini raises an error) - [PR](https://github.com/BerriAI/litellm/pull/11989)\\n        - Json schema translation improvement: Fix unpack_def handling of nested $ref inside anyof items - [PR](https://github.com/BerriAI/litellm/pull/11964)\\n    - **[Mistral](../../docs/providers/mistral)**\\n        - Fix thinking prompt to match hugging face recommendation - [PR](https://github.com/BerriAI/litellm/pull/12007)\\n        - Add `supports_response_schema: true` for all mistral models except codestral-mamba - [PR](https://github.com/BerriAI/litellm/pull/12024)\\n    - **[Ollama](../../docs/providers/ollama)**\\n        - Fix unnecessary await on embedding calls - [PR](https://github.com/BerriAI/litellm/pull/12024)\\n#### Features\\n    - **[Azure OpenAI](../../docs/providers/azure)**\\n        - Check if o-series model supports reasoning effort (enables drop_params to work for o1 models) \\n        - Assistant + tool use cost tracking - [PR](https://github.com/BerriAI/litellm/pull/12045)\\n    - **[Nvidia Nim](../../docs/providers/nvidia_nim)**\\n        - Add \u2018response_format\u2019 param support - [PR](https://github.com/BerriAI/litellm/pull/12003) @shagunb-acn\xa0\\n    - **[ElevenLabs](../../docs/providers/elevenlabs)**\\n        - New STT provider - [PR](https://github.com/BerriAI/litellm/pull/12119)\\n\\n---\\n## LLM API Endpoints\\n\\n#### Features\\n    - [**/mcp**](../../docs/mcp)\\n        - Send appropriate auth string value to `/tool/call` endpoint with `x-mcp-auth` - [PR](https://github.com/BerriAI/litellm/pull/11968) s/o [@wagnerjt](https://github.com/wagnerjt)\\n    - [**/v1/messages**](../../docs/anthropic_unified)\\n        - [Custom LLM](../../docs/providers/custom_llm_server#anthropic-v1messages) support - [PR](https://github.com/BerriAI/litellm/pull/12016)\\n    - [**/chat/completions**](../../docs/completion/input)\\n        - Azure Responses API via chat completion support - [PR](https://github.com/BerriAI/litellm/pull/12016)\\n    - [**/responses**](../../docs/response_api)\\n        - Add reasoning content support for non-openai providers - [PR](https://github.com/BerriAI/litellm/pull/12055)\\n    - **[NEW] /generateContent**\\n        - New endpoints for gemini cli support - [PR](https://github.com/BerriAI/litellm/pull/12040)\\n        - Support calling Google AI Studio / VertexAI Gemini models in their native format - [PR](https://github.com/BerriAI/litellm/pull/12046)\\n        - Add logging + cost tracking for stream + non-stream vertex/google ai studio routes - [PR](https://github.com/BerriAI/litellm/pull/12058)\\n        - Add Bridge from generateContent to /chat/completions - [PR](https://github.com/BerriAI/litellm/pull/12081)\\n    - [**/batches**](../../docs/batches)\\n        - Filter deployments to only those where managed file was written to - [PR](https://github.com/BerriAI/litellm/pull/12048)\\n        - Save all model / file id mappings in db (previously it was just the first one) - enables \u2018true\u2019 loadbalancing - [PR](https://github.com/BerriAI/litellm/pull/12048)\\n        - Support List Batches with target model name specified - [PR](https://github.com/BerriAI/litellm/pull/12049)\\n\\n---\\n## Spend Tracking / Budget Improvements\\n\\n#### Features\\n    - [**Passthrough**](../../docs/pass_through)\\n        - [Bedrock](../../docs/pass_through/bedrock) - cost tracking (`/invoke` + `/converse` routes) on streaming + non-streaming - [PR](https://github.com/BerriAI/litellm/pull/12123)\\n        - [VertexAI](../../docs/pass_through/vertex_ai) - anthropic cost calculation support - [PR](https://github.com/BerriAI/litellm/pull/11992)\\n    - [**Batches**](../../docs/batches)\\n        - Background job for cost tracking LiteLLM Managed batches - [PR](https://github.com/BerriAI/litellm/pull/12125)\\n\\n---\\n## Management Endpoints / UI\\n\\n#### Bugs\\n    - **General UI**\\n        - Fix today selector date mutation in dashboard components - [PR](https://github.com/BerriAI/litellm/pull/12042)\\n    - **Usage**\\n        - Aggregate usage data across all pages of paginated endpoint - [PR](https://github.com/BerriAI/litellm/pull/12033)\\n    - **Teams**\\n        - De-duplicate models in team settings dropdown - [PR](https://github.com/BerriAI/litellm/pull/12074)\\n    - **Models**\\n        - Preserve public model name when selecting \u2018test connect\u2019 with azure model (previously would reset) - [PR](https://github.com/BerriAI/litellm/pull/11713)\\n    - **Invitation Links**\\n        - Ensure Invite links email contain the correct invite id when using tf provider - [PR](https://github.com/BerriAI/litellm/pull/12130)\\n#### Features\\n    - **Models**\\n        - Add \u2018last success\u2019 column to health check table - [PR](https://github.com/BerriAI/litellm/pull/11903)\\n    - **MCP**\\n        - New UI component to support auth types: api key, bearer token, basic auth - [PR](https://github.com/BerriAI/litellm/pull/11968) s/o [@wagnerjt](https://github.com/wagnerjt)\\n        - Ensure internal users can access /mcp and /mcp/ routes - [PR](https://github.com/BerriAI/litellm/pull/12106)\\n    - **SCIM**\\n        - Ensure default_internal_user_params are applied for new users - [PR](https://github.com/BerriAI/litellm/pull/12015)\\n    - **Team**\\n        - Support default key expiry for team member keys - [PR](https://github.com/BerriAI/litellm/pull/12023)\\n        - Expand team member add check to cover user email - [PR](https://github.com/BerriAI/litellm/pull/12082)\\n    - **UI**\\n        - Restrict UI access by SSO group - [PR](https://github.com/BerriAI/litellm/pull/12023)\\n    - **Keys**\\n        - Add new\xa0new_key\xa0param for regenerating key - [PR](https://github.com/BerriAI/litellm/pull/12087)\\n    - **Test Keys**\\n        - New \u2018get code\u2019 button for getting runnable python code snippet based on ui configuration - [PR](https://github.com/BerriAI/litellm/pull/11629)\\n\\n--- \\n\\n## Logging / Guardrail Integrations\\n\\n#### Bugs\\n    - **Braintrust**\\n        - Adds model to metadata to enable braintrust cost estimation - [PR](https://github.com/BerriAI/litellm/pull/12022)\\n#### Features\\n    - **Callbacks**\\n        - (Enterprise) - disable logging callbacks in request headers - [PR](https://github.com/BerriAI/litellm/pull/11985)\\n        - Add List Callbacks API Endpoint - [PR](https://github.com/BerriAI/litellm/pull/11987)\\n    - **Bedrock Guardrail**\\n        - Don\'t raise exception on intervene action - [PR](https://github.com/BerriAI/litellm/pull/11875)\\n        - Ensure PII Masking is applied on response streaming or non streaming content when using post call\xa0- [PR](https://github.com/BerriAI/litellm/pull/12086)\\n    - **[NEW] Palo Alto Networks Prisma AIRS Guardrail**\\n        - [PR](https://github.com/BerriAI/litellm/pull/12116)\\n    - **ElasticSearch**\\n        - New Elasticsearch Logging Tutorial - [PR](https://github.com/BerriAI/litellm/pull/11761)\\n    - **Message Redaction**\\n        - Preserve usage / model information  for Embedding redaction - [PR](https://github.com/BerriAI/litellm/pull/12088)\\n\\n---\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n#### Bugs\\n    - **Team-only models**\\n        - Filter team-only models from routing logic for non-team calls\\n    - **Context Window Exceeded error**\\n        - Catch anthropic exceptions - [PR](https://github.com/BerriAI/litellm/pull/12113)\\n#### Features\\n    - **Router**\\n        - allow using dynamic cooldown time for a specific deployment - [PR](https://github.com/BerriAI/litellm/pull/12037)\\n        - handle cooldown_time = 0 for deployments - [PR](https://github.com/BerriAI/litellm/pull/12108)\\n    - **Redis**\\n        - Add better debugging to see what variables are set - [PR](https://github.com/BerriAI/litellm/pull/12073)\\n\\n---\\n\\n## General Proxy Improvements\\n\\n#### Bugs\\n    - **aiohttp**\\n        - Check HTTP_PROXY vars in networking requests\\n        - Allow using HTTP_ Proxy settings with trust_env\\n\\n#### Features\\n    - **Docs**\\n        - Add recommended spec - [PR](https://github.com/BerriAI/litellm/pull/11980)\\n    - **Swagger**\\n        - Introduce new environment variable NO_REDOC to opt-out Redoc - [PR](https://github.com/BerriAI/litellm/pull/12092)\\n\\n\\n---\\n\\n## New Contributors\\n* @mukesh-dream11 made their first contribution in https://github.com/BerriAI/litellm/pull/11969\\n* @cbjuan made their first contribution in https://github.com/BerriAI/litellm/pull/11854\\n* @ryan-castner made their first contribution in https://github.com/BerriAI/litellm/pull/12055\\n* @davis-featherstone made their first contribution in https://github.com/BerriAI/litellm/pull/12075\\n* @Gum-Joe made their first contribution in https://github.com/BerriAI/litellm/pull/12068\\n* @jroberts2600 made their first contribution in https://github.com/BerriAI/litellm/pull/12116\\n* @ohmeow made their first contribution in https://github.com/BerriAI/litellm/pull/12022\\n* @amarrella made their first contribution in https://github.com/BerriAI/litellm/pull/11942\\n* @zhangyoufu made their first contribution in https://github.com/BerriAI/litellm/pull/12092\\n* @bougou made their first contribution in https://github.com/BerriAI/litellm/pull/12088\\n* @codeugar made their first contribution in https://github.com/BerriAI/litellm/pull/11972\\n* @glgh made their first contribution in https://github.com/BerriAI/litellm/pull/12133\\n\\n## **[Git Diff](https://github.com/BerriAI/litellm/compare/v1.73.0-stable...v1.73.6.rc-draft)**"},{"id":"v1-73-0-stable","metadata":{"permalink":"/release_notes/v1-73-0-stable","source":"@site/release_notes/v1.73.0-stable/index.md","title":"v1.73.0-stable - Set default team for new users","description":"Known Issues","date":"2025-06-21T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.73.0-stable - Set default team for new users","slug":"v1-73-0-stable","date":"2025-06-21T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.73.6-stable","permalink":"/release_notes/v1-73-6-stable"},"nextItem":{"title":"v1.72.6-stable - MCP Gateway Permission Management","permalink":"/release_notes/v1-72-6-stable"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n\\n:::warning\\n\\n## Known Issues\\n\\nThe `non-root` docker image has a known issue around the UI not loading. If you use the `non-root` docker image we recommend waiting before upgrading to this version. We will post a patch fix for this.\\n\\n:::\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:v1.73.0-stable\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.73.0.post1\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n\\n## TLDR\\n\\n\\n* **Why Upgrade**\\n    - User Management: Set default team for new users - enables giving all users $10 API keys for exploration.\\n    - Passthrough Endpoints v2: Enhanced support for subroutes and custom cost tracking for passthrough endpoints.\\n    - Health Check Dashboard: New frontend UI for monitoring model health and status.\\n* **Who Should Read**\\n    - Teams using **Passthrough Endpoints**\\n    - Teams using **User Management** on LiteLLM\\n    - Teams using **Health Check Dashboard** for models\\n    - Teams using **Claude Code** with LiteLLM\\n* **Risk of Upgrade**\\n    - **Low**\\n        - No major breaking changes to existing functionality.\\n- **Major Changes**\\n    - `User Agent` will be auto-tracked as a tag in LiteLLM UI Logs Page. This means for all LLM requests you will see a `User Agent` tag in the logs page.\\n\\n---\\n\\n## Key Highlights\\n\\n\\n\\n### Set Default Team for New Users\\n\\n<Image img={require(\'../../img/default_teams_product_ss.jpg\')}/>\\n\\n<br/>\\n\\nv1.73.0 introduces the ability to assign new users to Default Teams. This makes it much easier to enable experimentation with LLMs within your company, while also **ensuring spend for exploration is tracked correctly.** \\n \\nWhat this means for **Proxy Admins**:\\n- Set a max budget per team member: This sets a max amount an individual can spend within a team. \\n- Set a default team for new users: When a new user signs in via SSO / invitation link, they will be automatically added to this team. \\n\\nWhat this means for **Developers**: \\n- View models across teams: You can now go to `Models + Endpoints` and view the models you have access to, across all teams you\'re a member of. \\n- Safe create key modal: If you have no model access outside of a team (default behaviour), you are now nudged to select a team on the Create Key modal. This resolves a common confusion point for new users onboarding to the proxy. \\n\\n[Get Started](https://docs.litellm.ai/docs/tutorials/default_team_self_serve)\\n\\n\\n### Passthrough Endpoints v2\\n\\n<Image img={require(\'../../img/release_notes/v2_pt.png\')}/>\\n\\n\\n<br/>\\n\\nThis release brings support for adding billing and full URL forwarding for passthrough endpoints. \\n\\nPreviously, you could only map simple endpoints, but now you can add just `/bria` and all subroutes automatically get forwarded - for example, `/bria/v1/text-to-image/base/model` and `/bria/v1/enhance_image` will both be forwarded to the target URL with the same path structure.\\n\\nThis means you as Proxy Admin can onboard third-party endpoints like Bria API and Mistral OCR, set a cost per request, and give your developers access to the complete API functionality.\\n\\n[Learn more about Passthrough Endpoints](../../docs/proxy/pass_through)\\n\\n\\n### v2 Health Checks \\n\\n<Image img={require(\'../../img/release_notes/v2_health.png\')}/>\\n\\n<br/>\\n\\nThis release brings support for Proxy Admins to select which specific models to health check and see the health status as soon as its individual check completes, along with last check times.\\n\\nThis allows Proxy Admins to immediately identify which specific models are in a bad state and view the full error stack trace for faster troubleshooting.\\n\\n---\\n\\n\\n## New / Updated Models\\n\\n### Pricing / Context Window Updates\\n\\n| Provider    | Model                                  | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Type |\\n| ----------- | -------------------------------------- | -------------- | ------------------- | -------------------- | ---- |\\n| Google VertexAI | `vertex_ai/imagen-4` | N/A | Image Generation | Image Generation | New |\\n| Google VertexAI | `vertex_ai/imagen-4-preview` | N/A | Image Generation | Image Generation | New |\\n| Gemini | `gemini-2.5-pro` | 2M | $1.25 | $5.00 | New |\\n| Gemini | `gemini-2.5-flash-lite` | 1M | $0.075 | $0.30 | New |\\n| OpenRouter | Various models | Updated | Updated | Updated | Updated |\\n| Azure | `azure/o3` | 200k | $2.00 | $8.00 | Updated |\\n| Azure | `azure/o3-pro` | 200k | $2.00 | $8.00 | Updated |\\n| Azure OpenAI | Azure Codex Models | Various | Various | Various | New |\\n\\n### Updated Models\\n\\n#### Features\\n- **[Azure](../../docs/providers/azure)**\\n    - Support for new /v1 preview Azure OpenAI API - [PR](https://github.com/BerriAI/litellm/pull/11934), [Get Started](../../docs/providers/azure/azure_responses#azure-codex-models)\\n    - Add Azure Codex Models support - [PR](https://github.com/BerriAI/litellm/pull/11934), [Get Started](../../docs/providers/azure/azure_responses#azure-codex-models)\\n    - Make Azure AD scope configurable - [PR](https://github.com/BerriAI/litellm/pull/11621)\\n    - Handle more GPT custom naming patterns - [PR](https://github.com/BerriAI/litellm/pull/11914)\\n    - Update o3 pricing to match OpenAI pricing - [PR](https://github.com/BerriAI/litellm/pull/11937)\\n- **[VertexAI](../../docs/providers/vertex)**\\n    - Add Vertex Imagen-4 models - [PR](https://github.com/BerriAI/litellm/pull/11767), [Get Started](../../docs/providers/vertex_image)\\n    - Anthropic streaming passthrough cost tracking - [PR](https://github.com/BerriAI/litellm/pull/11734)\\n- **[Gemini](../../docs/providers/gemini)**\\n    - Working Gemini TTS support via `/v1/speech` endpoint - [PR](https://github.com/BerriAI/litellm/pull/11832)\\n    - Fix gemini 2.5 flash config - [PR](https://github.com/BerriAI/litellm/pull/11830)\\n    - Add missing `flash-2.5-flash-lite` model and fix pricing - [PR](https://github.com/BerriAI/litellm/pull/11901)\\n    - Mark all gemini-2.5 models as supporting PDF input - [PR](https://github.com/BerriAI/litellm/pull/11907)\\n    - Add `gemini-2.5-pro` with reasoning support - [PR](https://github.com/BerriAI/litellm/pull/11927)\\n- **[AWS Bedrock](../../docs/providers/bedrock)**\\n    - AWS credentials no longer mandatory - [PR](https://github.com/BerriAI/litellm/pull/11765)\\n    - Add AWS Bedrock profiles for APAC region - [PR](https://github.com/BerriAI/litellm/pull/11883)\\n    - Fix AWS Bedrock Claude tool call index - [PR](https://github.com/BerriAI/litellm/pull/11842)\\n    - Handle base64 file data with `qs:..` prefix - [PR](https://github.com/BerriAI/litellm/pull/11908)\\n    - Add Mistral Small to BEDROCK_CONVERSE_MODELS - [PR](https://github.com/BerriAI/litellm/pull/11760)\\n- **[Mistral](../../docs/providers/mistral)**\\n    - Enhance Mistral API with parallel tool calls support - [PR](https://github.com/BerriAI/litellm/pull/11770)\\n- **[Meta Llama API](../../docs/providers/meta_llama)**\\n    - Enable tool calling for meta_llama models - [PR](https://github.com/BerriAI/litellm/pull/11895)\\n- **[Volcengine](../../docs/providers/volcengine)**\\n    - Add thinking parameter support - [PR](https://github.com/BerriAI/litellm/pull/11914)\\n\\n\\n#### Bugs\\n\\n- **[VertexAI](../../docs/providers/vertex)**\\n    - Handle missing tokenCount in promptTokensDetails - [PR](https://github.com/BerriAI/litellm/pull/11896)\\n    - Fix vertex AI claude thinking params - [PR](https://github.com/BerriAI/litellm/pull/11796)\\n- **[Gemini](../../docs/providers/gemini)**\\n    - Fix web search error with responses API - [PR](https://github.com/BerriAI/litellm/pull/11894), [Get Started](../../docs/completion/web_search#responses-litellmresponses)\\n- **[Custom LLM](../../docs/providers/custom_llm_server)**\\n    - Set anthropic custom LLM provider property - [PR](https://github.com/BerriAI/litellm/pull/11907)\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Bump anthropic package version - [PR](https://github.com/BerriAI/litellm/pull/11851)\\n- **[Ollama](../../docs/providers/ollama)**\\n    - Update ollama_embeddings to work on sync API - [PR](https://github.com/BerriAI/litellm/pull/11746)\\n    - Fix response_format not working - [PR](https://github.com/BerriAI/litellm/pull/11880)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n#### Features\\n- **[Responses API](../../docs/response_api)**\\n    - Day-0 support for OpenAI re-usable prompts Responses API - [PR](https://github.com/BerriAI/litellm/pull/11782), [Get Started](../../docs/providers/openai/responses_api#reusable-prompts)\\n    - Support passing image URLs in Completion-to-Responses bridge - [PR](https://github.com/BerriAI/litellm/pull/11833)\\n- **[MCP Gateway](../../docs/mcp)**\\n    - Add Allowed MCPs to Creating/Editing Organizations - [PR](https://github.com/BerriAI/litellm/pull/11893), [Get Started](../../docs/mcp#-mcp-permission-management)\\n    - Allow connecting to MCP with authentication headers - [PR](https://github.com/BerriAI/litellm/pull/11891), [Get Started](../../docs/mcp#using-your-mcp-with-client-side-credentials)\\n- **[Speech API](../../docs/speech)**\\n    - Working Gemini TTS support via OpenAI\'s `/v1/speech` endpoint - [PR](https://github.com/BerriAI/litellm/pull/11832)\\n- **[Passthrough Endpoints](../../docs/proxy/pass_through)**\\n    - Add support for subroutes for passthrough endpoints - [PR](https://github.com/BerriAI/litellm/pull/11827)\\n    - Support for setting custom cost per passthrough request - [PR](https://github.com/BerriAI/litellm/pull/11870)\\n    - Ensure \\"Request\\" is tracked for passthrough requests on LiteLLM Proxy - [PR](https://github.com/BerriAI/litellm/pull/11873)\\n    - Add V2 Passthrough endpoints on UI - [PR](https://github.com/BerriAI/litellm/pull/11905)\\n    - Move passthrough endpoints under Models + Endpoints in UI - [PR](https://github.com/BerriAI/litellm/pull/11871)\\n    - QA improvements for adding passthrough endpoints - [PR](https://github.com/BerriAI/litellm/pull/11909), [PR](https://github.com/BerriAI/litellm/pull/11939)\\n- **[Models API](../../docs/completion/model_alias)**\\n    - Allow `/models` to return correct models for custom wildcard prefixes - [PR](https://github.com/BerriAI/litellm/pull/11784)\\n\\n#### Bugs\\n\\n- **[Messages API](../../docs/anthropic_unified)**\\n    - Fix `/v1/messages` endpoint always using us-central1 with vertex_ai-anthropic models - [PR](https://github.com/BerriAI/litellm/pull/11831)\\n    - Fix model_group tracking for `/v1/messages` and `/moderations` - [PR](https://github.com/BerriAI/litellm/pull/11933)\\n    - Fix cost tracking and logging via `/v1/messages` API when using Claude Code - [PR](https://github.com/BerriAI/litellm/pull/11928)\\n- **[MCP Gateway](../../docs/mcp)**\\n    - Fix using MCPs defined on config.yaml - [PR](https://github.com/BerriAI/litellm/pull/11824)\\n- **[Chat Completion API](../../docs/completion/input)**\\n    - Allow dict for tool_choice argument in acompletion - [PR](https://github.com/BerriAI/litellm/pull/11860)\\n- **[Passthrough Endpoints](../../docs/pass_through/langfuse)**\\n    - Don\'t log request to Langfuse passthrough on Langfuse - [PR](https://github.com/BerriAI/litellm/pull/11768)\\n\\n---\\n\\n## Spend Tracking\\n\\n#### Features\\n- **[User Agent Tracking](../../docs/proxy/cost_tracking)**\\n    - Automatically track spend by user agent (allows cost tracking for Claude Code) - [PR](https://github.com/BerriAI/litellm/pull/11781)\\n    - Add user agent tags in spend logs payload - [PR](https://github.com/BerriAI/litellm/pull/11872)\\n- **[Tag Management](../../docs/proxy/cost_tracking)**\\n    - Support adding public model names in tag management - [PR](https://github.com/BerriAI/litellm/pull/11908)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Features\\n- **Test Key Page**\\n    - Allow testing `/v1/messages` on the Test Key Page - [PR](https://github.com/BerriAI/litellm/pull/11930)\\n- **[SSO](../../docs/proxy/sso)**\\n    - Allow passing additional headers - [PR](https://github.com/BerriAI/litellm/pull/11781)\\n- **[JWT Auth](../../docs/proxy/jwt_auth)**\\n    - Correctly return user email - [PR](https://github.com/BerriAI/litellm/pull/11783)\\n- **[Model Management](../../docs/proxy/model_management)**\\n    - Allow editing model access group for existing model - [PR](https://github.com/BerriAI/litellm/pull/11783)\\n- **[Team Management](../../docs/proxy/team_management)**\\n    - Allow setting default team for new users - [PR](https://github.com/BerriAI/litellm/pull/11874), [PR](https://github.com/BerriAI/litellm/pull/11877)\\n    - Fix default team settings - [PR](https://github.com/BerriAI/litellm/pull/11887)\\n- **[SCIM](../../docs/proxy/scim)**\\n    - Add error handling for existing user on SCIM - [PR](https://github.com/BerriAI/litellm/pull/11862)\\n    - Add SCIM PATCH and PUT operations for users - [PR](https://github.com/BerriAI/litellm/pull/11863)\\n- **Health Check Dashboard**\\n    - Implement health check backend API and storage functionality - [PR](https://github.com/BerriAI/litellm/pull/11852)\\n    - Add LiteLLM_HealthCheckTable to database schema - [PR](https://github.com/BerriAI/litellm/pull/11677)\\n    - Implement health check frontend UI components and dashboard integration - [PR](https://github.com/BerriAI/litellm/pull/11679)\\n    - Add success modal for health check responses - [PR](https://github.com/BerriAI/litellm/pull/11899)\\n    - Fix clickable model ID in health check table - [PR](https://github.com/BerriAI/litellm/pull/11898)\\n    - Fix health check UI table design - [PR](https://github.com/BerriAI/litellm/pull/11897)\\n\\n---\\n\\n## Logging / Guardrails Integrations\\n\\n#### Bugs\\n- **[Prometheus](../../docs/observability/prometheus)**\\n    - Fix bug for using prometheus metrics config - [PR](https://github.com/BerriAI/litellm/pull/11779)\\n\\n---\\n\\n## Security & Reliability\\n\\n#### Security Fixes\\n- **[Documentation Security](../../docs)**\\n    - Security fixes for docs - [PR](https://github.com/BerriAI/litellm/pull/11776)\\n    - Add Trivy Security Scan for UI + Docs folder - remove all vulnerabilities - [PR](https://github.com/BerriAI/litellm/pull/11778)\\n\\n#### Reliability Improvements\\n- **[Dependencies](../../docs)**\\n    - Fix aiohttp version requirement - [PR](https://github.com/BerriAI/litellm/pull/11777)\\n    - Bump next from 14.2.26 to 14.2.30 in UI dashboard - [PR](https://github.com/BerriAI/litellm/pull/11720)\\n- **[Networking](../../docs)**\\n    - Allow using CA Bundles - [PR](https://github.com/BerriAI/litellm/pull/11906)\\n    - Add workload identity federation between GCP and AWS - [PR](https://github.com/BerriAI/litellm/pull/10210)\\n\\n---\\n\\n## General Proxy Improvements\\n\\n#### Features\\n- **[Deployment](../../docs/proxy/deploy)**\\n    - Add deployment annotations for Kubernetes - [PR](https://github.com/BerriAI/litellm/pull/11849)\\n    - Add ciphers in command and pass to hypercorn for proxy - [PR](https://github.com/BerriAI/litellm/pull/11916)\\n- **[Custom Root Path](../../docs/proxy/deploy)**\\n    - Fix loading UI on custom root path - [PR](https://github.com/BerriAI/litellm/pull/11912)\\n- **[SDK Improvements](../../docs/proxy/reliability)**\\n    - LiteLLM SDK / Proxy improvement (don\'t transform message client-side) - [PR](https://github.com/BerriAI/litellm/pull/11908)\\n\\n#### Bugs\\n- **[Observability](../../docs/observability)**\\n    - Fix boto3 tracer wrapping for observability - [PR](https://github.com/BerriAI/litellm/pull/11869)\\n\\n\\n---\\n\\n## New Contributors\\n* @kjoth made their first contribution in [PR](https://github.com/BerriAI/litellm/pull/11621)\\n* @shagunb-acn made their first contribution in [PR](https://github.com/BerriAI/litellm/pull/11760)\\n* @MadsRC made their first contribution in [PR](https://github.com/BerriAI/litellm/pull/11765)\\n* @Abiji-2020 made their first contribution in [PR](https://github.com/BerriAI/litellm/pull/11746)\\n* @salzubi401 made their first contribution in [PR](https://github.com/BerriAI/litellm/pull/11803)\\n* @orolega made their first contribution in [PR](https://github.com/BerriAI/litellm/pull/11826)\\n* @X4tar made their first contribution in [PR](https://github.com/BerriAI/litellm/pull/11796)\\n* @karen-veigas made their first contribution in [PR](https://github.com/BerriAI/litellm/pull/11858)\\n* @Shankyg made their first contribution in [PR](https://github.com/BerriAI/litellm/pull/11859)\\n* @pascallim made their first contribution in [PR](https://github.com/BerriAI/litellm/pull/10210)\\n* @lgruen-vcgs made their first contribution in [PR](https://github.com/BerriAI/litellm/pull/11883)\\n* @rinormaloku made their first contribution in [PR](https://github.com/BerriAI/litellm/pull/11851)\\n* @InvisibleMan1306 made their first contribution in [PR](https://github.com/BerriAI/litellm/pull/11849)\\n* @ervwalter made their first contribution in [PR](https://github.com/BerriAI/litellm/pull/11937)\\n* @ThakeeNathees made their first contribution in [PR](https://github.com/BerriAI/litellm/pull/11880)\\n* @jnhyperion made their first contribution in [PR](https://github.com/BerriAI/litellm/pull/11842)\\n* @Jannchie made their first contribution in [PR](https://github.com/BerriAI/litellm/pull/11860)\\n\\n---\\n\\n## Demo Instance\\n\\nHere\'s a Demo Instance to test changes:\\n\\n- Instance: https://demo.litellm.ai/\\n- Login Credentials:\\n    - Username: admin\\n    - Password: sk-1234\\n\\n## [Git Diff](https://github.com/BerriAI/litellm/compare/v1.72.6-stable...v1.73.0.rc)"},{"id":"v1-72-6-stable","metadata":{"permalink":"/release_notes/v1-72-6-stable","source":"@site/release_notes/v1.72.6-stable/index.md","title":"v1.72.6-stable - MCP Gateway Permission Management","description":"Deploy this version","date":"2025-06-14T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.72.6-stable - MCP Gateway Permission Management","slug":"v1-72-6-stable","date":"2025-06-14T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.73.0-stable - Set default team for new users","permalink":"/release_notes/v1-73-0-stable"},"nextItem":{"title":"v1.72.2-stable","permalink":"/release_notes/v1-72-2-stable"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run\\n-e STORE_MODEL_IN_DB=True\\n-p 4000:4000\\ndocker.litellm.ai/berriai/litellm:main-v1.72.6-stable\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.72.6.post2\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n\\n## TLDR\\n\\n\\n* **Why Upgrade**\\n    - Codex-mini on Claude Code: You can now use `codex-mini` (OpenAI\u2019s code assistant model) via Claude Code.\\n    - MCP Permissions Management: Manage permissions for MCP Servers by Keys, Teams, Organizations (entities) on LiteLLM.\\n    - UI: Turn on/off auto refresh on logs view. \\n    - Rate Limiting: Support for output token-only rate limiting.  \\n* **Who Should Read**\\n    - Teams using `/v1/messages` API (Claude Code)\\n    - Teams using **MCP**\\n    - Teams giving access to self-hosted models and setting rate limits\\n* **Risk of Upgrade**\\n    - **Low**\\n        - No major changes to existing functionality or package updates.\\n\\n\\n---\\n\\n## Key Highlights\\n\\n\\n### MCP Permissions Management\\n\\n<Image img={require(\'../../img/release_notes/mcp_permissions.png\')}/>\\n\\nThis release brings support for managing permissions for MCP Servers by Keys, Teams, Organizations (entities) on LiteLLM. When a MCP client attempts to list tools, LiteLLM will only return the tools the entity has permissions to access.\\n\\nThis is great for use cases that require access to restricted data (e.g Jira MCP) that you don\'t want everyone to use.\\n\\nFor Proxy Admins, this enables centralized management of all MCP Servers with access control. For developers, this means you\'ll only see the MCP tools assigned to you.\\n\\n\\n\\n\\n### Codex-mini on Claude Code\\n\\n<Image img={require(\'../../img/release_notes/codex_on_claude_code.jpg\')} />\\n\\nThis release brings support for calling `codex-mini` (OpenAI\u2019s code assistant model) via Claude Code.\\n\\nThis is done by LiteLLM enabling any Responses API model (including `o3-pro`) to be called via `/chat/completions` and `/v1/messages` endpoints. This includes:\\n\\n- Streaming calls\\n- Non-streaming calls\\n- Cost Tracking on success + failure for Responses API models\\n\\nHere\'s how to use it [today](../../docs/tutorials/claude_responses_api)\\n\\n\\n\\n\\n---\\n\\n\\n## New / Updated Models\\n\\n### Pricing / Context Window Updates\\n\\n| Provider    | Model                                  | Context Window | Input ($/1M tokens) | Output ($/1M tokens) | Type |\\n| ----------- | -------------------------------------- | -------------- | ------------------- | -------------------- | -------------------- |\\n| VertexAI   | `vertex_ai/claude-opus-4`               | 200K           | $15.00              | $75.00               | New |\\n| OpenAI   | `gpt-4o-audio-preview-2025-06-03`             | 128k           | $2.5 (text), $40 (audio)              | $10 (text), $80 (audio)               | New |\\n| OpenAI | `o3-pro` | 200k | 20 | 80 | New |\\n| OpenAI | `o3-pro-2025-06-10` | 200k | 20 | 80 | New |\\n| OpenAI | `o3` | 200k | 2 | 8 | Updated |\\n| OpenAI | `o3-2025-04-16` | 200k | 2 | 8 | Updated |\\n| Azure | `azure/gpt-4o-mini-transcribe` | 16k | 1.25 (text), 3 (audio) | 5 (text) | New |\\n| Mistral | `mistral/magistral-medium-latest` | 40k | 2 | 5 | New |\\n| Mistral | `mistral/magistral-small-latest` | 40k | 0.5 | 1.5 | New |\\n\\n- Deepgram: `nova-3` cost per second pricing is [now supported](https://github.com/BerriAI/litellm/pull/11634).\\n\\n### Updated Models\\n#### Bugs\\n- **[Watsonx](../../docs/providers/watsonx)**\\n    - Ignore space id on Watsonx deployments (throws json errors) - [PR](https://github.com/BerriAI/litellm/pull/11527)\\n- **[Ollama](../../docs/providers/ollama)**\\n    - Set tool call id for streaming calls - [PR](https://github.com/BerriAI/litellm/pull/11528)\\n- **Gemini ([VertexAI](../../docs/providers/vertex) + [Google AI Studio](../../docs/providers/gemini))**\\n    - Fix tool call indexes - [PR](https://github.com/BerriAI/litellm/pull/11558)\\n    - Handle empty string for arguments in function calls - [PR](https://github.com/BerriAI/litellm/pull/11601)\\n    - Add audio/ogg mime type support when inferring from file url\u2019s - [PR](https://github.com/BerriAI/litellm/pull/11635)\\n- **[Custom LLM](../../docs/providers/custom_llm_server)**\\n    - Fix passing api_base, api_key, litellm_params_dict to custom_llm embedding methods - [PR](https://github.com/BerriAI/litellm/pull/11450) s/o [ElefHead](https://github.com/ElefHead)\\n- **[Huggingface](../../docs/providers/huggingface)**\\n    - Add /chat/completions to endpoint url when missing - [PR](https://github.com/BerriAI/litellm/pull/11630)\\n- **[Deepgram](../../docs/providers/deepgram)**\\n    - Support async httpx calls - [PR](https://github.com/BerriAI/litellm/pull/11641)\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Append prefix (if set) to assistant content start - [PR](https://github.com/BerriAI/litellm/pull/11719)\\n\\n#### Features\\n- **[VertexAI](../../docs/providers/vertex)**\\n    - Support vertex credentials set via env var on passthrough - [PR](https://github.com/BerriAI/litellm/pull/11527)\\n    - Support for choosing \u2018global\u2019 region when model is only available there - [PR](https://github.com/BerriAI/litellm/pull/11566)\\n    - Anthropic passthrough cost calculation + token tracking - [PR](https://github.com/BerriAI/litellm/pull/11611)\\n    - Support \u2018global\u2019 vertex region on passthrough - [PR](https://github.com/BerriAI/litellm/pull/11661)\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - \u2018none\u2019 tool choice param support - [PR](https://github.com/BerriAI/litellm/pull/11695), [Get Started](../../docs/providers/anthropic#disable-tool-calling)\\n- **[Perplexity](../../docs/providers/perplexity)**\\n    - Add \u2018reasoning_effort\u2019 support - [PR](https://github.com/BerriAI/litellm/pull/11562), [Get Started](../../docs/providers/perplexity#reasoning-effort)\\n- **[Mistral](../../docs/providers/mistral)**\\n    - Add mistral reasoning support - [PR](https://github.com/BerriAI/litellm/pull/11642), [Get Started](../../docs/providers/mistral#reasoning)\\n- **[SGLang](../../docs/providers/openai_compatible)**\\n    - Map context window exceeded error for proper handling - [PR](https://github.com/BerriAI/litellm/pull/11575/)\\n- **[Deepgram](../../docs/providers/deepgram)**\\n    - Provider specific params support - [PR](https://github.com/BerriAI/litellm/pull/11638)\\n- **[Azure](../../docs/providers/azure)**\\n    - Return content safety filter results - [PR](https://github.com/BerriAI/litellm/pull/11655)\\n---\\n\\n## LLM API Endpoints\\n\\n#### Bugs\\n- **[Chat Completion](../../docs/completion/input)**\\n    - Streaming - Ensure consistent \u2018created\u2019 across chunks - [PR](https://github.com/BerriAI/litellm/pull/11528)\\n#### Features\\n- **MCP**\\n    - Add controls for MCP Permission Management - [PR](https://github.com/BerriAI/litellm/pull/11598), [Docs](../../docs/mcp#-mcp-permission-management)\\n    - Add permission management for MCP List + Call Tool operations - [PR](https://github.com/BerriAI/litellm/pull/11682), [Docs](../../docs/mcp#-mcp-permission-management)\\n    - Streamable HTTP server support - [PR](https://github.com/BerriAI/litellm/pull/11628), [PR](https://github.com/BerriAI/litellm/pull/11645), [Docs](../../docs/mcp#using-your-mcp)\\n    - Use Experimental dedicated Rest endpoints for list, calling MCP tools - [PR](https://github.com/BerriAI/litellm/pull/11684)\\n- **[Responses API](../../docs/response_api)**\\n    - NEW API Endpoint - List input items - [PR](https://github.com/BerriAI/litellm/pull/11602) \\n    - Background mode for OpenAI + Azure OpenAI - [PR](https://github.com/BerriAI/litellm/pull/11640)\\n    - Langfuse/other Logging support on responses api requests - [PR](https://github.com/BerriAI/litellm/pull/11685)\\n- **[Chat Completions](../../docs/completion/input)**\\n    - Bridge for Responses API - allows calling codex-mini via `/chat/completions` and `/v1/messages` - [PR](https://github.com/BerriAI/litellm/pull/11632), [PR](https://github.com/BerriAI/litellm/pull/11685)\\n\\n\\n---\\n\\n## Spend Tracking\\n\\n#### Bugs\\n- **[End Users](../../docs/proxy/customers)**\\n    - Update enduser spend and budget reset date based on budget duration - [PR](https://github.com/BerriAI/litellm/pull/8460) (s/o [laurien16](https://github.com/laurien16))\\n- **[Custom Pricing](../../docs/proxy/custom_pricing)**\\n    - Convert scientific notation str to int - [PR](https://github.com/BerriAI/litellm/pull/11655)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n#### Bugs\\n- **[Users](../../docs/proxy/users)**\\n    - `/user/info` - fix passing user with `+` in user id\\n    - Add admin-initiated password reset flow - [PR](https://github.com/BerriAI/litellm/pull/11618)\\n    - Fixes default user settings UI rendering error - [PR](https://github.com/BerriAI/litellm/pull/11674)\\n- **[Budgets](../../docs/proxy/users)**\\n    - Correct success message when new user budget is created - [PR](https://github.com/BerriAI/litellm/pull/11608)\\n\\n#### Features\\n- **Leftnav**\\n    - Show remaining Enterprise users on UI\\n- **MCP**\\n    - New server add form - [PR](https://github.com/BerriAI/litellm/pull/11604)\\n    - Allow editing mcp servers - [PR](https://github.com/BerriAI/litellm/pull/11693)\\n- **Models**\\n    - Add deepgram models on UI\\n    - Model Access Group support on UI - [PR](https://github.com/BerriAI/litellm/pull/11719)\\n- **Keys**\\n    - Trim long user id\u2019s - [PR](https://github.com/BerriAI/litellm/pull/11488)\\n- **Logs**\\n    - Add live tail feature to logs view, allows user to disable auto refresh in high traffic - [PR](https://github.com/BerriAI/litellm/pull/11712)\\n    - Audit Logs - preview screenshot - [PR](https://github.com/BerriAI/litellm/pull/11715)\\n\\n---\\n\\n## Logging / Guardrails Integrations\\n\\n#### Bugs\\n- **[Arize](../../docs/observability/arize_integration)**\\n    - Change space_key header to space_id - [PR](https://github.com/BerriAI/litellm/pull/11595) (s/o [vanities](https://github.com/vanities))\\n- **[Prometheus](../../docs/proxy/prometheus)**\\n    - Fix total requests increment - [PR](https://github.com/BerriAI/litellm/pull/11718)\\n\\n#### Features\\n- **[Lasso Guardrails](../../docs/proxy/guardrails/lasso_security)**\\n    - [NEW] Lasso Guardrails support - [PR](https://github.com/BerriAI/litellm/pull/11565)\\n- **[Users](../../docs/proxy/users)**\\n    - New `organizations` param on `/user/new` - allows adding users to orgs on creation - [PR](https://github.com/BerriAI/litellm/pull/11572/files)\\n- **Prevent double logging when using bridge logic** - [PR](https://github.com/BerriAI/litellm/pull/11687)\\n\\n---\\n\\n## Performance / Reliability Improvements\\n\\n#### Bugs\\n- **[Tag based routing](../../docs/proxy/tag_routing)**\\n    - Do not consider \u2018default\u2019 models when request specifies a tag - [PR](https://github.com/BerriAI/litellm/pull/11454) (s/o [thiagosalvatore](https://github.com/thiagosalvatore))\\n\\n#### Features\\n- **[Caching](../../docs/caching/all_caches)**\\n    - New optional \u2018litellm[caching]\u2019 pip install for adding disk cache dependencies - [PR](https://github.com/BerriAI/litellm/pull/11600)\\n\\n---\\n\\n## General Proxy Improvements\\n\\n#### Bugs\\n- **aiohttp**\\n    - fixes for transfer encoding error on aiohttp transport - [PR](https://github.com/BerriAI/litellm/pull/11561)\\n\\n#### Features\\n- **aiohttp**\\n    - Enable System Proxy Support for aiohttp transport - [PR](https://github.com/BerriAI/litellm/pull/11616) (s/o [idootop](https://github.com/idootop))\\n- **CLI**\\n    - Make all commands show server URL - [PR](https://github.com/BerriAI/litellm/pull/10801)\\n- **Unicorn**\\n    - Allow setting keep alive timeout - [PR](https://github.com/BerriAI/litellm/pull/11594)\\n- **Experimental Rate Limiting v2** (enable via `EXPERIMENTAL_MULTI_INSTANCE_RATE_LIMITING=\\"True\\"`)\\n    - Support specifying rate limit by output_tokens only - [PR](https://github.com/BerriAI/litellm/pull/11646)\\n    - Decrement parallel requests on call failure - [PR](https://github.com/BerriAI/litellm/pull/11646)\\n    - In-memory only rate limiting support - [PR](https://github.com/BerriAI/litellm/pull/11646)\\n    - Return remaining rate limits by key/user/team - [PR](https://github.com/BerriAI/litellm/pull/11646)\\n- **Helm**\\n    - support extraContainers in migrations-job.yaml - [PR](https://github.com/BerriAI/litellm/pull/11649)\\n\\n\\n\\n\\n---\\n\\n## New Contributors\\n* @laurien16 made their first contribution in https://github.com/BerriAI/litellm/pull/8460\\n* @fengbohello made their first contribution in https://github.com/BerriAI/litellm/pull/11547\\n* @lapinek made their first contribution in https://github.com/BerriAI/litellm/pull/11570\\n* @yanwork made their first contribution in https://github.com/BerriAI/litellm/pull/11586\\n* @dhs-shine made their first contribution in https://github.com/BerriAI/litellm/pull/11575\\n* @ElefHead made their first contribution in https://github.com/BerriAI/litellm/pull/11450\\n* @idootop made their first contribution in https://github.com/BerriAI/litellm/pull/11616\\n* @stevenaldinger made their first contribution in https://github.com/BerriAI/litellm/pull/11649\\n* @thiagosalvatore made their first contribution in https://github.com/BerriAI/litellm/pull/11454\\n* @vanities made their first contribution in https://github.com/BerriAI/litellm/pull/11595\\n* @alvarosevilla95 made their first contribution in https://github.com/BerriAI/litellm/pull/11661\\n\\n---\\n\\n## Demo Instance\\n\\nHere\'s a Demo Instance to test changes:\\n\\n- Instance: https://demo.litellm.ai/\\n- Login Credentials:\\n    - Username: admin\\n    - Password: sk-1234\\n\\n## [Git Diff](https://github.com/BerriAI/litellm/compare/v1.72.2-stable...1.72.6.rc)"},{"id":"v1-72-2-stable","metadata":{"permalink":"/release_notes/v1-72-2-stable","source":"@site/release_notes/v1.72.2-stable/index.md","title":"v1.72.2-stable","description":"Deploy this version","date":"2025-06-07T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.72.2-stable","slug":"v1-72-2-stable","date":"2025-06-07T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.72.6-stable - MCP Gateway Permission Management","permalink":"/release_notes/v1-72-6-stable"},"nextItem":{"title":"v1.72.0-stable","permalink":"/release_notes/v1-72-0-stable"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run\\n-e STORE_MODEL_IN_DB=True\\n-p 4000:4000\\ndocker.litellm.ai/berriai/litellm:main-v1.72.2-stable\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.72.2.post1\\n```\\n\\n</TabItem>\\n</Tabs>\\n\\n## TLDR\\n\\n* **Why Upgrade**\\n    - Performance Improvements for /v1/messages: For this endpoint LiteLLM Proxy overhead is now down to 50ms at 250 RPS. \\n    - Accurate Rate Limiting: Multi-instance rate limiting now tracks rate limits across keys, models, teams, and users with 0 spillover.\\n    - Audit Logs on UI: Track when Keys, Teams, and Models were deleted by viewing Audit Logs on the LiteLLM UI.\\n    - /v1/messages all models support: You can now use all LiteLLM models (`gpt-4.1`, `o1-pro`, `gemini-2.5-pro`) with /v1/messages API. \\n    - [Anthropic MCP](../../docs/providers/anthropic#mcp-tool-calling): Use remote MCP Servers with Anthropic Models. \\n* **Who Should Read**\\n    - Teams using `/v1/messages` API (Claude Code)\\n    - Proxy Admins using LiteLLM Virtual Keys and setting rate limits\\n* **Risk of Upgrade**\\n    - **Medium**\\n        - Upgraded `ddtrace==3.8.0`, if you use DataDog tracing this is a medium level risk. We recommend monitoring logs for any issues.\\n\\n\\n\\n---\\n\\n## `/v1/messages` Performance Improvements\\n\\n<Image \\n  img={require(\'../../img/release_notes/v1_messages_perf.png\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n\\nThis release brings significant performance improvements to the /v1/messages API on LiteLLM. \\n\\nFor this endpoint LiteLLM Proxy overhead latency is now down to 50ms, and each instance can handle 250 RPS. We validated these improvements through load testing with payloads containing over 1,000 streaming chunks.\\n\\nThis is great for real time use cases with large requests (eg. multi turn conversations, Claude Code, etc.). \\n\\n## Multi-Instance Rate Limiting Improvements\\n\\n<Image \\n  img={require(\'../../img/release_notes/multi_instance_rate_limits_v3.jpg\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n\\nLiteLLM now accurately tracks rate limits across keys, models, teams, and users with 0 spillover.\\n\\nThis is a significant improvement over the previous version, which faced issues with leakage and spillover in high traffic, multi-instance setups.\\n\\n**Key Changes:**\\n- Redis is now part of the rate limit check, instead of being a background sync. This ensures accuracy and reduces read/write operations during low activity.\\n- LiteLLM now uses Lua scripts to ensure all checks are atomic.\\n- In-memory caching uses Redis values. This prevents drift, and reduces Redis queries once objects are over their limit.\\n\\nThese changes are currently behind the feature flag - `EXPERIMENTAL_ENABLE_MULTI_INSTANCE_RATE_LIMITING=True`. We plan to GA this in our next release - subject to feedback.\\n\\n## Audit Logs on UI\\n\\n<Image \\n  img={require(\'../../img/release_notes/ui_audit_log.png\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n\\nThis release introduces support for viewing audit logs in the UI. As a Proxy Admin, you can now check if and when a key was deleted, along with who performed the action.\\n\\nLiteLLM tracks changes to the following entities and actions: \\n\\n- **Entities:** Keys, Teams, Users, Models\\n- **Actions:** Create, Update, Delete, Regenerate\\n\\n\\n\\n## New Models / Updated Models\\n\\n**Newly Added Models**\\n\\n| Provider    | Model                                  | Context Window | Input ($/1M tokens) | Output ($/1M tokens) |\\n| ----------- | -------------------------------------- | -------------- | ------------------- | -------------------- |\\n| Anthropic   | `claude-4-opus-20250514`               | 200K           | $15.00              | $75.00               |\\n| Anthropic   | `claude-4-sonnet-20250514`             | 200K           | $3.00               | $15.00               |\\n| VertexAI, Google AI Studio      | `gemini-2.5-pro-preview-06-05`         | 1M             | $1.25               | $10.00               |\\n| OpenAI      | `codex-mini-latest`                    | 200K           | $1.50               | $6.00                |\\n| Cerebras    | `qwen-3-32b`                           | 128K           | $0.40               | $0.80                |\\n| SambaNova   | `DeepSeek-R1`                          | 32K            | $5.00               | $7.00                |\\n| SambaNova   | `DeepSeek-R1-Distill-Llama-70B`       | 131K           | $0.70               | $1.40                |\\n\\n\\n\\n### Model Updates\\n\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Cost tracking added for new Claude models - [PR](https://github.com/BerriAI/litellm/pull/11339)\\n        - `claude-4-opus-20250514`\\n        - `claude-4-sonnet-20250514`\\n    - Support for MCP tool calling with Anthropic models - [PR](https://github.com/BerriAI/litellm/pull/11474)\\n- **[Google AI Studio](../../docs/providers/gemini)**\\n    - Google Gemini 2.5 Pro Preview 06-05 support - [PR](https://github.com/BerriAI/litellm/pull/11447)\\n    - Gemini streaming thinking content parsing with `reasoning_content` - [PR](https://github.com/BerriAI/litellm/pull/11298)\\n    - Support for no reasoning option for Gemini models - [PR](https://github.com/BerriAI/litellm/pull/11393)\\n    - URL context support for Gemini models - [PR](https://github.com/BerriAI/litellm/pull/11351)\\n    - Gemini embeddings-001 model prices and context window - [PR](https://github.com/BerriAI/litellm/pull/11332)\\n- **[OpenAI](../../docs/providers/openai)**\\n    - Cost tracking for `codex-mini-latest` - [PR](https://github.com/BerriAI/litellm/pull/11492)\\n- **[Vertex AI](../../docs/providers/vertex)**\\n    - Cache token tracking on streaming calls - [PR](https://github.com/BerriAI/litellm/pull/11387)\\n    - Return response_id matching upstream response ID for stream and non-stream - [PR](https://github.com/BerriAI/litellm/pull/11456)\\n- **[Cerebras](../../docs/providers/cerebras)**\\n    - Cerebras/qwen-3-32b model pricing and context window - [PR](https://github.com/BerriAI/litellm/pull/11373)\\n- **[HuggingFace](../../docs/providers/huggingface)**\\n    - Fixed embeddings using non-default `input_type` - [PR](https://github.com/BerriAI/litellm/pull/11452)\\n- **[DataRobot](../../docs/providers/datarobot)**\\n    - New provider integration for enterprise AI workflows - [PR](https://github.com/BerriAI/litellm/pull/10385)\\n- **[DeepSeek](../../docs/providers/together_ai)**\\n    - DeepSeek R1 family model configuration via Together AI - [PR](https://github.com/BerriAI/litellm/pull/11394)\\n    - DeepSeek R1 pricing and context window configuration - [PR](https://github.com/BerriAI/litellm/pull/11339)\\n\\n---\\n\\n## LLM API Endpoints\\n\\n- **[Images API](../../docs/image_generation)**\\n    - Azure endpoint support for image endpoints - [PR](https://github.com/BerriAI/litellm/pull/11482)\\n- **[Anthropic Messages API](../../docs/completion/chat)**\\n    - Support for ALL LiteLLM Providers (OpenAI, Azure, Bedrock, Vertex, DeepSeek, etc.) on /v1/messages API Spec - [PR](https://github.com/BerriAI/litellm/pull/11502)\\n    - Performance improvements for /v1/messages route - [PR](https://github.com/BerriAI/litellm/pull/11421)\\n    - Return streaming usage statistics when using LiteLLM with Bedrock models - [PR](https://github.com/BerriAI/litellm/pull/11469)\\n- **[Embeddings API](../../docs/embedding/supported_embedding)**\\n    - Provider-specific optional params handling for embedding calls - [PR](https://github.com/BerriAI/litellm/pull/11346)\\n    - Proper Sagemaker request attribute usage for embeddings - [PR](https://github.com/BerriAI/litellm/pull/11362)\\n- **[Rerank API](../../docs/rerank/supported_rerank)**\\n    - New HuggingFace rerank provider support - [PR](https://github.com/BerriAI/litellm/pull/11438), [Guide](../../docs/providers/huggingface_rerank)\\n\\n---\\n\\n## Spend Tracking\\n\\n- Added token tracking for anthropic batch calls via /anthropic passthrough route- [PR](https://github.com/BerriAI/litellm/pull/11388)\\n\\n---\\n\\n## Management Endpoints / UI\\n\\n\\n- **SSO/Authentication**\\n    - SSO configuration endpoints and UI integration with persistent settings - [PR](https://github.com/BerriAI/litellm/pull/11417)\\n    - Update proxy admin ID role in DB + Handle SSO redirects with custom root path - [PR](https://github.com/BerriAI/litellm/pull/11384)\\n    - Support returning virtual key in custom auth - [PR](https://github.com/BerriAI/litellm/pull/11346)\\n    - User ID validation to ensure it is not an email or phone number - [PR](https://github.com/BerriAI/litellm/pull/10102)\\n- **Teams**\\n    - Fixed Create/Update team member API 500 error - [PR](https://github.com/BerriAI/litellm/pull/10479)\\n    - Enterprise feature gating for RegenerateKeyModal in KeyInfoView - [PR](https://github.com/BerriAI/litellm/pull/11400)\\n- **SCIM**\\n    - Fixed SCIM running patch operation case sensitivity - [PR](https://github.com/BerriAI/litellm/pull/11335)\\n- **General**\\n    - Converted action buttons to sticky footer action buttons - [PR](https://github.com/BerriAI/litellm/pull/11293)\\n    - Custom Server Root Path - support for serving UI on a custom root path - [Guide](../../docs/proxy/custom_root_ui)\\n---\\n\\n## Logging / Guardrails Integrations\\n\\n#### Logging\\n- **[S3](../../docs/proxy/logging#s3)**\\n    - Async + Batched S3 Logging for improved performance - [PR](https://github.com/BerriAI/litellm/pull/11340)\\n- **[DataDog](../../docs/observability/datadog_integration)**\\n    - Add instrumentation for streaming chunks - [PR](https://github.com/BerriAI/litellm/pull/11338)\\n    - Add DD profiler to monitor Python profile of LiteLLM CPU% - [PR](https://github.com/BerriAI/litellm/pull/11375)\\n    - Bump DD trace version - [PR](https://github.com/BerriAI/litellm/pull/11426)\\n- **[Prometheus](../../docs/proxy/prometheus)**\\n    - Pass custom metadata labels in litellm_total_token metrics - [PR](https://github.com/BerriAI/litellm/pull/11414)\\n- **[GCS](../../docs/proxy/logging#google-cloud-storage)**\\n    - Update GCSBucketBase to handle GSM project ID if passed - [PR](https://github.com/BerriAI/litellm/pull/11409)\\n\\n#### Guardrails\\n- **[Presidio](../../docs/proxy/guardrails/presidio)**\\n    - Add presidio_language yaml configuration support for guardrails - [PR](https://github.com/BerriAI/litellm/pull/11331)\\n\\n---\\n\\n## Performance / Reliability Improvements\\n\\n- **Performance Optimizations**\\n    - Don\'t run auth on /health/liveliness endpoints - [PR](https://github.com/BerriAI/litellm/pull/11378)\\n    - Don\'t create 1 task for every hanging request alert - [PR](https://github.com/BerriAI/litellm/pull/11385)\\n    - Add debugging endpoint to track active /asyncio-tasks - [PR](https://github.com/BerriAI/litellm/pull/11382)\\n    - Make batch size for maximum retention in spend logs controllable - [PR](https://github.com/BerriAI/litellm/pull/11459)\\n    - Expose flag to disable token counter - [PR](https://github.com/BerriAI/litellm/pull/11344)\\n    - Support pipeline redis lpop for older redis versions - [PR](https://github.com/BerriAI/litellm/pull/11425)\\n---\\n\\n## Bug Fixes\\n\\n- **LLM API Fixes**\\n    - **Anthropic**: Fix regression when passing file url\'s to the \'file_id\' parameter - [PR](https://github.com/BerriAI/litellm/pull/11387)\\n    - **Vertex AI**: Fix Vertex AI any_of issues for Description and Default. - [PR](https://github.com/BerriAI/litellm/issues/11383) \\n    - Fix transcription model name mapping - [PR](https://github.com/BerriAI/litellm/pull/11333)\\n    - **Image Generation**: Fix None values in usage field for gpt-image-1 model responses - [PR](https://github.com/BerriAI/litellm/pull/11448)\\n    - **Responses API**: Fix _transform_responses_api_content_to_chat_completion_content doesn\'t support file content type - [PR](https://github.com/BerriAI/litellm/pull/11494)\\n    - **Fireworks AI**: Fix rate limit exception mapping - detect \\"rate limit\\" text in error messages - [PR](https://github.com/BerriAI/litellm/pull/11455)\\n- **Spend Tracking/Budgets**\\n    - Respect user_header_name property for budget selection and user identification - [PR](https://github.com/BerriAI/litellm/pull/11419)\\n- **MCP Server**\\n    - Remove duplicate server_id MCP config servers - [PR](https://github.com/BerriAI/litellm/pull/11327)\\n- **Function Calling**\\n    - supports_function_calling works with llm_proxy models - [PR](https://github.com/BerriAI/litellm/pull/11381)\\n- **Knowledge Base**\\n    - Fixed Knowledge Base Call returning error - [PR](https://github.com/BerriAI/litellm/pull/11467)\\n\\n---\\n\\n## New Contributors\\n* [@mjnitz02](https://github.com/mjnitz02) made their first contribution in [#10385](https://github.com/BerriAI/litellm/pull/10385)\\n* [@hagan](https://github.com/hagan) made their first contribution in [#10479](https://github.com/BerriAI/litellm/pull/10479)\\n* [@wwells](https://github.com/wwells) made their first contribution in [#11409](https://github.com/BerriAI/litellm/pull/11409)\\n* [@likweitan](https://github.com/likweitan) made their first contribution in [#11400](https://github.com/BerriAI/litellm/pull/11400)\\n* [@raz-alon](https://github.com/raz-alon) made their first contribution in [#10102](https://github.com/BerriAI/litellm/pull/10102)\\n* [@jtsai-quid](https://github.com/jtsai-quid) made their first contribution in [#11394](https://github.com/BerriAI/litellm/pull/11394)\\n* [@tmbo](https://github.com/tmbo) made their first contribution in [#11362](https://github.com/BerriAI/litellm/pull/11362)\\n* [@wangsha](https://github.com/wangsha) made their first contribution in [#11351](https://github.com/BerriAI/litellm/pull/11351)\\n* [@seankwalker](https://github.com/seankwalker) made their first contribution in [#11452](https://github.com/BerriAI/litellm/pull/11452)\\n* [@pazevedo-hyland](https://github.com/pazevedo-hyland) made their first contribution in [#11381](https://github.com/BerriAI/litellm/pull/11381)\\n* [@cainiaoit](https://github.com/cainiaoit) made their first contribution in [#11438](https://github.com/BerriAI/litellm/pull/11438)\\n* [@vuanhtu52](https://github.com/vuanhtu52) made their first contribution in [#11508](https://github.com/BerriAI/litellm/pull/11508)\\n\\n---\\n\\n## Demo Instance\\n\\nHere\'s a Demo Instance to test changes:\\n\\n- Instance: https://demo.litellm.ai/\\n- Login Credentials:\\n    - Username: admin\\n    - Password: sk-1234\\n\\n## [Git Diff](https://github.com/BerriAI/litellm/releases/tag/v1.72.2-stable)"},{"id":"v1-72-0-stable","metadata":{"permalink":"/release_notes/v1-72-0-stable","source":"@site/release_notes/v1.72.0-stable/index.md","title":"v1.72.0-stable","description":"Deploy this version","date":"2025-05-31T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.72.0-stable","slug":"v1-72-0-stable","date":"2025-05-31T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.72.2-stable","permalink":"/release_notes/v1-72-2-stable"},"nextItem":{"title":"v1.71.1-stable - 2x Higher Requests Per Second (RPS)","permalink":"/release_notes/v1.71.1-stable"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run\\n-e STORE_MODEL_IN_DB=True\\n-p 4000:4000\\ndocker.litellm.ai/berriai/litellm:main-v1.72.0-stable\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.72.0\\n```\\n</TabItem>\\n</Tabs>\\n\\n\\n## Key Highlights\\n\\nLiteLLM v1.72.0-stable.rc is live now. Here are the key highlights of this release:\\n\\n- **Vector Store Permissions**: Control Vector Store access at the Key, Team, and Organization level.\\n- **Rate Limiting Sliding Window support**: Improved accuracy for Key/Team/User rate limits with request tracking across minutes.\\n- **Aiohttp Transport used by default**: Aiohttp transport is now the default transport for LiteLLM networking requests. This gives users 2x higher RPS per instance with a 40ms median latency overhead.\\n- **Bedrock Agents**: Call Bedrock Agents with `/chat/completions`, `/response` endpoints.\\n- **Anthropic File API**: Upload and analyze CSV files with Claude-4 on Anthropic via LiteLLM.\\n- **Prometheus**: End users (`end_user`) will no longer be tracked by default on Prometheus. Tracking end_users on prometheus is now opt-in. This is done to prevent the response from `/metrics` from  becoming too large. [Read More](../../docs/proxy/prometheus#tracking-end_user-on-prometheus)\\n\\n\\n---\\n\\n## Vector Store Permissions\\n\\nThis release brings support for managing permissions for vector stores by Keys, Teams, Organizations (entities) on LiteLLM. When a request attempts to query a vector store, LiteLLM will block it if the requesting entity lacks the proper permissions.\\n\\nThis is great for use cases that require access to restricted data that you don\'t want everyone to use. \\n\\nOver the next week we plan on adding permission management for MCP Servers.\\n\\n---\\n## Aiohttp Transport used by default\\n\\nAiohttp transport is now the default transport for LiteLLM networking requests. This gives users 2x higher RPS per instance with a 40ms median latency overhead. This has been live on LiteLLM Cloud for a week + gone through alpha users testing for a week.\\n\\n\\nIf you encounter any issues, you can disable using the aiohttp transport in the following ways:\\n\\n**On LiteLLM Proxy**\\n\\nSet the `DISABLE_AIOHTTP_TRANSPORT=True` in the environment variables. \\n\\n```yaml showLineNumbers title=\\"Environment Variable\\"\\nexport DISABLE_AIOHTTP_TRANSPORT=\\"True\\"\\n```\\n\\n**On LiteLLM Python SDK**\\n\\nSet the `disable_aiohttp_transport=True` to disable aiohttp transport. \\n\\n```python showLineNumbers title=\\"Python SDK\\"\\nimport litellm\\n\\nlitellm.disable_aiohttp_transport = True # default is False, enable this to disable aiohttp transport\\nresult = litellm.completion(\\n    model=\\"openai/gpt-4o\\",\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": \\"Hello, world!\\"}],\\n)\\nprint(result)\\n```\\n\\n---\\n\\n\\n## New Models / Updated Models\\n\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Video support for Bedrock Converse - [PR](https://github.com/BerriAI/litellm/pull/11166)\\n    - InvokeAgents support as /chat/completions route - [PR](https://github.com/BerriAI/litellm/pull/11239), [Get Started](../../docs/providers/bedrock_agents)\\n    - AI21 Jamba models compatibility fixes - [PR](https://github.com/BerriAI/litellm/pull/11233)\\n    - Fixed duplicate maxTokens parameter for Claude with thinking - [PR](https://github.com/BerriAI/litellm/pull/11181)\\n- **[Gemini (Google AI Studio + Vertex AI)](https://docs.litellm.ai/docs/providers/gemini)**\\n    - Parallel tool calling support with `parallel_tool_calls` parameter - [PR](https://github.com/BerriAI/litellm/pull/11125)\\n    - All Gemini models now support parallel function calling - [PR](https://github.com/BerriAI/litellm/pull/11225)\\n- **[VertexAI](../../docs/providers/vertex)**\\n    - codeExecution tool support and anyOf handling - [PR](https://github.com/BerriAI/litellm/pull/11195)\\n    - Vertex AI Anthropic support on /v1/messages - [PR](https://github.com/BerriAI/litellm/pull/11246)\\n    - Thinking, global regions, and parallel tool calling improvements - [PR](https://github.com/BerriAI/litellm/pull/11194)\\n    - Web Search Support [PR](https://github.com/BerriAI/litellm/commit/06484f6e5a7a2f4e45c490266782ed28b51b7db6)\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Thinking blocks on streaming support - [PR](https://github.com/BerriAI/litellm/pull/11194)\\n    - Files API with form-data support on passthrough - [PR](https://github.com/BerriAI/litellm/pull/11256)\\n    - File ID support on /chat/completion - [PR](https://github.com/BerriAI/litellm/pull/11256)\\n- **[xAI](../../docs/providers/xai)**\\n    - Web Search Support [PR](https://github.com/BerriAI/litellm/commit/06484f6e5a7a2f4e45c490266782ed28b51b7db6)\\n- **[Google AI Studio](../../docs/providers/gemini)**\\n    - Web Search Support [PR](https://github.com/BerriAI/litellm/commit/06484f6e5a7a2f4e45c490266782ed28b51b7db6)\\n- **[Mistral](../../docs/providers/mistral)**\\n    - Updated mistral-medium prices and context sizes - [PR](https://github.com/BerriAI/litellm/pull/10729)\\n- **[Ollama](../../docs/providers/ollama)**\\n    - Tool calls parsing on streaming - [PR](https://github.com/BerriAI/litellm/pull/11171)\\n- **[Cohere](../../docs/providers/cohere)**\\n    - Swapped Cohere and Cohere Chat provider positioning - [PR](https://github.com/BerriAI/litellm/pull/11173)\\n- **[Nebius AI Studio](../../docs/providers/nebius)**\\n    - New provider integration - [PR](https://github.com/BerriAI/litellm/pull/11143)\\n\\n## LLM API Endpoints\\n\\n- **[Image Edits API](../../docs/image_generation)**\\n    - Azure support for /v1/images/edits - [PR](https://github.com/BerriAI/litellm/pull/11160)\\n    - Cost tracking for image edits endpoint (OpenAI, Azure) - [PR](https://github.com/BerriAI/litellm/pull/11186)\\n- **[Completions API](../../docs/completion/chat)**\\n    - Codestral latency overhead tracking on /v1/completions - [PR](https://github.com/BerriAI/litellm/pull/10879)\\n- **[Audio Transcriptions API](../../docs/audio/speech)**\\n    - GPT-4o mini audio preview pricing without date - [PR](https://github.com/BerriAI/litellm/pull/11207)\\n    - Non-default params support for audio transcription - [PR](https://github.com/BerriAI/litellm/pull/11212)\\n- **[Responses API](../../docs/response_api)**\\n    - Session management fixes for using Non-OpenAI models - [PR](https://github.com/BerriAI/litellm/pull/11254)\\n\\n## Management Endpoints / UI\\n\\n- **Vector Stores**\\n    - Permission management for LiteLLM Keys, Teams, and Organizations - [PR](https://github.com/BerriAI/litellm/pull/11213)\\n    - UI display of vector store permissions - [PR](https://github.com/BerriAI/litellm/pull/11277)\\n    - Vector store access controls enforcement - [PR](https://github.com/BerriAI/litellm/pull/11281)\\n    - Object permissions fixes and QA improvements - [PR](https://github.com/BerriAI/litellm/pull/11291)\\n- **Teams**\\n    - \\"All proxy models\\" display when no models selected - [PR](https://github.com/BerriAI/litellm/pull/11187)\\n    - Removed redundant teamInfo call, using existing teamsList - [PR](https://github.com/BerriAI/litellm/pull/11051)\\n    - Improved model tags display on Keys, Teams and Org pages - [PR](https://github.com/BerriAI/litellm/pull/11022)\\n- **SSO/SCIM**\\n    - Bug fixes for showing SCIM token on UI - [PR](https://github.com/BerriAI/litellm/pull/11220)\\n- **General UI**\\n    - Fix \\"UI Session Expired. Logging out\\" - [PR](https://github.com/BerriAI/litellm/pull/11279)\\n    - Support for forwarding /sso/key/generate to server root path URL - [PR](https://github.com/BerriAI/litellm/pull/11165)\\n\\n\\n## Logging / Guardrails Integrations\\n\\n#### Logging\\n- **[Prometheus](../../docs/proxy/prometheus)**\\n    - End users will no longer be tracked by default on Prometheus. Tracking end_users on prometheus is now opt-in. [PR](https://github.com/BerriAI/litellm/pull/11192)\\n- **[Langfuse](../../docs/proxy/logging#langfuse)**\\n    - Performance improvements: Fixed \\"Max langfuse clients reached\\" issue - [PR](https://github.com/BerriAI/litellm/pull/11285)\\n- **[Helicone](../../docs/observability/helicone_integration)**\\n    - Base URL support - [PR](https://github.com/BerriAI/litellm/pull/11211)\\n- **[Sentry](../../docs/proxy/logging#sentry)**\\n    - Added sentry sample rate configuration - [PR](https://github.com/BerriAI/litellm/pull/10283)\\n\\n#### Guardrails\\n- **[Bedrock Guardrails](../../docs/proxy/guardrails/bedrock)**\\n    - Streaming support for bedrock post guard - [PR](https://github.com/BerriAI/litellm/pull/11247)\\n    - Auth parameter persistence fixes - [PR](https://github.com/BerriAI/litellm/pull/11270)\\n- **[Pangea Guardrails](../../docs/proxy/guardrails/pangea)**\\n    - Added Pangea provider to Guardrails hook - [PR](https://github.com/BerriAI/litellm/pull/10775)\\n\\n\\n## Performance / Reliability Improvements\\n- **aiohttp Transport**\\n    - Handling for aiohttp.ClientPayloadError - [PR](https://github.com/BerriAI/litellm/pull/11162)\\n    - SSL verification settings support - [PR](https://github.com/BerriAI/litellm/pull/11162)\\n    - Rollback to httpx==0.27.0 for stability - [PR](https://github.com/BerriAI/litellm/pull/11146)\\n- **Request Limiting**\\n    - Sliding window logic for parallel request limiter v2 - [PR](https://github.com/BerriAI/litellm/pull/11283)\\n\\n\\n## Bug Fixes\\n\\n- **LLM API Fixes**\\n    - Added missing request_kwargs to get_available_deployment call - [PR](https://github.com/BerriAI/litellm/pull/11202)\\n    - Fixed calling Azure O-series models - [PR](https://github.com/BerriAI/litellm/pull/11212)\\n    - Support for dropping non-OpenAI params via additional_drop_params - [PR](https://github.com/BerriAI/litellm/pull/11246)\\n    - Fixed frequency_penalty to repeat_penalty parameter mapping - [PR](https://github.com/BerriAI/litellm/pull/11284)\\n    - Fix for embedding cache hits on string input - [PR](https://github.com/BerriAI/litellm/pull/11211)\\n- **General**\\n    - OIDC provider improvements and audience bug fix - [PR](https://github.com/BerriAI/litellm/pull/10054)\\n    - Removed AzureCredentialType restriction on AZURE_CREDENTIAL - [PR](https://github.com/BerriAI/litellm/pull/11272)\\n    - Prevention of sensitive key leakage to Langfuse - [PR](https://github.com/BerriAI/litellm/pull/11165)\\n    - Fixed healthcheck test using curl when curl not in image - [PR](https://github.com/BerriAI/litellm/pull/9737)\\n\\n## New Contributors\\n* [@agajdosi](https://github.com/agajdosi) made their first contribution in [#9737](https://github.com/BerriAI/litellm/pull/9737)\\n* [@ketangangal](https://github.com/ketangangal) made their first contribution in [#11161](https://github.com/BerriAI/litellm/pull/11161)\\n* [@Aktsvigun](https://github.com/Aktsvigun) made their first contribution in [#11143](https://github.com/BerriAI/litellm/pull/11143)\\n* [@ryanmeans](https://github.com/ryanmeans) made their first contribution in [#10775](https://github.com/BerriAI/litellm/pull/10775)\\n* [@nikoizs](https://github.com/nikoizs) made their first contribution in [#10054](https://github.com/BerriAI/litellm/pull/10054)\\n* [@Nitro963](https://github.com/Nitro963) made their first contribution in [#11202](https://github.com/BerriAI/litellm/pull/11202)\\n* [@Jacobh2](https://github.com/Jacobh2) made their first contribution in [#11207](https://github.com/BerriAI/litellm/pull/11207)\\n* [@regismesquita](https://github.com/regismesquita) made their first contribution in [#10729](https://github.com/BerriAI/litellm/pull/10729)\\n* [@Vinnie-Singleton-NN](https://github.com/Vinnie-Singleton-NN) made their first contribution in [#10283](https://github.com/BerriAI/litellm/pull/10283)\\n* [@trashhalo](https://github.com/trashhalo) made their first contribution in [#11219](https://github.com/BerriAI/litellm/pull/11219)\\n* [@VigneshwarRajasekaran](https://github.com/VigneshwarRajasekaran) made their first contribution in [#11223](https://github.com/BerriAI/litellm/pull/11223)\\n* [@AnilAren](https://github.com/AnilAren) made their first contribution in [#11233](https://github.com/BerriAI/litellm/pull/11233)\\n* [@fadil4u](https://github.com/fadil4u) made their first contribution in [#11242](https://github.com/BerriAI/litellm/pull/11242)\\n* [@whitfin](https://github.com/whitfin) made their first contribution in [#11279](https://github.com/BerriAI/litellm/pull/11279)\\n* [@hcoona](https://github.com/hcoona) made their first contribution in [#11272](https://github.com/BerriAI/litellm/pull/11272)\\n* [@keyute](https://github.com/keyute) made their first contribution in [#11173](https://github.com/BerriAI/litellm/pull/11173)\\n* [@emmanuel-ferdman](https://github.com/emmanuel-ferdman) made their first contribution in [#11230](https://github.com/BerriAI/litellm/pull/11230)\\n\\n## Demo Instance\\n\\nHere\'s a Demo Instance to test changes:\\n\\n- Instance: https://demo.litellm.ai/\\n- Login Credentials:\\n    - Username: admin\\n    - Password: sk-1234\\n\\n## [Git Diff](https://github.com/BerriAI/litellm/releases)"},{"id":"v1.71.1-stable","metadata":{"permalink":"/release_notes/v1.71.1-stable","source":"@site/release_notes/v1.71.1-stable/index.md","title":"v1.71.1-stable - 2x Higher Requests Per Second (RPS)","description":"Deploy this version","date":"2025-05-24T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.71.1-stable - 2x Higher Requests Per Second (RPS)","slug":"v1.71.1-stable","date":"2025-05-24T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.72.0-stable","permalink":"/release_notes/v1-72-0-stable"},"nextItem":{"title":"v1.70.1-stable - Gemini Realtime API Support","permalink":"/release_notes/v1.70.1-stable"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run\\n-e STORE_MODEL_IN_DB=True\\n-p 4000:4000\\ndocker.litellm.ai/berriai/litellm:main-v1.71.1-stable\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.71.1\\n```\\n</TabItem>\\n</Tabs>\\n\\n## Key Highlights\\n\\nLiteLLM v1.71.1-stable is live now. Here are the key highlights of this release:\\n\\n- **Performance improvements**: LiteLLM can now scale to 200 RPS per instance with a 74ms median response time. \\n- **File Permissions**:  Control file access across OpenAI, Azure, VertexAI. \\n- **MCP x OpenAI**: Use MCP servers with OpenAI Responses API.\\n\\n\\n\\n## Performance Improvements\\n\\n<Image img={require(\'../../img/perf_imp.png\')}  style={{ width: \'800px\', height: \'auto\' }} />\\n\\n<br/>\\n\\n\\nThis release brings aiohttp support for all LLM api providers. This means that LiteLLM can now scale to 200 RPS per instance with a 40ms median latency overhead. \\n\\nThis change doubles the RPS LiteLLM can scale to at this latency overhead.\\n\\nYou can opt into this by enabling the flag below. (We expect to make this the default in 1 week.)\\n\\n\\n### Flag to enable\\n\\n**On LiteLLM Proxy**\\n\\nSet the `USE_AIOHTTP_TRANSPORT=True` in the environment variables. \\n\\n```yaml showLineNumbers title=\\"Environment Variable\\"\\nexport USE_AIOHTTP_TRANSPORT=\\"True\\"\\n```\\n\\n**On LiteLLM Python SDK**\\n\\nSet the `use_aiohttp_transport=True` to enable aiohttp transport. \\n\\n```python showLineNumbers title=\\"Python SDK\\"\\nimport litellm\\n\\nlitellm.use_aiohttp_transport = True # default is False, enable this to use aiohttp transport\\nresult = litellm.completion(\\n    model=\\"openai/gpt-4o\\",\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": \\"Hello, world!\\"}],\\n)\\nprint(result)\\n```\\n\\n## File Permissions\\n\\n<Image img={require(\'../../img/files_api_graphic.png\')}  style={{ width: \'800px\', height: \'auto\' }} />\\n\\n<br/>\\n\\nThis release brings support for [File Permissions](../../docs/proxy/litellm_managed_files#file-permissions) and [Finetuning APIs](../../docs/proxy/managed_finetuning) to [LiteLLM Managed Files](../../docs/proxy/litellm_managed_files). This is great for: \\n\\n- **Proxy Admins**: as users can only view/edit/delete files they\u2019ve created - even when using shared OpenAI/Azure/Vertex deployments.\\n- **Developers**: get a standard interface to use Files across Chat/Finetuning/Batch APIs.\\n\\n\\n## New Models / Updated Models\\n\\n- **Gemini [VertexAI](https://docs.litellm.ai/docs/providers/vertex), [Google AI Studio](https://docs.litellm.ai/docs/providers/gemini)**\\n    - New gemini models - [PR 1](https://github.com/BerriAI/litellm/pull/10991), [PR 2](https://github.com/BerriAI/litellm/pull/10998)\\n        - `gemini-2.5-flash-preview-tts`\\n        - `gemini-2.0-flash-preview-image-generation`\\n        - `gemini/gemini-2.5-flash-preview-05-20`\\n        - `gemini-2.5-flash-preview-05-20`\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - Claude-4 model family support - [PR](https://github.com/BerriAI/litellm/pull/11060)\\n- **[Bedrock](../../docs/providers/bedrock)**\\n    - Claude-4 model family support - [PR](https://github.com/BerriAI/litellm/pull/11060)\\n    - Support for `reasoning_effort` and `thinking` parameters for Claude-4 - [PR](https://github.com/BerriAI/litellm/pull/11114)\\n- **[VertexAI](../../docs/providers/vertex)**\\n    - Claude-4 model family support - [PR](https://github.com/BerriAI/litellm/pull/11060)\\n    - Global endpoints support - [PR](https://github.com/BerriAI/litellm/pull/10658)\\n    - authorized_user credentials type support - [PR](https://github.com/BerriAI/litellm/pull/10899)\\n- **[xAI](../../docs/providers/xai)**\\n    - `xai/grok-3` pricing information - [PR](https://github.com/BerriAI/litellm/pull/11028)\\n- **[LM Studio](../../docs/providers/lm_studio)**\\n    - Structured JSON schema outputs support - [PR](https://github.com/BerriAI/litellm/pull/10929)\\n- **[SambaNova](../../docs/providers/sambanova)**\\n    - Updated models and parameters - [PR](https://github.com/BerriAI/litellm/pull/10900)\\n- **[Databricks](../../docs/providers/databricks)**\\n    - Llama 4 Maverick model cost - [PR](https://github.com/BerriAI/litellm/pull/11008)\\n    - Claude 3.7 Sonnet output token cost correction - [PR](https://github.com/BerriAI/litellm/pull/11007)\\n- **[Azure](../../docs/providers/azure)**\\n    - Mistral Medium 25.05 support - [PR](https://github.com/BerriAI/litellm/pull/11063)\\n    - Certificate-based authentication support - [PR](https://github.com/BerriAI/litellm/pull/11069)\\n- **[Mistral](../../docs/providers/mistral)**\\n    - devstral-small-2505 model pricing and context window - [PR](https://github.com/BerriAI/litellm/pull/11103)\\n- **[Ollama](../../docs/providers/ollama)**\\n    - Wildcard model support - [PR](https://github.com/BerriAI/litellm/pull/10982)\\n- **[CustomLLM](../../docs/providers/custom_llm_server)**\\n    - Embeddings support added - [PR](https://github.com/BerriAI/litellm/pull/10980)\\n- **[Featherless AI](../../docs/providers/featherless_ai)**\\n    - Access to 4200+ models - [PR](https://github.com/BerriAI/litellm/pull/10596)\\n\\n## LLM API Endpoints\\n\\n- **[Image Edits](../../docs/image_generation)**\\n    - `/v1/images/edits` - Support for /images/edits endpoint - [PR](https://github.com/BerriAI/litellm/pull/11020) [PR](https://github.com/BerriAI/litellm/pull/11123)\\n    - Content policy violation error mapping - [PR](https://github.com/BerriAI/litellm/pull/11113)\\n- **[Responses API](../../docs/response_api)**\\n    - MCP support for Responses API - [PR](https://github.com/BerriAI/litellm/pull/11029)\\n- **[Files API](../../docs/fine_tuning)**\\n    - LiteLLM Managed Files support for finetuning - [PR](https://github.com/BerriAI/litellm/pull/11039) [PR](https://github.com/BerriAI/litellm/pull/11040)\\n    - Validation for file operations (retrieve/list/delete) - [PR](https://github.com/BerriAI/litellm/pull/11081)\\n\\n## Management Endpoints / UI\\n\\n- **Teams**\\n    - Key and member count display - [PR](https://github.com/BerriAI/litellm/pull/10950)\\n    - Spend rounded to 4 decimal points - [PR](https://github.com/BerriAI/litellm/pull/11013)\\n    - Organization and team create buttons repositioned - [PR](https://github.com/BerriAI/litellm/pull/10948)\\n- **Keys**\\n    - Key reassignment and \'updated at\' column - [PR](https://github.com/BerriAI/litellm/pull/10960)\\n    - Show model access groups during creation - [PR](https://github.com/BerriAI/litellm/pull/10965)\\n- **Logs**\\n    - Model filter on logs - [PR](https://github.com/BerriAI/litellm/pull/11048)\\n    - Passthrough endpoint error logs support - [PR](https://github.com/BerriAI/litellm/pull/10990)\\n- **Guardrails**\\n    - Config.yaml guardrails display - [PR](https://github.com/BerriAI/litellm/pull/10959)\\n- **Organizations/Users**\\n    - Spend rounded to 4 decimal points - [PR](https://github.com/BerriAI/litellm/pull/11023)\\n    - Show clear error when adding a user to a team - [PR](https://github.com/BerriAI/litellm/pull/10978)\\n- **Audit Logs**\\n    - `/list` and `/info` endpoints for Audit Logs - [PR](https://github.com/BerriAI/litellm/pull/11102)\\n\\n## Logging / Alerting Integrations\\n\\n- **[Prometheus](../../docs/proxy/prometheus)**\\n    - Track `route` on proxy_* metrics - [PR](https://github.com/BerriAI/litellm/pull/10992)\\n- **[Langfuse](../../docs/proxy/logging#langfuse)**\\n    - Support for `prompt_label` parameter - [PR](https://github.com/BerriAI/litellm/pull/11018)\\n    - Consistent modelParams logging - [PR](https://github.com/BerriAI/litellm/pull/11018)\\n- **[DeepEval/ConfidentAI](../../docs/proxy/logging#deepeval)**\\n    - Logging enabled for proxy and SDK - [PR](https://github.com/BerriAI/litellm/pull/10649)\\n- **[Logfire](../../docs/proxy/logging)**\\n    - Fix otel proxy server initialization when using Logfire - [PR](https://github.com/BerriAI/litellm/pull/11091)\\n\\n## Authentication & Security\\n\\n- **[JWT Authentication](../../docs/proxy/token_auth)**\\n    - Support for applying default internal user parameters when upserting a user via JWT authentication - [PR](https://github.com/BerriAI/litellm/pull/10995)\\n    - Map a user to a team when upserting a user via JWT authentication - [PR](https://github.com/BerriAI/litellm/pull/11108)\\n- **Custom Auth**\\n    - Support for switching between custom auth and API key auth - [PR](https://github.com/BerriAI/litellm/pull/11070)\\n\\n## Performance / Reliability Improvements\\n\\n- **aiohttp Transport**\\n    - 97% lower median latency (feature flagged) - [PR](https://github.com/BerriAI/litellm/pull/11097) [PR](https://github.com/BerriAI/litellm/pull/11132)\\n- **Background Health Checks**\\n    - Improved reliability - [PR](https://github.com/BerriAI/litellm/pull/10887)\\n- **Response Handling**\\n    - Better streaming status code detection - [PR](https://github.com/BerriAI/litellm/pull/10962)\\n    - Response ID propagation improvements - [PR](https://github.com/BerriAI/litellm/pull/11006)\\n- **Thread Management**\\n    - Removed error-creating threads for reliability - [PR](https://github.com/BerriAI/litellm/pull/11066)\\n\\n## General Proxy Improvements\\n\\n- **[Proxy CLI](../../docs/proxy/cli)**\\n    - Skip server startup flag - [PR](https://github.com/BerriAI/litellm/pull/10665)\\n    - Avoid DATABASE_URL override when provided - [PR](https://github.com/BerriAI/litellm/pull/11076)\\n- **Model Management**\\n    - Clear cache and reload after model updates - [PR](https://github.com/BerriAI/litellm/pull/10853)\\n    - Computer use support tracking - [PR](https://github.com/BerriAI/litellm/pull/10881)\\n- **Helm Chart**\\n    - LoadBalancer class support - [PR](https://github.com/BerriAI/litellm/pull/11064)\\n\\n## Bug Fixes\\n\\nThis release includes numerous bug fixes to improve stability and reliability:\\n\\n- **LLM Provider Fixes**\\n    - VertexAI: \\n        - Fixed quota_project_id parameter issue - [PR](https://github.com/BerriAI/litellm/pull/10915)\\n        - Fixed credential refresh exceptions - [PR](https://github.com/BerriAI/litellm/pull/10969)\\n    - Cohere: \\n        Fixes for adding Cohere models through LiteLLM UI - [PR](https://github.com/BerriAI/litellm/pull/10822)\\n    - Anthropic: \\n        - Fixed streaming dict object handling for /v1/messages - [PR](https://github.com/BerriAI/litellm/pull/11032)\\n    - OpenRouter: \\n        - Fixed stream usage ID issues - [PR](https://github.com/BerriAI/litellm/pull/11004)\\n\\n- **Authentication & Users**\\n    - Fixed invitation email link generation - [PR](https://github.com/BerriAI/litellm/pull/10958) \\n    - Fixed JWT authentication default role - [PR](https://github.com/BerriAI/litellm/pull/10995)\\n    - Fixed user budget reset functionality - [PR](https://github.com/BerriAI/litellm/pull/10993)\\n    - Fixed SSO user compatibility and email validation - [PR](https://github.com/BerriAI/litellm/pull/11106)\\n\\n- **Database & Infrastructure**\\n    - Fixed DB connection parameter handling - [PR](https://github.com/BerriAI/litellm/pull/10842)\\n    - Fixed email invitation link  - [PR](https://github.com/BerriAI/litellm/pull/11031)\\n\\n- **UI & Display**\\n    - Fixed MCP tool rendering when no arguments required - [PR](https://github.com/BerriAI/litellm/pull/11012)\\n    - Fixed team model alias deletion - [PR](https://github.com/BerriAI/litellm/pull/11121)\\n    - Fixed team viewer permissions - [PR](https://github.com/BerriAI/litellm/pull/11127)\\n\\n- **Model & Routing**\\n    - Fixed team model mapping in route requests - [PR](https://github.com/BerriAI/litellm/pull/11111)\\n    - Fixed standard optional parameter passing - [PR](https://github.com/BerriAI/litellm/pull/11124)\\n\\n\\n## New Contributors\\n* [@DarinVerheijke](https://github.com/DarinVerheijke) made their first contribution in PR [#10596](https://github.com/BerriAI/litellm/pull/10596)\\n* [@estsauver](https://github.com/estsauver) made their first contribution in PR [#10929](https://github.com/BerriAI/litellm/pull/10929)\\n* [@mohittalele](https://github.com/mohittalele) made their first contribution in PR [#10665](https://github.com/BerriAI/litellm/pull/10665)\\n* [@pselden](https://github.com/pselden) made their first contribution in PR [#10899](https://github.com/BerriAI/litellm/pull/10899)\\n* [@unrealandychan](https://github.com/unrealandychan) made their first contribution in PR [#10842](https://github.com/BerriAI/litellm/pull/10842)\\n* [@dastaiger](https://github.com/dastaiger) made their first contribution in PR [#10946](https://github.com/BerriAI/litellm/pull/10946)\\n* [@slytechnical](https://github.com/slytechnical) made their first contribution in PR [#10881](https://github.com/BerriAI/litellm/pull/10881)\\n* [@daarko10](https://github.com/daarko10) made their first contribution in PR [#11006](https://github.com/BerriAI/litellm/pull/11006)\\n* [@sorenmat](https://github.com/sorenmat) made their first contribution in PR [#10658](https://github.com/BerriAI/litellm/pull/10658)\\n* [@matthid](https://github.com/matthid) made their first contribution in PR [#10982](https://github.com/BerriAI/litellm/pull/10982)\\n* [@jgowdy-godaddy](https://github.com/jgowdy-godaddy) made their first contribution in PR [#11032](https://github.com/BerriAI/litellm/pull/11032)\\n* [@bepotp](https://github.com/bepotp) made their first contribution in PR [#11008](https://github.com/BerriAI/litellm/pull/11008)\\n* [@jmorenoc-o](https://github.com/jmorenoc-o) made their first contribution in PR [#11031](https://github.com/BerriAI/litellm/pull/11031)\\n* [@martin-liu](https://github.com/martin-liu) made their first contribution in PR [#11076](https://github.com/BerriAI/litellm/pull/11076)\\n* [@gunjan-solanki](https://github.com/gunjan-solanki) made their first contribution in PR [#11064](https://github.com/BerriAI/litellm/pull/11064)\\n* [@tokoko](https://github.com/tokoko) made their first contribution in PR [#10980](https://github.com/BerriAI/litellm/pull/10980)\\n* [@spike-spiegel-21](https://github.com/spike-spiegel-21) made their first contribution in PR [#10649](https://github.com/BerriAI/litellm/pull/10649)\\n* [@kreatoo](https://github.com/kreatoo) made their first contribution in PR [#10927](https://github.com/BerriAI/litellm/pull/10927)\\n* [@baejooc](https://github.com/baejooc) made their first contribution in PR [#10887](https://github.com/BerriAI/litellm/pull/10887)\\n* [@keykbd](https://github.com/keykbd) made their first contribution in PR [#11114](https://github.com/BerriAI/litellm/pull/11114)\\n* [@dalssoft](https://github.com/dalssoft) made their first contribution in PR [#11088](https://github.com/BerriAI/litellm/pull/11088)\\n* [@jtong99](https://github.com/jtong99) made their first contribution in PR [#10853](https://github.com/BerriAI/litellm/pull/10853)\\n\\n## Demo Instance\\n\\nHere\'s a Demo Instance to test changes:\\n\\n- Instance: https://demo.litellm.ai/\\n- Login Credentials:\\n    - Username: admin\\n    - Password: sk-1234\\n\\n## [Git Diff](https://github.com/BerriAI/litellm/releases)"},{"id":"v1.70.1-stable","metadata":{"permalink":"/release_notes/v1.70.1-stable","source":"@site/release_notes/v1.70.1-stable/index.md","title":"v1.70.1-stable - Gemini Realtime API Support","description":"Deploy this version","date":"2025-05-17T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.70.1-stable - Gemini Realtime API Support","slug":"v1.70.1-stable","date":"2025-05-17T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.71.1-stable - 2x Higher Requests Per Second (RPS)","permalink":"/release_notes/v1.71.1-stable"},"nextItem":{"title":"v1.69.0-stable - Loadbalance Batch API Models","permalink":"/release_notes/v1.69.0-stable"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run\\n-e STORE_MODEL_IN_DB=True\\n-p 4000:4000\\ndocker.litellm.ai/berriai/litellm:main-v1.70.1-stable\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.70.1\\n```\\n</TabItem>\\n</Tabs>\\n\\n\\n## Key Highlights\\n\\nLiteLLM v1.70.1-stable is live now. Here are the key highlights of this release:\\n\\n- **Gemini Realtime API**: You can now call Gemini\'s Live API via the OpenAI /v1/realtime API\\n- **Spend Logs Retention Period**: Enable deleting spend logs older than a certain period.\\n- **PII Masking 2.0**: Easily configure masking or blocking specific PII/PHI entities on the UI\\n\\n## Gemini Realtime API\\n\\n<Image img={require(\'../../img/gemini_realtime.png\')}/>\\n\\n\\nThis release brings support for calling Gemini\'s realtime models (e.g. gemini-2.0-flash-live) via OpenAI\'s\xa0/v1/realtime\xa0API. This is great for developers as it lets them easily switch from OpenAI to Gemini by just changing the model name. \\n\\nKey Highlights: \\n- Support for text + audio input/output\\n- Support for setting session configurations (modality, instructions, activity detection) in the OpenAI format\\n- Support for logging + usage tracking for realtime sessions\\n\\nThis is currently supported via Google AI Studio. We plan to release VertexAI support over the coming week.\\n\\n[**Read more**](../../docs/providers/google_ai_studio/realtime)\\n\\n## Spend Logs Retention Period\\n\\n<Image img={require(\'../../img/delete_spend_logs.jpg\')}/>\\n\\n\\n\\nThis release enables deleting LiteLLM Spend Logs older than a certain period. Since we now enable storing the raw request/response in the logs, deleting old logs ensures the database remains performant in production. \\n\\n[**Read more**](../../docs/proxy/spend_logs_deletion)\\n\\n## PII Masking 2.0\\n\\n<Image img={require(\'../../img/pii_masking_v2.png\')}/>\\n\\nThis release brings improvements to our Presidio PII Integration. As a Proxy Admin, you now have the ability to:\\n\\n- Mask or block specific entities (e.g., block medical licenses while masking other entities like emails).\\n- Monitor guardrails in production. LiteLLM Logs will now show you the guardrail run, the entities it detected, and its confidence score for each entity.\\n\\n[**Read more**](../../docs/proxy/guardrails/pii_masking_v2)\\n\\n## New Models / Updated Models\\n\\n- **Gemini ([VertexAI](https://docs.litellm.ai/docs/providers/vertex#usage-with-litellm-proxy-server) + [Google AI Studio](https://docs.litellm.ai/docs/providers/gemini))**\\n    - `/chat/completion`\\n        - Handle audio input - [PR](https://github.com/BerriAI/litellm/pull/10739)\\n        - Fixes maximum recursion depth issue when using deeply nested response schemas with Vertex AI by Increasing DEFAULT_MAX_RECURSE_DEPTH from 10 to 100 in constants. [PR](https://github.com/BerriAI/litellm/pull/10798)\\n        - Capture reasoning tokens in streaming mode - [PR](https://github.com/BerriAI/litellm/pull/10789)\\n- **[Google AI Studio](../../docs/providers/google_ai_studio/realtime)**\\n    - `/realtime`\\n        - Gemini Multimodal Live API support\\n        - Audio input/output support, optional param mapping, accurate usage calculation - [PR](https://github.com/BerriAI/litellm/pull/10909)\\n- **[VertexAI](../../docs/providers/vertex#metallama-api)**\\n    - `/chat/completion`\\n        - Fix llama streaming error - where model response was nested in returned streaming chunk - [PR](https://github.com/BerriAI/litellm/pull/10878)\\n- **[Ollama](../../docs/providers/ollama)**\\n    - `/chat/completion`\\n        - structure responses fix - [PR](https://github.com/BerriAI/litellm/pull/10617)\\n- **[Bedrock](../../docs/providers/bedrock#litellm-proxy-usage)**\\n    - [`/chat/completion`](../../docs/providers/bedrock#litellm-proxy-usage)\\n        - Handle thinking_blocks when assistant.content is None - [PR](https://github.com/BerriAI/litellm/pull/10688)\\n        - Fixes to only allow accepted fields for tool json schema - [PR](https://github.com/BerriAI/litellm/pull/10062)\\n        - Add bedrock sonnet prompt caching cost information\\n        - Mistral Pixtral support - [PR](https://github.com/BerriAI/litellm/pull/10439)\\n        - Tool caching support - [PR](https://github.com/BerriAI/litellm/pull/10897)\\n    - [`/messages`](../../docs/anthropic_unified)\\n        - allow using dynamic AWS Params - [PR](https://github.com/BerriAI/litellm/pull/10769)\\n- **[Nvidia NIM](../../docs/providers/nvidia_nim)**\\n    - [`/chat/completion`](../../docs/providers/nvidia_nim#usage---litellm-proxy-server)\\n        - Add tools, tool_choice, parallel_tool_calls support - [PR](https://github.com/BerriAI/litellm/pull/10763)\\n- **[Novita AI](../../docs/providers/novita)**\\n    - New Provider added for `/chat/completion` routes - [PR](https://github.com/BerriAI/litellm/pull/9527)\\n- **[Azure](../../docs/providers/azure)**\\n    - [`/image/generation`](../../docs/providers/azure#image-generation)\\n        - Fix azure dall e 3 call with custom model name - [PR](https://github.com/BerriAI/litellm/pull/10776)\\n- **[Cohere](../../docs/providers/cohere)**\\n    - [`/embeddings`](../../docs/providers/cohere#embedding)\\n        - Migrate embedding to use `/v2/embed` - adds support for output_dimensions param - [PR](https://github.com/BerriAI/litellm/pull/10809)\\n- **[Anthropic](../../docs/providers/anthropic)**\\n    - [`/chat/completion`](../../docs/providers/anthropic#usage-with-litellm-proxy)\\n        - Web search tool support - native + openai format - [Get Started](../../docs/providers/anthropic#anthropic-hosted-tools-computer-text-editor-web-search)\\n- **[VLLM](../../docs/providers/vllm)**\\n    - [`/embeddings`](../../docs/providers/vllm#embeddings)\\n        - Support embedding input as list of integers\\n- **[OpenAI](../../docs/providers/openai)**\\n    - [`/chat/completion`](../../docs/providers/openai#usage---litellm-proxy-server)\\n        - Fix - b64 file data input handling - [Get Started](../../docs/providers/openai#pdf-file-parsing)\\n        - Add \u2018supports_pdf_input\u2019 to all vision models - [PR](https://github.com/BerriAI/litellm/pull/10897)\\n\\n## LLM API Endpoints\\n- [**Responses API**](../../docs/response_api)\\n    - Fix delete API support - [PR](https://github.com/BerriAI/litellm/pull/10845)\\n- [**Rerank API**](../../docs/rerank)\\n    - `/v2/rerank` now registered as \u2018llm_api_route\u2019 - enabling non-admins to call it - [PR](https://github.com/BerriAI/litellm/pull/10861)\\n\\n## Spend Tracking Improvements\\n- **`/chat/completion`, `/messages`**\\n    - Anthropic - web search tool cost tracking - [PR](https://github.com/BerriAI/litellm/pull/10846)\\n    - Groq - update model max tokens + cost information - [PR](https://github.com/BerriAI/litellm/pull/10077)\\n- **`/audio/transcription`**\\n    - Azure - Add gpt-4o-mini-tts pricing - [PR](https://github.com/BerriAI/litellm/pull/10807)\\n    - Proxy - Fix tracking spend by tag - [PR](https://github.com/BerriAI/litellm/pull/10832)\\n- **`/embeddings`**\\n    - Azure AI - Add cohere embed v4 pricing - [PR](https://github.com/BerriAI/litellm/pull/10806)\\n\\n## Management Endpoints / UI\\n- **Models**\\n    - Ollama - adds api base param to UI \\n- **Logs**\\n    - Add team id, key alias, key hash filter on logs - https://github.com/BerriAI/litellm/pull/10831\\n    - Guardrail tracing now in Logs UI - https://github.com/BerriAI/litellm/pull/10893\\n- **Teams**\\n    - Patch for updating team info when team in org and members not in org - https://github.com/BerriAI/litellm/pull/10835\\n- **Guardrails**\\n    - Add Bedrock, Presidio, Lakers guardrails on UI - https://github.com/BerriAI/litellm/pull/10874\\n    - See guardrail info page - https://github.com/BerriAI/litellm/pull/10904\\n    - Allow editing guardrails on UI - https://github.com/BerriAI/litellm/pull/10907\\n- **Test Key**\\n    - select guardrails to test on UI \\n\\n\\n\\n## Logging / Alerting Integrations\\n- **[StandardLoggingPayload](../../docs/proxy/logging_spec)**\\n    - Log any `x-` headers in requester metadata - [Get Started](../../docs/proxy/logging_spec#standardloggingmetadata)\\n    - Guardrail tracing now in standard logging payload - [Get Started](../../docs/proxy/logging_spec#standardloggingguardrailinformation)\\n- **[Generic API Logger](../../docs/proxy/logging#custom-callback-apis-async)**\\n    - Support passing application/json header \\n- **[Arize Phoenix](../../docs/observability/phoenix_integration)**\\n    - fix: URL encode OTEL_EXPORTER_OTLP_TRACES_HEADERS for Phoenix Integration - [PR](https://github.com/BerriAI/litellm/pull/10654)\\n    - add guardrail tracing to OTEL, Arize phoenix - [PR](https://github.com/BerriAI/litellm/pull/10896)\\n- **[PagerDuty](../../docs/proxy/pagerduty)**\\n    - Pagerduty is now a free feature - [PR](https://github.com/BerriAI/litellm/pull/10857)\\n- **[Alerting](../../docs/proxy/alerting)**\\n    - Sending slack alerts on virtual key/user/team updates is now free - [PR](https://github.com/BerriAI/litellm/pull/10863)\\n\\n\\n## Guardrails\\n- **Guardrails**\\n    - New `/apply_guardrail` endpoint for directly testing a guardrail - [PR](https://github.com/BerriAI/litellm/pull/10867)\\n- **[Lakera](../../docs/proxy/guardrails/lakera_ai)**\\n    - `/v2` endpoints support - [PR](https://github.com/BerriAI/litellm/pull/10880)\\n- **[Presidio](../../docs/proxy/guardrails/pii_masking_v2)**\\n    - Fixes handling of message content on presidio guardrail integration - [PR](https://github.com/BerriAI/litellm/pull/10197)\\n    - Allow specifying PII Entities Config - [PR](https://github.com/BerriAI/litellm/pull/10810)\\n- **[Aim Security](../../docs/proxy/guardrails/aim_security)**\\n    - Support for anonymization in AIM Guardrails - [PR](https://github.com/BerriAI/litellm/pull/10757)\\n\\n\\n\\n## Performance / Loadbalancing / Reliability improvements\\n- **Allow overriding all constants using a .env variable** - [PR](https://github.com/BerriAI/litellm/pull/10803)\\n- **[Maximum retention period for spend logs](../../docs/proxy/spend_logs_deletion)**\\n    - Add retention flag to config - [PR](https://github.com/BerriAI/litellm/pull/10815)\\n    - Support for cleaning up logs based on configured time period - [PR](https://github.com/BerriAI/litellm/pull/10872)\\n\\n## General Proxy Improvements\\n- **Authentication**\\n    - Handle\xa0Bearer $LITELLM_API_KEY\xa0in\xa0x-litellm-api-key\xa0custom header [PR](https://github.com/BerriAI/litellm/pull/10776)\\n- **New Enterprise pip package** - `litellm-enterprise` - fixes issue where `enterprise` folder was not found when using pip package  \\n- **[Proxy CLI](../../docs/proxy/management_cli)**\\n    - Add `models import` command - [PR](https://github.com/BerriAI/litellm/pull/10581)\\n- **[OpenWebUI](../../docs/tutorials/openweb_ui#per-user-tracking)**\\n    - Configure LiteLLM to Parse User Headers from Open Web UI\\n- **[LiteLLM Proxy w/ LiteLLM SDK](../../docs/providers/litellm_proxy#send-all-sdk-requests-to-litellm-proxy)**\\n    - Option to force/always use the litellm proxy when calling via LiteLLM SDK\\n\\n\\n## New Contributors\\n* [@imdigitalashish](https://github.com/imdigitalashish) made their first contribution in PR [#10617](https://github.com/BerriAI/litellm/pull/10617)\\n* [@LouisShark](https://github.com/LouisShark) made their first contribution in PR [#10688](https://github.com/BerriAI/litellm/pull/10688)\\n* [@OscarSavNS](https://github.com/OscarSavNS) made their first contribution in PR [#10764](https://github.com/BerriAI/litellm/pull/10764)\\n* [@arizedatngo](https://github.com/arizedatngo) made their first contribution in PR [#10654](https://github.com/BerriAI/litellm/pull/10654)\\n* [@jugaldb](https://github.com/jugaldb) made their first contribution in PR [#10805](https://github.com/BerriAI/litellm/pull/10805)\\n* [@daikeren](https://github.com/daikeren) made their first contribution in PR [#10781](https://github.com/BerriAI/litellm/pull/10781)\\n* [@naliotopier](https://github.com/naliotopier) made their first contribution in PR [#10077](https://github.com/BerriAI/litellm/pull/10077)\\n* [@damienpontifex](https://github.com/damienpontifex) made their first contribution in PR [#10813](https://github.com/BerriAI/litellm/pull/10813)\\n* [@Dima-Mediator](https://github.com/Dima-Mediator) made their first contribution in PR [#10789](https://github.com/BerriAI/litellm/pull/10789)\\n* [@igtm](https://github.com/igtm) made their first contribution in PR [#10814](https://github.com/BerriAI/litellm/pull/10814)\\n* [@shibaboy](https://github.com/shibaboy) made their first contribution in PR [#10752](https://github.com/BerriAI/litellm/pull/10752)\\n* [@camfarineau](https://github.com/camfarineau) made their first contribution in PR [#10629](https://github.com/BerriAI/litellm/pull/10629)\\n* [@ajac-zero](https://github.com/ajac-zero) made their first contribution in PR [#10439](https://github.com/BerriAI/litellm/pull/10439)\\n* [@damgem](https://github.com/damgem) made their first contribution in PR [#9802](https://github.com/BerriAI/litellm/pull/9802)\\n* [@hxdror](https://github.com/hxdror) made their first contribution in PR [#10757](https://github.com/BerriAI/litellm/pull/10757)\\n* [@wwwillchen](https://github.com/wwwillchen) made their first contribution in PR [#10894](https://github.com/BerriAI/litellm/pull/10894)\\n\\n\\n## Demo Instance\\n\\nHere\'s a Demo Instance to test changes:\\n\\n- Instance: https://demo.litellm.ai/\\n- Login Credentials:\\n    - Username: admin\\n    - Password: sk-1234\\n\\n\\n## [Git Diff](https://github.com/BerriAI/litellm/releases)"},{"id":"v1.69.0-stable","metadata":{"permalink":"/release_notes/v1.69.0-stable","source":"@site/release_notes/v1.69.0-stable/index.md","title":"v1.69.0-stable - Loadbalance Batch API Models","description":"Deploy this version","date":"2025-05-10T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.69.0-stable - Loadbalance Batch API Models","slug":"v1.69.0-stable","date":"2025-05-10T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.70.1-stable - Gemini Realtime API Support","permalink":"/release_notes/v1.70.1-stable"},"nextItem":{"title":"v1.68.0-stable","permalink":"/release_notes/v1.68.0-stable"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run\\n-e STORE_MODEL_IN_DB=True\\n-p 4000:4000\\ndocker.litellm.ai/berriai/litellm:main-v1.69.0-stable\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.69.0.post1\\n```\\n</TabItem>\\n</Tabs>\\n\\n## Key Highlights\\n\\nLiteLLM v1.69.0-stable brings the following key improvements:\\n\\n- **Loadbalance Batch API Models**: Easily loadbalance across multiple azure batch deployments using LiteLLM Managed Files\\n- **Email Invites 2.0**: Send new users onboarded to LiteLLM an email invite.\\n- **Nscale**: LLM API for compliance with European regulations.\\n- **Bedrock /v1/messages**: Use Bedrock Anthropic models with Anthropic\'s /v1/messages.\\n\\n## Batch API Load Balancing\\n\\n<Image \\nimg={require(\'../../img/release_notes/lb_batch.png\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'0 0 2rem 0\'}}\\n/>\\n\\n\\nThis release brings LiteLLM Managed File support to Batches. This is great for:\\n\\n- Proxy Admins: You can now control which Batch models users can call.\\n- Developers: You no longer need to know the Azure deployment name when creating your batch .jsonl files - just specify the model your LiteLLM key has access to. \\n\\nOver time, we expect LiteLLM Managed Files to be the way most teams use Files across `/chat/completions`, `/batch`, `/fine_tuning` endpoints. \\n\\n[Read more here](https://docs.litellm.ai/docs/proxy/managed_batches)\\n\\n\\n## Email Invites\\n\\n<Image \\n  img={require(\'../../img/email_2_0.png\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'0 0 2rem 0\'}}\\n/>\\n\\nThis release brings the following improvements to our email invite integration:\\n- New templates for user invited and key created events.\\n- Fixes for using SMTP email providers.\\n- Native support for Resend API.\\n- Ability for Proxy Admins to control email events. \\n\\nFor LiteLLM Cloud Users, please reach out to us if you want this enabled for your instance. \\n\\n[Read more here](https://docs.litellm.ai/docs/proxy/email)\\n\\n\\n## New Models / Updated Models\\n- **Gemini ([VertexAI](https://docs.litellm.ai/docs/providers/vertex#usage-with-litellm-proxy-server) + [Google AI Studio](https://docs.litellm.ai/docs/providers/gemini))**\\n    - Added `gemini-2.5-pro-preview-05-06` models with pricing and context window info - [PR](https://github.com/BerriAI/litellm/pull/10597)\\n    - Set correct context window length for all Gemini 2.5 variants - [PR](https://github.com/BerriAI/litellm/pull/10690)\\n- **[Perplexity](../../docs/providers/perplexity)**: \\n    - Added new Perplexity models - [PR](https://github.com/BerriAI/litellm/pull/10652) \\n    - Added sonar-deep-research model pricing - [PR](https://github.com/BerriAI/litellm/pull/10537)\\n- **[Azure OpenAI](../../docs/providers/azure)**: \\n  - Fixed passing through of azure_ad_token_provider parameter - [PR](https://github.com/BerriAI/litellm/pull/10694)\\n- **[OpenAI](../../docs/providers/openai)**:\\n    - Added support for pdf url\'s in \'file\' parameter - [PR](https://github.com/BerriAI/litellm/pull/10640)\\n- **[Sagemaker](../../docs/providers/aws_sagemaker)**:\\n    - Fix content length for `sagemaker_chat` provider - [PR](https://github.com/BerriAI/litellm/pull/10607)\\n- **[Azure AI Foundry](../../docs/providers/azure_ai)**: \\n    - Added cost tracking for the following models [PR](https://github.com/BerriAI/litellm/pull/9956)\\n        - DeepSeek V3 0324\\n        - Llama 4 Scout\\n        - Llama 4 Maverick\\n- **[Bedrock](../../docs/providers/bedrock)**: \\n    - Added cost tracking for Bedrock Llama 4 models - [PR](https://github.com/BerriAI/litellm/pull/10582)\\n    - Fixed template conversion for Llama 4 models in Bedrock - [PR](https://github.com/BerriAI/litellm/pull/10582)\\n    - Added support for using Bedrock Anthropic models with /v1/messages format - [PR](https://github.com/BerriAI/litellm/pull/10681)\\n    - Added streaming support for Bedrock Anthropic models with /v1/messages format - [PR](https://github.com/BerriAI/litellm/pull/10710)\\n- **[OpenAI](../../docs/providers/openai)**: Added `reasoning_effort` support for `o3` models - [PR](https://github.com/BerriAI/litellm/pull/10591)\\n- **[Databricks](../../docs/providers/databricks)**:\\n    - Fixed issue when Databricks uses external model and delta could be empty - [PR](https://github.com/BerriAI/litellm/pull/10540)\\n- **[Cerebras](../../docs/providers/cerebras)**: Fixed Llama-3.1-70b model pricing and context window - [PR](https://github.com/BerriAI/litellm/pull/10648)\\n- **[Ollama](../../docs/providers/ollama)**: \\n    - Fixed custom price cost tracking and added \'max_completion_token\' support - [PR](https://github.com/BerriAI/litellm/pull/10636)\\n    - Fixed KeyError when using JSON response format - [PR](https://github.com/BerriAI/litellm/pull/10611)\\n- \ud83c\udd95 **[Nscale](../../docs/providers/nscale)**: \\n    - Added support for chat, image generation endpoints - [PR](https://github.com/BerriAI/litellm/pull/10638)\\n\\n## LLM API Endpoints\\n- **[Messages API](../../docs/anthropic_unified)**: \\n    - \ud83c\udd95 Added support for using Bedrock Anthropic models with /v1/messages format - [PR](https://github.com/BerriAI/litellm/pull/10681) and streaming support - [PR](https://github.com/BerriAI/litellm/pull/10710)\\n- **[Moderations API](../../docs/moderations)**: \\n    - Fixed bug to allow using LiteLLM UI credentials for /moderations API - [PR](https://github.com/BerriAI/litellm/pull/10723)  \\n- **[Realtime API](../../docs/realtime)**: \\n    - Fixed setting \'headers\' in scope for websocket auth requests and infinite loop issues - [PR](https://github.com/BerriAI/litellm/pull/10679)\\n- **[Files API](../../docs/proxy/litellm_managed_files)**:\\n    - Unified File ID output support - [PR](https://github.com/BerriAI/litellm/pull/10713)\\n    - Support for writing files to all deployments - [PR](https://github.com/BerriAI/litellm/pull/10708)\\n    - Added target model name validation - [PR](https://github.com/BerriAI/litellm/pull/10722)\\n- **[Batches API](../../docs/batches)**:\\n    - Complete unified batch ID support - replacing model in jsonl to be deployment model name - [PR](https://github.com/BerriAI/litellm/pull/10719)\\n  - Beta support for unified file ID (managed files) for batches - [PR](https://github.com/BerriAI/litellm/pull/10650)\\n\\n\\n## Spend Tracking / Budget Improvements\\n- Bug Fix - PostgreSQL Integer Overflow Error in DB Spend Tracking - [PR](https://github.com/BerriAI/litellm/pull/10697)\\n\\n## Management Endpoints / UI\\n- **Models**\\n    - Fixed model info overwriting when editing a model on UI - [PR](https://github.com/BerriAI/litellm/pull/10726)\\n    - Fixed team admin model updates and organization creation with specific models - [PR](https://github.com/BerriAI/litellm/pull/10539)\\n- **Logs**:\\n  - Bug Fix -  copying Request/Response on Logs Page - [PR](https://github.com/BerriAI/litellm/pull/10720)\\n  - Bug Fix -  log did not remain in focus on QA Logs page + text overflow on error logs - [PR](https://github.com/BerriAI/litellm/pull/10725)\\n  - Added index for session_id on LiteLLM_SpendLogs for better query performance - [PR](https://github.com/BerriAI/litellm/pull/10727)\\n- **User Management**:\\n  - Added user management functionality to Python client library & CLI - [PR](https://github.com/BerriAI/litellm/pull/10627)\\n  - Bug Fix - Fixed SCIM token creation on Admin UI - [PR](https://github.com/BerriAI/litellm/pull/10628)\\n  - Bug Fix - Added 404 response when trying to delete verification tokens that don\'t exist - [PR](https://github.com/BerriAI/litellm/pull/10605)\\n\\n## Logging / Guardrail Integrations\\n- **Custom Logger API**: v2 Custom Callback API (send llm logs to custom api) - [PR](https://github.com/BerriAI/litellm/pull/10575), [Get Started](https://docs.litellm.ai/docs/proxy/logging#custom-callback-apis-async)\\n- **OpenTelemetry**:\\n  - Fixed OpenTelemetry to follow genai semantic conventions + support for \'instructions\' param for TTS - [PR](https://github.com/BerriAI/litellm/pull/10608)\\n- ** Bedrock PII**:\\n  - Add support for PII Masking with bedrock guardrails - [Get Started](https://docs.litellm.ai/docs/proxy/guardrails/bedrock#pii-masking-with-bedrock-guardrails), [PR](https://github.com/BerriAI/litellm/pull/10608)\\n- **Documentation**:\\n  - Added documentation for StandardLoggingVectorStoreRequest - [PR](https://github.com/BerriAI/litellm/pull/10535)\\n\\n## Performance / Reliability Improvements\\n- **Python Compatibility**:\\n  - Added support for Python 3.11- (fixed datetime UTC handling) - [PR](https://github.com/BerriAI/litellm/pull/10701)\\n  - Fixed UnicodeDecodeError: \'charmap\' on Windows during litellm import - [PR](https://github.com/BerriAI/litellm/pull/10542)\\n- **Caching**:\\n  - Fixed embedding string caching result - [PR](https://github.com/BerriAI/litellm/pull/10700)\\n  - Fixed cache miss for Gemini models with response_format - [PR](https://github.com/BerriAI/litellm/pull/10635)\\n\\n## General Proxy Improvements\\n- **Proxy CLI**:\\n  - Added `--version` flag to `litellm-proxy` CLI - [PR](https://github.com/BerriAI/litellm/pull/10704)\\n  - Added dedicated `litellm-proxy` CLI - [PR](https://github.com/BerriAI/litellm/pull/10578)\\n- **Alerting**:\\n  - Fixed Slack alerting not working when using a DB - [PR](https://github.com/BerriAI/litellm/pull/10370)\\n- **Email Invites**:\\n  - Added V2 Emails with fixes for sending emails when creating keys + Resend API support - [PR](https://github.com/BerriAI/litellm/pull/10602)\\n  - Added user invitation emails - [PR](https://github.com/BerriAI/litellm/pull/10615)\\n  - Added endpoints to manage email settings - [PR](https://github.com/BerriAI/litellm/pull/10646)\\n- **General**:\\n  - Fixed bug where duplicate JSON logs were getting emitted - [PR](https://github.com/BerriAI/litellm/pull/10580)\\n\\n\\n## New Contributors\\n- [@zoltan-ongithub](https://github.com/zoltan-ongithub) made their first contribution in [PR #10568](https://github.com/BerriAI/litellm/pull/10568)\\n- [@mkavinkumar1](https://github.com/mkavinkumar1) made their first contribution in [PR #10548](https://github.com/BerriAI/litellm/pull/10548)\\n- [@thomelane](https://github.com/thomelane) made their first contribution in [PR #10549](https://github.com/BerriAI/litellm/pull/10549)\\n- [@frankzye](https://github.com/frankzye) made their first contribution in [PR #10540](https://github.com/BerriAI/litellm/pull/10540)\\n- [@aholmberg](https://github.com/aholmberg) made their first contribution in [PR #10591](https://github.com/BerriAI/litellm/pull/10591)\\n- [@aravindkarnam](https://github.com/aravindkarnam) made their first contribution in [PR #10611](https://github.com/BerriAI/litellm/pull/10611)\\n- [@xsg22](https://github.com/xsg22) made their first contribution in [PR #10648](https://github.com/BerriAI/litellm/pull/10648)\\n- [@casparhsws](https://github.com/casparhsws) made their first contribution in [PR #10635](https://github.com/BerriAI/litellm/pull/10635)\\n- [@hypermoose](https://github.com/hypermoose) made their first contribution in [PR #10370](https://github.com/BerriAI/litellm/pull/10370)\\n- [@tomukmatthews](https://github.com/tomukmatthews) made their first contribution in [PR #10638](https://github.com/BerriAI/litellm/pull/10638)\\n- [@keyute](https://github.com/keyute) made their first contribution in [PR #10652](https://github.com/BerriAI/litellm/pull/10652)\\n- [@GPTLocalhost](https://github.com/GPTLocalhost) made their first contribution in [PR #10687](https://github.com/BerriAI/litellm/pull/10687)\\n- [@husnain7766](https://github.com/husnain7766) made their first contribution in [PR #10697](https://github.com/BerriAI/litellm/pull/10697)\\n- [@claralp](https://github.com/claralp) made their first contribution in [PR #10694](https://github.com/BerriAI/litellm/pull/10694)\\n- [@mollux](https://github.com/mollux) made their first contribution in [PR #10690](https://github.com/BerriAI/litellm/pull/10690)"},{"id":"v1.68.0-stable","metadata":{"permalink":"/release_notes/v1.68.0-stable","source":"@site/release_notes/v1.68.0-stable/index.md","title":"v1.68.0-stable","description":"Deploy this version","date":"2025-05-03T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.68.0-stable","slug":"v1.68.0-stable","date":"2025-05-03T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.69.0-stable - Loadbalance Batch API Models","permalink":"/release_notes/v1.69.0-stable"},"nextItem":{"title":"v1.67.4-stable - Improved User Management","permalink":"/release_notes/v1.67.4-stable"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run\\n-e STORE_MODEL_IN_DB=True\\n-p 4000:4000\\ndocker.litellm.ai/berriai/litellm:main-v1.68.0-stable\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.68.0.post1\\n```\\n</TabItem>\\n</Tabs>\\n\\n## Key Highlights\\n\\nLiteLLM v1.68.0-stable will be live soon. Here are the key highlights of this release:\\n\\n- **Bedrock Knowledge Base**: You can now call query your Bedrock Knowledge Base with all LiteLLM models via `/chat/completion` or `/responses` API.\\n- **Rate Limits**: This release brings accurate rate limiting across multiple instances, reducing spillover to at most 10 additional requests in high traffic. \\n- **Meta Llama API**: Added support for Meta Llama API [Get Started](https://docs.litellm.ai/docs/providers/meta_llama)\\n- **LlamaFile**: Added support for LlamaFile [Get Started](https://docs.litellm.ai/docs/providers/llamafile)\\n\\n## Bedrock Knowledge Base (Vector Store)\\n\\n<Image img={require(\'../../img/release_notes/bedrock_kb.png\')}/>\\n<br/>\\n\\nThis release adds support for Bedrock vector stores (knowledge bases) in LiteLLM. With this update, you can:\\n\\n- Use Bedrock vector stores in the OpenAI /chat/completions spec with all LiteLLM supported models. \\n- View all available vector stores through the LiteLLM UI or API.\\n- Configure vector stores to be always active for specific models.\\n- Track vector store usage in LiteLLM Logs.\\n\\nFor the next release we plan on allowing you to set key, user, team, org permissions for vector stores. \\n\\n[Read more here](https://docs.litellm.ai/docs/completion/knowledgebase)\\n\\n## Rate Limiting\\n\\n<Image img={require(\'../../img/multi_instance_rate_limiting.png\')}/>\\n<br/>\\n\\n\\nThis release brings accurate multi-instance rate limiting across keys/users/teams. Outlining key engineering changes below:\\n\\n- **Change**: Instances now increment cache value instead of setting it. To avoid calling Redis on each request, this is synced every 0.01s.\\n- **Accuracy**: In testing, we saw a maximum spill over from expected of 10 requests, in high traffic (100 RPS, 3 instances), vs. current 189 request spillover\\n- **Performance**: Our load tests show this to reduce median response time by 100ms in high traffic\xa0\\n\\nThis is currently behind a feature flag, and we plan to have this be the default by next week. To enable this today, just add this environment variable:\\n\\n```\\nexport LITELLM_RATE_LIMIT_ACCURACY=true\\n```\\n\\n[Read more here](../../docs/proxy/users#beta-multi-instance-rate-limiting) \\n\\n\\n\\n## New Models / Updated Models\\n- **Gemini ([VertexAI](https://docs.litellm.ai/docs/providers/vertex#usage-with-litellm-proxy-server) + [Google AI Studio](https://docs.litellm.ai/docs/providers/gemini))**\\n    - Handle more json schema - openapi schema conversion edge cases [PR](https://github.com/BerriAI/litellm/pull/10351)\\n    - Tool calls - return \u2018finish_reason=\u201ctool_calls\u201d\u2019 on gemini tool calling response [PR](https://github.com/BerriAI/litellm/pull/10485)\\n- **[VertexAI](../../docs/providers/vertex#metallama-api)**\\n    - Meta/llama-4 model support [PR](https://github.com/BerriAI/litellm/pull/10492)\\n    - Meta/llama3 - handle tool call result in content [PR](https://github.com/BerriAI/litellm/pull/10492)\\n    - Meta/* - return \u2018finish_reason=\u201ctool_calls\u201d\u2019 on tool calling response [PR](https://github.com/BerriAI/litellm/pull/10492)\\n- **[Bedrock](../../docs/providers/bedrock#litellm-proxy-usage)**\\n    - [Image Generation](../../docs/providers/bedrock#image-generation) - Support new \u2018stable-image-core\u2019 models - [PR](https://github.com/BerriAI/litellm/pull/10351)\\n    - [Knowledge Bases](../../docs/completion/knowledgebase) - support using Bedrock knowledge bases with `/chat/completions` [PR](https://github.com/BerriAI/litellm/pull/10413)\\n    - [Anthropic](../../docs/providers/bedrock#litellm-proxy-usage) - add \u2018supports_pdf_input\u2019 for claude-3.7-bedrock models [PR](https://github.com/BerriAI/litellm/pull/9917), [Get Started](../../docs/completion/document_understanding#checking-if-a-model-supports-pdf-input)\\n- **[OpenAI](../../docs/providers/openai)**\\n    - Support OPENAI_BASE_URL in addition to OPENAI_API_BASE [PR](https://github.com/BerriAI/litellm/pull/10423)\\n    - Correctly re-raise 504 timeout errors [PR](https://github.com/BerriAI/litellm/pull/10462)\\n    - Native Gpt-4o-mini-tts support [PR](https://github.com/BerriAI/litellm/pull/10462)\\n- \ud83c\udd95 **[Meta Llama API](../../docs/providers/meta_llama)** provider [PR](https://github.com/BerriAI/litellm/pull/10451)\\n- \ud83c\udd95 **[LlamaFile](../../docs/providers/llamafile)** provider [PR](https://github.com/BerriAI/litellm/pull/10482)\\n\\n## LLM API Endpoints\\n- **[Response API](../../docs/response_api)** \\n    - Fix for handling multi turn sessions [PR](https://github.com/BerriAI/litellm/pull/10415)\\n- **[Embeddings](../../docs/embedding/supported_embedding)**\\n    - Caching fixes - [PR](https://github.com/BerriAI/litellm/pull/10424)\\n        - handle str -> list cache\\n        - Return usage tokens for cache hit \\n        - Combine usage tokens on partial cache hits \\n- \ud83c\udd95 **[Vector Stores](../../docs/completion/knowledgebase)**\\n    - Allow defining Vector Store Configs - [PR](https://github.com/BerriAI/litellm/pull/10448)\\n    - New StandardLoggingPayload field for requests made when a vector store is used - [PR](https://github.com/BerriAI/litellm/pull/10509)\\n    - Show Vector Store / KB Request on LiteLLM Logs Page  - [PR](https://github.com/BerriAI/litellm/pull/10514)\\n    - Allow using vector store in OpenAI API spec with tools - [PR](https://github.com/BerriAI/litellm/pull/10516)\\n- **[MCP](../../docs/mcp)**\\n    - Ensure Non-Admin virtual keys can access /mcp routes - [PR](https://github.com/BerriAI/litellm/pull/10473)\\n      \\n      **Note:** Currently, all Virtual Keys are able to access the MCP endpoints. We are working on a feature to allow restricting MCP access by keys/teams/users/orgs. Follow [here](https://github.com/BerriAI/litellm/discussions/9891) for updates.\\n- **Moderations**\\n    - Add logging callback support for `/moderations` API - [PR](https://github.com/BerriAI/litellm/pull/10390)\\n\\n\\n## Spend Tracking / Budget Improvements\\n- **[OpenAI](../../docs/providers/openai)**\\n    - [computer-use-preview](../../docs/providers/openai/responses_api#computer-use) cost tracking / pricing [PR](https://github.com/BerriAI/litellm/pull/10422)\\n    - [gpt-4o-mini-tts](../../docs/providers/openai/text_to_speech) input cost tracking - [PR](https://github.com/BerriAI/litellm/pull/10462)\\n- **[Fireworks AI](../../docs/providers/fireworks_ai)** - pricing updates - new `0-4b` model pricing tier + llama4 model pricing\\n- **[Budgets](../../docs/proxy/users#set-budgets)**\\n    - [Budget resets](../../docs/proxy/users#reset-budgets) now happen as start of day/week/month - [PR](https://github.com/BerriAI/litellm/pull/10333)\\n    - Trigger [Soft Budget Alerts](../../docs/proxy/alerting#soft-budget-alerts-for-virtual-keys) When Key Crosses Threshold - [PR](https://github.com/BerriAI/litellm/pull/10491)\\n- **[Token Counting](../../docs/completion/token_usage#3-token_counter)**\\n    - Rewrite of token_counter() function to handle to prevent undercounting tokens - [PR](https://github.com/BerriAI/litellm/pull/10409)\\n\\n\\n## Management Endpoints / UI\\n- **Virtual Keys**\\n    - Fix filtering on key alias - [PR](https://github.com/BerriAI/litellm/pull/10455)\\n    - Support global filtering on keys - [PR](https://github.com/BerriAI/litellm/pull/10455)\\n    - Pagination - fix clicking on next/back buttons on table - [PR](https://github.com/BerriAI/litellm/pull/10528)\\n- **Models**\\n    - Triton - Support adding model/provider on UI - [PR](https://github.com/BerriAI/litellm/pull/10456)\\n    - VertexAI - Fix adding vertex models with reusable credentials - [PR](https://github.com/BerriAI/litellm/pull/10528)\\n    - LLM Credentials - show existing credentials for easy editing - [PR](https://github.com/BerriAI/litellm/pull/10519)\\n- **Teams**\\n    - Allow reassigning team to other org - [PR](https://github.com/BerriAI/litellm/pull/10527)\\n- **Organizations**\\n    - Fix showing org budget on table - [PR](https://github.com/BerriAI/litellm/pull/10528)\\n\\n\\n\\n## Logging / Guardrail Integrations\\n- **[Langsmith](../../docs/observability/langsmith_integration)**\\n    - Respect [langsmith_batch_size](../../docs/observability/langsmith_integration#local-testing---control-batch-size) param - [PR](https://github.com/BerriAI/litellm/pull/10411)\\n\\n## Performance / Loadbalancing / Reliability improvements\\n- **[Redis](../../docs/proxy/caching)**\\n    - Ensure all redis queues are periodically flushed, this fixes an issue where redis queue size was growing indefinitely when request tags were used - [PR](https://github.com/BerriAI/litellm/pull/10393)\\n- **[Rate Limits](../../docs/proxy/users#set-rate-limit)**\\n    - [Multi-instance rate limiting](../../docs/proxy/users#beta-multi-instance-rate-limiting) support across keys/teams/users/customers - [PR](https://github.com/BerriAI/litellm/pull/10458), [PR](https://github.com/BerriAI/litellm/pull/10497), [PR](https://github.com/BerriAI/litellm/pull/10500)\\n- **[Azure OpenAI OIDC](../../docs/providers/azure#entra-id---use-azure_ad_token)**\\n    - allow using litellm defined params for [OIDC Auth](../../docs/providers/azure#entra-id---use-azure_ad_token) - [PR](https://github.com/BerriAI/litellm/pull/10394)\\n\\n\\n## General Proxy Improvements\\n- **Security**\\n    - Allow [blocking web crawlers](../../docs/proxy/enterprise#blocking-web-crawlers) - [PR](https://github.com/BerriAI/litellm/pull/10420)\\n- **Auth**\\n    - Support [`x-litellm-api-key` header param by default](../../docs/pass_through/vertex_ai#use-with-virtual-keys), this fixes an issue from the prior release where `x-litellm-api-key` was not being used on vertex ai passthrough requests - [PR](https://github.com/BerriAI/litellm/pull/10392)\\n    - Allow key at max budget to call non-llm api endpoints - [PR](https://github.com/BerriAI/litellm/pull/10392)\\n- \ud83c\udd95 **[Python Client Library](../../docs/proxy/management_cli) for LiteLLM Proxy management endpoints**\\n    - Initial PR - [PR](https://github.com/BerriAI/litellm/pull/10445)\\n    - Support for doing HTTP requests - [PR](https://github.com/BerriAI/litellm/pull/10452)\\n- **Dependencies**\\n    - Don\u2019t require uvloop for windows - [PR](https://github.com/BerriAI/litellm/pull/10483)"},{"id":"v1.67.4-stable","metadata":{"permalink":"/release_notes/v1.67.4-stable","source":"@site/release_notes/v1.67.4-stable/index.md","title":"v1.67.4-stable - Improved User Management","description":"Deploy this version","date":"2025-04-26T10:00:00.000Z","tags":[{"inline":true,"label":"responses_api","permalink":"/release_notes/tags/responses-api"},{"inline":true,"label":"ui_improvements","permalink":"/release_notes/tags/ui-improvements"},{"inline":true,"label":"security","permalink":"/release_notes/tags/security"},{"inline":true,"label":"session_management","permalink":"/release_notes/tags/session-management"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.67.4-stable - Improved User Management","slug":"v1.67.4-stable","date":"2025-04-26T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"tags":["responses_api","ui_improvements","security","session_management"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.68.0-stable","permalink":"/release_notes/v1.68.0-stable"},"nextItem":{"title":"v1.67.0-stable - SCIM Integration","permalink":"/release_notes/v1.67.0-stable"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run\\n-e STORE_MODEL_IN_DB=True\\n-p 4000:4000\\ndocker.litellm.ai/berriai/litellm:main-v1.67.4-stable\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.67.4.post1\\n```\\n</TabItem>\\n</Tabs>\\n\\n## Key Highlights\\n\\n- **Improved User Management**: This release enables search and filtering across users, keys, teams, and models.\\n- **Responses API Load Balancing**: Route requests across provider regions and ensure session continuity. \\n- **UI Session Logs**: Group several requests to LiteLLM into a session. \\n\\n## Improved User Management\\n\\n<Image img={require(\'../../img/release_notes/ui_search_users.png\')}/>\\n<br/>\\n\\nThis release makes it easier to manage users and keys on LiteLLM. You can now search and filter across users, keys, teams, and models, and control user settings more easily.\\n\\nNew features include:\\n\\n- Search for users by email, ID, role, or team.\\n- See all of a user\'s models, teams, and keys in one place.\\n- Change user roles and model access right from the Users Tab.\\n\\nThese changes help you spend less time on user setup and management on LiteLLM.\\n\\n## Responses API Load Balancing\\n\\n<Image img={require(\'../../img/release_notes/ui_responses_lb.png\')}/>\\n<br/>\\n\\nThis release introduces load balancing for the Responses API, allowing you to route requests across provider regions and ensure session continuity. It works as follows:\\n\\n- If a `previous_response_id` is provided, LiteLLM will route the request to the original deployment that generated the prior response \u2014 ensuring session continuity.\\n- If no `previous_response_id` is provided, LiteLLM will load-balance requests across your available deployments.\\n\\n[Read more](https://docs.litellm.ai/docs/response_api#load-balancing-with-session-continuity)\\n\\n## UI Session Logs\\n\\n<Image img={require(\'../../img/ui_session_logs.png\')}/>\\n<br/>\\n\\nThis release allow you to group requests to LiteLLM proxy into a session. If you specify a litellm_session_id in your request LiteLLM will automatically group all logs in the same session. This allows you to easily track usage and request content per session. \\n\\n[Read more](https://docs.litellm.ai/docs/proxy/ui_logs_sessions)\\n\\n## New Models / Updated Models\\n\\n- **OpenAI**\\n    1. Added `gpt-image-1` cost tracking [Get Started](https://docs.litellm.ai/docs/image_generation)\\n    2. Bug fix: added cost tracking for gpt-image-1 when quality is unspecified [PR](https://github.com/BerriAI/litellm/pull/10247)\\n- **Azure**\\n    1. Fixed timestamp granularities passing to whisper in Azure [Get Started](https://docs.litellm.ai/docs/audio_transcription)\\n    2. Added azure/gpt-image-1 pricing [Get Started](https://docs.litellm.ai/docs/image_generation), [PR](https://github.com/BerriAI/litellm/pull/10327)\\n    3. Added cost tracking for `azure/computer-use-preview`, `azure/gpt-4o-audio-preview-2024-12-17`, `azure/gpt-4o-mini-audio-preview-2024-12-17` [PR](https://github.com/BerriAI/litellm/pull/10178)\\n- **Bedrock**\\n    1. Added support for all compatible Bedrock parameters when model=\\"arn:..\\" (Bedrock application inference profile models) [Get started](https://docs.litellm.ai/docs/providers/bedrock#bedrock-application-inference-profile), [PR](https://github.com/BerriAI/litellm/pull/10256)\\n    2. Fixed wrong system prompt transformation [PR](https://github.com/BerriAI/litellm/pull/10120)\\n- **VertexAI / Google AI Studio**\\n    1. Allow setting `budget_tokens=0` for `gemini-2.5-flash` [Get Started](https://docs.litellm.ai/docs/providers/gemini#usage---thinking--reasoning_content),[PR](https://github.com/BerriAI/litellm/pull/10198)\\n    2. Ensure returned `usage` includes thinking token usage [PR](https://github.com/BerriAI/litellm/pull/10198)\\n    3. Added cost tracking for `gemini-2.5-pro-preview-03-25` [PR](https://github.com/BerriAI/litellm/pull/10178)\\n- **Cohere**\\n    1. Added support for cohere command-a-03-2025 [Get Started](https://docs.litellm.ai/docs/providers/cohere), [PR](https://github.com/BerriAI/litellm/pull/10295)\\n- **SageMaker**\\n    1. Added support for max_completion_tokens parameter [Get Started](https://docs.litellm.ai/docs/providers/sagemaker), [PR](https://github.com/BerriAI/litellm/pull/10300)\\n- **Responses API**\\n    1. Added support for GET and DELETE operations - `/v1/responses/{response_id}` [Get Started](../../docs/response_api)\\n    2. Added session management support for all supported models [PR](https://github.com/BerriAI/litellm/pull/10321)\\n    3. Added routing affinity to maintain model consistency within sessions [Get Started](https://docs.litellm.ai/docs/response_api#load-balancing-with-routing-affinity), [PR](https://github.com/BerriAI/litellm/pull/10193)\\n\\n\\n## Spend Tracking Improvements\\n\\n- **Bug Fix**: Fixed spend tracking bug, ensuring default litellm params aren\'t modified in memory [PR](https://github.com/BerriAI/litellm/pull/10167)\\n- **Deprecation Dates**: Added deprecation dates for Azure, VertexAI models [PR](https://github.com/BerriAI/litellm/pull/10308)\\n\\n## Management Endpoints / UI\\n\\n#### Users\\n- **Filtering and Searching**: \\n  - Filter users by user_id, role, team, sso_id \\n  - Search users by email\\n\\n  <br/>\\n\\n  <Image img={require(\'../../img/release_notes/user_filters.png\')}/>\\n\\n- **User Info Panel**: Added a new user information pane [PR](https://github.com/BerriAI/litellm/pull/10213)\\n  - View teams, keys, models associated with User \\n  - Edit user role, model permissions \\n\\n\\n\\n#### Teams\\n- **Filtering and Searching**: \\n    - Filter teams by Organization, Team ID [PR](https://github.com/BerriAI/litellm/pull/10324)\\n    - Search teams by Team Name [PR](https://github.com/BerriAI/litellm/pull/10324)\\n\\n  <br/>\\n\\n  <Image img={require(\'../../img/release_notes/team_filters.png\')}/>\\n\\n\\n\\n#### Keys\\n- **Key Management**: \\n  - Support for cross-filtering and filtering by key hash [PR](https://github.com/BerriAI/litellm/pull/10322)\\n  - Fixed key alias reset when resetting filters [PR](https://github.com/BerriAI/litellm/pull/10099)\\n  - Fixed table rendering on key creation [PR](https://github.com/BerriAI/litellm/pull/10224)\\n\\n#### UI Logs Page\\n\\n- **Session Logs**: Added UI Session Logs [Get Started](https://docs.litellm.ai/docs/proxy/ui_logs_sessions)\\n\\n\\n#### UI Authentication & Security\\n- **Required Authentication**: Authentication now required for all dashboard pages [PR](https://github.com/BerriAI/litellm/pull/10229)\\n- **SSO Fixes**: Fixed SSO user login invalid token error [PR](https://github.com/BerriAI/litellm/pull/10298)\\n- [BETA] **Encrypted Tokens**: Moved UI to encrypted token usage [PR](https://github.com/BerriAI/litellm/pull/10302)\\n- **Token Expiry**: Support token refresh by re-routing to login page (fixes issue where expired token would show a blank page) [PR](https://github.com/BerriAI/litellm/pull/10250)\\n\\n#### UI General fixes\\n- **Fixed UI Flicker**: Addressed UI flickering issues in Dashboard [PR](https://github.com/BerriAI/litellm/pull/10261)\\n- **Improved Terminology**: Better loading and no-data states on Keys and Tools pages [PR](https://github.com/BerriAI/litellm/pull/10253)\\n- **Azure Model Support**: Fixed editing Azure public model names and changing model names after creation [PR](https://github.com/BerriAI/litellm/pull/10249)\\n- **Team Model Selector**: Bug fix for team model selection [PR](https://github.com/BerriAI/litellm/pull/10171)\\n\\n\\n## Logging / Guardrail Integrations\\n\\n- **Datadog**:\\n    1. Fixed Datadog LLM observability logging [Get Started](https://docs.litellm.ai/docs/proxy/logging#datadog), [PR](https://github.com/BerriAI/litellm/pull/10206)\\n- **Prometheus / Grafana**: \\n    1. Enable datasource selection on LiteLLM Grafana Template [Get Started](https://docs.litellm.ai/docs/proxy/prometheus#-litellm-maintained-grafana-dashboards-), [PR](https://github.com/BerriAI/litellm/pull/10257)\\n- **AgentOps**: \\n    1. Added AgentOps Integration [Get Started](https://docs.litellm.ai/docs/observability/agentops_integration), [PR](https://github.com/BerriAI/litellm/pull/9685)\\n- **Arize**: \\n    1. Added missing attributes for Arize & Phoenix Integration [Get Started](https://docs.litellm.ai/docs/observability/arize_integration), [PR](https://github.com/BerriAI/litellm/pull/10215)\\n\\n\\n## General Proxy Improvements\\n\\n- **Caching**: Fixed caching to account for `thinking` or `reasoning_effort` when calculating cache key [PR](https://github.com/BerriAI/litellm/pull/10140)\\n- **Model Groups**: Fixed handling for cases where user sets model_group inside model_info [PR](https://github.com/BerriAI/litellm/pull/10191)\\n- **Passthrough Endpoints**: Ensured `PassthroughStandardLoggingPayload` is logged with method, URL, request/response body [PR](https://github.com/BerriAI/litellm/pull/10194)\\n- **Fix SQL Injection**: Fixed potential SQL injection vulnerability in spend_management_endpoints.py [PR](https://github.com/BerriAI/litellm/pull/9878)\\n\\n\\n\\n## Helm\\n\\n- Fixed serviceAccountName on migration job [PR](https://github.com/BerriAI/litellm/pull/10258)\\n\\n## Full Changelog\\n\\nThe complete list of changes can be found in the [GitHub release notes](https://github.com/BerriAI/litellm/compare/v1.67.0-stable...v1.67.4-stable)."},{"id":"v1.67.0-stable","metadata":{"permalink":"/release_notes/v1.67.0-stable","source":"@site/release_notes/v1.67.0-stable/index.md","title":"v1.67.0-stable - SCIM Integration","description":"Key Highlights","date":"2025-04-19T10:00:00.000Z","tags":[{"inline":true,"label":"sso","permalink":"/release_notes/tags/sso"},{"inline":true,"label":"unified_file_id","permalink":"/release_notes/tags/unified-file-id"},{"inline":true,"label":"cost_tracking","permalink":"/release_notes/tags/cost-tracking"},{"inline":true,"label":"security","permalink":"/release_notes/tags/security"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.67.0-stable - SCIM Integration","slug":"v1.67.0-stable","date":"2025-04-19T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"tags":["sso","unified_file_id","cost_tracking","security"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.67.4-stable - Improved User Management","permalink":"/release_notes/v1.67.4-stable"},"nextItem":{"title":"v1.66.0-stable - Realtime API Cost Tracking","permalink":"/release_notes/v1.66.0-stable"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Key Highlights\\n\\n- **SCIM Integration**: Enables identity providers (Okta, Azure AD, OneLogin, etc.) to automate user and team (group) provisioning, updates, and deprovisioning\\n- **Team and Tag based usage tracking**: You can now see usage and spend by team and tag at 1M+ spend logs.\\n- **Unified Responses API**: Support for calling Anthropic, Gemini, Groq, etc. via OpenAI\'s new Responses API.\\n\\nLet\'s dive in.\\n\\n## SCIM Integration\\n\\n<Image img={require(\'../../img/scim_integration.png\')}/>\\n\\nThis release adds SCIM support to LiteLLM. This allows your SSO provider (Okta, Azure AD, etc) to automatically create/delete users, teams, and memberships on LiteLLM. This means that when you remove a team on your SSO provider, your SSO provider will automatically delete the corresponding team on LiteLLM. \\n\\n[Read more](../../docs/tutorials/scim_litellm)\\n## Team and Tag based usage tracking\\n\\n<Image img={require(\'../../img/release_notes/new_team_usage_highlight.jpg\')}/>\\n\\n\\nThis release improves team and tag based usage tracking at 1m+ spend logs, making it easy to monitor your LLM API Spend in production. This covers:\\n\\n- View **daily spend** by teams + tags\\n- View **usage / spend by key**, within teams\\n- View **spend by multiple tags**\\n- Allow **internal users** to view spend of teams they\'re a member of\\n\\n[Read more](#management-endpoints--ui)\\n\\n## Unified Responses API\\n\\nThis release allows you to call Azure OpenAI, Anthropic, AWS Bedrock, and Google Vertex AI models via the POST /v1/responses endpoint on LiteLLM. This means you can now use popular tools like [OpenAI Codex](https://docs.litellm.ai/docs/tutorials/openai_codex) with your own models. \\n\\n<Image img={require(\'../../img/release_notes/unified_responses_api_rn.png\')}/>\\n\\n\\n[Read more](https://docs.litellm.ai/docs/response_api)\\n\\n\\n## New Models / Updated Models\\n\\n- **OpenAI**\\n    1. gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, o3, o3-mini, o4-mini pricing - [Get Started](../../docs/providers/openai#usage), [PR](https://github.com/BerriAI/litellm/pull/9990)\\n    2. o4 - correctly map o4 to openai o_series model\\n- **Azure AI**\\n    1. Phi-4 output cost per token fix - [PR](https://github.com/BerriAI/litellm/pull/9880)\\n    2. Responses API support [Get Started](../../docs/providers/azure#azure-responses-api),[PR](https://github.com/BerriAI/litellm/pull/10116)\\n- **Anthropic**\\n    1. redacted message thinking support - [Get Started](../../docs/providers/anthropic#usage---thinking--reasoning_content),[PR](https://github.com/BerriAI/litellm/pull/10129)\\n- **Cohere**\\n    1. `/v2/chat` Passthrough endpoint support w/ cost tracking - [Get Started](../../docs/pass_through/cohere), [PR](https://github.com/BerriAI/litellm/pull/9997)\\n- **Azure**\\n    1. Support azure tenant_id/client_id env vars - [Get Started](../../docs/providers/azure#entra-id---use-tenant_id-client_id-client_secret), [PR](https://github.com/BerriAI/litellm/pull/9993)\\n    2. Fix response_format check for 2025+ api versions - [PR](https://github.com/BerriAI/litellm/pull/9993)\\n    3. Add gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, o3, o3-mini, o4-mini pricing\\n- **VLLM**\\n    1. Files - Support \'file\' message type for VLLM video url\'s - [Get Started](../../docs/providers/vllm#send-video-url-to-vllm), [PR](https://github.com/BerriAI/litellm/pull/10129)\\n    2. Passthrough - new `/vllm/` passthrough endpoint support [Get Started](../../docs/pass_through/vllm), [PR](https://github.com/BerriAI/litellm/pull/10002)\\n- **Mistral**\\n    1. new `/mistral` passthrough endpoint support [Get Started](../../docs/pass_through/mistral), [PR](https://github.com/BerriAI/litellm/pull/10002)\\n- **AWS**\\n    1. New mapped bedrock regions - [PR](https://github.com/BerriAI/litellm/pull/9430)\\n- **VertexAI / Google AI Studio**\\n    1. Gemini - Response format - Retain schema field ordering for google gemini and vertex by specifying\xa0propertyOrdering - [Get Started](../../docs/providers/vertex#json-schema), [PR](https://github.com/BerriAI/litellm/pull/9828)\\n    2. Gemini-2.5-flash - return reasoning content [Google AI Studio](../../docs/providers/gemini#usage---thinking--reasoning_content), [Vertex AI](../../docs/providers/vertex#thinking--reasoning_content)\\n    3. Gemini-2.5-flash - pricing + model information [PR](https://github.com/BerriAI/litellm/pull/10125)\\n    4. Passthrough - new `/vertex_ai/discovery` route - enables calling AgentBuilder API routes [Get Started](../../docs/pass_through/vertex_ai#supported-api-endpoints), [PR](https://github.com/BerriAI/litellm/pull/10084)\\n- **Fireworks AI**\\n    1. return tool calling responses in `tool_calls` field (fireworks incorrectly returns this as a json str in content) [PR](https://github.com/BerriAI/litellm/pull/10130)\\n- **Triton**\\n    1. Remove fixed remove bad_words / stop words\xa0from `/generate` call - [Get Started](../../docs/providers/triton-inference-server#triton-generate---chat-completion), [PR](https://github.com/BerriAI/litellm/pull/10163)\\n- **Other**\\n    1. Support for all litellm providers on Responses API (works with Codex) - [Get Started](../../docs/tutorials/openai_codex), [PR](https://github.com/BerriAI/litellm/pull/10132)\\n    2. Fix combining multiple tool calls in streaming response - [Get Started](../../docs/completion/stream#helper-function), [PR](https://github.com/BerriAI/litellm/pull/10040)\\n\\n\\n## Spend Tracking Improvements\\n\\n- **Cost Control** - inject cache control points in prompt for cost reduction [Get Started](../../docs/tutorials/prompt_caching), [PR](https://github.com/BerriAI/litellm/pull/10000)\\n- **Spend Tags** - spend tags in headers - support x-litellm-tags even if tag based routing not enabled [Get Started](../../docs/proxy/request_headers#litellm-headers), [PR](https://github.com/BerriAI/litellm/pull/10000)\\n- **Gemini-2.5-flash** - support cost calculation for reasoning tokens [PR](https://github.com/BerriAI/litellm/pull/10141)\\n\\n## Management Endpoints / UI\\n- **Users**\\n    1. Show created_at and updated_at on users page - [PR](https://github.com/BerriAI/litellm/pull/10033)\\n- **Virtual Keys**\\n    1. Filter by key alias - https://github.com/BerriAI/litellm/pull/10085\\n- **Usage Tab**\\n\\n    1. Team based usage\\n        \\n        - New `LiteLLM_DailyTeamSpend` Table for aggregate team based usage logging - [PR](https://github.com/BerriAI/litellm/pull/10039)\\n        \\n        - New Team based usage dashboard + new `/team/daily/activity` API - [PR](https://github.com/BerriAI/litellm/pull/10081)\\n        - Return team alias on\xa0/team/daily/activity\xa0API - [PR](https://github.com/BerriAI/litellm/pull/10157)\\n        - allow internal user view spend for teams they belong to - [PR](https://github.com/BerriAI/litellm/pull/10157)\\n        - allow viewing top keys by team - [PR](https://github.com/BerriAI/litellm/pull/10157)\\n\\n        <Image img={require(\'../../img/release_notes/new_team_usage.png\')}/>\\n\\n    2. Tag Based Usage\\n        - New `LiteLLM_DailyTagSpend` Table for aggregate tag based usage logging - [PR](https://github.com/BerriAI/litellm/pull/10071)\\n        - Restrict to only Proxy Admins - [PR](https://github.com/BerriAI/litellm/pull/10157)\\n        - allow viewing top keys by tag\\n        - Return tags passed in request (i.e. dynamic tags) on `/tag/list` API - [PR](https://github.com/BerriAI/litellm/pull/10157)\\n        <Image img={require(\'../../img/release_notes/new_tag_usage.png\')}/>\\n    3. Track prompt caching metrics in daily user, team, tag tables - [PR](https://github.com/BerriAI/litellm/pull/10029)\\n    4. Show usage by key (on all up, team, and tag usage dashboards) - [PR](https://github.com/BerriAI/litellm/pull/10157)\\n    5. swap old usage with new usage tab\\n- **Models**\\n    1. Make columns resizable/hideable - [PR](https://github.com/BerriAI/litellm/pull/10119)\\n- **API Playground**\\n    1. Allow internal user to call api playground - [PR](https://github.com/BerriAI/litellm/pull/10157)\\n- **SCIM**\\n    1. Add LiteLLM SCIM Integration for Team and User management - [Get Started](../../docs/tutorials/scim_litellm), [PR](https://github.com/BerriAI/litellm/pull/10072)\\n\\n\\n## Logging / Guardrail Integrations\\n- **GCS**\\n    1. Fix gcs pub sub logging with env var GCS_PROJECT_ID - [Get Started](../../docs/observability/gcs_bucket_integration#usage), [PR](https://github.com/BerriAI/litellm/pull/10042)\\n- **AIM**\\n    1. Add litellm call id passing to Aim guardrails on pre and post-hooks calls - [Get Started](../../docs/proxy/guardrails/aim_security), [PR](https://github.com/BerriAI/litellm/pull/10021)\\n- **Azure blob storage**\\n    1. Ensure logging works in high throughput scenarios - [Get Started](../../docs/proxy/logging#azure-blob-storage), [PR](https://github.com/BerriAI/litellm/pull/9962)\\n\\n## General Proxy Improvements\\n\\n- **Support setting `litellm.modify_params` via env var** [PR](https://github.com/BerriAI/litellm/pull/9964)\\n- **Model Discovery** - Check provider\u2019s `/models` endpoints when calling proxy\u2019s `/v1/models` endpoint - [Get Started](../../docs/proxy/model_discovery), [PR](https://github.com/BerriAI/litellm/pull/9958)\\n- **`/utils/token_counter`** - fix retrieving custom tokenizer for db models - [Get Started](../../docs/proxy/configs#set-custom-tokenizer), [PR](https://github.com/BerriAI/litellm/pull/10047)\\n- **Prisma migrate** - handle existing columns in db table - [PR](https://github.com/BerriAI/litellm/pull/10138)"},{"id":"v1.66.0-stable","metadata":{"permalink":"/release_notes/v1.66.0-stable","source":"@site/release_notes/v1.66.0-stable/index.md","title":"v1.66.0-stable - Realtime API Cost Tracking","description":"Deploy this version","date":"2025-04-12T10:00:00.000Z","tags":[{"inline":true,"label":"sso","permalink":"/release_notes/tags/sso"},{"inline":true,"label":"unified_file_id","permalink":"/release_notes/tags/unified-file-id"},{"inline":true,"label":"cost_tracking","permalink":"/release_notes/tags/cost-tracking"},{"inline":true,"label":"security","permalink":"/release_notes/tags/security"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.66.0-stable - Realtime API Cost Tracking","slug":"v1.66.0-stable","date":"2025-04-12T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"tags":["sso","unified_file_id","cost_tracking","security"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.67.0-stable - SCIM Integration","permalink":"/release_notes/v1.67.0-stable"},"nextItem":{"title":"v1.65.4-stable","permalink":"/release_notes/v1.65.4-stable"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run\\n-e STORE_MODEL_IN_DB=True\\n-p 4000:4000\\ndocker.litellm.ai/berriai/litellm:main-v1.66.0-stable\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.66.0.post1\\n```\\n</TabItem>\\n</Tabs>\\n\\nv1.66.0-stable is live now, here are the key highlights of this release\\n\\n## Key Highlights\\n- **Realtime API Cost Tracking**: Track cost of realtime API calls\\n- **Microsoft SSO Auto-sync**: Auto-sync groups and group members from Azure Entra ID to LiteLLM\\n- **xAI grok-3**: Added support for `xai/grok-3` models\\n- **Security Fixes**: Fixed [CVE-2025-0330](https://www.cve.org/CVERecord?id=CVE-2025-0330) and [CVE-2024-6825](https://www.cve.org/CVERecord?id=CVE-2024-6825) vulnerabilities\\n\\nLet\'s dive in.\\n\\n## Realtime API Cost Tracking\\n\\n<Image \\n  img={require(\'../../img/realtime_api.png\')}\\n  style={{width: \'100%\', display: \'block\'}}\\n/>\\n\\n\\nThis release adds Realtime API logging + cost tracking. \\n- **Logging**: LiteLLM now logs the complete response from realtime calls to all logging integrations (DB, S3, Langfuse, etc.) \\n- **Cost Tracking**: You can now set \'base_model\' and custom pricing for realtime models. [Custom Pricing](../../docs/proxy/custom_pricing)\\n- **Budgets**: Your key/user/team budgets now work for realtime models as well.\\n\\nStart [here](https://docs.litellm.ai/docs/realtime)\\n\\n\\n\\n## Microsoft SSO Auto-sync\\n\\n<Image \\n  img={require(\'../../img/release_notes/sso_sync.png\')}\\n  style={{width: \'100%\', display: \'block\'}}\\n/>\\n<p style={{textAlign: \'left\', color: \'#666\'}}>\\n  Auto-sync groups and members from Azure Entra ID to LiteLLM\\n</p>\\n\\nThis release adds support for auto-syncing groups and members on Microsoft Entra ID with LiteLLM. This means that LiteLLM proxy administrators can spend less time managing teams and members and LiteLLM handles the following: \\n\\n- Auto-create teams that exist on Microsoft Entra ID \\n- Sync team members on Microsoft Entra ID with LiteLLM teams\\n\\nGet started with this [here](https://docs.litellm.ai/docs/tutorials/msft_sso)\\n\\n\\n## New Models / Updated Models\\n\\n- **xAI**\\n    1. Added reasoning_effort support for `xai/grok-3-mini-beta` [Get Started](https://docs.litellm.ai/docs/providers/xai#reasoning-usage)\\n    2. Added cost tracking for `xai/grok-3` models [PR](https://github.com/BerriAI/litellm/pull/9920)\\n\\n- **Hugging Face**\\n    1. Added inference providers support [Get Started](https://docs.litellm.ai/docs/providers/huggingface#serverless-inference-providers)\\n\\n- **Azure**\\n    1. Added azure/gpt-4o-realtime-audio cost tracking [PR](https://github.com/BerriAI/litellm/pull/9893)\\n\\n- **VertexAI**\\n    1. Added enterpriseWebSearch tool support [Get Started](https://docs.litellm.ai/docs/providers/vertex#grounding---web-search)\\n    2. Moved to only passing keys accepted by the Vertex AI response schema [PR](https://github.com/BerriAI/litellm/pull/8992)\\n\\n- **Google AI Studio**\\n    1. Added cost tracking for `gemini-2.5-pro` [PR](https://github.com/BerriAI/litellm/pull/9837)\\n    2. Fixed pricing for \'gemini/gemini-2.5-pro-preview-03-25\' [PR](https://github.com/BerriAI/litellm/pull/9896)\\n    3. Fixed handling file_data being passed in [PR](https://github.com/BerriAI/litellm/pull/9786)\\n\\n- **Azure**\\n    1. Updated Azure Phi-4 pricing [PR](https://github.com/BerriAI/litellm/pull/9862)\\n    2. Added azure/gpt-4o-realtime-audio cost tracking [PR](https://github.com/BerriAI/litellm/pull/9893)\\n\\n- **Databricks**\\n    1. Removed reasoning_effort from parameters [PR](https://github.com/BerriAI/litellm/pull/9811)\\n    2. Fixed custom endpoint check for Databricks [PR](https://github.com/BerriAI/litellm/pull/9925)\\n\\n- **General**\\n    1. Added litellm.supports_reasoning() util to track if an llm supports reasoning [Get Started](https://docs.litellm.ai/docs/providers/anthropic#reasoning)\\n    2. Function Calling - Handle pydantic base model in message tool calls, handle tools = [], and support fake streaming on tool calls for meta.llama3-3-70b-instruct-v1:0 [PR](https://github.com/BerriAI/litellm/pull/9774)\\n    3. LiteLLM Proxy - Allow passing `thinking` param to litellm proxy via client sdk [PR](https://github.com/BerriAI/litellm/pull/9386)\\n    4. Fixed correctly translating \'thinking\' param for litellm [PR](https://github.com/BerriAI/litellm/pull/9904)\\n\\n\\n## Spend Tracking Improvements\\n- **OpenAI, Azure**\\n    1. Realtime API Cost tracking with token usage metrics in spend logs [Get Started](https://docs.litellm.ai/docs/realtime)\\n- **Anthropic**\\n    1. Fixed Claude Haiku cache read pricing per token [PR](https://github.com/BerriAI/litellm/pull/9834)\\n    2. Added cost tracking for Claude responses with base_model [PR](https://github.com/BerriAI/litellm/pull/9897)\\n    3. Fixed Anthropic prompt caching cost calculation and trimmed logged message in db [PR](https://github.com/BerriAI/litellm/pull/9838)\\n- **General**\\n    1. Added token tracking and log usage object in spend logs [PR](https://github.com/BerriAI/litellm/pull/9843)\\n    2. Handle custom pricing at deployment level [PR](https://github.com/BerriAI/litellm/pull/9855)\\n\\n\\n## Management Endpoints / UI\\n\\n- **Test Key Tab**\\n    1. Added rendering of Reasoning content, ttft, usage metrics on test key page [PR](https://github.com/BerriAI/litellm/pull/9931)\\n\\n    <Image \\n    img={require(\'../../img/release_notes/chat_metrics.png\')}\\n    style={{width: \'100%\', display: \'block\'}}\\n    />\\n    <p style={{textAlign: \'left\', color: \'#666\'}}>\\n    View input, output, reasoning tokens, ttft metrics.\\n    </p>\\n- **Tag / Policy Management**\\n    1. Added Tag/Policy Management. Create routing rules based on request metadata. This allows you to enforce that requests with `tags=\\"private\\"` only go to specific models. [Get Started](https://docs.litellm.ai/docs/tutorials/tag_management)\\n\\n    <br />\\n\\n    <Image \\n    img={require(\'../../img/release_notes/tag_management.png\')}\\n    style={{width: \'100%\', display: \'block\'}}\\n    />\\n    <p style={{textAlign: \'left\', color: \'#666\'}}>\\n    Create and manage tags.\\n    </p>\\n- **Redesigned Login Screen**\\n    1. Polished login screen [PR](https://github.com/BerriAI/litellm/pull/9778)\\n- **Microsoft SSO Auto-Sync**\\n    1. Added debug route to allow admins to debug SSO JWT fields [PR](https://github.com/BerriAI/litellm/pull/9835)\\n    2. Added ability to use MSFT Graph API to assign users to teams [PR](https://github.com/BerriAI/litellm/pull/9865)\\n    3. Connected litellm to Azure Entra ID Enterprise Application [PR](https://github.com/BerriAI/litellm/pull/9872)\\n    4. Added ability for admins to set `default_team_params` for when litellm SSO creates default teams [PR](https://github.com/BerriAI/litellm/pull/9895)\\n    5. Fixed MSFT SSO to use correct field for user email [PR](https://github.com/BerriAI/litellm/pull/9886)\\n    6. Added UI support for setting Default Team setting when litellm SSO auto creates teams [PR](https://github.com/BerriAI/litellm/pull/9918)\\n- **UI Bug Fixes**\\n    1. Prevented team, key, org, model numerical values changing on scrolling [PR](https://github.com/BerriAI/litellm/pull/9776)\\n    2. Instantly reflect key and team updates in UI [PR](https://github.com/BerriAI/litellm/pull/9825)\\n\\n## Logging / Guardrail Improvements\\n\\n- **Prometheus**\\n    1. Emit Key and Team Budget metrics on a cron job schedule [Get Started](https://docs.litellm.ai/docs/proxy/prometheus#initialize-budget-metrics-on-startup)\\n\\n## Security Fixes\\n\\n- Fixed [CVE-2025-0330](https://www.cve.org/CVERecord?id=CVE-2025-0330) - Leakage of Langfuse API keys in team exception handling [PR](https://github.com/BerriAI/litellm/pull/9830)\\n- Fixed [CVE-2024-6825](https://www.cve.org/CVERecord?id=CVE-2024-6825) - Remote code execution in post call rules [PR](https://github.com/BerriAI/litellm/pull/9826)\\n\\n## Helm\\n\\n- Added service annotations to litellm-helm chart [PR](https://github.com/BerriAI/litellm/pull/9840)\\n- Added extraEnvVars to the helm deployment [PR](https://github.com/BerriAI/litellm/pull/9292)\\n\\n## Demo\\n\\nTry this on the demo instance [today](https://docs.litellm.ai/docs/proxy/demo)\\n\\n## Complete Git Diff\\n\\nSee the complete git diff since v1.65.4-stable, [here](https://github.com/BerriAI/litellm/releases/tag/v1.66.0-stable)"},{"id":"v1.65.4-stable","metadata":{"permalink":"/release_notes/v1.65.4-stable","source":"@site/release_notes/v1.65.4-stable/index.md","title":"v1.65.4-stable","description":"Deploy this version","date":"2025-04-05T10:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.65.4-stable","slug":"v1.65.4-stable","date":"2025-04-05T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"tags":[],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.66.0-stable - Realtime API Cost Tracking","permalink":"/release_notes/v1.66.0-stable"},"nextItem":{"title":"v1.65.0-stable - Model Context Protocol","permalink":"/release_notes/v1.65.0-stable"}},"content":"import Image from \'@theme/IdealImage\';\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n## Deploy this version\\n\\n<Tabs>\\n<TabItem value=\\"docker\\" label=\\"Docker\\">\\n\\n``` showLineNumbers title=\\"docker run litellm\\"\\ndocker run\\n-e STORE_MODEL_IN_DB=True\\n-p 4000:4000\\ndocker.litellm.ai/berriai/litellm:main-v1.65.4-stable\\n```\\n</TabItem>\\n\\n<TabItem value=\\"pip\\" label=\\"Pip\\">\\n\\n``` showLineNumbers title=\\"pip install litellm\\"\\npip install litellm==1.65.4.post1\\n```\\n</TabItem>\\n</Tabs>\\n\\nv1.65.4-stable is live. Here are the improvements since v1.65.0-stable.\\n\\n## Key Highlights\\n- **Preventing DB Deadlocks**: Fixes a high-traffic issue when multiple instances were writing to the DB at the same time. \\n- **New Usage Tab**: Enables viewing spend by model and customizing date range\\n\\nLet\'s dive in. \\n\\n### Preventing DB Deadlocks\\n\\n<Image img={require(\'../../img/prevent_deadlocks.jpg\')} />\\n\\nThis release fixes the DB deadlocking issue that users faced in high traffic (10K+ RPS). This is great because it enables user/key/team spend tracking works at that scale.\\n\\nRead more about the new architecture [here](https://docs.litellm.ai/docs/proxy/db_deadlocks)\\n\\n\\n### New Usage Tab\\n\\n<Image img={require(\'../../img/release_notes/spend_by_model.jpg\')} />\\n\\nThe new Usage tab now brings the ability to track daily spend by model. This makes it easier to catch any spend tracking or token counting errors, when combined with the ability to view successful requests, and token usage.\\n\\nTo test this out, just go to Experimental > New Usage > Activity.\\n\\n\\n## New Models / Updated Models\\n\\n1. Databricks - claude-3-7-sonnet cost tracking [PR](https://github.com/BerriAI/litellm/blob/52b35cd8093b9ad833987b24f494586a1e923209/model_prices_and_context_window.json#L10350)\\n2. VertexAI - `gemini-2.5-pro-exp-03-25` cost tracking [PR](https://github.com/BerriAI/litellm/blob/52b35cd8093b9ad833987b24f494586a1e923209/model_prices_and_context_window.json#L4492)\\n3. VertexAI - `gemini-2.0-flash` cost tracking [PR](https://github.com/BerriAI/litellm/blob/52b35cd8093b9ad833987b24f494586a1e923209/model_prices_and_context_window.json#L4689)\\n4. Groq - add whisper ASR models to model cost map [PR](https://github.com/BerriAI/litellm/blob/52b35cd8093b9ad833987b24f494586a1e923209/model_prices_and_context_window.json#L3324)\\n5. IBM - Add watsonx/ibm/granite-3-8b-instruct to model cost map [PR](https://github.com/BerriAI/litellm/blob/52b35cd8093b9ad833987b24f494586a1e923209/model_prices_and_context_window.json#L91)\\n6. Google AI Studio - add gemini/gemini-2.5-pro-preview-03-25 to model cost map [PR](https://github.com/BerriAI/litellm/blob/52b35cd8093b9ad833987b24f494586a1e923209/model_prices_and_context_window.json#L4850)\\n\\n## LLM Translation\\n1. Vertex AI - Support anyOf param for OpenAI json schema translation [Get Started](https://docs.litellm.ai/docs/providers/vertex#json-schema)\\n2. Anthropic- response_format + thinking param support  (works across Anthropic API, Bedrock, Vertex) [Get Started](https://docs.litellm.ai/docs/reasoning_content)\\n3. Anthropic - if thinking token is specified and max tokens is not - ensure max token to anthropic is higher than thinking tokens (works across Anthropic API, Bedrock, Vertex) [PR](https://github.com/BerriAI/litellm/pull/9594)\\n4. Bedrock - latency optimized inference support [Get Started](https://docs.litellm.ai/docs/providers/bedrock#usage---latency-optimized-inference)\\n5. Sagemaker - handle special tokens + multibyte character code in response [Get Started](https://docs.litellm.ai/docs/providers/aws_sagemaker)\\n6. MCP - add support for using SSE MCP servers [Get Started](https://docs.litellm.ai/docs/mcp#usage)\\n8. Anthropic - new `litellm.messages.create` interface for calling Anthropic `/v1/messages` via passthrough [Get Started](https://docs.litellm.ai/docs/anthropic_unified#usage)\\n11. Anthropic - support \u2018file\u2019 content type in message param (works across Anthropic API, Bedrock, Vertex) [Get Started](https://docs.litellm.ai/docs/providers/anthropic#usage---pdf)\\n12. Anthropic - map openai \'reasoning_effort\' to anthropic \'thinking\' param (works across Anthropic API, Bedrock, Vertex) [Get Started](https://docs.litellm.ai/docs/providers/anthropic#usage---thinking--reasoning_content)\\n13. Google AI Studio (Gemini) - [BETA] `/v1/files` upload support [Get Started](../../docs/providers/google_ai_studio/files) \\n14. Azure - fix o-series tool calling [Get Started](../../docs/providers/azure#tool-calling--function-calling)\\n15. Unified file id - [ALPHA] allow calling multiple providers with same file id [PR](https://github.com/BerriAI/litellm/pull/9718)\\n    - This is experimental, and not recommended for production use.\\n    - We plan to have a production-ready implementation by next week.\\n16. Google AI Studio (Gemini) - return logprobs [PR](https://github.com/BerriAI/litellm/pull/9713)\\n17. Anthropic - Support prompt caching for Anthropic tool calls [Get Started](https://docs.litellm.ai/docs/completion/prompt_caching)\\n18. OpenRouter - unwrap extra body on open router calls [PR](https://github.com/BerriAI/litellm/pull/9747)\\n19. VertexAI - fix credential caching issue [PR](https://github.com/BerriAI/litellm/pull/9756)\\n20. XAI - filter out \'name\' param for XAI [PR](https://github.com/BerriAI/litellm/pull/9761)\\n21. Gemini - image generation output support [Get Started](../../docs/providers/gemini#image-generation)\\n22. Databricks - support claude-3-7-sonnet w/ thinking + response_format [Get Started](../../docs/providers/databricks#usage---thinking--reasoning_content)\\n\\n## Spend Tracking Improvements\\n1. Reliability fix  - Check sent and received model for cost calculation [PR](https://github.com/BerriAI/litellm/pull/9669)\\n2. Vertex AI - Multimodal embedding cost tracking [Get Started](https://docs.litellm.ai/docs/providers/vertex#multi-modal-embeddings), [PR](https://github.com/BerriAI/litellm/pull/9623)\\n\\n## Management Endpoints / UI\\n\\n<Image img={require(\'../../img/release_notes/new_activity_tab.png\')} />\\n\\n1. New Usage Tab\\n    - Report \'total_tokens\' + report success/failure calls\\n    - Remove double bars on scroll\\n    - Ensure \u2018daily spend\u2019 chart ordered from earliest to latest date\\n    - showing spend per model per day\\n    - show key alias on usage tab\\n    - Allow non-admins to view their activity\\n    - Add date picker to new usage tab\\n2. Virtual Keys Tab\\n    - remove \'default key\' on user signup\\n    - fix showing user models available for personal key creation\\n3. Test Key Tab\\n    - Allow testing image generation models\\n4. Models Tab\\n    - Fix bulk adding models \\n    - support reusable credentials for passthrough endpoints\\n    - Allow team members to see team models\\n5. Teams Tab\\n    - Fix json serialization error on update team metadata\\n6. Request Logs Tab\\n    - Add reasoning_content token tracking across all providers on streaming\\n7. API \\n    - return key alias on /user/daily/activity [Get Started](../../docs/proxy/cost_tracking#daily-spend-breakdown-api)\\n8. SSO\\n    - Allow assigning SSO users to teams on MSFT SSO [PR](https://github.com/BerriAI/litellm/pull/9745)\\n\\n## Logging / Guardrail Integrations\\n\\n1. Console Logs - Add json formatting for uncaught exceptions [PR](https://github.com/BerriAI/litellm/pull/9619)\\n2. Guardrails - AIM Guardrails support for virtual key based policies [Get Started](../../docs/proxy/guardrails/aim_security)\\n3. Logging - fix completion start time tracking [PR](https://github.com/BerriAI/litellm/pull/9688)\\n4. Prometheus\\n    - Allow adding authentication on Prometheus /metrics endpoints [PR](https://github.com/BerriAI/litellm/pull/9766)\\n    - Distinguish LLM Provider Exception vs. LiteLLM Exception in metric naming [PR](https://github.com/BerriAI/litellm/pull/9760)\\n    - Emit operational metrics for new DB Transaction architecture [PR](https://github.com/BerriAI/litellm/pull/9719)\\n\\n## Performance / Loadbalancing / Reliability improvements\\n1. Preventing Deadlocks\\n    - Reduce DB Deadlocks by storing spend updates in Redis and then committing to DB [PR](https://github.com/BerriAI/litellm/pull/9608)\\n    - Ensure no deadlocks occur when updating\xa0DailyUserSpendTransaction [PR](https://github.com/BerriAI/litellm/pull/9690)\\n    - High Traffic fix - ensure new DB + Redis architecture accurately tracks spend [PR](https://github.com/BerriAI/litellm/pull/9673)\\n    - Use Redis for PodLock Manager instead of PG (ensures no deadlocks occur) [PR](https://github.com/BerriAI/litellm/pull/9715)\\n    - v2 DB Deadlock Reduction Architecture \u2013 Add Max Size for In-Memory Queue + Backpressure Mechanism [PR](https://github.com/BerriAI/litellm/pull/9759)\\n    \\n2. Prisma Migrations [Get Started](../../docs/proxy/prod#9-use-prisma-migrate-deploy)\\n    - connects litellm proxy to litellm\'s prisma migration files\\n    - Handle db schema updates from new `litellm-proxy-extras` sdk\\n3. Redis - support password for sync sentinel clients [PR](https://github.com/BerriAI/litellm/pull/9622)\\n4. Fix \\"Circular reference detected\\" error when max_parallel_requests = 0 [PR](https://github.com/BerriAI/litellm/pull/9671)\\n5. Code QA - Ban hardcoded numbers [PR](https://github.com/BerriAI/litellm/pull/9709)\\n\\n## Helm\\n1. fix: wrong indentation of ttlSecondsAfterFinished in chart [PR](https://github.com/BerriAI/litellm/pull/9611)\\n\\n## General Proxy Improvements\\n1. Fix - only apply service_account_settings.enforced_params on service accounts [PR](https://github.com/BerriAI/litellm/pull/9683)\\n2. Fix - handle metadata null on `/chat/completion` [PR](https://github.com/BerriAI/litellm/issues/9717)\\n3. Fix - Move daily user transaction logging outside of \'disable_spend_logs\' flag, as they\u2019re unrelated [PR](https://github.com/BerriAI/litellm/pull/9772)\\n\\n## Demo\\n\\nTry this on the demo instance [today](https://docs.litellm.ai/docs/proxy/demo)\\n\\n## Complete Git Diff\\n\\nSee the complete git diff since v1.65.0-stable, [here](https://github.com/BerriAI/litellm/releases/tag/v1.65.4-stable)"},{"id":"v1.65.0-stable","metadata":{"permalink":"/release_notes/v1.65.0-stable","source":"@site/release_notes/v1.65.0-stable/index.md","title":"v1.65.0-stable - Model Context Protocol","description":"v1.65.0-stable is live now. Here are the key highlights of this release:","date":"2025-03-30T10:00:00.000Z","tags":[{"inline":true,"label":"mcp","permalink":"/release_notes/tags/mcp"},{"inline":true,"label":"custom_prompt_management","permalink":"/release_notes/tags/custom-prompt-management"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.65.0-stable - Model Context Protocol","slug":"v1.65.0-stable","date":"2025-03-30T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"tags":["mcp","custom_prompt_management"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.65.4-stable","permalink":"/release_notes/v1.65.4-stable"},"nextItem":{"title":"v1.65.0 - Team Model Add - update","permalink":"/release_notes/v1.65.0"}},"content":"import Image from \'@theme/IdealImage\';\\n\\nv1.65.0-stable is live now. Here are the key highlights of this release:\\n- **MCP Support**: Support for adding and using MCP servers on the LiteLLM proxy.\\n- **UI view total usage after 1M+ logs**: You can now view usage analytics after crossing 1M+ logs in DB. \\n\\n## Model Context Protocol (MCP)\\n\\nThis release introduces support for centrally adding MCP servers on LiteLLM. This allows you to add MCP server endpoints and your developers can `list` and `call` MCP tools through LiteLLM.\\n\\nRead more about MCP [here](https://docs.litellm.ai/docs/mcp).\\n\\n<Image \\n  img={require(\'../../img/release_notes/mcp_ui.png\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n<p style={{textAlign: \'left\', color: \'#666\'}}>\\n  Expose and use MCP servers through LiteLLM\\n</p>\\n\\n## UI view total usage after 1M+ logs\\n\\nThis release brings the ability to view total usage analytics even after exceeding 1M+ logs in your database. We\'ve implemented a scalable architecture that stores only aggregate usage data, resulting in significantly more efficient queries and reduced database CPU utilization.\\n\\n\\n<Image \\n  img={require(\'../../img/release_notes/ui_usage.png\')}\\n  style={{width: \'100%\', display: \'block\', margin: \'2rem auto\'}}\\n/>\\n<p style={{textAlign: \'left\', color: \'#666\'}}>\\n  View total usage after 1M+ logs\\n</p>\\n\\n\\n- How this works:\\n    - We now aggregate usage data into a dedicated DailyUserSpend table, significantly reducing query load and CPU usage even beyond 1M+ logs.\\n\\n- Daily Spend Breakdown API:\\n\\n    - Retrieve granular daily usage data (by model, provider, and API key) with a single endpoint.\\n    Example Request:\\n\\n    ```shell title=\\"Daily Spend Breakdown API\\" showLineNumbers\\n    curl -L -X GET \'http://localhost:4000/user/daily/activity?start_date=2025-03-20&end_date=2025-03-27\' \\\\\\n    -H \'Authorization: Bearer sk-...\'\\n    ```\\n\\n    ```json title=\\"Daily Spend Breakdown API Response\\" showLineNumbers\\n    {\\n        \\"results\\": [\\n            {\\n                \\"date\\": \\"2025-03-27\\",\\n                \\"metrics\\": {\\n                    \\"spend\\": 0.0177072,\\n                    \\"prompt_tokens\\": 111,\\n                    \\"completion_tokens\\": 1711,\\n                    \\"total_tokens\\": 1822,\\n                    \\"api_requests\\": 11\\n                },\\n                \\"breakdown\\": {\\n                    \\"models\\": {\\n                        \\"gpt-4o-mini\\": {\\n                            \\"spend\\": 1.095e-05,\\n                            \\"prompt_tokens\\": 37,\\n                            \\"completion_tokens\\": 9,\\n                            \\"total_tokens\\": 46,\\n                            \\"api_requests\\": 1\\n                    },\\n                    \\"providers\\": { \\"openai\\": { ... }, \\"azure_ai\\": { ... } },\\n                    \\"api_keys\\": { \\"3126b6eaf1...\\": { ... } }\\n                }\\n            }\\n        ],\\n        \\"metadata\\": {\\n            \\"total_spend\\": 0.7274667,\\n            \\"total_prompt_tokens\\": 280990,\\n            \\"total_completion_tokens\\": 376674,\\n            \\"total_api_requests\\": 14\\n        }\\n    }\\n    ```\\n\\n\\n\\n\\n## New Models / Updated Models\\n- Support for Vertex AI gemini-2.0-flash-lite & Google AI Studio gemini-2.0-flash-lite [PR](https://github.com/BerriAI/litellm/pull/9523)\\n- Support for Vertex AI Fine-Tuned LLMs [PR](https://github.com/BerriAI/litellm/pull/9542)\\n- Nova Canvas image generation support [PR](https://github.com/BerriAI/litellm/pull/9525)\\n- OpenAI gpt-4o-transcribe support [PR](https://github.com/BerriAI/litellm/pull/9517)\\n- Added new Vertex AI text embedding model [PR](https://github.com/BerriAI/litellm/pull/9476)\\n\\n## LLM Translation\\n- OpenAI Web Search Tool Call Support [PR](https://github.com/BerriAI/litellm/pull/9465)\\n- Vertex AI topLogprobs support [PR](https://github.com/BerriAI/litellm/pull/9518) \\n- Support for sending images and video to Vertex AI multimodal embedding [Doc](https://docs.litellm.ai/docs/providers/vertex#multi-modal-embeddings)\\n- Support litellm.api_base for Vertex AI + Gemini across completion, embedding, image_generation [PR](https://github.com/BerriAI/litellm/pull/9516)\\n- Bug fix for returning `response_cost` when using litellm python SDK with LiteLLM Proxy [PR](https://github.com/BerriAI/litellm/commit/6fd18651d129d606182ff4b980e95768fc43ca3d)\\n- Support for `max_completion_tokens` on Mistral API [PR](https://github.com/BerriAI/litellm/pull/9606)\\n- Refactored Vertex AI passthrough routes - fixes unpredictable behaviour with auto-setting default_vertex_region on router model add [PR](https://github.com/BerriAI/litellm/pull/9467)\\n\\n## Spend Tracking Improvements\\n- Log \'api_base\' on spend logs [PR](https://github.com/BerriAI/litellm/pull/9509)\\n- Support for Gemini audio token cost tracking [PR](https://github.com/BerriAI/litellm/pull/9535)\\n- Fixed OpenAI audio input token cost tracking [PR](https://github.com/BerriAI/litellm/pull/9535)\\n\\n## UI\\n\\n### Model Management\\n- Allowed team admins to add/update/delete models on UI [PR](https://github.com/BerriAI/litellm/pull/9572)\\n- Added render supports_web_search on model hub [PR](https://github.com/BerriAI/litellm/pull/9469)\\n\\n### Request Logs\\n- Show API base and model ID on request logs [PR](https://github.com/BerriAI/litellm/pull/9572)\\n- Allow viewing keyinfo on request logs [PR](https://github.com/BerriAI/litellm/pull/9568)\\n\\n### Usage Tab\\n- Added Daily User Spend Aggregate view - allows UI Usage tab to work > 1m rows [PR](https://github.com/BerriAI/litellm/pull/9538)\\n- Connected UI to \\"LiteLLM_DailyUserSpend\\" spend table [PR](https://github.com/BerriAI/litellm/pull/9603)\\n\\n## Logging Integrations\\n- Fixed StandardLoggingPayload for GCS Pub Sub Logging Integration [PR](https://github.com/BerriAI/litellm/pull/9508)\\n- Track `litellm_model_name` on `StandardLoggingPayload` [Docs](https://docs.litellm.ai/docs/proxy/logging_spec#standardlogginghiddenparams)\\n\\n## Performance / Reliability Improvements\\n- LiteLLM Redis semantic caching implementation [PR](https://github.com/BerriAI/litellm/pull/9356)\\n- Gracefully handle exceptions when DB is having an outage [PR](https://github.com/BerriAI/litellm/pull/9533)\\n- Allow Pods to startup + passing /health/readiness when allow_requests_on_db_unavailable: True and DB is down [PR](https://github.com/BerriAI/litellm/pull/9569)\\n\\n\\n## General Improvements\\n- Support for exposing MCP tools on litellm proxy [PR](https://github.com/BerriAI/litellm/pull/9426)\\n- Support discovering Gemini, Anthropic, xAI models by calling their /v1/model endpoint [PR](https://github.com/BerriAI/litellm/pull/9530)\\n- Fixed route check for non-proxy admins on JWT auth [PR](https://github.com/BerriAI/litellm/pull/9454)\\n- Added baseline Prisma database migrations [PR](https://github.com/BerriAI/litellm/pull/9565)\\n- View all wildcard models on /model/info [PR](https://github.com/BerriAI/litellm/pull/9572)\\n\\n\\n## Security\\n- Bumped next from 14.2.21 to 14.2.25 in UI dashboard [PR](https://github.com/BerriAI/litellm/pull/9458)\\n\\n## Complete Git Diff\\n\\n[Here\'s the complete git diff](https://github.com/BerriAI/litellm/compare/v1.63.14-stable.patch1...v1.65.0-stable)"},{"id":"v1.65.0","metadata":{"permalink":"/release_notes/v1.65.0","source":"@site/release_notes/v1.65.0/index.md","title":"v1.65.0 - Team Model Add - update","description":"v1.65.0 updates the /model/new endpoint to prevent non-team admins from creating team models.","date":"2025-03-28T10:00:00.000Z","tags":[{"inline":true,"label":"management endpoints","permalink":"/release_notes/tags/management-endpoints"},{"inline":true,"label":"team models","permalink":"/release_notes/tags/team-models"},{"inline":true,"label":"ui","permalink":"/release_notes/tags/ui"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.65.0 - Team Model Add - update","slug":"v1.65.0","date":"2025-03-28T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"tags":["management endpoints","team models","ui"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.65.0-stable - Model Context Protocol","permalink":"/release_notes/v1.65.0-stable"},"nextItem":{"title":"v1.63.14-stable","permalink":"/release_notes/v1.63.14-stable"}},"content":"import Image from \'@theme/IdealImage\';\\n\\nv1.65.0 updates the `/model/new` endpoint to prevent non-team admins from creating team models.\\n\\nThis means that only proxy admins or team admins can create team models.\\n\\n## Additional Changes\\n\\n- Allows team admins to call `/model/update` to update team models.\\n- Allows team admins to call `/model/delete` to delete team models.\\n- Introduces new `user_models_only` param to `/v2/model/info` - only return models added by this user.\\n\\n\\nThese changes enable team admins to add and manage models for their team on the LiteLLM UI + API.\\n\\n\\n<Image img={require(\'../../img/release_notes/team_model_add.png\')} />"},{"id":"v1.63.14-stable","metadata":{"permalink":"/release_notes/v1.63.14-stable","source":"@site/release_notes/v1.63.14/index.md","title":"v1.63.14-stable","description":"These are the changes since v1.63.11-stable.","date":"2025-03-22T10:00:00.000Z","tags":[{"inline":true,"label":"credential management","permalink":"/release_notes/tags/credential-management"},{"inline":true,"label":"thinking content","permalink":"/release_notes/tags/thinking-content"},{"inline":true,"label":"responses api","permalink":"/release_notes/tags/responses-api"},{"inline":true,"label":"snowflake","permalink":"/release_notes/tags/snowflake"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.63.14-stable","slug":"v1.63.14-stable","date":"2025-03-22T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"tags":["credential management","thinking content","responses api","snowflake"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.65.0 - Team Model Add - update","permalink":"/release_notes/v1.65.0"},"nextItem":{"title":"v1.63.11-stable","permalink":"/release_notes/v1.63.11-stable"}},"content":"import Image from \'@theme/IdealImage\';\\n\\nThese are the changes since `v1.63.11-stable`.\\n\\nThis release brings:\\n- LLM Translation Improvements (MCP Support and Bedrock Application Profiles)\\n- Perf improvements for Usage-based Routing\\n- Streaming guardrail support via websockets\\n- Azure OpenAI client perf fix (from previous release)\\n\\n## Docker Run LiteLLM Proxy\\n\\n```\\ndocker run\\n-e STORE_MODEL_IN_DB=True\\n-p 4000:4000\\ndocker.litellm.ai/berriai/litellm:main-v1.63.14-stable.patch1\\n```\\n\\n## Demo Instance\\n\\nHere\'s a Demo Instance to test changes:\\n- Instance: https://demo.litellm.ai/\\n- Login Credentials:\\n    - Username: admin\\n    - Password: sk-1234\\n\\n\\n\\n## New Models / Updated Models\\n\\n- Azure gpt-4o - fixed pricing to latest global pricing - [PR](https://github.com/BerriAI/litellm/pull/9361)\\n- O1-Pro - add pricing + model information - [PR](https://github.com/BerriAI/litellm/pull/9397)\\n- Azure AI - mistral 3.1 small pricing added - [PR](https://github.com/BerriAI/litellm/pull/9453)\\n- Azure - gpt-4.5-preview pricing added - [PR](https://github.com/BerriAI/litellm/pull/9453)\\n\\n\\n\\n## LLM Translation\\n\\n1. **New LLM Features**\\n\\n- Bedrock: Support bedrock application inference profiles [Docs](https://docs.litellm.ai/docs/providers/bedrock#bedrock-application-inference-profile)\\n   - Infer aws region from bedrock application profile id - (`arn:aws:bedrock:us-east-1:...`)\\n- Ollama - support calling via `/v1/completions` [Get Started](../../docs/providers/ollama#using-ollama-fim-on-v1completions)\\n- Bedrock - support `us.deepseek.r1-v1:0` model name [Docs](../../docs/providers/bedrock#supported-aws-bedrock-models)\\n- OpenRouter - `OPENROUTER_API_BASE` env var support [Docs](../../docs/providers/openrouter.md)\\n- Azure - add audio model parameter support - [Docs](../../docs/providers/azure#azure-audio-model)\\n- OpenAI - PDF File support [Docs](../../docs/completion/document_understanding#openai-file-message-type)\\n- OpenAI - o1-pro Responses API streaming support [Docs](../../docs/response_api.md#streaming)\\n- [BETA] MCP - Use MCP Tools with LiteLLM SDK [Docs](../../docs/mcp)\\n\\n2. **Bug Fixes**\\n\\n- Voyage: prompt token on embedding tracking fix - [PR](https://github.com/BerriAI/litellm/commit/56d3e75b330c3c3862dc6e1c51c1210e48f1068e)\\n- Sagemaker - Fix \u2018Too little data for declared Content-Length\u2019 error - [PR](https://github.com/BerriAI/litellm/pull/9326)\\n- OpenAI-compatible models - fix issue when calling openai-compatible models w/ custom_llm_provider set - [PR](https://github.com/BerriAI/litellm/pull/9355)\\n- VertexAI - Embedding \u2018outputDimensionality\u2019 support - [PR](https://github.com/BerriAI/litellm/commit/437dbe724620675295f298164a076cbd8019d304)\\n- Anthropic - return consistent json response format on streaming/non-streaming - [PR](https://github.com/BerriAI/litellm/pull/9437)\\n\\n## Spend Tracking Improvements\\n\\n- `litellm_proxy/` - support reading litellm response cost header from proxy, when using client sdk \\n- Reset Budget Job - fix budget reset error on keys/teams/users [PR](https://github.com/BerriAI/litellm/pull/9329)\\n- Streaming - Prevents final chunk w/ usage from being ignored (impacted bedrock streaming + cost tracking) [PR](https://github.com/BerriAI/litellm/pull/9314)\\n\\n\\n## UI\\n\\n1. Users Page\\n   - Feature: Control default internal user settings [PR](https://github.com/BerriAI/litellm/pull/9328)\\n2. Icons:\\n   - Feature: Replace external \\"artificialanalysis.ai\\" icons by local svg [PR](https://github.com/BerriAI/litellm/pull/9374)\\n3. Sign In/Sign Out\\n   - Fix: Default login when `default_user_id` user does not exist in DB [PR](https://github.com/BerriAI/litellm/pull/9395)\\n\\n\\n## Logging Integrations\\n\\n- Support post-call guardrails for streaming responses [Get Started](../../docs/proxy/guardrails/custom_guardrail#1-write-a-customguardrail-class)\\n- Arize [Get Started](../../docs/observability/arize_integration)\\n   - fix invalid package import [PR](https://github.com/BerriAI/litellm/pull/9338)\\n   - migrate to using standardloggingpayload for metadata, ensures spans land successfully [PR](https://github.com/BerriAI/litellm/pull/9338)\\n   - fix logging to just log the LLM I/O [PR](https://github.com/BerriAI/litellm/pull/9353)\\n   - Dynamic API Key/Space param support [Get Started](../../docs/observability/arize_integration#pass-arize-spacekey-per-request)\\n- StandardLoggingPayload - Log litellm_model_name in payload. Allows knowing what the model sent to API provider was [Get Started](../../docs/proxy/logging_spec#standardlogginghiddenparams)\\n- Prompt Management - Allow building custom prompt management integration [Get Started](../../docs/proxy/custom_prompt_management.md)\\n\\n## Performance / Reliability improvements\\n\\n- Redis Caching - add 5s default timeout, prevents hanging redis connection from impacting llm calls [PR](https://github.com/BerriAI/litellm/commit/db92956ae33ed4c4e3233d7e1b0c7229817159bf)\\n- Allow disabling all spend updates / writes to DB - patch to allow disabling all spend updates to DB with a flag [PR](https://github.com/BerriAI/litellm/pull/9331)\\n- Azure OpenAI - correctly re-use azure openai client, fixes perf issue from previous Stable release [PR](https://github.com/BerriAI/litellm/commit/f2026ef907c06d94440930917add71314b901413)\\n- Azure OpenAI - uses litellm.ssl_verify on Azure/OpenAI clients [PR](https://github.com/BerriAI/litellm/commit/f2026ef907c06d94440930917add71314b901413)\\n- Usage-based routing - Wildcard model support [Get Started](../../docs/proxy/usage_based_routing#wildcard-model-support)\\n- Usage-based routing - Support batch writing increments to redis - reduces latency to same as \u2018simple-shuffle\u2019 [PR](https://github.com/BerriAI/litellm/pull/9357)\\n- Router - show reason for model cooldown on \u2018no healthy deployments available error\u2019 [PR](https://github.com/BerriAI/litellm/pull/9438)\\n- Caching - add max value limit to an item in in-memory cache (1MB) - prevents OOM errors on large image url\u2019s being sent through proxy [PR](https://github.com/BerriAI/litellm/pull/9448)\\n\\n\\n## General Improvements\\n\\n- Passthrough Endpoints - support returning api-base on pass-through endpoints Response Headers [Docs](../../docs/proxy/response_headers#litellm-specific-headers)\\n- SSL - support reading ssl security level from env var - Allows user to specify lower security settings [Get Started](../../docs/guides/security_settings)\\n- Credentials - only poll Credentials table when `STORE_MODEL_IN_DB` is True [PR](https://github.com/BerriAI/litellm/pull/9376)\\n- Image URL Handling - new architecture doc on image url handling [Docs](../../docs/proxy/image_handling)\\n- OpenAI - bump to pip install \\"openai==1.68.2\\" [PR](https://github.com/BerriAI/litellm/commit/e85e3bc52a9de86ad85c3dbb12d87664ee567a5a)\\n- Gunicorn - security fix - bump gunicorn==23.0.0 [PR](https://github.com/BerriAI/litellm/commit/7e9fc92f5c7fea1e7294171cd3859d55384166eb)\\n\\n\\n## Complete Git Diff\\n\\n[Here\'s the complete git diff](https://github.com/BerriAI/litellm/compare/v1.63.11-stable...v1.63.14.rc)"},{"id":"v1.63.11-stable","metadata":{"permalink":"/release_notes/v1.63.11-stable","source":"@site/release_notes/v1.63.11-stable/index.md","title":"v1.63.11-stable","description":"These are the changes since v1.63.2-stable.","date":"2025-03-15T10:00:00.000Z","tags":[{"inline":true,"label":"credential management","permalink":"/release_notes/tags/credential-management"},{"inline":true,"label":"thinking content","permalink":"/release_notes/tags/thinking-content"},{"inline":true,"label":"responses api","permalink":"/release_notes/tags/responses-api"},{"inline":true,"label":"snowflake","permalink":"/release_notes/tags/snowflake"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.63.11-stable","slug":"v1.63.11-stable","date":"2025-03-15T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"tags":["credential management","thinking content","responses api","snowflake"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.63.14-stable","permalink":"/release_notes/v1.63.14-stable"},"nextItem":{"title":"v1.63.2-stable","permalink":"/release_notes/v1.63.2-stable"}},"content":"import Image from \'@theme/IdealImage\';\\n\\nThese are the changes since `v1.63.2-stable`.\\n\\nThis release is primarily focused on:\\n- [Beta] Responses API Support\\n- Snowflake Cortex Support, Amazon Nova Image Generation\\n- UI - Credential Management, re-use credentials when adding new models\\n- UI - Test Connection to LLM Provider before adding a model\\n\\n## Known Issues\\n- \ud83d\udea8 Known issue on Azure OpenAI - We don\'t recommend upgrading if you use Azure OpenAI. This version failed our Azure OpenAI load test\\n\\n\\n## Docker Run LiteLLM Proxy\\n\\n```\\ndocker run\\n-e STORE_MODEL_IN_DB=True\\n-p 4000:4000\\ndocker.litellm.ai/berriai/litellm:main-v1.63.11-stable\\n```\\n\\n## Demo Instance\\n\\nHere\'s a Demo Instance to test changes:\\n- Instance: https://demo.litellm.ai/\\n- Login Credentials:\\n    - Username: admin\\n    - Password: sk-1234\\n\\n\\n\\n## New Models / Updated Models\\n\\n- Image Generation support for Amazon Nova Canvas [Getting Started](https://docs.litellm.ai/docs/providers/bedrock#image-generation)\\n- Add pricing for Jamba new models [PR](https://github.com/BerriAI/litellm/pull/9032/files)\\n- Add pricing for Amazon EU models [PR](https://github.com/BerriAI/litellm/pull/9056/files)\\n- Add Bedrock Deepseek R1 model pricing [PR](https://github.com/BerriAI/litellm/pull/9108/files)\\n- Update Gemini pricing: Gemma 3, Flash 2 thinking update, LearnLM [PR](https://github.com/BerriAI/litellm/pull/9190/files)\\n- Mark Cohere Embedding 3 models as Multimodal [PR](https://github.com/BerriAI/litellm/pull/9176/commits/c9a576ce4221fc6e50dc47cdf64ab62736c9da41)\\n- Add Azure Data Zone pricing [PR](https://github.com/BerriAI/litellm/pull/9185/files#diff-19ad91c53996e178c1921cbacadf6f3bae20cfe062bd03ee6bfffb72f847ee37)\\n   - LiteLLM Tracks cost for `azure/eu` and `azure/us` models\\n\\n\\n\\n## LLM Translation\\n\\n<Image img={require(\'../../img/release_notes/responses_api.png\')} />\\n\\n1. **New Endpoints**\\n- [Beta] POST `/responses` API. [Getting Started](https://docs.litellm.ai/docs/response_api)\\n\\n2. **New LLM Providers**\\n- Snowflake Cortex [Getting Started](https://docs.litellm.ai/docs/providers/snowflake)\\n\\n3. **New LLM Features**\\n\\n- Support OpenRouter `reasoning_content` on streaming [Getting Started](https://docs.litellm.ai/docs/reasoning_content)\\n\\n4. **Bug Fixes**\\n\\n- OpenAI: Return `code`, `param` and `type` on bad request error [More information on litellm exceptions](https://docs.litellm.ai/docs/exception_mapping)\\n- Bedrock: Fix converse chunk parsing to only return empty dict on tool use [PR](https://github.com/BerriAI/litellm/pull/9166)\\n- Bedrock: Support extra_headers [PR](https://github.com/BerriAI/litellm/pull/9113)\\n- Azure: Fix Function Calling Bug & Update Default API Version to `2025-02-01-preview` [PR](https://github.com/BerriAI/litellm/pull/9191)\\n- Azure: Fix AI services URL [PR](https://github.com/BerriAI/litellm/pull/9185)\\n- Vertex AI: Handle HTTP 201 status code in response [PR](https://github.com/BerriAI/litellm/pull/9193)\\n- Perplexity: Fix incorrect streaming response [PR](https://github.com/BerriAI/litellm/pull/9081)\\n- Triton: Fix streaming completions bug [PR](https://github.com/BerriAI/litellm/pull/8386)\\n- Deepgram: Support bytes.IO when handling audio files for transcription [PR](https://github.com/BerriAI/litellm/pull/9071)\\n- Ollama: Fix \\"system\\" role has become unacceptable [PR](https://github.com/BerriAI/litellm/pull/9261)\\n- All Providers (Streaming): Fix String `data:` stripped from entire content in streamed responses [PR](https://github.com/BerriAI/litellm/pull/9070)\\n\\n\\n\\n## Spend Tracking Improvements\\n\\n1. Support Bedrock converse cache token tracking [Getting Started](https://docs.litellm.ai/docs/completion/prompt_caching)\\n2. Cost Tracking for Responses API [Getting Started](https://docs.litellm.ai/docs/response_api)\\n3. Fix Azure Whisper cost tracking [Getting Started](https://docs.litellm.ai/docs/audio_transcription)\\n\\n\\n## UI\\n\\n### Re-Use Credentials on UI\\n\\nYou can now onboard LLM provider credentials on LiteLLM UI. Once these credentials are added you can re-use them when adding new models [Getting Started](https://docs.litellm.ai/docs/proxy/ui_credentials)\\n\\n<Image img={require(\'../../img/release_notes/credentials.jpg\')} />\\n\\n\\n### Test Connections before adding models\\n\\nBefore adding a model you can test the connection to the LLM provider to verify you have setup your API Base + API Key correctly\\n\\n<Image img={require(\'../../img/release_notes/litellm_test_connection.gif\')} />\\n\\n### General UI Improvements\\n1. Add Models Page\\n   - Allow adding Cerebras, Sambanova, Perplexity, Fireworks, Openrouter, TogetherAI Models, Text-Completion OpenAI on Admin UI\\n   - Allow adding EU OpenAI models\\n   - Fix: Instantly show edit + deletes to models\\n2. Keys Page\\n   - Fix: Instantly show newly created keys on Admin UI (don\'t require refresh)\\n   - Fix: Allow clicking into Top Keys when showing users Top API Key\\n   - Fix: Allow Filter Keys by Team Alias, Key Alias and Org\\n   - UI Improvements: Show 100 Keys Per Page, Use full height, increase width of key alias\\n3. Users Page\\n   - Fix: Show correct count of internal user keys on Users Page\\n   - Fix: Metadata not updating in Team UI\\n4. Logs Page\\n   - UI Improvements: Keep expanded log in focus on LiteLLM UI\\n   - UI Improvements: Minor improvements to logs page\\n   - Fix: Allow internal user to query their own logs\\n   - Allow switching off storing Error Logs in DB [Getting Started](https://docs.litellm.ai/docs/proxy/ui_logs)\\n5. Sign In/Sign Out\\n   - Fix: Correctly use `PROXY_LOGOUT_URL` when set [Getting Started](https://docs.litellm.ai/docs/proxy/self_serve#setting-custom-logout-urls)\\n\\n\\n## Security\\n\\n1. Support for Rotating Master Keys [Getting Started](https://docs.litellm.ai/docs/proxy/master_key_rotations)\\n2. Fix: Internal User Viewer Permissions, don\'t allow `internal_user_viewer` role to see `Test Key Page` or `Create Key Button` [More information on role based access controls](https://docs.litellm.ai/docs/proxy/access_control)\\n3. Emit audit logs on All user + model Create/Update/Delete endpoints [Getting Started](https://docs.litellm.ai/docs/proxy/multiple_admins)\\n4. JWT\\n    - Support multiple JWT OIDC providers [Getting Started](https://docs.litellm.ai/docs/proxy/token_auth)\\n    - Fix JWT access with Groups not working when team is assigned All Proxy Models access\\n5. Using K/V pairs in 1 AWS Secret [Getting Started](https://docs.litellm.ai/docs/secret#using-kv-pairs-in-1-aws-secret)\\n\\n\\n## Logging Integrations\\n\\n1. Prometheus: Track Azure LLM API latency metric [Getting Started](https://docs.litellm.ai/docs/proxy/prometheus#request-latency-metrics)\\n2. Athina: Added tags, user_feedback and model_options to additional_keys which can be sent to Athina [Getting Started](https://docs.litellm.ai/docs/observability/athina_integration)\\n\\n\\n## Performance / Reliability improvements\\n\\n1. Redis + litellm router - Fix Redis cluster mode for litellm router [PR](https://github.com/BerriAI/litellm/pull/9010)\\n\\n\\n## General Improvements\\n\\n1. OpenWebUI Integration - display `thinking` tokens\\n- Guide on getting started with LiteLLM x OpenWebUI. [Getting Started](https://docs.litellm.ai/docs/tutorials/openweb_ui)\\n- Display `thinking` tokens on OpenWebUI (Bedrock, Anthropic, Deepseek) [Getting Started](https://docs.litellm.ai/docs/tutorials/openweb_ui#render-thinking-content-on-openweb-ui)\\n\\n<Image img={require(\'../../img/litellm_thinking_openweb.gif\')} />\\n\\n\\n## Complete Git Diff\\n\\n[Here\'s the complete git diff](https://github.com/BerriAI/litellm/compare/v1.63.2-stable...v1.63.11-stable)"},{"id":"v1.63.2-stable","metadata":{"permalink":"/release_notes/v1.63.2-stable","source":"@site/release_notes/v1.63.2-stable/index.md","title":"v1.63.2-stable","description":"These are the changes since v1.61.20-stable.","date":"2025-03-08T10:00:00.000Z","tags":[{"inline":true,"label":"llm translation","permalink":"/release_notes/tags/llm-translation"},{"inline":true,"label":"thinking","permalink":"/release_notes/tags/thinking"},{"inline":true,"label":"reasoning_content","permalink":"/release_notes/tags/reasoning-content"},{"inline":true,"label":"claude-3-7-sonnet","permalink":"/release_notes/tags/claude-3-7-sonnet"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.63.2-stable","slug":"v1.63.2-stable","date":"2025-03-08T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"tags":["llm translation","thinking","reasoning_content","claude-3-7-sonnet"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.63.11-stable","permalink":"/release_notes/v1.63.11-stable"},"nextItem":{"title":"v1.63.0 - Anthropic \'thinking\' response update","permalink":"/release_notes/v1.63.0"}},"content":"import Image from \'@theme/IdealImage\';\\n\\n\\nThese are the changes since `v1.61.20-stable`.\\n\\nThis release is primarily focused on:\\n- LLM Translation improvements (more `thinking` content improvements)\\n- UI improvements (Error logs now shown on UI)\\n\\n\\n:::info\\n\\nThis release will be live on 03/09/2025\\n\\n::: \\n\\n<Image img={require(\'../../img/release_notes/v1632_release.jpg\')} />\\n\\n\\n## Demo Instance\\n\\nHere\'s a Demo Instance to test changes:\\n- Instance: https://demo.litellm.ai/\\n- Login Credentials:\\n    - Username: admin\\n    - Password: sk-1234\\n\\n\\n## New Models / Updated Models\\n\\n1. Add\xa0`supports_pdf_input`\xa0for specific Bedrock Claude models\xa0[PR](https://github.com/BerriAI/litellm/commit/f63cf0030679fe1a43d03fb196e815a0f28dae92)\\n2. Add pricing for amazon `eu` models [PR](https://github.com/BerriAI/litellm/commits/main/model_prices_and_context_window.json)\\n3. Fix Azure O1 mini pricing [PR](https://github.com/BerriAI/litellm/commit/52de1949ef2f76b8572df751f9c868a016d4832c)\\n\\n## LLM Translation\\n\\n<Image img={require(\'../../img/release_notes/anthropic_thinking.jpg\')}/>\\n\\n1. Support `/openai/` passthrough for Assistant endpoints. [Get Started](https://docs.litellm.ai/docs/pass_through/openai_passthrough)\\n2. Bedrock Claude - fix tool calling transformation on invoke route. [Get Started](../../docs/providers/bedrock#usage---function-calling--tool-calling)\\n3. Bedrock Claude - response_format support for claude on invoke route. [Get Started](../../docs/providers/bedrock#usage---structured-output--json-mode)\\n4. Bedrock - pass `description` if set in response_format. [Get Started](../../docs/providers/bedrock#usage---structured-output--json-mode)\\n5. Bedrock - Fix passing\xa0response_format: `{\\"type\\": \\"text\\"}`. [PR](https://github.com/BerriAI/litellm/commit/c84b489d5897755139aa7d4e9e54727ebe0fa540)\\n6. OpenAI - Handle sending\xa0image_url\xa0as str to openai. [Get Started](https://docs.litellm.ai/docs/completion/vision)\\n7. Deepseek - return \'reasoning_content\' missing on streaming. [Get Started](https://docs.litellm.ai/docs/reasoning_content)\\n8. Caching - Support caching on reasoning content. [Get Started](https://docs.litellm.ai/docs/proxy/caching)\\n9. Bedrock - handle thinking blocks in assistant message. [Get Started](https://docs.litellm.ai/docs/providers/bedrock#usage---thinking--reasoning-content)\\n10. Anthropic - Return `signature` on streaming. [Get Started](https://docs.litellm.ai/docs/providers/bedrock#usage---thinking--reasoning-content)\\n- Note: We\'ve also migrated from `signature_delta` to `signature`. [Read more](https://docs.litellm.ai/release_notes/v1.63.0)\\n11. Support\xa0format\xa0param for specifying image type. [Get Started](../../docs/completion/vision.md#explicitly-specify-image-type)\\n12. Anthropic - `/v1/messages` endpoint - `thinking` param support. [Get Started](../../docs/anthropic_unified.md)\\n- Note: this refactors the [BETA] unified `/v1/messages` endpoint, to just work for the Anthropic API. \\n13. Vertex AI - handle $id in response schema when calling vertex ai. [Get Started](https://docs.litellm.ai/docs/providers/vertex#json-schema)\\n\\n## Spend Tracking Improvements\\n\\n1. Batches API - Fix cost calculation to run on retrieve_batch. [Get Started](https://docs.litellm.ai/docs/batches)\\n2. Batches API - Log batch models in spend logs / standard logging payload. [Get Started](../../docs/proxy/logging_spec.md#standardlogginghiddenparams)\\n\\n## Management Endpoints / UI\\n\\n<Image img={require(\'../../img/release_notes/error_logs.jpg\')} />\\n\\n1. Virtual Keys Page\\n    - Allow team/org filters to be searchable on the Create Key Page\\n    - Add\xa0created_by\xa0and\xa0updated_by\xa0fields to Keys table\\n    - Show \'user_email\' on key table\\n    - Show 100 Keys Per Page, Use full height, increase width of key alias\\n2. Logs Page\\n    - Show Error Logs on LiteLLM UI\\n    - Allow Internal Users to View their own logs\\n3. Internal Users Page \\n    - Allow admin to control default model access for internal users\\n7. Fix session handling with cookies\\n\\n## Logging / Guardrail Integrations\\n\\n1. Fix prometheus metrics w/ custom metrics, when keys containing team_id make requests. [PR](https://github.com/BerriAI/litellm/pull/8935)\\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n1. Cooldowns - Support cooldowns on models called with client side credentials. [Get Started](https://docs.litellm.ai/docs/proxy/clientside_auth#pass-user-llm-api-keys--api-base)\\n2. Tag-based Routing - ensures tag-based routing across all endpoints (`/embeddings`, `/image_generation`, etc.). [Get Started](https://docs.litellm.ai/docs/proxy/tag_routing)\\n\\n## General Proxy Improvements\\n\\n1. Raise\xa0BadRequestError\xa0when unknown model passed in request\\n2. Enforce model access restrictions on Azure OpenAI proxy route\\n3. Reliability fix - Handle emoji\u2019s in text - fix orjson error\\n4. Model Access Patch - don\'t overwrite litellm.anthropic_models when running auth checks\\n5. Enable setting timezone information in docker image \\n\\n## Complete Git Diff\\n\\n[Here\'s the complete git diff](https://github.com/BerriAI/litellm/compare/v1.61.20-stable...v1.63.2-stable)"},{"id":"v1.63.0","metadata":{"permalink":"/release_notes/v1.63.0","source":"@site/release_notes/v1.63.0/index.md","title":"v1.63.0 - Anthropic \'thinking\' response update","description":"v1.63.0 fixes Anthropic \'thinking\' response on streaming to return the signature block. Github Issue","date":"2025-03-05T10:00:00.000Z","tags":[{"inline":true,"label":"llm translation","permalink":"/release_notes/tags/llm-translation"},{"inline":true,"label":"thinking","permalink":"/release_notes/tags/thinking"},{"inline":true,"label":"reasoning_content","permalink":"/release_notes/tags/reasoning-content"},{"inline":true,"label":"claude-3-7-sonnet","permalink":"/release_notes/tags/claude-3-7-sonnet"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.63.0 - Anthropic \'thinking\' response update","slug":"v1.63.0","date":"2025-03-05T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"tags":["llm translation","thinking","reasoning_content","claude-3-7-sonnet"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.63.2-stable","permalink":"/release_notes/v1.63.2-stable"},"nextItem":{"title":"v1.61.20-stable","permalink":"/release_notes/v1.61.20-stable"}},"content":"v1.63.0 fixes Anthropic \'thinking\' response on streaming to return the `signature` block. [Github Issue](https://github.com/BerriAI/litellm/issues/8964)\\n\\n\\n\\nIt also moves the response structure from `signature_delta` to `signature` to be the same as Anthropic. [Anthropic Docs](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#implementing-extended-thinking)\\n\\n\\n## Diff \\n\\n```bash\\n\\"message\\": {\\n    ...\\n    \\"reasoning_content\\": \\"The capital of France is Paris.\\",\\n    \\"thinking_blocks\\": [\\n        {\\n            \\"type\\": \\"thinking\\",\\n            \\"thinking\\": \\"The capital of France is Paris.\\",\\n-            \\"signature_delta\\": \\"EqoBCkgIARABGAIiQL2UoU0b1OHYi+...\\" # \ud83d\udc48 OLD FORMAT\\n+            \\"signature\\": \\"EqoBCkgIARABGAIiQL2UoU0b1OHYi+...\\" # \ud83d\udc48 KEY CHANGE\\n        }\\n    ]\\n}\\n```"},{"id":"v1.61.20-stable","metadata":{"permalink":"/release_notes/v1.61.20-stable","source":"@site/release_notes/v1.61.20-stable/index.md","title":"v1.61.20-stable","description":"These are the changes since v1.61.13-stable.","date":"2025-03-01T10:00:00.000Z","tags":[{"inline":true,"label":"llm translation","permalink":"/release_notes/tags/llm-translation"},{"inline":true,"label":"rerank","permalink":"/release_notes/tags/rerank"},{"inline":true,"label":"ui","permalink":"/release_notes/tags/ui"},{"inline":true,"label":"thinking","permalink":"/release_notes/tags/thinking"},{"inline":true,"label":"reasoning_content","permalink":"/release_notes/tags/reasoning-content"},{"inline":true,"label":"claude-3-7-sonnet","permalink":"/release_notes/tags/claude-3-7-sonnet"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.61.20-stable","slug":"v1.61.20-stable","date":"2025-03-01T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"tags":["llm translation","rerank","ui","thinking","reasoning_content","claude-3-7-sonnet"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.63.0 - Anthropic \'thinking\' response update","permalink":"/release_notes/v1.63.0"},"nextItem":{"title":"v1.59.8-stable","permalink":"/release_notes/v1.59.8-stable"}},"content":"import Image from \'@theme/IdealImage\';\\n\\n\\n\\nThese are the changes since `v1.61.13-stable`.\\n\\nThis release is primarily focused on:\\n- LLM Translation improvements (claude-3-7-sonnet + \'thinking\'/\'reasoning_content\' support)\\n- UI improvements (add model flow, user management, etc)\\n\\n## Demo Instance\\n\\nHere\'s a Demo Instance to test changes:\\n- Instance: https://demo.litellm.ai/\\n- Login Credentials:\\n    - Username: admin\\n    - Password: sk-1234\\n\\n## New Models / Updated Models\\n\\n1. Anthropic 3-7 sonnet support + cost tracking (Anthropic API + Bedrock + Vertex AI + OpenRouter) \\n    1. Anthropic API [Start here](https://docs.litellm.ai/docs/providers/anthropic#usage---thinking--reasoning_content)\\n    2. Bedrock API [Start here](https://docs.litellm.ai/docs/providers/bedrock#usage---thinking--reasoning-content)\\n    3. Vertex AI API [See here](../../docs/providers/vertex#usage---thinking--reasoning_content)\\n    4. OpenRouter [See here](https://github.com/BerriAI/litellm/blob/ba5bdce50a0b9bc822de58c03940354f19a733ed/model_prices_and_context_window.json#L5626)\\n2. Gpt-4.5-preview support + cost tracking [See here](https://github.com/BerriAI/litellm/blob/ba5bdce50a0b9bc822de58c03940354f19a733ed/model_prices_and_context_window.json#L79)\\n3. Azure AI - Phi-4 cost tracking [See here](https://github.com/BerriAI/litellm/blob/ba5bdce50a0b9bc822de58c03940354f19a733ed/model_prices_and_context_window.json#L1773)\\n4. Claude-3.5-sonnet - vision support updated on Anthropic API [See here](https://github.com/BerriAI/litellm/blob/ba5bdce50a0b9bc822de58c03940354f19a733ed/model_prices_and_context_window.json#L2888)\\n5. Bedrock llama vision support [See here](https://github.com/BerriAI/litellm/blob/ba5bdce50a0b9bc822de58c03940354f19a733ed/model_prices_and_context_window.json#L7714)\\n6. Cerebras llama3.3-70b pricing [See here](https://github.com/BerriAI/litellm/blob/ba5bdce50a0b9bc822de58c03940354f19a733ed/model_prices_and_context_window.json#L2697)\\n\\n## LLM Translation\\n\\n1. Infinity Rerank - support returning documents when return_documents=True [Start here](../../docs/providers/infinity#usage---returning-documents)\\n2. Amazon Deepseek - `<think>` param extraction into \u2018reasoning_content\u2019 [Start here](https://docs.litellm.ai/docs/providers/bedrock#bedrock-imported-models-deepseek-deepseek-r1)\\n3. Amazon Titan Embeddings - filter out \u2018aws_\u2019 params from request body [Start here](https://docs.litellm.ai/docs/providers/bedrock#bedrock-embedding)\\n4. Anthropic \u2018thinking\u2019 + \u2018reasoning_content\u2019 translation support (Anthropic API, Bedrock, Vertex AI)  [Start here](https://docs.litellm.ai/docs/reasoning_content)\\n5. VLLM - support \u2018video_url\u2019 [Start here](../../docs/providers/vllm#send-video-url-to-vllm)\\n6. Call proxy via litellm SDK: Support `litellm_proxy/` for embedding, image_generation, transcription, speech, rerank [Start here](https://docs.litellm.ai/docs/providers/litellm_proxy)\\n7. OpenAI Pass-through - allow using Assistants GET, DELETE on /openai pass through routes [Start here](https://docs.litellm.ai/docs/pass_through/openai_passthrough)\\n8. Message Translation - fix openai message for assistant msg if role is missing - openai allows this\\n9. O1/O3 - support \u2018drop_params\u2019 for o3-mini and o1 parallel_tool_calls param (not supported currently) [See here](https://docs.litellm.ai/docs/completion/drop_params)\\n\\n## Spend Tracking Improvements\\n\\n1. Cost tracking for rerank via Bedrock [See PR](https://github.com/BerriAI/litellm/commit/b682dc4ec8fd07acf2f4c981d2721e36ae2a49c5)\\n2. Anthropic pass-through - fix race condition causing cost to not be tracked [See PR](https://github.com/BerriAI/litellm/pull/8874)\\n3. Anthropic pass-through: Ensure accurate token counting [See PR](https://github.com/BerriAI/litellm/pull/8880)\\n\\n## Management Endpoints / UI\\n\\n1. Models Page - Allow sorting models by \u2018created at\u2019\\n2. Models Page - Edit Model Flow Improvements\\n3. Models Page - Fix Adding Azure, Azure AI Studio models on UI \\n4. Internal Users Page - Allow Bulk Adding Internal Users on UI \\n5. Internal Users Page - Allow sorting users by \u2018created at\u2019 \\n6. Virtual Keys Page - Allow searching for UserIDs on the dropdown when assigning a user to a team [See PR](https://github.com/BerriAI/litellm/pull/8844)\\n7. Virtual Keys Page - allow creating a user when assigning keys to users [See PR](https://github.com/BerriAI/litellm/pull/8844)\\n8. Model Hub Page  - fix text overflow issue [See PR](https://github.com/BerriAI/litellm/pull/8749)\\n9. Admin Settings Page - Allow adding MSFT SSO on UI \\n10. Backend - don\'t allow creating duplicate internal users in DB\\n\\n## Helm\\n\\n1. support ttlSecondsAfterFinished on the migration job - [See PR](https://github.com/BerriAI/litellm/pull/8593)\\n2. enhance migrations job with additional configurable properties - [See PR](https://github.com/BerriAI/litellm/pull/8636)\\n\\n## Logging / Guardrail Integrations\\n\\n1. Arize Phoenix support \\n2. \u2018No-log\u2019 - fix \u2018no-log\u2019 param support on embedding calls \\n\\n## Performance / Loadbalancing / Reliability improvements\\n\\n1. Single Deployment Cooldown logic - Use allowed_fails or allowed_fail_policy if set [Start here](https://docs.litellm.ai/docs/routing#advanced-custom-retries-cooldowns-based-on-error-type)\\n\\n## General Proxy Improvements\\n\\n1. Hypercorn - fix reading / parsing request body \\n2. Windows - fix running proxy in windows \\n3. DD-Trace - fix dd-trace enablement on proxy\\n\\n## Complete Git Diff\\n\\nView the complete git diff [here](https://github.com/BerriAI/litellm/compare/v1.61.13-stable...v1.61.20-stable)."},{"id":"v1.59.8-stable","metadata":{"permalink":"/release_notes/v1.59.8-stable","source":"@site/release_notes/v1.59.8-stable/index.md","title":"v1.59.8-stable","description":"Get a 7 day free trial for LiteLLM Enterprise here.","date":"2025-01-31T10:00:00.000Z","tags":[{"inline":true,"label":"admin ui","permalink":"/release_notes/tags/admin-ui"},{"inline":true,"label":"logging","permalink":"/release_notes/tags/logging"},{"inline":true,"label":"db schema","permalink":"/release_notes/tags/db-schema"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.59.8-stable","slug":"v1.59.8-stable","date":"2025-01-31T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"tags":["admin ui","logging","db schema"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.61.20-stable","permalink":"/release_notes/v1.61.20-stable"},"nextItem":{"title":"v1.59.0","permalink":"/release_notes/v1.59.0"}},"content":"import Image from \'@theme/IdealImage\';\\n\\n\\n\\n\\n:::info\\n\\nGet a 7 day free trial for LiteLLM Enterprise [here](https://litellm.ai/#trial).\\n\\n**no call needed**\\n\\n:::\\n\\n\\n## New Models / Updated Models \\n\\n1. New OpenAI `/image/variations` endpoint BETA support [Docs](../../docs/image_variations)\\n2. Topaz API support on OpenAI `/image/variations` BETA endpoint [Docs](../../docs/providers/topaz)\\n3. Deepseek - r1 support w/ reasoning_content ([Deepseek API](../../docs/providers/deepseek#reasoning-models), [Vertex AI](../../docs/providers/vertex#model-garden), [Bedrock](../../docs/providers/bedrock#deepseek)) \\n4. Azure - Add azure o1 pricing [See Here](https://github.com/BerriAI/litellm/blob/b8b927f23bc336862dacb89f59c784a8d62aaa15/model_prices_and_context_window.json#L952)\\n5. Anthropic - handle `-latest` tag in model for cost calculation\\n6. Gemini-2.0-flash-thinking - add model pricing (it\u2019s 0.0) [See Here](https://github.com/BerriAI/litellm/blob/b8b927f23bc336862dacb89f59c784a8d62aaa15/model_prices_and_context_window.json#L3393)\\n7. Bedrock - add stability sd3 model pricing [See Here](https://github.com/BerriAI/litellm/blob/b8b927f23bc336862dacb89f59c784a8d62aaa15/model_prices_and_context_window.json#L6814)  (s/o [Marty Sullivan](https://github.com/marty-sullivan))\\n8. Bedrock - add us.amazon.nova-lite-v1:0 to model cost map [See Here](https://github.com/BerriAI/litellm/blob/b8b927f23bc336862dacb89f59c784a8d62aaa15/model_prices_and_context_window.json#L5619)\\n9. TogetherAI - add new together_ai llama3.3 models [See Here](https://github.com/BerriAI/litellm/blob/b8b927f23bc336862dacb89f59c784a8d62aaa15/model_prices_and_context_window.json#L6985)\\n\\n## LLM Translation\\n\\n1. LM Studio -> fix async embedding call \\n2. Gpt 4o models - fix response_format translation \\n3. Bedrock nova - expand supported document types to include .md, .csv, etc. [Start Here](../../docs/providers/bedrock#usage---pdf--document-understanding)\\n4. Bedrock - docs on IAM role based access for bedrock - [Start Here](https://docs.litellm.ai/docs/providers/bedrock#sts-role-based-auth)\\n5. Bedrock - cache IAM role credentials when used \\n6. Google AI Studio (`gemini/`) - support gemini \'frequency_penalty\' and \'presence_penalty\'\\n7. Azure O1 - fix model name check \\n8. WatsonX - ZenAPIKey support for WatsonX [Docs](../../docs/providers/watsonx)\\n9. Ollama Chat - support json schema response format [Start Here](../../docs/providers/ollama#json-schema-support)\\n10. Bedrock - return correct bedrock status code and error message if error during streaming\\n11. Anthropic - Supported nested json schema on anthropic calls\\n12. OpenAI - `metadata` param preview support \\n    1. SDK - enable via `litellm.enable_preview_features = True` \\n    2. PROXY - enable via `litellm_settings::enable_preview_features: true` \\n13. Replicate - retry completion response on status=processing \\n\\n## Spend Tracking Improvements\\n\\n1. Bedrock - QA asserts all bedrock regional models have same `supported_` as base model \\n2. Bedrock - fix bedrock converse cost tracking w/ region name specified\\n3. Spend Logs reliability fix - when `user` passed in request body is int instead of string \\n4. Ensure \u2018base_model\u2019 cost tracking works across all endpoints \\n5. Fixes for Image generation cost tracking \\n6. Anthropic - fix anthropic end user cost tracking\\n7. JWT / OIDC Auth - add end user id tracking from jwt auth\\n\\n## Management Endpoints / UI\\n\\n1. allows team member to become admin post-add (ui + endpoints) \\n2. New edit/delete button for updating team membership on UI \\n3. If team admin - show all team keys \\n4. Model Hub - clarify cost of models is per 1m tokens \\n5. Invitation Links - fix invalid url generated\\n6. New - SpendLogs Table Viewer - allows proxy admin to view spend logs on UI \\n    1. New spend logs - allow proxy admin to \u2018opt in\u2019 to logging request/response in spend logs table - enables easier abuse detection \\n    2. Show country of origin in spend logs \\n    3. Add pagination + filtering by key name/team name \\n7. `/key/delete` - allow team admin to delete team keys \\n8. Internal User \u2018view\u2019 - fix spend calculation when team selected\\n9. Model Analytics is now on Free  \\n10. Usage page - shows days when spend = 0, and round spend on charts to 2 sig figs \\n11. Public Teams - allow admins to expose teams for new users to \u2018join\u2019 on UI - [Start Here](https://docs.litellm.ai/docs/proxy/public_teams)\\n12. Guardrails\\n    1. set/edit guardrails on a virtual key \\n    2. Allow setting guardrails on a team \\n    3. Set guardrails on team create + edit page\\n13. Support temporary budget increases on `/key/update` - new `temp_budget_increase` and `temp_budget_expiry` fields - [Start Here](../../docs/proxy/virtual_keys#temporary-budget-increase)\\n14. Support writing new key alias to AWS Secret Manager - on key rotation [Start Here](../../docs/secret#aws-secret-manager)\\n\\n## Helm\\n\\n1. add securityContext and pull policy values to migration job (s/o https://github.com/Hexoplon) \\n2. allow specifying envVars on values.yaml\\n3. new helm lint test\\n\\n## Logging / Guardrail Integrations\\n\\n1. Log the used prompt when prompt management used. [Start Here](../../docs/proxy/prompt_management)\\n2. Support s3 logging with team alias prefixes - [Start Here](https://docs.litellm.ai/docs/proxy/logging#team-alias-prefix-in-object-key)\\n3. Prometheus [Start Here](../../docs/proxy/prometheus)\\n    1. fix litellm_llm_api_time_to_first_token_metric not populating for bedrock models\\n    2. emit remaining team budget metric on regular basis (even when call isn\u2019t made) - allows for more stable metrics on Grafana/etc. \\n    3. add key and team level budget metrics\\n    4. emit `litellm_overhead_latency_metric` \\n    5. Emit `litellm_team_budget_reset_at_metric` and `litellm_api_key_budget_remaining_hours_metric` \\n4. Datadog - support logging spend tags to Datadog. [Start Here](../../docs/proxy/enterprise#tracking-spend-for-custom-tags)\\n5. Langfuse - fix logging request tags, read from standard logging payload \\n6. GCS - don\u2019t truncate payload on logging \\n7. New GCS Pub/Sub logging support [Start Here](https://docs.litellm.ai/docs/proxy/logging#google-cloud-storage---pubsub-topic)\\n8. Add AIM Guardrails support [Start Here](../../docs/proxy/guardrails/aim_security)\\n\\n## Security\\n\\n1. New Enterprise SLA for patching security vulnerabilities. [See Here](../../docs/enterprise#slas--professional-support)\\n2. Hashicorp - support using vault namespace for TLS auth. [Start Here](../../docs/secret#hashicorp-vault)\\n3. Azure - DefaultAzureCredential support \\n\\n## Health Checks\\n\\n1. Cleanup pricing-only model names from wildcard route list - prevent bad health checks \\n2. Allow specifying a health check model for wildcard routes - https://docs.litellm.ai/docs/proxy/health#wildcard-routes\\n3. New \u2018health_check_timeout \u2018 param with default 1min upperbound to prevent bad model from health check to hang and cause pod restarts. [Start Here](../../docs/proxy/health#health-check-timeout)\\n4. Datadog - add data dog service health check + expose new `/health/services` endpoint. [Start Here](../../docs/proxy/health#healthservices)\\n\\n## Performance / Reliability improvements\\n\\n1. 3x increase in RPS - moving to orjson for reading request body \\n2. LLM Routing speedup - using cached get model group info \\n3. SDK speedup - using cached get model info helper - reduces CPU work to get model info \\n4. Proxy speedup - only read request body 1 time per request \\n5. Infinite loop detection scripts added to codebase \\n6. Bedrock - pure async image transformation requests \\n7. Cooldowns - single deployment model group if 100% calls fail in high traffic - prevents an o1 outage from impacting other calls \\n8. Response Headers - return \\n    1. `x-litellm-timeout` \\n    2. `x-litellm-attempted-retries`\\n    3. `x-litellm-overhead-duration-ms` \\n    4. `x-litellm-response-duration-ms` \\n9. ensure duplicate callbacks are not added to proxy\\n10. Requirements.txt - bump certifi version\\n\\n## General Proxy Improvements\\n\\n1. JWT / OIDC Auth - new `enforce_rbac` param,allows proxy admin to prevent any unmapped yet authenticated jwt tokens from calling proxy. [Start Here](../../docs/proxy/token_auth#enforce-role-based-access-control-rbac)\\n2. fix custom openapi schema generation for customized swagger\u2019s \\n3. Request Headers - support reading `x-litellm-timeout` param from request headers. Enables model timeout control when using Vercel\u2019s AI SDK + LiteLLM Proxy. [Start Here](../../docs/proxy/request_headers#litellm-headers)\\n4. JWT / OIDC Auth - new `role` based permissions for model authentication. [See Here](https://docs.litellm.ai/docs/proxy/jwt_auth_arch)\\n\\n## Complete Git Diff\\n\\nThis is the diff between v1.57.8-stable and v1.59.8-stable.\\n\\nUse this to see the changes in the codebase.\\n\\n[**Git Diff**](https://github.com/BerriAI/litellm/compare/v1.57.8-stable...v1.59.8-stable)"},{"id":"v1.59.0","metadata":{"permalink":"/release_notes/v1.59.0","source":"@site/release_notes/v1.59.0/index.md","title":"v1.59.0","description":"Get a 7 day free trial for LiteLLM Enterprise here.","date":"2025-01-17T10:00:00.000Z","tags":[{"inline":true,"label":"admin ui","permalink":"/release_notes/tags/admin-ui"},{"inline":true,"label":"logging","permalink":"/release_notes/tags/logging"},{"inline":true,"label":"db schema","permalink":"/release_notes/tags/db-schema"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.59.0","slug":"v1.59.0","date":"2025-01-17T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"tags":["admin ui","logging","db schema"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.59.8-stable","permalink":"/release_notes/v1.59.8-stable"},"nextItem":{"title":"v1.57.8-stable","permalink":"/release_notes/v1.57.8-stable"}},"content":"import Image from \'@theme/IdealImage\';\\n\\n\\n\\n\\n:::info\\n\\nGet a 7 day free trial for LiteLLM Enterprise [here](https://litellm.ai/#trial).\\n\\n**no call needed**\\n\\n:::\\n\\n## UI Improvements\\n\\n### [Opt In] Admin UI - view messages / responses \\n\\nYou can now view messages and response logs on Admin UI.\\n\\n<Image img={require(\'../../img/release_notes/ui_logs.png\')} />\\n\\nHow to enable it - add `store_prompts_in_spend_logs: true` to your `proxy_config.yaml`\\n\\nOnce this flag is enabled, your `messages` and `responses` will be stored in the `LiteLLM_Spend_Logs` table.\\n\\n```yaml\\ngeneral_settings:\\n  store_prompts_in_spend_logs: true\\n```\\n\\n## DB Schema Change\\n\\nAdded `messages` and `responses` to the `LiteLLM_Spend_Logs` table.\\n\\n**By default this is not logged.** If you want `messages` and `responses` to be logged, you need to opt in with this setting \\n\\n```yaml\\ngeneral_settings:\\n  store_prompts_in_spend_logs: true\\n```"},{"id":"v1.57.8-stable","metadata":{"permalink":"/release_notes/v1.57.8-stable","source":"@site/release_notes/v1.57.8-stable/index.md","title":"v1.57.8-stable","description":"alerting, prometheus, secret management, management endpoints, ui, prompt management, finetuning, batch","date":"2025-01-11T10:00:00.000Z","tags":[{"inline":true,"label":"langfuse","permalink":"/release_notes/tags/langfuse"},{"inline":true,"label":"humanloop","permalink":"/release_notes/tags/humanloop"},{"inline":true,"label":"alerting","permalink":"/release_notes/tags/alerting"},{"inline":true,"label":"prometheus","permalink":"/release_notes/tags/prometheus"},{"inline":true,"label":"secret management","permalink":"/release_notes/tags/secret-management"},{"inline":true,"label":"management endpoints","permalink":"/release_notes/tags/management-endpoints"},{"inline":true,"label":"ui","permalink":"/release_notes/tags/ui"},{"inline":true,"label":"prompt management","permalink":"/release_notes/tags/prompt-management"},{"inline":true,"label":"finetuning","permalink":"/release_notes/tags/finetuning"},{"inline":true,"label":"batch","permalink":"/release_notes/tags/batch"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.57.8-stable","slug":"v1.57.8-stable","date":"2025-01-11T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"tags":["langfuse","humanloop","alerting","prometheus","secret management","management endpoints","ui","prompt management","finetuning","batch"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.59.0","permalink":"/release_notes/v1.59.0"},"nextItem":{"title":"v1.57.7","permalink":"/release_notes/v1.57.7"}},"content":"`alerting`, `prometheus`, `secret management`, `management endpoints`, `ui`, `prompt management`, `finetuning`, `batch`\\n\\n\\n## New / Updated Models\\n\\n1. Mistral large pricing - https://github.com/BerriAI/litellm/pull/7452\\n2. Cohere command-r7b-12-2024 pricing - https://github.com/BerriAI/litellm/pull/7553/files\\n3. Voyage - new models, prices and context window information - https://github.com/BerriAI/litellm/pull/7472\\n4. Anthropic - bump Bedrock claude-3-5-haiku max_output_tokens to 8192\\n\\n## General Proxy Improvements\\n\\n1. Health check support for realtime models \\n2. Support calling Azure realtime routes via virtual keys \\n3. Support custom tokenizer on `/utils/token_counter` - useful when checking token count for self-hosted models \\n4. Request Prioritization - support on `/v1/completion` endpoint as well \\n\\n## LLM Translation Improvements\\n\\n1. Deepgram STT support. [Start Here](https://docs.litellm.ai/docs/providers/deepgram)\\n2. OpenAI Moderations - `omni-moderation-latest` support. [Start Here](https://docs.litellm.ai/docs/moderation)\\n3. Azure O1 - fake streaming support. This ensures if a `stream=true` is passed, the response is streamed. [Start Here](https://docs.litellm.ai/docs/providers/azure)\\n4. Anthropic - non-whitespace char stop sequence handling - [PR](https://github.com/BerriAI/litellm/pull/7484)\\n5. Azure OpenAI - support Entra ID username + password based auth. [Start Here](https://docs.litellm.ai/docs/providers/azure#entra-id---use-tenant_id-client_id-client_secret)\\n6. LM Studio - embedding route support. [Start Here](https://docs.litellm.ai/docs/providers/lm-studio)\\n7. WatsonX - ZenAPIKeyAuth support. [Start Here](https://docs.litellm.ai/docs/providers/watsonx)\\n    \\n## Prompt Management Improvements\\n\\n1. Langfuse integration\\n2. HumanLoop integration \\n3. Support for using load balanced models \\n4. Support for loading optional params from prompt manager \\n\\n[Start Here](https://docs.litellm.ai/docs/proxy/prompt_management)\\n\\n## Finetuning + Batch APIs Improvements\\n\\n1. Improved unified endpoint support for Vertex AI finetuning - [PR](https://github.com/BerriAI/litellm/pull/7487)\\n2. Add support for retrieving vertex api batch jobs - [PR](https://github.com/BerriAI/litellm/commit/13f364682d28a5beb1eb1b57f07d83d5ef50cbdc)\\n\\n## *NEW* Alerting Integration\\n\\nPagerDuty Alerting Integration. \\n\\nHandles two types of alerts:\\n\\n- High LLM API Failure Rate. Configure X fails in Y seconds to trigger an alert.\\n- High Number of Hanging LLM Requests. Configure X hangs in Y seconds to trigger an alert.\\n\\n\\n[Start Here](https://docs.litellm.ai/docs/proxy/pagerduty)\\n\\n## Prometheus Improvements\\n\\nAdded support for tracking latency/spend/tokens based on custom metrics. [Start Here](https://docs.litellm.ai/docs/proxy/prometheus#beta-custom-metrics)\\n\\n## *NEW* Hashicorp Secret Manager Support \\n\\nSupport for reading credentials + writing LLM API keys. [Start Here](https://docs.litellm.ai/docs/secret#hashicorp-vault)\\n\\n## Management Endpoints / UI Improvements\\n\\n1. Create and view organizations + assign org admins on the Proxy UI\\n2. Support deleting keys by key_alias\\n3. Allow assigning teams to org on UI\\n4. Disable using ui session token for \'test key\' pane\\n5. Show model used in \'test key\' pane \\n6. Support markdown output in \'test key\' pane\\n\\n## Helm Improvements\\n\\n1. Prevent istio injection for db migrations cron job\\n2. allow using migrationJob.enabled variable within job\\n\\n## Logging Improvements\\n\\n1. braintrust logging: respect project_id, add more metrics  - https://github.com/BerriAI/litellm/pull/7613\\n2. Athina - support base url - `ATHINA_BASE_URL`\\n3. Lunary - Allow passing custom parent run id to LLM Calls \\n\\n\\n\\n## Git Diff \\n\\nThis is the diff between v1.56.3-stable and v1.57.8-stable. \\n\\nUse this to see the changes in the codebase. \\n\\n[Git Diff](https://github.com/BerriAI/litellm/compare/v1.56.3-stable...189b67760011ea313ca58b1f8bd43aa74fbd7f55)"},{"id":"v1.57.7","metadata":{"permalink":"/release_notes/v1.57.7","source":"@site/release_notes/v1.57.7/index.md","title":"v1.57.7","description":"langfuse, management endpoints, ui, prometheus, secret management","date":"2025-01-10T10:00:00.000Z","tags":[{"inline":true,"label":"langfuse","permalink":"/release_notes/tags/langfuse"},{"inline":true,"label":"management endpoints","permalink":"/release_notes/tags/management-endpoints"},{"inline":true,"label":"ui","permalink":"/release_notes/tags/ui"},{"inline":true,"label":"prometheus","permalink":"/release_notes/tags/prometheus"},{"inline":true,"label":"secret management","permalink":"/release_notes/tags/secret-management"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.57.7","slug":"v1.57.7","date":"2025-01-10T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"tags":["langfuse","management endpoints","ui","prometheus","secret management"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.57.8-stable","permalink":"/release_notes/v1.57.8-stable"},"nextItem":{"title":"v1.57.3 - New Base Docker Image","permalink":"/release_notes/v1.57.3"}},"content":"`langfuse`, `management endpoints`, `ui`, `prometheus`, `secret management`\\n\\n## Langfuse Prompt Management \\n\\nLangfuse Prompt Management is being labelled as BETA. This allows us to iterate quickly on the feedback we\'re receiving, and making the status clearer to users. We expect to make this feature to be stable by next month (February 2025).\\n\\nChanges:\\n- Include the client message in the LLM API Request. (Previously only the prompt template was sent, and the client message was ignored).\\n- Log the prompt template in the logged request (e.g. to s3/langfuse). \\n- Log the \'prompt_id\' and \'prompt_variables\' in the logged request (e.g. to s3/langfuse). \\n\\n\\n[Start Here](https://docs.litellm.ai/docs/proxy/prompt_management)\\n\\n## Team/Organization Management + UI Improvements\\n\\nManaging teams and organizations on the UI is now easier. \\n\\nChanges:\\n- Support for editing user role within team on UI. \\n- Support updating team member role to admin via api - `/team/member_update`\\n- Show team admins all keys for their team. \\n- Add organizations with budgets\\n- Assign teams to orgs on the UI\\n- Auto-assign SSO users to teams\\n\\n[Start Here](https://docs.litellm.ai/docs/proxy/self_serve)\\n\\n## Hashicorp Vault Support\\n\\nWe now support writing LiteLLM Virtual API keys to Hashicorp Vault. \\n\\n[Start Here](https://docs.litellm.ai/docs/proxy/vault)\\n\\n## Custom Prometheus Metrics\\n\\nDefine custom prometheus metrics, and track usage/latency/no. of requests against them\\n\\nThis allows for more fine-grained tracking - e.g. on prompt template passed in request metadata\\n\\n[Start Here](https://docs.litellm.ai/docs/proxy/prometheus#beta-custom-metrics)"},{"id":"v1.57.3","metadata":{"permalink":"/release_notes/v1.57.3","source":"@site/release_notes/v1.57.3/index.md","title":"v1.57.3 - New Base Docker Image","description":"docker image, security, vulnerability","date":"2025-01-08T10:00:00.000Z","tags":[{"inline":true,"label":"docker image","permalink":"/release_notes/tags/docker-image"},{"inline":true,"label":"security","permalink":"/release_notes/tags/security"},{"inline":true,"label":"vulnerability","permalink":"/release_notes/tags/vulnerability"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.57.3 - New Base Docker Image","slug":"v1.57.3","date":"2025-01-08T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"tags":["docker image","security","vulnerability"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.57.7","permalink":"/release_notes/v1.57.7"},"nextItem":{"title":"v1.56.4","permalink":"/release_notes/v1.56.4"}},"content":"import Image from \'@theme/IdealImage\';\\n\\n`docker image`, `security`, `vulnerability`\\n\\n# 0 Critical/High Vulnerabilities\\n\\n<Image img={require(\'../../img/release_notes/security.png\')} />\\n\\n## What changed?\\n- LiteLLMBase image now uses `cgr.dev/chainguard/python:latest-dev`\\n\\n## Why the change?\\n\\nTo ensure there are 0 critical/high vulnerabilities on LiteLLM Docker Image\\n\\n## Migration Guide\\n\\n- If you use a custom dockerfile with litellm as a base image + `apt-get`\\n\\nInstead of `apt-get` use `apk`, the base litellm image will no longer have `apt-get` installed.\\n\\n**You are only impacted if you use `apt-get` in your Dockerfile**\\n```shell\\n# Use the provided base image\\nFROM docker.litellm.ai/berriai/litellm:main-latest\\n\\n# Set the working directory\\nWORKDIR /app\\n\\n# Install dependencies - CHANGE THIS to `apk`\\nRUN apt-get update && apt-get install -y dumb-init \\n```\\n\\n\\nBefore Change\\n```\\nRUN apt-get update && apt-get install -y dumb-init\\n```\\n\\nAfter Change\\n```\\nRUN apk update && apk add --no-cache dumb-init\\n```"},{"id":"v1.56.4","metadata":{"permalink":"/release_notes/v1.56.4","source":"@site/release_notes/v1.56.4/index.md","title":"v1.56.4","description":"deepgram, fireworks ai, vision, admin ui, dependency upgrades","date":"2024-12-29T10:00:00.000Z","tags":[{"inline":true,"label":"deepgram","permalink":"/release_notes/tags/deepgram"},{"inline":true,"label":"fireworks ai","permalink":"/release_notes/tags/fireworks-ai"},{"inline":true,"label":"vision","permalink":"/release_notes/tags/vision"},{"inline":true,"label":"admin ui","permalink":"/release_notes/tags/admin-ui"},{"inline":true,"label":"dependency upgrades","permalink":"/release_notes/tags/dependency-upgrades"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.56.4","slug":"v1.56.4","date":"2024-12-29T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"tags":["deepgram","fireworks ai","vision","admin ui","dependency upgrades"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.57.3 - New Base Docker Image","permalink":"/release_notes/v1.57.3"},"nextItem":{"title":"v1.56.3","permalink":"/release_notes/v1.56.3"}},"content":"import Image from \'@theme/IdealImage\';\\n\\n\\n`deepgram`, `fireworks ai`, `vision`, `admin ui`, `dependency upgrades`\\n\\n## New Models\\n\\n### **Deepgram Speech to Text**\\n\\nNew Speech to Text support for Deepgram models. [**Start Here**](https://docs.litellm.ai/docs/providers/deepgram)\\n\\n```python\\nfrom litellm import transcription\\nimport os \\n\\n# set api keys \\nos.environ[\\"DEEPGRAM_API_KEY\\"] = \\"\\"\\naudio_file = open(\\"/path/to/audio.mp3\\", \\"rb\\")\\n\\nresponse = transcription(model=\\"deepgram/nova-2\\", file=audio_file)\\n\\nprint(f\\"response: {response}\\")\\n```\\n\\n### **Fireworks AI - Vision** support for all models\\nLiteLLM supports document inlining for Fireworks AI models. This is useful for models that are not vision models, but still need to parse documents/images/etc.\\nLiteLLM will add `#transform=inline` to the url of the image_url, if the model is not a vision model [See Code](https://github.com/BerriAI/litellm/blob/1ae9d45798bdaf8450f2dfdec703369f3d2212b7/litellm/llms/fireworks_ai/chat/transformation.py#L114)\\n\\n\\n## Proxy Admin UI\\n\\n- `Test Key` Tab displays `model` used in response\\n\\n<Image img={require(\'../../img/release_notes/ui_model.png\')} />\\n\\n- `Test Key` Tab renders content in `.md`, `.py` (any code/markdown format)\\n\\n<Image img={require(\'../../img/release_notes/ui_format.png\')} />\\n\\n\\n## Dependency Upgrades\\n\\n- (Security fix) Upgrade to `fastapi==0.115.5` https://github.com/BerriAI/litellm/pull/7447\\n\\n## Bug Fixes\\n\\n- Add health check support for realtime models [Here](https://docs.litellm.ai/docs/proxy/health#realtime-models)\\n- Health check error with audio_transcription model https://github.com/BerriAI/litellm/issues/5999"},{"id":"v1.56.3","metadata":{"permalink":"/release_notes/v1.56.3","source":"@site/release_notes/v1.56.3/index.md","title":"v1.56.3","description":"guardrails, logging, virtual key management, new models","date":"2024-12-28T10:00:00.000Z","tags":[{"inline":true,"label":"guardrails","permalink":"/release_notes/tags/guardrails"},{"inline":true,"label":"logging","permalink":"/release_notes/tags/logging"},{"inline":true,"label":"virtual key management","permalink":"/release_notes/tags/virtual-key-management"},{"inline":true,"label":"new models","permalink":"/release_notes/tags/new-models"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.56.3","slug":"v1.56.3","date":"2024-12-28T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"tags":["guardrails","logging","virtual key management","new models"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.56.4","permalink":"/release_notes/v1.56.4"},"nextItem":{"title":"v1.56.1","permalink":"/release_notes/v1.56.1"}},"content":"import Image from \'@theme/IdealImage\';\\n\\n`guardrails`, `logging`, `virtual key management`, `new models`\\n\\n:::info\\n\\nGet a 7 day free trial for LiteLLM Enterprise [here](https://litellm.ai/#trial).\\n\\n**no call needed**\\n\\n:::\\n\\n## New Features\\n\\n### \u2728 Log Guardrail Traces \\n\\nTrack guardrail failure rate and if a guardrail is going rogue and failing requests. [Start here](https://docs.litellm.ai/docs/proxy/guardrails/quick_start)\\n\\n\\n#### Traced Guardrail Success\\n\\n<Image img={require(\'../../img/gd_success.png\')} />\\n\\n#### Traced Guardrail Failure\\n\\n<Image img={require(\'../../img/gd_fail.png\')} />\\n\\n\\n### `/guardrails/list` \\n\\n`/guardrails/list` allows clients to view available guardrails + supported guardrail params\\n\\n\\n```shell\\ncurl -X GET \'http://0.0.0.0:4000/guardrails/list\'\\n```\\n\\nExpected response\\n\\n```json\\n{\\n    \\"guardrails\\": [\\n        {\\n        \\"guardrail_name\\": \\"aporia-post-guard\\",\\n        \\"guardrail_info\\": {\\n            \\"params\\": [\\n            {\\n                \\"name\\": \\"toxicity_score\\",\\n                \\"type\\": \\"float\\",\\n                \\"description\\": \\"Score between 0-1 indicating content toxicity level\\"\\n            },\\n            {\\n                \\"name\\": \\"pii_detection\\",\\n                \\"type\\": \\"boolean\\"\\n            }\\n            ]\\n        }\\n        }\\n    ]\\n}\\n```\\n\\n\\n### \u2728 Guardrails with Mock LLM \\n\\n\\nSend `mock_response` to test guardrails without making an LLM call. More info on `mock_response` [here](https://docs.litellm.ai/docs/proxy/guardrails/quick_start)\\n\\n```shell\\ncurl -i http://localhost:4000/v1/chat/completions \\\\\\n  -H \\"Content-Type: application/json\\" \\\\\\n  -H \\"Authorization: Bearer sk-npnwjPQciVRok5yNZgKmFQ\\" \\\\\\n  -d \'{\\n    \\"model\\": \\"gpt-3.5-turbo\\",\\n    \\"messages\\": [\\n      {\\"role\\": \\"user\\", \\"content\\": \\"hi my email is ishaan@berri.ai\\"}\\n    ],\\n    \\"mock_response\\": \\"This is a mock response\\",\\n    \\"guardrails\\": [\\"aporia-pre-guard\\", \\"aporia-post-guard\\"]\\n  }\'\\n```\\n\\n\\n\\n### Assign Keys to Users\\n\\nYou can now assign keys to users via Proxy UI\\n\\n\\n<Image img={require(\'../../img/ui_key.png\')} />\\n\\n## New Models\\n\\n- `openrouter/openai/o1`\\n- `vertex_ai/mistral-large@2411`\\n\\n## Fixes \\n\\n- Fix `vertex_ai/` mistral model pricing: https://github.com/BerriAI/litellm/pull/7345\\n- Missing model_group field in logs for aspeech call types https://github.com/BerriAI/litellm/pull/7392"},{"id":"v1.56.1","metadata":{"permalink":"/release_notes/v1.56.1","source":"@site/release_notes/v1.56.1/index.md","title":"v1.56.1","description":"key management, budgets/rate limits, logging, guardrails","date":"2024-12-27T10:00:00.000Z","tags":[{"inline":true,"label":"key management","permalink":"/release_notes/tags/key-management"},{"inline":true,"label":"budgets/rate limits","permalink":"/release_notes/tags/budgets-rate-limits"},{"inline":true,"label":"logging","permalink":"/release_notes/tags/logging"},{"inline":true,"label":"guardrails","permalink":"/release_notes/tags/guardrails"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.56.1","slug":"v1.56.1","date":"2024-12-27T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"tags":["key management","budgets/rate limits","logging","guardrails"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.56.3","permalink":"/release_notes/v1.56.3"},"nextItem":{"title":"v1.55.10","permalink":"/release_notes/v1.55.10"}},"content":"import Image from \'@theme/IdealImage\';\\n\\n\\n`key management`, `budgets/rate limits`, `logging`, `guardrails`\\n\\n:::info\\n\\nGet a 7 day free trial for LiteLLM Enterprise [here](https://litellm.ai/#trial).\\n\\n**no call needed**\\n\\n:::\\n\\n## \u2728 Budget / Rate Limit Tiers\\n\\nDefine tiers with rate limits. Assign them to keys. \\n\\nUse this to control access and budgets across a lot of keys.\\n\\n**[Start here](https://docs.litellm.ai/docs/proxy/rate_limit_tiers)**\\n\\n```bash\\ncurl -L -X POST \'http://0.0.0.0:4000/budget/new\' \\\\\\n-H \'Authorization: Bearer sk-1234\' \\\\\\n-H \'Content-Type: application/json\' \\\\\\n-d \'{\\n    \\"budget_id\\": \\"high-usage-tier\\",\\n    \\"model_max_budget\\": {\\n        \\"gpt-4o\\": {\\"rpm_limit\\": 1000000}\\n    }\\n}\'\\n```\\n\\n\\n## OTEL Bug Fix\\n\\nLiteLLM was double logging litellm_request span. This is now fixed.\\n\\n[Relevant PR](https://github.com/BerriAI/litellm/pull/7435)\\n\\n## Logging for Finetuning Endpoints \\n\\nLogs for finetuning requests are now available on all logging providers (e.g. Datadog). \\n\\nWhat\'s logged per request:\\n\\n- file_id\\n- finetuning_job_id\\n- any key/team metadata\\n\\n\\n**Start Here:**\\n- [Setup Finetuning](https://docs.litellm.ai/docs/fine_tuning)\\n- [Setup Logging](https://docs.litellm.ai/docs/proxy/logging#datadog)\\n\\n## Dynamic Params for Guardrails \\n\\nYou can now set custom parameters (like success threshold) for your guardrails in each request.\\n\\n[See guardrails spec for more details](https://docs.litellm.ai/docs/proxy/guardrails/custom_guardrail#-pass-additional-parameters-to-guardrail)"},{"id":"v1.55.10","metadata":{"permalink":"/release_notes/v1.55.10","source":"@site/release_notes/v1.55.10/index.md","title":"v1.55.10","description":"batches, guardrails, team management, custom auth","date":"2024-12-24T10:00:00.000Z","tags":[{"inline":true,"label":"batches","permalink":"/release_notes/tags/batches"},{"inline":true,"label":"guardrails","permalink":"/release_notes/tags/guardrails"},{"inline":true,"label":"team management","permalink":"/release_notes/tags/team-management"},{"inline":true,"label":"custom auth","permalink":"/release_notes/tags/custom-auth"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.55.10","slug":"v1.55.10","date":"2024-12-24T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"tags":["batches","guardrails","team management","custom auth"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.56.1","permalink":"/release_notes/v1.56.1"},"nextItem":{"title":"v1.55.8-stable","permalink":"/release_notes/v1.55.8-stable"}},"content":"import Image from \'@theme/IdealImage\';\\n\\n\\n`batches`, `guardrails`, `team management`, `custom auth`\\n\\n\\n<Image img={require(\'../../img/batches_cost_tracking.png\')} />\\n\\n<br/>\\n\\n:::info\\n\\nGet a free 7-day LiteLLM Enterprise trial here. [Start here](https://www.litellm.ai/enterprise#trial)\\n\\n**No call needed**\\n\\n:::\\n\\n## \u2728 Cost Tracking, Logging for Batches API (`/batches`)\\n\\nTrack cost, usage for Batch Creation Jobs. [Start here](https://docs.litellm.ai/docs/batches)\\n\\n## \u2728 `/guardrails/list` endpoint \\n\\nShow available guardrails to users. [Start here](https://litellm-api.up.railway.app/#/Guardrails)\\n\\n\\n## \u2728 Allow teams to add models\\n\\nThis enables team admins to call their own finetuned models via litellm proxy. [Start here](https://docs.litellm.ai/docs/proxy/team_model_add)\\n\\n\\n## \u2728 Common checks for custom auth\\n\\nCalling the internal common_checks function in custom auth is now enforced as an enterprise feature. This allows admins to use litellm\'s default budget/auth checks within their custom auth implementation. [Start here](https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth)\\n\\n\\n## \u2728 Assigning team admins\\n\\nTeam admins is graduating from beta and moving to our enterprise tier. This allows proxy admins to allow others to manage keys/models for their own teams (useful for projects in production). [Start here](https://docs.litellm.ai/docs/proxy/virtual_keys#restricting-key-generation)"},{"id":"v1.55.8-stable","metadata":{"permalink":"/release_notes/v1.55.8-stable","source":"@site/release_notes/v1.55.8-stable/index.md","title":"v1.55.8-stable","description":"A new LiteLLM Stable release just went out. Here are 5 updates since v1.52.2-stable.","date":"2024-12-22T10:00:00.000Z","tags":[{"inline":true,"label":"langfuse","permalink":"/release_notes/tags/langfuse"},{"inline":true,"label":"fallbacks","permalink":"/release_notes/tags/fallbacks"},{"inline":true,"label":"new models","permalink":"/release_notes/tags/new-models"},{"inline":true,"label":"azure_storage","permalink":"/release_notes/tags/azure-storage"}],"hasTruncateMarker":false,"authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","socials":{},"key":null,"page":null},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","socials":{},"key":null,"page":null}],"frontMatter":{"title":"v1.55.8-stable","slug":"v1.55.8-stable","date":"2024-12-22T10:00:00.000Z","authors":[{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"name":"Ishaan Jaffer","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"tags":["langfuse","fallbacks","new models","azure_storage"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"v1.55.10","permalink":"/release_notes/v1.55.10"}},"content":"import Image from \'@theme/IdealImage\';\\n\\n\\nA new LiteLLM Stable release [just went out](https://github.com/BerriAI/litellm/releases/tag/v1.55.8-stable). Here are 5 updates since v1.52.2-stable. \\n\\n`langfuse`, `fallbacks`, `new models`, `azure_storage`\\n\\n<Image img={require(\'../../img/langfuse_prmpt_mgmt.png\')} />\\n\\n## Langfuse Prompt Management\\n\\nThis makes it easy to run experiments or change the specific models `gpt-4o` to `gpt-4o-mini` on Langfuse, instead of making changes in your applications. [Start here](https://docs.litellm.ai/docs/proxy/prompt_management)\\n\\n## Control fallback prompts client-side \\n\\n> Claude prompts are different than OpenAI\\n\\nPass in prompts specific to model when doing fallbacks. [Start here](https://docs.litellm.ai/docs/proxy/reliability#control-fallback-prompts)\\n\\n\\n## New Providers / Models\\n\\n- [NVIDIA Triton](https://developer.nvidia.com/triton-inference-server) `/infer` endpoint. [Start here](https://docs.litellm.ai/docs/providers/triton-inference-server)\\n- [Infinity](https://github.com/michaelfeil/infinity) Rerank Models [Start here](https://docs.litellm.ai/docs/providers/infinity)\\n\\n\\n## \u2728 Azure Data Lake Storage Support\\n\\nSend LLM usage (spend, tokens) data to [Azure Data Lake](https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction). This makes it easy to consume usage data on other services (eg. Databricks)\\n [Start here](https://docs.litellm.ai/docs/proxy/logging#azure-blob-storage)\\n\\n## Docker Run LiteLLM\\n\\n```shell\\ndocker run \\\\\\n-e STORE_MODEL_IN_DB=True \\\\\\n-p 4000:4000 \\\\\\ndocker.litellm.ai/berriai/litellm:litellm_stable_release_branch-v1.55.8-stable\\n```\\n\\n## Get Daily Updates\\n\\nLiteLLM ships new releases every day. [Follow us on LinkedIn](https://www.linkedin.com/company/berri-ai/) to get daily updates."}]}}')}}]);