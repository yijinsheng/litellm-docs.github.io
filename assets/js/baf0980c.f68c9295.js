"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[42666],{7227(e,n,s){s.d(n,{A:()=>r});s(96540);var i=s(18215);const t="tabItem_Ymn6";var o=s(74848);function r({children:e,hidden:n,className:s}){return(0,o.jsx)("div",{role:"tabpanel",className:(0,i.A)(t,s),hidden:n,children:e})}},28453(e,n,s){s.d(n,{R:()=>r,x:()=>l});var i=s(96540);const t={},o=i.createContext(t);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(o.Provider,{value:n},e.children)}},31538(e,n,s){s.r(n),s.d(n,{assets:()=>h,contentTitle:()=>c,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>d});var i=s(83815),t=s(74848),o=s(28453),r=s(49489),l=s(7227);const a={slug:"gemini_3",title:"DAY 0 Support: Gemini 3 on LiteLLM",date:new Date("2025-11-19T10:00:00.000Z"),authors:[{name:"Sameer Kankute",title:"SWE @ LiteLLM (LLM Translation)",url:"https://www.linkedin.com/in/sameer-kankute/",image_url:"https://pbs.twimg.com/profile_images/2001352686994907136/ONgNuSk5_400x400.jpg"},{name:"Krrish Dholakia",title:"CEO, LiteLLM",url:"https://www.linkedin.com/in/krish-d/",image_url:"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{name:"Ishaan Jaff",title:"CTO, LiteLLM",url:"https://www.linkedin.com/in/reffajnaahsi/",image_url:"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],tags:["gemini","day 0 support","llms"],hide_table_of_contents:!1},c=void 0,h={authorsImageUrls:[void 0,void 0,void 0]},d=[{value:"Quick Start",id:"quick-start",level:2},{value:"Supported Endpoints",id:"supported-endpoints",level:2},{value:"Thought Signatures",id:"thought-signatures",level:2},{value:"What are Thought Signatures?",id:"what-are-thought-signatures",level:4},{value:"How Thought Signatures Work",id:"how-thought-signatures-work",level:4},{value:"Example: Multi-Turn Function Calling",id:"example-multi-turn-function-calling",level:2},{value:"Streaming with Thought Signatures",id:"streaming-with-thought-signatures",level:4},{value:"Important Notes on Thought Signatures",id:"important-notes-on-thought-signatures",level:4},{value:"Conversation History: Switching from Non-Gemini-3 Models",id:"conversation-history-switching-from-non-gemini-3-models",level:2},{value:"Common Question: Will switching from a non-Gemini-3 model to Gemini-3 break conversation history?",id:"common-question-will-switching-from-a-non-gemini-3-model-to-gemini-3-break-conversation-history",level:4},{value:"How It Works",id:"how-it-works",level:4},{value:"Example: Switching Models Mid-Conversation",id:"example-switching-models-mid-conversation",level:4},{value:"Dummy Signature Details",id:"dummy-signature-details",level:4},{value:"Thinking Level Parameter",id:"thinking-level-parameter",level:2},{value:"How <code>reasoning_effort</code> Maps to <code>thinking_level</code>",id:"how-reasoning_effort-maps-to-thinking_level",level:4},{value:"Default Behavior",id:"default-behavior",level:4},{value:"Example Usage",id:"example-usage",level:3},{value:"Important Notes",id:"important-notes",level:2},{value:"Cost Tracking: Prompt Caching &amp; Context Window",id:"cost-tracking-prompt-caching--context-window",level:2},{value:"Prompt Caching Cost Tracking",id:"prompt-caching-cost-tracking",level:3},{value:"How It Works",id:"how-it-works-1",level:4},{value:"Context Window Tiered Pricing",id:"context-window-tiered-pricing",level:3},{value:"Automatic Tier Detection",id:"automatic-tier-detection",level:4},{value:"Cost Breakdown",id:"cost-breakdown",level:4},{value:"Example: Viewing Cost Breakdown",id:"example-viewing-cost-breakdown",level:3},{value:"Cost Optimization Tips",id:"cost-optimization-tips",level:3},{value:"Integration with LiteLLM Proxy",id:"integration-with-litellm-proxy",level:3},{value:"Using with Claude Code CLI",id:"using-with-claude-code-cli",level:2},{value:"Setup",id:"setup",level:3},{value:"Example Usage",id:"example-usage-1",level:3},{value:"Benefits",id:"benefits",level:3},{value:"Troubleshooting",id:"troubleshooting",level:3},{value:"Responses API Support",id:"responses-api-support",level:2},{value:"Example: Using Responses API with Gemini 3",id:"example-using-responses-api-with-gemini-3",level:3},{value:"Responses API Benefits",id:"responses-api-benefits",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Always Include Thought Signatures in Conversation History",id:"1-always-include-thought-signatures-in-conversation-history",level:4},{value:"2. Use Appropriate Thinking Levels",id:"2-use-appropriate-thinking-levels",level:4},{value:"3. Keep Temperature at Default",id:"3-keep-temperature-at-default",level:4},{value:"4. Handle Model Switches Gracefully",id:"4-handle-model-switches-gracefully",level:4},{value:"Troubleshooting",id:"troubleshooting-1",level:2},{value:"Issue: Missing Thought Signatures",id:"issue-missing-thought-signatures",level:4},{value:"Issue: Conversation Breaks When Switching Models",id:"issue-conversation-breaks-when-switching-models",level:4},{value:"Issue: Infinite Loops or Poor Performance",id:"issue-infinite-loops-or-poor-performance",level:4},{value:"Additional Resources",id:"additional-resources",level:2}];function u(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["This guide covers common questions and best practices for using ",(0,t.jsx)(n.code,{children:"gemini-3-pro-preview"})," with LiteLLM Proxy and SDK."]})}),"\n",(0,t.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(l.A,{value:"sdk",label:"Python SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion\nimport os\n\nos.environ["GEMINI_API_KEY"] = "your-api-key"\n\nresponse = completion(\n    model="gemini/gemini-3-pro-preview",\n    messages=[{"role": "user", "content": "Hello!"}],\n    reasoning_effort="low"\n)\n\nprint(response.choices[0].message.content)\n'})})}),(0,t.jsxs)(l.A,{value:"proxy",label:"LiteLLM Proxy",children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"1. Add to config.yaml:"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: gemini-3-pro-preview\n    litellm_params:\n      model: gemini/gemini-3-pro-preview\n      api_key: os.environ/GEMINI_API_KEY\n"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"2. Start proxy:"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"3. Make request:"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl http://0.0.0.0:4000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer sk-1234" \\\n  -d \'{\n    "model": "gemini-3-pro-preview",\n    "messages": [{"role": "user", "content": "Hello!"}],\n    "reasoning_effort": "low"\n  }\'\n'})})]})]}),"\n",(0,t.jsx)(n.h2,{id:"supported-endpoints",children:"Supported Endpoints"}),"\n",(0,t.jsxs)(n.p,{children:["LiteLLM provides ",(0,t.jsx)(n.strong,{children:"full end-to-end support"})," for Gemini 3 Pro Preview on:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.code,{children:"/v1/chat/completions"})," - OpenAI-compatible chat completions endpoint"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.code,{children:"/v1/responses"})," - OpenAI Responses API endpoint (streaming and non-streaming)"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.a,{href:"../../docs/anthropic_unified",children:(0,t.jsx)(n.code,{children:"/v1/messages"})})," - Anthropic-compatible messages endpoint"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.code,{children:"/v1/generateContent"})," \u2013 ",(0,t.jsx)(n.a,{href:"https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini#rest",children:"Google Gemini API"})," compatible endpoint (for code, see: ",(0,t.jsx)(n.code,{children:"client.models.generate_content(...)"}),")"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"All endpoints support:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Streaming and non-streaming responses"}),"\n",(0,t.jsx)(n.li,{children:"Function calling with thought signatures"}),"\n",(0,t.jsx)(n.li,{children:"Multi-turn conversations"}),"\n",(0,t.jsx)(n.li,{children:"All Gemini 3-specific features"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"thought-signatures",children:"Thought Signatures"}),"\n",(0,t.jsx)(n.h4,{id:"what-are-thought-signatures",children:"What are Thought Signatures?"}),"\n",(0,t.jsx)(n.p,{children:"Thought signatures are encrypted representations of the model's internal reasoning process. They're essential for maintaining context across multi-turn conversations, especially with function calling."}),"\n",(0,t.jsx)(n.h4,{id:"how-thought-signatures-work",children:"How Thought Signatures Work"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Automatic Extraction"}),": When Gemini 3 returns a function call, LiteLLM automatically extracts the ",(0,t.jsx)(n.code,{children:"thought_signature"})," from the response"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Storage"}),": Thought signatures are stored in ",(0,t.jsx)(n.code,{children:"provider_specific_fields.thought_signature"})," of tool calls"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Automatic Preservation"}),": When you include the assistant's message in conversation history, LiteLLM automatically preserves and returns thought signatures to Gemini"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"example-multi-turn-function-calling",children:"Example: Multi-Turn Function Calling"}),"\n",(0,t.jsx)(n.h4,{id:"streaming-with-thought-signatures",children:"Streaming with Thought Signatures"}),"\n",(0,t.jsxs)(n.p,{children:["When using streaming mode with ",(0,t.jsx)(n.code,{children:"stream_chunk_builder()"}),", thought signatures are now automatically preserved:"]}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsxs)(l.A,{value:"streaming",label:"Streaming SDK",children:[(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import os\nimport litellm\nfrom litellm import completion\n\nos.environ["GEMINI_API_KEY"] = "your-api-key"\n\nMODEL = "gemini/gemini-3-pro-preview"\n\nmessages = [\n    {"role": "system", "content": "You are a helpful assistant. Use the calculate tool."},\n    {"role": "user", "content": "What is 2+2?"},\n]\n\ntools = [{\n    "type": "function",\n    "function": {\n        "name": "calculate",\n        "description": "Calculate a mathematical expression",\n        "parameters": {\n            "type": "object",\n            "properties": {"expression": {"type": "string"}},\n            "required": ["expression"],\n        },\n    },\n}]\n\nprint("Step 1: Sending request with stream=True...")\nresponse = completion(\n    model=MODEL,\n    messages=messages,\n    stream=True,\n    tools=tools,\n    reasoning_effort="low"\n)\n\n# Collect all chunks\nchunks = []\nfor part in response:\n    chunks.append(part)\n\n# Reconstruct message using stream_chunk_builder\n# Thought signatures are now preserved automatically!\nfull_response = litellm.stream_chunk_builder(chunks, messages=messages)\nprint(f"Full response: {full_response}")\n\nassistant_msg = full_response.choices[0].message\n\n# \u2705 Thought signature is now preserved in provider_specific_fields\nif assistant_msg.tool_calls and assistant_msg.tool_calls[0].provider_specific_fields:\n    thought_sig = assistant_msg.tool_calls[0].provider_specific_fields.get("thought_signature")\n    print(f"Thought signature preserved: {thought_sig is not None}")\n\n# Append assistant message (includes thought signatures automatically)\nmessages.append(assistant_msg)\n\n# Mock tool execution\nmessages.append({\n    "role": "tool",\n    "content": "4",\n    "tool_call_id": assistant_msg.tool_calls[0].id\n})\n\nprint("\\nStep 2: Sending tool result back to model...")\nresponse_2 = completion(\n    model=MODEL,\n    messages=messages,\n    stream=True,\n    tools=tools,\n    reasoning_effort="low"\n)\n\nfor part in response_2:\n    if part.choices[0].delta.content:\n        print(part.choices[0].delta.content, end="")\nprint()  # New line\n'})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Points:"})}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.code,{children:"stream_chunk_builder()"})," now preserves ",(0,t.jsx)(n.code,{children:"provider_specific_fields"})," including thought signatures"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 Thought signatures are automatically included when appending ",(0,t.jsx)(n.code,{children:"assistant_msg"})," to conversation history"]}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Multi-turn conversations work seamlessly with streaming"}),"\n"]})]}),(0,t.jsxs)(l.A,{value:"sdk",label:"Non-Streaming SDK",children:[(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\nimport json\n\nclient = OpenAI(api_key="sk-1234", base_url="http://localhost:4000")\n\n# Define tools\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "get_weather",\n            "description": "Get the current weather",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "location": {"type": "string"}\n                },\n                "required": ["location"]\n            }\n        }\n    }\n]\n\n# Step 1: Initial request\nmessages = [{"role": "user", "content": "What\'s the weather in Tokyo?"}]\n\nresponse = client.chat.completions.create(\n    model="gemini-3-pro-preview",\n    messages=messages,\n    tools=tools,\n    reasoning_effort="low"\n)\n\n# Step 2: Append assistant message (thought signatures automatically preserved)\nmessages.append(response.choices[0].message)\n\n# Step 3: Execute tool and append result\nfor tool_call in response.choices[0].message.tool_calls:\n    if tool_call.function.name == "get_weather":\n        result = {"temperature": 30, "unit": "celsius"}\n        messages.append({\n            "role": "tool",\n            "content": json.dumps(result),\n            "tool_call_id": tool_call.id\n        })\n\n# Step 4: Follow-up request (thought signatures automatically included)\nresponse2 = client.chat.completions.create(\n    model="gemini-3-pro-preview",\n    messages=messages,\n    tools=tools,\n    reasoning_effort="low"\n)\n\nprint(response2.choices[0].message.content)\n'})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Points:"})}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u2705 Thought signatures are automatically extracted from ",(0,t.jsx)(n.code,{children:"response.choices[0].message.tool_calls[].provider_specific_fields.thought_signature"})]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 When you append ",(0,t.jsx)(n.code,{children:"response.choices[0].message"})," to your conversation history, thought signatures are automatically preserved"]}),"\n",(0,t.jsx)(n.li,{children:"\u2705 You don't need to manually extract or manage thought signatures"}),"\n"]})]}),(0,t.jsxs)(l.A,{value:"proxy",label:"cURL",children:[(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Step 1: Initial request\ncurl http://localhost:4000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer sk-1234" \\\n  -d \'{\n    "model": "gemini-3-pro-preview",\n    "messages": [\n      {"role": "user", "content": "What\'\\\'\'s the weather in Tokyo?"}\n    ],\n    "tools": [\n      {\n        "type": "function",\n        "function": {\n          "name": "get_weather",\n          "description": "Get the current weather",\n          "parameters": {\n            "type": "object",\n            "properties": {\n              "location": {"type": "string"}\n            },\n            "required": ["location"]\n          }\n        }\n      }\n    ],\n    "reasoning_effort": "low"\n  }\'\n'})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Response includes thought signature:"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n  "choices": [{\n    "message": {\n      "role": "assistant",\n      "tool_calls": [{\n        "id": "call_abc123",\n        "type": "function",\n        "function": {\n          "name": "get_weather",\n          "arguments": "{\\"location\\": \\"Tokyo\\"}"\n        },\n        "provider_specific_fields": {\n          "thought_signature": "CpcHAdHtim9+q4rstcbvQC0ic4x1/vqQlCJWgE+UZ6dTLYGHMMBkF/AxqL5UmP6SY46uYC8t4BTFiXG5zkw6EMJ..."\n        }\n      }]\n    }\n  }]\n}\n'})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Step 2: Follow-up request (include assistant message with thought signature)\ncurl http://localhost:4000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer sk-1234" \\\n  -d \'{\n    "model": "gemini-3-pro-preview",\n    "messages": [\n      {"role": "user", "content": "What\'\\\'\'s the weather in Tokyo?"},\n      {\n        "role": "assistant",\n        "content": null,\n        "tool_calls": [{\n          "id": "call_abc123",\n          "type": "function",\n          "function": {\n            "name": "get_weather",\n            "arguments": "{\\"location\\": \\"Tokyo\\"}"\n          },\n          "provider_specific_fields": {\n            "thought_signature": "CpcHAdHtim9+q4rstcbvQC0ic4x1/vqQlCJWgE+UZ6dTLYGHMMBkF/AxqL5UmP6SY46uYC8t4BTFiXG5zkw6EMJ..."\n          }\n        }]\n      },\n      {\n        "role": "tool",\n        "content": "{\\"temperature\\": 30, \\"unit\\": \\"celsius\\"}",\n        "tool_call_id": "call_abc123"\n      }\n    ],\n    "tools": [...],\n    "reasoning_effort": "low"\n  }\'\n'})})]})]}),"\n",(0,t.jsx)(n.h4,{id:"important-notes-on-thought-signatures",children:"Important Notes on Thought Signatures"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Automatic Handling"}),": LiteLLM automatically extracts and preserves thought signatures. You don't need to manually manage them."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Parallel Function Calls"}),": When the model makes parallel function calls, only the ",(0,t.jsx)(n.strong,{children:"first function call"})," has a thought signature."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sequential Function Calls"}),": In multi-step function calling, each step's first function call has its own thought signature that must be preserved."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Required for Context"}),": Thought signatures are essential for maintaining reasoning context. Without them, the model may lose context of its previous reasoning."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"conversation-history-switching-from-non-gemini-3-models",children:"Conversation History: Switching from Non-Gemini-3 Models"}),"\n",(0,t.jsx)(n.h4,{id:"common-question-will-switching-from-a-non-gemini-3-model-to-gemini-3-break-conversation-history",children:"Common Question: Will switching from a non-Gemini-3 model to Gemini-3 break conversation history?"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Answer: No!"})," LiteLLM automatically handles this by adding dummy thought signatures when needed."]}),"\n",(0,t.jsx)(n.h4,{id:"how-it-works",children:"How It Works"}),"\n",(0,t.jsxs)(n.p,{children:["When you switch from a model that doesn't use thought signatures (e.g., ",(0,t.jsx)(n.code,{children:"gemini-2.5-flash"}),") to Gemini 3, LiteLLM:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Detects missing signatures"}),": Identifies assistant messages with tool calls that lack thought signatures"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Adds dummy signature"}),": Automatically injects a dummy thought signature (",(0,t.jsx)(n.code,{children:"skip_thought_signature_validator"}),") for compatibility"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Maintains conversation flow"}),": Your conversation history continues to work seamlessly"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"example-switching-models-mid-conversation",children:"Example: Switching Models Mid-Conversation"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(l.A,{value:"sdk",label:"Python SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI(api_key="sk-1234", base_url="http://localhost:4000")\n\n# Step 1: Start with gemini-2.5-flash (no thought signatures)\nmessages = [{"role": "user", "content": "What\'s the weather?"}]\n\nresponse1 = client.chat.completions.create(\n    model="gemini-2.5-flash",\n    messages=messages,\n    tools=[...],\n    reasoning_effort="low"\n)\n\n# Append assistant message (no tool call thought signature from gemini-2.5-flash)\nmessages.append(response1.choices[0].message)\n\n# Step 2: Switch to gemini-3-pro-preview\n# LiteLLM automatically adds dummy thought signature to the previous assistant message\nresponse2 = client.chat.completions.create(\n    model="gemini-3-pro-preview",  # \ud83d\udc48 Switched model\n    messages=messages,  # \ud83d\udc48 Same conversation history\n    tools=[...],\n    reasoning_effort="low"\n)\n\n# \u2705 Works seamlessly! No errors, no breaking changes\nprint(response2.choices[0].message.content)\n'})})}),(0,t.jsx)(l.A,{value:"proxy",label:"cURL",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Step 1: Start with gemini-2.5-flash\ncurl http://localhost:4000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer sk-1234" \\\n  -d \'{\n    "model": "gemini-2.5-flash",\n    "messages": [{"role": "user", "content": "What\'\\\'\'s the weather?"}],\n    "tools": [...],\n    "reasoning_effort": "low"\n  }\'\n\n# Step 2: Switch to gemini-3-pro-preview with same conversation history\n# LiteLLM automatically handles the missing thought signature\ncurl http://localhost:4000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer sk-1234" \\\n  -d \'{\n    "model": "gemini-3-pro-preview",  # \ud83d\udc48 Switched model\n    "messages": [\n      {"role": "user", "content": "What\'\\\'\'s the weather?"},\n      {\n        "role": "assistant",\n        "tool_calls": [...]  # \ud83d\udc48 No thought_signature from gemini-2.5-flash\n      }\n    ],\n    "tools": [...],\n    "reasoning_effort": "low"\n  }\'\n# \u2705 Works! LiteLLM adds dummy signature automatically\n'})})})]}),"\n",(0,t.jsx)(n.h4,{id:"dummy-signature-details",children:"Dummy Signature Details"}),"\n",(0,t.jsxs)(n.p,{children:["The dummy signature used is: ",(0,t.jsx)(n.code,{children:'base64("skip_thought_signature_validator")'})]}),"\n",(0,t.jsx)(n.p,{children:"This is the recommended approach by Google for handling conversation history from models that don't support thought signatures. It allows Gemini 3 to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Accept the conversation history without validation errors"}),"\n",(0,t.jsx)(n.li,{children:"Continue the conversation seamlessly"}),"\n",(0,t.jsx)(n.li,{children:"Maintain context across model switches"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"thinking-level-parameter",children:"Thinking Level Parameter"}),"\n",(0,t.jsxs)(n.h4,{id:"how-reasoning_effort-maps-to-thinking_level",children:["How ",(0,t.jsx)(n.code,{children:"reasoning_effort"})," Maps to ",(0,t.jsx)(n.code,{children:"thinking_level"})]}),"\n",(0,t.jsxs)(n.p,{children:["For Gemini 3 Pro Preview, LiteLLM automatically maps ",(0,t.jsx)(n.code,{children:"reasoning_effort"})," to the new ",(0,t.jsx)(n.code,{children:"thinking_level"})," parameter:"]}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:(0,t.jsx)(n.code,{children:"reasoning_effort"})}),(0,t.jsx)(n.th,{children:(0,t.jsx)(n.code,{children:"thinking_level"})}),(0,t.jsx)(n.th,{children:"Notes"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'"minimal"'})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'"low"'})}),(0,t.jsx)(n.td,{children:"Maps to low thinking level"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'"low"'})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'"low"'})}),(0,t.jsx)(n.td,{children:"Default for most use cases"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'"medium"'})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'"high"'})}),(0,t.jsx)(n.td,{children:"Medium not available yet, maps to high"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'"high"'})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'"high"'})}),(0,t.jsx)(n.td,{children:"Maximum reasoning depth"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'"disable"'})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'"low"'})}),(0,t.jsx)(n.td,{children:"Gemini 3 cannot fully disable thinking"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'"none"'})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:'"low"'})}),(0,t.jsx)(n.td,{children:"Gemini 3 cannot fully disable thinking"})]})]})]}),"\n",(0,t.jsx)(n.h4,{id:"default-behavior",children:"Default Behavior"}),"\n",(0,t.jsxs)(n.p,{children:["If you don't specify ",(0,t.jsx)(n.code,{children:"reasoning_effort"}),", LiteLLM automatically sets ",(0,t.jsx)(n.code,{children:'thinking_level="low"'})," for Gemini 3 models, to avoid high costs."]}),"\n",(0,t.jsx)(n.h3,{id:"example-usage",children:"Example Usage"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsx)(l.A,{value:"sdk",label:"Python SDK",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion\n\n# Low thinking level (faster, lower cost)\nresponse = completion(\n    model="gemini/gemini-3-pro-preview",\n    messages=[{"role": "user", "content": "What\'s the weather?"}],\n    reasoning_effort="low"  # Maps to thinking_level="low"\n)\n\n# High thinking level (deeper reasoning, higher cost)\nresponse = completion(\n    model="gemini/gemini-3-pro-preview",\n    messages=[{"role": "user", "content": "Solve this complex math problem step by step."}],\n    reasoning_effort="high"  # Maps to thinking_level="high"\n)\n'})})}),(0,t.jsx)(l.A,{value:"proxy",label:"LiteLLM Proxy",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Low thinking level\ncurl http://localhost:4000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer sk-1234" \\\n  -d \'{\n    "model": "gemini-3-pro-preview",\n    "messages": [{"role": "user", "content": "What\'\\\'\'s the weather?"}],\n    "reasoning_effort": "low"\n  }\'\n\n# High thinking level\ncurl http://localhost:4000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer sk-1234" \\\n  -d \'{\n    "model": "gemini-3-pro-preview",\n    "messages": [{"role": "user", "content": "Solve this complex problem."}],\n    "reasoning_effort": "high"\n  }\'\n'})})})]}),"\n",(0,t.jsx)(n.h2,{id:"important-notes",children:"Important Notes"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Gemini 3 Cannot Disable Thinking"}),": Unlike Gemini 2.5 models, Gemini 3 cannot fully disable thinking. Even when you set ",(0,t.jsx)(n.code,{children:'reasoning_effort="none"'})," or ",(0,t.jsx)(n.code,{children:'"disable"'}),", it maps to ",(0,t.jsx)(n.code,{children:'thinking_level="low"'}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Temperature Recommendation"}),": For Gemini 3 models, LiteLLM defaults ",(0,t.jsx)(n.code,{children:"temperature"})," to ",(0,t.jsx)(n.code,{children:"1.0"})," and strongly recommends keeping it at this default. Setting ",(0,t.jsx)(n.code,{children:"temperature < 1.0"})," can cause:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Infinite loops"}),"\n",(0,t.jsx)(n.li,{children:"Degraded reasoning performance"}),"\n",(0,t.jsx)(n.li,{children:"Failure on complex tasks"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Automatic Defaults"}),": If you don't specify ",(0,t.jsx)(n.code,{children:"reasoning_effort"}),", LiteLLM automatically sets ",(0,t.jsx)(n.code,{children:'thinking_level="low"'})," for optimal performance."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"cost-tracking-prompt-caching--context-window",children:"Cost Tracking: Prompt Caching & Context Window"}),"\n",(0,t.jsx)(n.p,{children:"LiteLLM provides comprehensive cost tracking for Gemini 3 Pro Preview, including support for prompt caching and tiered pricing based on context window size."}),"\n",(0,t.jsx)(n.h3,{id:"prompt-caching-cost-tracking",children:"Prompt Caching Cost Tracking"}),"\n",(0,t.jsx)(n.p,{children:"Gemini 3 supports prompt caching, which allows you to cache frequently used prompt prefixes to reduce costs. LiteLLM automatically tracks and calculates costs for:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cache Hit Tokens"}),": Tokens that are read from cache (charged at a lower rate)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cache Creation Tokens"}),": Tokens that are written to cache (one-time cost)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Text Tokens"}),": Regular prompt tokens that are processed normally"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"how-it-works-1",children:"How It Works"}),"\n",(0,t.jsxs)(n.p,{children:["LiteLLM extracts caching information from the ",(0,t.jsx)(n.code,{children:"prompt_tokens_details"})," field in the usage object:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'{\n  "usage": {\n    "prompt_tokens": 50000,\n    "completion_tokens": 1000,\n    "total_tokens": 51000,\n    "prompt_tokens_details": {\n      "cached_tokens": 30000,  # Cache hit tokens\n      "cache_creation_tokens": 5000,  # Tokens written to cache\n      "text_tokens": 15000  # Regular processed tokens\n    }\n  }\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"context-window-tiered-pricing",children:"Context Window Tiered Pricing"}),"\n",(0,t.jsx)(n.p,{children:"Gemini 3 Pro Preview supports up to 1M tokens of context, with tiered pricing that automatically applies when your prompt exceeds 200k tokens."}),"\n",(0,t.jsx)(n.h4,{id:"automatic-tier-detection",children:"Automatic Tier Detection"}),"\n",(0,t.jsx)(n.p,{children:"LiteLLM automatically detects when your prompt exceeds the 200k token threshold and applies the appropriate tiered pricing:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion_cost\n\n# Example: Small prompt (< 200k tokens)\nresponse_small = completion(\n    model="gemini/gemini-3-pro-preview",\n    messages=[{"role": "user", "content": "Hello!"}]\n)\n# Uses base pricing: $0.000002/input token, $0.000012/output token\n\n# Example: Large prompt (> 200k tokens)\nresponse_large = completion(\n    model="gemini/gemini-3-pro-preview",\n    messages=[{"role": "user", "content": "..." * 250000}]  # 250k tokens\n)\n# Automatically uses tiered pricing: $0.000004/input token, $0.000018/output token\n'})}),"\n",(0,t.jsx)(n.h4,{id:"cost-breakdown",children:"Cost Breakdown"}),"\n",(0,t.jsx)(n.p,{children:"The cost calculation includes:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Text Processing Cost"}),": Regular tokens processed at base or tiered rate"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cache Read Cost"}),": Cached tokens read at discounted rate"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cache Creation Cost"}),": One-time cost for writing tokens to cache (applies tiered rate if above 200k)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Output Cost"}),": Generated tokens at base or tiered rate"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"example-viewing-cost-breakdown",children:"Example: Viewing Cost Breakdown"}),"\n",(0,t.jsx)(n.p,{children:"You can view the detailed cost breakdown using LiteLLM's cost tracking:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from litellm import completion, completion_cost\n\nresponse = completion(\n    model="gemini/gemini-3-pro-preview",\n    messages=[{"role": "user", "content": "Explain prompt caching"}],\n    caching=True  # Enable prompt caching\n)\n\n# Get total cost\ntotal_cost = completion_cost(completion_response=response)\nprint(f"Total cost: ${total_cost:.6f}")\n\n# Access usage details\nusage = response.usage\nprint(f"Prompt tokens: {usage.prompt_tokens}")\nprint(f"Completion tokens: {usage.completion_tokens}")\n\n# Access caching details\nif usage.prompt_tokens_details:\n    print(f"Cache hit tokens: {usage.prompt_tokens_details.cached_tokens}")\n    print(f"Cache creation tokens: {usage.prompt_tokens_details.cache_creation_tokens}")\n    print(f"Text tokens: {usage.prompt_tokens_details.text_tokens}")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"cost-optimization-tips",children:"Cost Optimization Tips"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use Prompt Caching"}),": For repeated prompt prefixes, enable caching to reduce costs by up to 90% for cached portions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Monitor Context Size"}),": Be aware that prompts above 200k tokens use tiered pricing (2x for input, 1.5x for output)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cache Management"}),": Cache creation tokens are charged once when writing to cache, then subsequent reads are much cheaper"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Track Usage"}),": Use LiteLLM's built-in cost tracking to monitor spending across different token types"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"integration-with-litellm-proxy",children:"Integration with LiteLLM Proxy"}),"\n",(0,t.jsx)(n.p,{children:"When using LiteLLM Proxy, all cost tracking is automatically logged and available through:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Usage Logs"}),": Detailed token and cost breakdowns in proxy logs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Budget Management"}),": Set budgets and alerts based on actual usage"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Analytics Dashboard"}),": View cost trends and breakdowns by token type"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'# config.yaml\nmodel_list:\n  - model_name: gemini-3-pro-preview\n    litellm_params:\n      model: gemini/gemini-3-pro-preview\n      api_key: os.environ/GEMINI_API_KEY\n\nlitellm_settings:\n  # Enable detailed cost tracking\n  success_callback: ["langfuse"]  # or your preferred logging service\n'})}),"\n",(0,t.jsx)(n.h2,{id:"using-with-claude-code-cli",children:"Using with Claude Code CLI"}),"\n",(0,t.jsxs)(n.p,{children:["You can use ",(0,t.jsx)(n.code,{children:"gemini-3-pro-preview"})," with ",(0,t.jsx)(n.strong,{children:"Claude Code CLI"})," - Anthropic's command-line interface. This allows you to use Gemini 3 Pro Preview with Claude Code's native syntax and workflows."]}),"\n",(0,t.jsx)(n.h3,{id:"setup",children:"Setup"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:["1. Add Gemini 3 Pro Preview to your ",(0,t.jsx)(n.code,{children:"config.yaml"}),":"]})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"model_list:\n  - model_name: gemini-3-pro-preview\n    litellm_params:\n      model: gemini/gemini-3-pro-preview\n      api_key: os.environ/GEMINI_API_KEY\n\nlitellm_settings:\n  master_key: os.environ/LITELLM_MASTER_KEY\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"2. Set environment variables:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'export GEMINI_API_KEY="your-gemini-api-key"\nexport LITELLM_MASTER_KEY="sk-1234567890"  # Generate a secure key\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"3. Start LiteLLM Proxy:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"litellm --config /path/to/config.yaml\n\n# RUNNING on http://0.0.0.0:4000\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"4. Configure Claude Code to use LiteLLM Proxy:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'export ANTHROPIC_BASE_URL="http://0.0.0.0:4000"\nexport ANTHROPIC_AUTH_TOKEN="$LITELLM_MASTER_KEY"\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"5. Use Gemini 3 Pro Preview with Claude Code:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Claude Code will use gemini-3-pro-preview from your LiteLLM proxy\nclaude --model gemini-3-pro-preview\n\n"})}),"\n",(0,t.jsx)(n.h3,{id:"example-usage-1",children:"Example Usage"}),"\n",(0,t.jsx)(n.p,{children:"Once configured, you can interact with Gemini 3 Pro Preview using Claude Code's native interface:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"$ claude --model gemini-3-pro-preview\n> Explain how thought signatures work in multi-turn conversations.\n\n# Gemini 3 Pro Preview responds through Claude Code interface\n"})}),"\n",(0,t.jsx)(n.h3,{id:"benefits",children:"Benefits"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Native Claude Code Experience"}),": Use Gemini 3 Pro Preview with Claude Code's familiar CLI interface"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Unified Authentication"}),": Single API key for all models through LiteLLM proxy"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Cost Tracking"}),": All usage tracked through LiteLLM's centralized logging"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Seamless Model Switching"}),": Easily switch between Claude and Gemini models"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Full Feature Support"}),": All Gemini 3 features (thought signatures, function calling, etc.) work through Claude Code"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Claude Code not finding the model:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Ensure the model name in Claude Code matches exactly: ",(0,t.jsx)(n.code,{children:"gemini-3-pro-preview"})]}),"\n",(0,t.jsxs)(n.li,{children:["Verify your proxy is running: ",(0,t.jsx)(n.code,{children:"curl http://0.0.0.0:4000/health"})]}),"\n",(0,t.jsxs)(n.li,{children:["Check that ",(0,t.jsx)(n.code,{children:"ANTHROPIC_BASE_URL"})," points to your LiteLLM proxy"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Authentication errors:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Verify ",(0,t.jsx)(n.code,{children:"ANTHROPIC_AUTH_TOKEN"})," matches your LiteLLM master key"]}),"\n",(0,t.jsxs)(n.li,{children:["Ensure ",(0,t.jsx)(n.code,{children:"GEMINI_API_KEY"})," is set correctly"]}),"\n",(0,t.jsx)(n.li,{children:"Check LiteLLM proxy logs for detailed error messages"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"responses-api-support",children:"Responses API Support"}),"\n",(0,t.jsx)(n.p,{children:"LiteLLM fully supports the OpenAI Responses API for Gemini 3 Pro Preview, including both streaming and non-streaming modes. The Responses API provides a structured way to handle multi-turn conversations with function calling, and LiteLLM automatically preserves thought signatures throughout the conversation."}),"\n",(0,t.jsx)(n.h3,{id:"example-using-responses-api-with-gemini-3",children:"Example: Using Responses API with Gemini 3"}),"\n",(0,t.jsxs)(r.A,{children:[(0,t.jsxs)(l.A,{value:"sdk",label:"Non-Streaming",children:[(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\nimport json\n\nclient = OpenAI()\n\n# 1. Define a list of callable tools for the model\ntools = [\n    {\n        "type": "function",\n        "name": "get_horoscope",\n        "description": "Get today\'s horoscope for an astrological sign.",\n        "parameters": {\n            "type": "object",\n            "properties": {\n                "sign": {\n                    "type": "string",\n                    "description": "An astrological sign like Taurus or Aquarius",\n                },\n            },\n            "required": ["sign"],\n        },\n    },\n]\n\ndef get_horoscope(sign):\n    return f"{sign}: Next Tuesday you will befriend a baby otter."\n\n# Create a running input list we will add to over time\ninput_list = [\n    {"role": "user", "content": "What is my horoscope? I am an Aquarius."}\n]\n\n# 2. Prompt the model with tools defined\nresponse = client.responses.create(\n    model="gemini-3-pro-preview",\n    tools=tools,\n    input=input_list,\n)\n\n# Save function call outputs for subsequent requests\ninput_list += response.output\n\nfor item in response.output:\n    if item.type == "function_call":\n        if item.name == "get_horoscope":\n            # 3. Execute the function logic for get_horoscope\n            horoscope = get_horoscope(json.loads(item.arguments))\n            \n            # 4. Provide function call results to the model\n            input_list.append({\n                "type": "function_call_output",\n                "call_id": item.call_id,\n                "output": json.dumps({\n                  "horoscope": horoscope\n                })\n            })\n\nprint("Final input:")\nprint(input_list)\n\nresponse = client.responses.create(\n    model="gemini-3-pro-preview",\n    instructions="Respond only with a horoscope generated by a tool.",\n    tools=tools,\n    input=input_list,\n)\n\n# 5. The model should be able to give a response!\nprint("Final output:")\nprint(response.model_dump_json(indent=2))\nprint("\\n" + response.output_text)\n'})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Points:"})}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u2705 Thought signatures are automatically preserved in function calls"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Works seamlessly with multi-turn conversations"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 All Gemini 3-specific features are fully supported"}),"\n"]})]}),(0,t.jsxs)(l.A,{value:"streaming",label:"Streaming",children:[(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\nimport json\n\nclient = OpenAI()\n\ntools = [\n    {\n        "type": "function",\n        "name": "get_horoscope",\n        "description": "Get today\'s horoscope for an astrological sign.",\n        "parameters": {\n            "type": "object",\n            "properties": {\n                "sign": {\n                    "type": "string",\n                    "description": "An astrological sign like Taurus or Aquarius",\n                },\n            },\n            "required": ["sign"],\n        },\n    },\n]\n\ndef get_horoscope(sign):\n    return f"{sign}: Next Tuesday you will befriend a baby otter."\n\ninput_list = [\n    {"role": "user", "content": "What is my horoscope? I am an Aquarius."}\n]\n\n# Streaming mode\nresponse = client.responses.create(\n    model="gemini-3-pro-preview",\n    tools=tools,\n    input=input_list,\n    stream=True,\n)\n\n# Collect all chunks\nchunks = []\nfor chunk in response:\n    chunks.append(chunk)\n    # Process streaming chunks as they arrive\n    print(chunk)\n\n# Thought signatures are automatically preserved in streaming mode\n'})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Points:"})}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u2705 Streaming mode fully supported"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Thought signatures preserved across streaming chunks"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Real-time processing of function calls and responses"}),"\n"]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"responses-api-benefits",children:"Responses API Benefits"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Structured Output"}),": Responses API provides a clear structure for handling function calls and multi-turn conversations"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Thought Signature Preservation"}),": LiteLLM automatically preserves thought signatures in both streaming and non-streaming modes"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Seamless Integration"}),": Works with existing OpenAI SDK patterns"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Full Feature Support"}),": All Gemini 3 features (thought signatures, function calling, reasoning) are fully supported"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsx)(n.h4,{id:"1-always-include-thought-signatures-in-conversation-history",children:"1. Always Include Thought Signatures in Conversation History"}),"\n",(0,t.jsx)(n.p,{children:"When building multi-turn conversations with function calling:"}),"\n",(0,t.jsxs)(n.p,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Do:"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Append the full assistant message (includes thought signatures)\nmessages.append(response.choices[0].message)\n"})}),"\n",(0,t.jsxs)(n.p,{children:["\u274c ",(0,t.jsx)(n.strong,{children:"Don't:"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Don\'t manually construct assistant messages without thought signatures\nmessages.append({\n    "role": "assistant",\n    "tool_calls": [...]  # Missing thought signatures!\n})\n'})}),"\n",(0,t.jsx)(n.h4,{id:"2-use-appropriate-thinking-levels",children:"2. Use Appropriate Thinking Levels"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:'reasoning_effort="low"'})}),": For simple queries, quick responses, cost optimization"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:'reasoning_effort="high"'})}),": For complex problems requiring deep reasoning"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"3-keep-temperature-at-default",children:"3. Keep Temperature at Default"}),"\n",(0,t.jsxs)(n.p,{children:["For Gemini 3 models, always use ",(0,t.jsx)(n.code,{children:"temperature=1.0"})," (default). Lower temperatures can cause issues."]}),"\n",(0,t.jsx)(n.h4,{id:"4-handle-model-switches-gracefully",children:"4. Handle Model Switches Gracefully"}),"\n",(0,t.jsx)(n.p,{children:"When switching from non-Gemini-3 to Gemini-3:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u2705 LiteLLM automatically handles missing thought signatures"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 No manual intervention needed"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Conversation history continues seamlessly"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting-1",children:"Troubleshooting"}),"\n",(0,t.jsx)(n.h4,{id:"issue-missing-thought-signatures",children:"Issue: Missing Thought Signatures"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Symptom"}),": Error when including assistant messages in conversation history"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Ensure you're appending the full assistant message from the response:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"messages.append(response.choices[0].message)  # \u2705 Includes thought signatures\n"})}),"\n",(0,t.jsx)(n.h4,{id:"issue-conversation-breaks-when-switching-models",children:"Issue: Conversation Breaks When Switching Models"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Symptom"}),": Errors when switching from gemini-2.5-flash to gemini-3-pro-preview"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": This should work automatically! LiteLLM adds dummy signatures. If you see errors, ensure you're using the latest LiteLLM version."]}),"\n",(0,t.jsx)(n.h4,{id:"issue-infinite-loops-or-poor-performance",children:"Issue: Infinite Loops or Poor Performance"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Symptom"}),": Model gets stuck or produces poor results"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Ensure ",(0,t.jsx)(n.code,{children:"temperature=1.0"})," (default for Gemini 3)"]}),"\n",(0,t.jsxs)(n.li,{children:["Check that ",(0,t.jsx)(n.code,{children:"reasoning_effort"})," is set appropriately"]}),"\n",(0,t.jsxs)(n.li,{children:["Verify you're using the correct model name: ",(0,t.jsx)(n.code,{children:"gemini/gemini-3-pro-preview"})]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../gemini.md",children:"Gemini Provider Documentation"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../gemini.md#thought-signatures",children:"Thought Signatures Guide"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../../reasoning_content.md",children:"Reasoning Content Documentation"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../../function_calling.md",children:"Function Calling Guide"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(u,{...e})}):u(e)}},49489(e,n,s){s.d(n,{A:()=>k});var i=s(96540),t=s(18215),o=s(24245),r=s(56347),l=s(36494),a=s(62814),c=s(45167),h=s(69900);function d(e){return i.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,i.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function u(e){const{values:n,children:s}=e;return(0,i.useMemo)(()=>{const e=n??function(e){return d(e).map(({props:{value:e,label:n,attributes:s,default:i}})=>({value:e,label:n,attributes:s,default:i}))}(s);return function(e){const n=(0,c.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,s])}function p({value:e,tabValues:n}){return n.some(n=>n.value===e)}function g({queryString:e=!1,groupId:n}){const s=(0,r.W6)(),t=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,a.aZ)(t),(0,i.useCallback)(e=>{if(!t)return;const n=new URLSearchParams(s.location.search);n.set(t,e),s.replace({...s.location,search:n.toString()})},[t,s])]}function m(e){const{defaultValue:n,queryString:s=!1,groupId:t}=e,o=u(e),[r,a]=(0,i.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const s=n.find(e=>e.default)??n[0];if(!s)throw new Error("Unexpected error: 0 tabValues");return s.value}({defaultValue:n,tabValues:o})),[c,d]=g({queryString:s,groupId:t}),[m,x]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[s,t]=(0,h.Dv)(n);return[s,(0,i.useCallback)(e=>{n&&t.set(e)},[n,t])]}({groupId:t}),j=(()=>{const e=c??m;return p({value:e,tabValues:o})?e:null})();(0,l.A)(()=>{j&&a(j)},[j]);return{selectedValue:r,selectValue:(0,i.useCallback)(e=>{if(!p({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);a(e),d(e),x(e)},[d,x,o]),tabValues:o}}var x=s(11062);const j="tabList__CuJ",f="tabItem_LNqP";var v=s(74848);function w({className:e,block:n,selectedValue:s,selectValue:i,tabValues:r}){const l=[],{blockElementScrollPositionUntilNextRender:a}=(0,o.a_)(),c=e=>{const n=e.currentTarget,t=l.indexOf(n),o=r[t].value;o!==s&&(a(n),i(o))},h=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const s=l.indexOf(e.currentTarget)+1;n=l[s]??l[0];break}case"ArrowLeft":{const s=l.indexOf(e.currentTarget)-1;n=l[s]??l[l.length-1];break}}n?.focus()};return(0,v.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,t.A)("tabs",{"tabs--block":n},e),children:r.map(({value:e,label:n,attributes:i})=>(0,v.jsx)("li",{role:"tab",tabIndex:s===e?0:-1,"aria-selected":s===e,ref:e=>{l.push(e)},onKeyDown:h,onClick:c,...i,className:(0,t.A)("tabs__item",f,i?.className,{"tabs__item--active":s===e}),children:n??e},e))})}function y({lazy:e,children:n,selectedValue:s}){const o=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=o.find(e=>e.props.value===s);return e?(0,i.cloneElement)(e,{className:(0,t.A)("margin-top--md",e.props.className)}):null}return(0,v.jsx)("div",{className:"margin-top--md",children:o.map((e,n)=>(0,i.cloneElement)(e,{key:n,hidden:e.props.value!==s}))})}function _(e){const n=m(e);return(0,v.jsxs)("div",{className:(0,t.A)("tabs-container",j),children:[(0,v.jsx)(w,{...n,...e}),(0,v.jsx)(y,{...n,...e})]})}function k(e){const n=(0,x.A)();return(0,v.jsx)(_,{...e,children:d(e.children)},String(n))}},83815(e){e.exports=JSON.parse('{"permalink":"/blog/gemini_3","source":"@site/blog/gemini_3/index.md","title":"DAY 0 Support: Gemini 3 on LiteLLM","description":"This guide covers common questions and best practices for using gemini-3-pro-preview with LiteLLM Proxy and SDK.","date":"2025-11-19T10:00:00.000Z","tags":[{"inline":true,"label":"gemini","permalink":"/blog/tags/gemini"},{"inline":true,"label":"day 0 support","permalink":"/blog/tags/day-0-support"},{"inline":true,"label":"llms","permalink":"/blog/tags/llms"}],"hasTruncateMarker":false,"authors":[{"name":"Sameer Kankute","title":"SWE @ LiteLLM (LLM Translation)","url":"https://www.linkedin.com/in/sameer-kankute/","image_url":"https://pbs.twimg.com/profile_images/2001352686994907136/ONgNuSk5_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/2001352686994907136/ONgNuSk5_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","socials":{},"key":null,"page":null},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"gemini_3","title":"DAY 0 Support: Gemini 3 on LiteLLM","date":"2025-11-19T10:00:00.000Z","authors":[{"name":"Sameer Kankute","title":"SWE @ LiteLLM (LLM Translation)","url":"https://www.linkedin.com/in/sameer-kankute/","image_url":"https://pbs.twimg.com/profile_images/2001352686994907136/ONgNuSk5_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/2001352686994907136/ONgNuSk5_400x400.jpg"},{"name":"Krrish Dholakia","title":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image_url":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"name":"Ishaan Jaff","title":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image_url":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg","imageURL":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"tags":["gemini","day 0 support","llms"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Day 0 Support: Claude 4.5 Opus (+Advanced Features)","permalink":"/blog/anthropic_advanced_features"}}')}}]);