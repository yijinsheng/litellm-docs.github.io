"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[15945],{10533(e,n,o){o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>t,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"type":"mdx","permalink":"/observability/helicone_integration","source":"@site/src/pages/observability/helicone_integration.md","title":"Helicone Tutorial","description":"Helicone is an open source observability platform that proxies your OpenAI traffic and provides you key insights into your spend, latency and usage.","frontMatter":{},"unlisted":false}');var l=o(74848),r=o(28453);const i={},t="Helicone Tutorial",c={},a=[{value:"Use Helicone to log requests across all LLM Providers (OpenAI, Azure, Anthropic, Cohere, Replicate, PaLM)",id:"use-helicone-to-log-requests-across-all-llm-providers-openai-azure-anthropic-cohere-replicate-palm",level:2},{value:"Approach 1: Use Callbacks",id:"approach-1-use-callbacks",level:3},{value:"Approach 2: [OpenAI + Azure only] Use Helicone as a proxy",id:"approach-2-openai--azure-only-use-helicone-as-a-proxy",level:3}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"helicone-tutorial",children:"Helicone Tutorial"})}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.a,{href:"https://helicone.ai/",children:"Helicone"})," is an open source observability platform that proxies your OpenAI traffic and provides you key insights into your spend, latency and usage."]}),"\n",(0,l.jsx)(n.h2,{id:"use-helicone-to-log-requests-across-all-llm-providers-openai-azure-anthropic-cohere-replicate-palm",children:"Use Helicone to log requests across all LLM Providers (OpenAI, Azure, Anthropic, Cohere, Replicate, PaLM)"}),"\n",(0,l.jsxs)(n.p,{children:["liteLLM provides ",(0,l.jsx)(n.code,{children:"success_callbacks"})," and ",(0,l.jsx)(n.code,{children:"failure_callbacks"}),", making it easy for you to send data to a particular provider depending on the status of your responses."]}),"\n",(0,l.jsx)(n.p,{children:"In this case, we want to log requests to Helicone when a request succeeds."}),"\n",(0,l.jsx)(n.h3,{id:"approach-1-use-callbacks",children:"Approach 1: Use Callbacks"}),"\n",(0,l.jsxs)(n.p,{children:["Use just 1 line of code, to instantly log your responses ",(0,l.jsx)(n.strong,{children:"across all providers"})," with helicone:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:'litellm.success_callback=["helicone"]\n'})}),"\n",(0,l.jsx)(n.p,{children:"Complete code"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from litellm import completion\n\n## set env variables\nos.environ["HELICONE_API_KEY"] = "your-helicone-key" \nos.environ["OPENAI_API_KEY"], os.environ["COHERE_API_KEY"] = "", ""\n\n# set callbacks\nlitellm.success_callback=["helicone"]\n\n#openai call\nresponse = completion(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hi \ud83d\udc4b - i\'m openai"}]) \n\n#cohere call\nresponse = completion(model="command-nightly", messages=[{"role": "user", "content": "Hi \ud83d\udc4b - i\'m cohere"}]) \n'})}),"\n",(0,l.jsx)(n.h3,{id:"approach-2-openai--azure-only-use-helicone-as-a-proxy",children:"Approach 2: [OpenAI + Azure only] Use Helicone as a proxy"}),"\n",(0,l.jsx)(n.p,{children:"Helicone provides advanced functionality like caching, etc. Helicone currently supports this for Azure and OpenAI."}),"\n",(0,l.jsx)(n.p,{children:"If you want to use Helicone to proxy your OpenAI/Azure requests, then you can -"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Set helicone as your base url via: ",(0,l.jsx)(n.code,{children:"litellm.api_url"})]}),"\n",(0,l.jsxs)(n.li,{children:["Pass in helicone request headers via: ",(0,l.jsx)(n.code,{children:"litellm.headers"})]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"Complete Code"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:'import litellm\nfrom litellm import completion\n\nlitellm.api_base = "https://oai.hconeai.com/v1"\nlitellm.headers = {"Helicone-Auth": f"Bearer {os.getenv(\'HELICONE_API_KEY\')}"}\n\nresponse = litellm.completion(\n    model="gpt-3.5-turbo",\n    messages=[{"role": "user", "content": "how does a court case get to the Supreme Court?"}]\n)\n\nprint(response)\n'})})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(p,{...e})}):p(e)}},28453(e,n,o){o.d(n,{R:()=>i,x:()=>t});var s=o(96540);const l={},r=s.createContext(l);function i(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:i(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);