<!doctype html>
<html lang="zh-CN" dir="ltr" class="blog-wrapper blog-list-page plugin-blog plugin-id-release_notes" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Release Notes | liteLLM</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docs.litellm.ai/img/docusaurus-social-card.png"><meta data-rh="true" name="twitter:image" content="https://docs.litellm.ai/img/docusaurus-social-card.png"><meta data-rh="true" property="og:url" content="https://docs.litellm.ai/release_notes"><meta data-rh="true" property="og:locale" content="zh_CN"><meta data-rh="true" name="docusaurus_locale" content="zh-CN"><meta data-rh="true" name="docsearch:language" content="zh-CN"><meta data-rh="true" property="og:title" content="Release Notes | liteLLM"><meta data-rh="true" name="description" content="Blog"><meta data-rh="true" property="og:description" content="Blog"><meta data-rh="true" name="docusaurus_tag" content="blog_posts_list"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_posts_list"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docs.litellm.ai/release_notes"><link data-rh="true" rel="alternate" href="https://docs.litellm.ai/release_notes" hreflang="zh-CN"><link data-rh="true" rel="alternate" href="https://docs.litellm.ai/release_notes" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"Blog","@id":"https://docs.litellm.ai/release_notes","mainEntityOfPage":"https://docs.litellm.ai/release_notes","headline":"Release Notes","description":"Blog","blogPost":[{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-80-11","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-80-11","url":"https://docs.litellm.ai/release_notes/v1-80-11","headline":"[Preview] v1.80.11 - Google Interactions API","name":"[Preview] v1.80.11 - Google Interactions API","description":"Deploy this version","datePublished":"2025-12-20T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaff","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-80-10","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-80-10","url":"https://docs.litellm.ai/release_notes/v1-80-10","headline":"[Preview] v1.80.10.rc.1 - Agent Gateway: Azure Foundry & Bedrock AgentCore","name":"[Preview] v1.80.10.rc.1 - Agent Gateway: Azure Foundry & Bedrock AgentCore","description":"Deploy this version","datePublished":"2025-12-13T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaff","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-80-8","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-80-8","url":"https://docs.litellm.ai/release_notes/v1-80-8","headline":"v1.80.8-stable - Introducing A2A Agent Gateway","name":"v1.80.8-stable - Introducing A2A Agent Gateway","description":"Deploy this version","datePublished":"2025-12-06T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaff","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-80-5","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-80-5","url":"https://docs.litellm.ai/release_notes/v1-80-5","headline":"v1.80.5-stable - Gemini 3.0 Support","name":"v1.80.5-stable - Gemini 3.0 Support","description":"Deploy this version","datePublished":"2025-11-22T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaff","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-80-0","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-80-0","url":"https://docs.litellm.ai/release_notes/v1-80-0","headline":"v1.80.0-stable - Introducing Agent Hub: Register, Publish, and Share Agents","name":"v1.80.0-stable - Introducing Agent Hub: Register, Publish, and Share Agents","description":"Deploy this version","datePublished":"2025-11-15T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaff","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-79-3","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-79-3","url":"https://docs.litellm.ai/release_notes/v1-79-3","headline":"v1.79.3-stable - Built-in Guardrails on AI Gateway","name":"v1.79.3-stable - Built-in Guardrails on AI Gateway","description":"Deploy this version","datePublished":"2025-11-08T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaff","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-79-1","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-79-1","url":"https://docs.litellm.ai/release_notes/v1-79-1","headline":"v1.79.1-stable - Guardrail Playground","name":"v1.79.1-stable - Guardrail Playground","description":"Deploy this version","datePublished":"2025-11-01T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaff","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-79-0","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-79-0","url":"https://docs.litellm.ai/release_notes/v1-79-0","headline":"v1.79.0-stable - Search APIs","name":"v1.79.0-stable - Search APIs","description":"Deploy this version","datePublished":"2025-10-26T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaff","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-78-5","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-78-5","url":"https://docs.litellm.ai/release_notes/v1-78-5","headline":"v1.78.5-stable - Native OCR Support","name":"v1.78.5-stable - Native OCR Support","description":"Deploy this version","datePublished":"2025-10-18T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaff","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-78-0","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-78-0","url":"https://docs.litellm.ai/release_notes/v1-78-0","headline":"v1.78.0-stable - MCP Gateway: Control Tool Access by Team, Key","name":"v1.78.0-stable - MCP Gateway: Control Tool Access by Team, Key","description":"Deploy this version","datePublished":"2025-10-11T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaff","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-77-7","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-77-7","url":"https://docs.litellm.ai/release_notes/v1-77-7","headline":"v1.77.7-stable - 2.9x Lower Median Latency","name":"v1.77.7-stable - 2.9x Lower Median Latency","description":"Deploy this version","datePublished":"2025-10-04T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaff","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-77-5","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-77-5","url":"https://docs.litellm.ai/release_notes/v1-77-5","headline":"v1.77.5-stable - MCP OAuth 2.0 Support","name":"v1.77.5-stable - MCP OAuth 2.0 Support","description":"Deploy this version","datePublished":"2025-09-29T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaff","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-77-3","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-77-3","url":"https://docs.litellm.ai/release_notes/v1-77-3","headline":"v1.77.3-stable - Priority Based Rate Limiting","name":"v1.77.3-stable - Priority Based Rate Limiting","description":"Deploy this version","datePublished":"2025-09-21T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaff","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-77-2","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-77-2","url":"https://docs.litellm.ai/release_notes/v1-77-2","headline":"v1.77.2-stable - Bedrock Batches API","name":"v1.77.2-stable - Bedrock Batches API","description":"Deploy this version","datePublished":"2025-09-13T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-76-3","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-76-3","url":"https://docs.litellm.ai/release_notes/v1-76-3","headline":"v1.76.3-stable - Performance, Video Generation & CloudZero Integration","name":"v1.76.3-stable - Performance, Video Generation & CloudZero Integration","description":"This release has a known issue where startup is leading to Out of Memory errors when deploying on Kubernetes. We recommend waiting before upgrading to this version.","datePublished":"2025-09-06T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-76-1","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-76-1","url":"https://docs.litellm.ai/release_notes/v1-76-1","headline":"v1.76.1-stable - Gemini 2.5 Flash Image","name":"v1.76.1-stable - Gemini 2.5 Flash Image","description":"Deploy this version","datePublished":"2025-08-30T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-76-0","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-76-0","url":"https://docs.litellm.ai/release_notes/v1-76-0","headline":"v1.76.0-stable - RPS Improvements","name":"v1.76.0-stable - RPS Improvements","description":"LiteLLM is hiring a Founding Backend Engineer, in San Francisco.","datePublished":"2025-08-23T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-75-8","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-75-8","url":"https://docs.litellm.ai/release_notes/v1-75-8","headline":"v1.75.8-stable - Team Member Rate Limits","name":"v1.75.8-stable - Team Member Rate Limits","description":"Deploy this version","datePublished":"2025-08-16T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-75-5","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-75-5","url":"https://docs.litellm.ai/release_notes/v1-75-5","headline":"v1.75.5-stable - Redis latency improvements","name":"v1.75.5-stable - Redis latency improvements","description":"Deploy this version","datePublished":"2025-08-10T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-74-15","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-74-15","url":"https://docs.litellm.ai/release_notes/v1-74-15","headline":"v1.74.15-stable","name":"v1.74.15-stable","description":"Deploy this version","datePublished":"2025-08-02T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-74-9","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-74-9","url":"https://docs.litellm.ai/release_notes/v1-74-9","headline":"v1.74.9-stable - Auto-Router","name":"v1.74.9-stable - Auto-Router","description":"Deploy this version","datePublished":"2025-07-27T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-74-7","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-74-7","url":"https://docs.litellm.ai/release_notes/v1-74-7","headline":"v1.74.7-stable","name":"v1.74.7-stable","description":"Deploy this version","datePublished":"2025-07-19T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-74-3-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-74-3-stable","url":"https://docs.litellm.ai/release_notes/v1-74-3-stable","headline":"v1.74.3-stable","name":"v1.74.3-stable","description":"Deploy this version","datePublished":"2025-07-12T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-74-0-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-74-0-stable","url":"https://docs.litellm.ai/release_notes/v1-74-0-stable","headline":"v1.74.0-stable","name":"v1.74.0-stable","description":"Deploy this version","datePublished":"2025-07-05T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-73-6-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-73-6-stable","url":"https://docs.litellm.ai/release_notes/v1-73-6-stable","headline":"v1.73.6-stable","name":"v1.73.6-stable","description":"Deploy this version","datePublished":"2025-06-28T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-73-0-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-73-0-stable","url":"https://docs.litellm.ai/release_notes/v1-73-0-stable","headline":"v1.73.0-stable - Set default team for new users","name":"v1.73.0-stable - Set default team for new users","description":"Known Issues","datePublished":"2025-06-21T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-72-6-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-72-6-stable","url":"https://docs.litellm.ai/release_notes/v1-72-6-stable","headline":"v1.72.6-stable - MCP Gateway Permission Management","name":"v1.72.6-stable - MCP Gateway Permission Management","description":"Deploy this version","datePublished":"2025-06-14T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-72-2-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-72-2-stable","url":"https://docs.litellm.ai/release_notes/v1-72-2-stable","headline":"v1.72.2-stable","name":"v1.72.2-stable","description":"Deploy this version","datePublished":"2025-06-07T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1-72-0-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1-72-0-stable","url":"https://docs.litellm.ai/release_notes/v1-72-0-stable","headline":"v1.72.0-stable","name":"v1.72.0-stable","description":"Deploy this version","datePublished":"2025-05-31T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.71.1-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.71.1-stable","url":"https://docs.litellm.ai/release_notes/v1.71.1-stable","headline":"v1.71.1-stable - 2x Higher Requests Per Second (RPS)","name":"v1.71.1-stable - 2x Higher Requests Per Second (RPS)","description":"Deploy this version","datePublished":"2025-05-24T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.70.1-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.70.1-stable","url":"https://docs.litellm.ai/release_notes/v1.70.1-stable","headline":"v1.70.1-stable - Gemini Realtime API Support","name":"v1.70.1-stable - Gemini Realtime API Support","description":"Deploy this version","datePublished":"2025-05-17T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.69.0-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.69.0-stable","url":"https://docs.litellm.ai/release_notes/v1.69.0-stable","headline":"v1.69.0-stable - Loadbalance Batch API Models","name":"v1.69.0-stable - Loadbalance Batch API Models","description":"Deploy this version","datePublished":"2025-05-10T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.68.0-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.68.0-stable","url":"https://docs.litellm.ai/release_notes/v1.68.0-stable","headline":"v1.68.0-stable","name":"v1.68.0-stable","description":"Deploy this version","datePublished":"2025-05-03T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.67.4-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.67.4-stable","url":"https://docs.litellm.ai/release_notes/v1.67.4-stable","headline":"v1.67.4-stable - Improved User Management","name":"v1.67.4-stable - Improved User Management","description":"Deploy this version","datePublished":"2025-04-26T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.67.0-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.67.0-stable","url":"https://docs.litellm.ai/release_notes/v1.67.0-stable","headline":"v1.67.0-stable - SCIM Integration","name":"v1.67.0-stable - SCIM Integration","description":"Key Highlights","datePublished":"2025-04-19T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.66.0-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.66.0-stable","url":"https://docs.litellm.ai/release_notes/v1.66.0-stable","headline":"v1.66.0-stable - Realtime API Cost Tracking","name":"v1.66.0-stable - Realtime API Cost Tracking","description":"Deploy this version","datePublished":"2025-04-12T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.65.4-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.65.4-stable","url":"https://docs.litellm.ai/release_notes/v1.65.4-stable","headline":"v1.65.4-stable","name":"v1.65.4-stable","description":"Deploy this version","datePublished":"2025-04-05T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.65.0-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.65.0-stable","url":"https://docs.litellm.ai/release_notes/v1.65.0-stable","headline":"v1.65.0-stable - Model Context Protocol","name":"v1.65.0-stable - Model Context Protocol","description":"v1.65.0-stable is live now. Here are the key highlights of this release:","datePublished":"2025-03-30T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.65.0","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.65.0","url":"https://docs.litellm.ai/release_notes/v1.65.0","headline":"v1.65.0 - Team Model Add - update","name":"v1.65.0 - Team Model Add - update","description":"v1.65.0 updates the /model/new endpoint to prevent non-team admins from creating team models.","datePublished":"2025-03-28T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.63.14-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.63.14-stable","url":"https://docs.litellm.ai/release_notes/v1.63.14-stable","headline":"v1.63.14-stable","name":"v1.63.14-stable","description":"These are the changes since v1.63.11-stable.","datePublished":"2025-03-22T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.63.11-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.63.11-stable","url":"https://docs.litellm.ai/release_notes/v1.63.11-stable","headline":"v1.63.11-stable","name":"v1.63.11-stable","description":"These are the changes since v1.63.2-stable.","datePublished":"2025-03-15T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.63.2-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.63.2-stable","url":"https://docs.litellm.ai/release_notes/v1.63.2-stable","headline":"v1.63.2-stable","name":"v1.63.2-stable","description":"These are the changes since v1.61.20-stable.","datePublished":"2025-03-08T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.63.0","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.63.0","url":"https://docs.litellm.ai/release_notes/v1.63.0","headline":"v1.63.0 - Anthropic 'thinking' response update","name":"v1.63.0 - Anthropic 'thinking' response update","description":"v1.63.0 fixes Anthropic 'thinking' response on streaming to return the signature block. Github Issue","datePublished":"2025-03-05T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.61.20-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.61.20-stable","url":"https://docs.litellm.ai/release_notes/v1.61.20-stable","headline":"v1.61.20-stable","name":"v1.61.20-stable","description":"These are the changes since v1.61.13-stable.","datePublished":"2025-03-01T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.59.8-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.59.8-stable","url":"https://docs.litellm.ai/release_notes/v1.59.8-stable","headline":"v1.59.8-stable","name":"v1.59.8-stable","description":"Get a 7 day free trial for LiteLLM Enterprise here.","datePublished":"2025-01-31T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.59.0","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.59.0","url":"https://docs.litellm.ai/release_notes/v1.59.0","headline":"v1.59.0","name":"v1.59.0","description":"Get a 7 day free trial for LiteLLM Enterprise here.","datePublished":"2025-01-17T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.57.8-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.57.8-stable","url":"https://docs.litellm.ai/release_notes/v1.57.8-stable","headline":"v1.57.8-stable","name":"v1.57.8-stable","description":"alerting, prometheus, secret management, management endpoints, ui, prompt management, finetuning, batch","datePublished":"2025-01-11T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.57.7","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.57.7","url":"https://docs.litellm.ai/release_notes/v1.57.7","headline":"v1.57.7","name":"v1.57.7","description":"langfuse, management endpoints, ui, prometheus, secret management","datePublished":"2025-01-10T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.57.3","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.57.3","url":"https://docs.litellm.ai/release_notes/v1.57.3","headline":"v1.57.3 - New Base Docker Image","name":"v1.57.3 - New Base Docker Image","description":"docker image, security, vulnerability","datePublished":"2025-01-08T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.56.4","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.56.4","url":"https://docs.litellm.ai/release_notes/v1.56.4","headline":"v1.56.4","name":"v1.56.4","description":"deepgram, fireworks ai, vision, admin ui, dependency upgrades","datePublished":"2024-12-29T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.56.3","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.56.3","url":"https://docs.litellm.ai/release_notes/v1.56.3","headline":"v1.56.3","name":"v1.56.3","description":"guardrails, logging, virtual key management, new models","datePublished":"2024-12-28T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.56.1","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.56.1","url":"https://docs.litellm.ai/release_notes/v1.56.1","headline":"v1.56.1","name":"v1.56.1","description":"key management, budgets/rate limits, logging, guardrails","datePublished":"2024-12-27T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.55.10","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.55.10","url":"https://docs.litellm.ai/release_notes/v1.55.10","headline":"v1.55.10","name":"v1.55.10","description":"batches, guardrails, team management, custom auth","datePublished":"2024-12-24T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"keywords":[]},{"@type":"BlogPosting","@id":"https://docs.litellm.ai/release_notes/v1.55.8-stable","mainEntityOfPage":"https://docs.litellm.ai/release_notes/v1.55.8-stable","url":"https://docs.litellm.ai/release_notes/v1.55.8-stable","headline":"v1.55.8-stable","name":"v1.55.8-stable","description":"A new LiteLLM Stable release just went out. Here are 5 updates since v1.52.2-stable.","datePublished":"2024-12-22T10:00:00.000Z","author":[{"@type":"Person","name":"Krrish Dholakia","description":"CEO, LiteLLM","url":"https://www.linkedin.com/in/krish-d/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&v=beta&t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"},{"@type":"Person","name":"Ishaan Jaffer","description":"CTO, LiteLLM","url":"https://www.linkedin.com/in/reffajnaahsi/","image":"https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&v=beta&t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"}],"keywords":[]}]}</script><link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-K7K215ZVNC"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-K7K215ZVNC",{anonymize_ip:!0})</script>





<link rel="alternate" type="application/rss+xml" href="/release_notes/rss.xml" title="liteLLM RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/release_notes/atom.xml" title="liteLLM Atom Feed">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="liteLLM RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="liteLLM Atom Feed">
<script>window.$crisp=[],window.CRISP_WEBSITE_ID="be07a4d6-dba0-4df7-961d-9302c86b7ebc",d=document,s=d.createElement("script"),s.src="https://client.crisp.chat/l.js",s.async=1,d.getElementsByTagName("head")[0].appendChild(s)</script>

<script async src="https://www.feedbackrocket.io/sdk/v1.2.js" data-fr-id="GQwepB0f0L-x_ZH63kR_V" data-fr-theme="dynamic"></script><link rel="stylesheet" href="/assets/css/styles.ab6b1187.css">
<script src="/assets/js/runtime~main.e2ac0700.js" defer="defer"></script>
<script src="/assets/js/main.cc386e68.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg"><link rel="preload" as="image" href="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg"><link rel="preload" as="image" href="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8"><link rel="preload" as="image" href="/assets/images/litellm_test_connection-029765a2de4dcabccfe3be9a8d33dbdd.gif"><link rel="preload" as="image" href="/assets/images/litellm_thinking_openweb-5ec7dddb7e7b6a10252694c27cfc177d.gif"><link rel="preload" as="image" href="https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&amp;v=beta&amp;t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc"><div role="region" aria-label=""><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback"></a></div><nav aria-label="" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><b class="navbar__title text--truncate"> LiteLLM</b></a><a class="navbar__item navbar__link" href="/docs/">Docs</a><a class="navbar__item navbar__link" sidebarid="integrationsSidebar" href="/docs/integrations">Integrations</a><a class="navbar__item navbar__link" sidebarid="tutorialSidebar" href="/docs/enterprise">Enterprise</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/release_notes">Release Notes</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://models.litellm.ai/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"> LLM Model Cost Map<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><a href="https://github.com/BerriAI/litellm" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><a href="https://www.litellm.ai/support" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Slack/Discord<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="/system mode"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div id="inkeep-shadowradix-_R_uclq5_" style="display:contents"></div><span></span></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label=""><div class="sidebarItemTitle_pO2u margin-bottom--md">Releases</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-80-11">[Preview] v1.80.11 - Google Interactions API</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-80-10">[Preview] v1.80.10.rc.1 - Agent Gateway: Azure Foundry &amp; Bedrock AgentCore</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-80-8">v1.80.8-stable - Introducing A2A Agent Gateway</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-80-5">v1.80.5-stable - Gemini 3.0 Support</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-80-0">v1.80.0-stable - Introducing Agent Hub: Register, Publish, and Share Agents</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-79-3">v1.79.3-stable - Built-in Guardrails on AI Gateway</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-79-1">v1.79.1-stable - Guardrail Playground</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-79-0">v1.79.0-stable - Search APIs</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-78-5">v1.78.5-stable - Native OCR Support</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-78-0">v1.78.0-stable - MCP Gateway: Control Tool Access by Team, Key</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-77-7">v1.77.7-stable - 2.9x Lower Median Latency</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-77-5">v1.77.5-stable - MCP OAuth 2.0 Support</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-77-3">v1.77.3-stable - Priority Based Rate Limiting</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-77-2">v1.77.2-stable - Bedrock Batches API</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-76-3">v1.76.3-stable - Performance, Video Generation &amp; CloudZero Integration</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-76-1">v1.76.1-stable - Gemini 2.5 Flash Image</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-76-0">v1.76.0-stable - RPS Improvements</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-75-8">v1.75.8-stable - Team Member Rate Limits</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-75-5">v1.75.5-stable - Redis latency improvements</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-74-15">v1.74.15-stable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-74-9">v1.74.9-stable - Auto-Router</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-74-7">v1.74.7-stable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-74-3-stable">v1.74.3-stable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-74-0-stable">v1.74.0-stable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-73-6-stable">v1.73.6-stable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-73-0-stable">v1.73.0-stable - Set default team for new users</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-72-6-stable">v1.72.6-stable - MCP Gateway Permission Management</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-72-2-stable">v1.72.2-stable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1-72-0-stable">v1.72.0-stable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.71.1-stable">v1.71.1-stable - 2x Higher Requests Per Second (RPS)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.70.1-stable">v1.70.1-stable - Gemini Realtime API Support</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.69.0-stable">v1.69.0-stable - Loadbalance Batch API Models</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.68.0-stable">v1.68.0-stable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.67.4-stable">v1.67.4-stable - Improved User Management</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.67.0-stable">v1.67.0-stable - SCIM Integration</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.66.0-stable">v1.66.0-stable - Realtime API Cost Tracking</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.65.4-stable">v1.65.4-stable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.65.0-stable">v1.65.0-stable - Model Context Protocol</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.65.0">v1.65.0 - Team Model Add - update</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.63.14-stable">v1.63.14-stable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.63.11-stable">v1.63.11-stable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.63.2-stable">v1.63.2-stable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.63.0">v1.63.0 - Anthropic &#x27;thinking&#x27; response update</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.61.20-stable">v1.61.20-stable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.59.8-stable">v1.59.8-stable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.59.0">v1.59.0</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.57.8-stable">v1.57.8-stable</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.57.7">v1.57.7</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.57.3">v1.57.3 - New Base Docker Image</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.56.4">v1.56.4</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.56.3">v1.56.3</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.56.1">v1.56.1</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.55.10">v1.55.10</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/release_notes/v1.55.8-stable">v1.55.8-stable</a></li></ul></div></nav></aside><main class="col col--7"><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-80-11">[Preview] v1.80.11 - Google Interactions API</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-12-20T10:00:00.000Z">20251220</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaff"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaff</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.80.11.rc.1</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.80.11</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>Gemini 3 Flash Preview</strong> - <a href="/docs/providers/gemini">Day 0 support for Google&#x27;s Gemini 3 Flash Preview with reasoning capabilities</a></li>
<li><strong>Stability AI Image Generation</strong> - <a href="/docs/providers/stability">New provider for Stability AI image generation and editing</a></li>
<li><strong>LiteLLM Content Filter</strong> - <a href="/docs/proxy/guardrails/litellm_content_filter">Built-in guardrails for harmful content, bias, and PII detection with image support</a></li>
<li><strong>New Provider: Venice.ai</strong> - Support for Venice.ai API via providers.json</li>
<li><strong>Unified Skills API</strong> - <a href="/docs/skills">Skills API works across Anthropic, Vertex, Azure, and Bedrock</a></li>
<li><strong>Azure Sentinel Logging</strong> - <a href="/docs/observability/azure_sentinel">New logging integration for Azure Sentinel</a></li>
<li><strong>Guardrails Load Balancing</strong> - <a href="/docs/proxy/guardrails">Load balance between multiple guardrail providers</a></li>
<li><strong>Email Budget Alerts</strong> - <a href="/docs/proxy/email">Send email notifications when budgets are reached</a></li>
<li><strong>Cloudzero Integration on UI</strong> - Setup your Cloudzero Integration Directly on the UI</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cloudzero-integration-on-ui">Cloudzero Integration on UI<a href="#cloudzero-integration-on-ui" class="hash-link" aria-label="Cloudzero Integration on UI" title="Cloudzero Integration on UI"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAc0lEQVR4nFWNuw7CMBAE/f8/R08JFUgQZOLE9xx0oklW2mY0q22iyrYZ6+pMFdQMVSMiiEjCnYykuTvL8qb3D7uNv2iGVd2JOcGdVssxBqpKhuEm1EvxY1pm8u0dEeX2Ch49T8JJVBWmKJc7XJ+Fk+LH/gCW6MRTg1/h0gAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="309"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/ui_cloudzero.2349047.640.png" srcset="/assets/ideal-img/ui_cloudzero.2349047.640.png 640w,/assets/ideal-img/ui_cloudzero.7c5cc2f.1005.png 1005w" width="640" height="309"></noscript></div>
<p>Users can now configure their Cloudzero Integration directly on the UI.</p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="performance-50-reduction-in-memory-usage-and-import-latency-for-the-litellm-sdk">Performance: 50% Reduction in Memory Usage and Import Latency for the LiteLLM SDK<a href="#performance-50-reduction-in-memory-usage-and-import-latency-for-the-litellm-sdk" class="hash-link" aria-label="Performance: 50% Reduction in Memory Usage and Import Latency for the LiteLLM SDK" title="Performance: 50% Reduction in Memory Usage and Import Latency for the LiteLLM SDK"></a></h3>
<p>We&#x27;ve completely restructured <code>litellm.__init__.py</code> to defer heavy imports until they&#x27;re actually needed, implementing lazy loading for <strong>109 components</strong>.</p>
<p>This refactoring includes <strong>41 provider config classes</strong>, <strong>40 utility functions</strong>, cache implementations (Redis, DualCache, InMemoryCache), HTTP handlers, logging, types, and other heavy dependencies. Heavy libraries like tiktoken and boto3 are now loaded on-demand rather than eagerly at import time.</p>
<p>This makes LiteLLM especially beneficial for serverless functions, Lambda deployments, and containerized environments where cold start times and memory footprint matter.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-providers-and-endpoints">New Providers and Endpoints<a href="#new-providers-and-endpoints" class="hash-link" aria-label="New Providers and Endpoints" title="New Providers and Endpoints"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="new-providers-5-new-providers">New Providers (5 new providers)<a href="#new-providers-5-new-providers" class="hash-link" aria-label="New Providers (5 new providers)" title="New Providers (5 new providers)"></a></h3>
<table><thead><tr><th>Provider</th><th>Supported LiteLLM Endpoints</th><th>Description</th></tr></thead><tbody><tr><td><a href="/docs/providers/stability">Stability AI</a></td><td><code>/images/generations</code>, <code>/images/edits</code></td><td>Stable Diffusion 3, SD3.5, image editing and generation</td></tr><tr><td>Venice.ai</td><td><code>/chat/completions</code>, <code>/messages</code>, <code>/responses</code></td><td>Venice.ai API integration via providers.json</td></tr><tr><td><a href="/docs/providers/pydantic_ai_agent">Pydantic AI Agents</a></td><td><code>/a2a</code></td><td>Pydantic AI agents for A2A protocol workflows</td></tr><tr><td><a href="/docs/providers/vertex_ai_agent_engine">VertexAI Agent Engine</a></td><td><code>/a2a</code></td><td>Google Vertex AI Agent Engine for agentic workflows</td></tr><tr><td><a href="/docs/search/linkup">LinkUp Search</a></td><td><code>/search</code></td><td>LinkUp web search API integration</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="new-llm-api-endpoints-2-new-endpoints">New LLM API Endpoints (2 new endpoints)<a href="#new-llm-api-endpoints-2-new-endpoints" class="hash-link" aria-label="New LLM API Endpoints (2 new endpoints)" title="New LLM API Endpoints (2 new endpoints)"></a></h3>
<table><thead><tr><th>Endpoint</th><th>Method</th><th>Description</th><th>Documentation</th></tr></thead><tbody><tr><td><code>/interactions</code></td><td>POST</td><td>Google Interactions API for conversational AI</td><td><a href="/docs/interactions">Docs</a></td></tr><tr><td><code>/search</code></td><td>POST</td><td>RAG Search API with rerankers</td><td><a href="/docs/search/index">Docs</a></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-model-support-55-new-models">New Model Support (55+ new models)<a href="#new-model-support-55-new-models" class="hash-link" aria-label="New Model Support (55+ new models)" title="New Model Support (55+ new models)"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Features</th></tr></thead><tbody><tr><td>Gemini</td><td><code>gemini/gemini-3-flash-preview</code></td><td>1M</td><td>$0.50</td><td>$3.00</td><td>Reasoning, vision, audio, video, PDF</td></tr><tr><td>Vertex AI</td><td><code>vertex_ai/gemini-3-flash-preview</code></td><td>1M</td><td>$0.50</td><td>$3.00</td><td>Reasoning, vision, audio, video, PDF</td></tr><tr><td>Azure AI</td><td><code>azure_ai/deepseek-v3.2</code></td><td>164K</td><td>$0.58</td><td>$1.68</td><td>Reasoning, function calling, caching</td></tr><tr><td>Azure AI</td><td><code>azure_ai/cohere-rerank-v4.0-pro</code></td><td>32K</td><td>$0.0025/query</td><td>-</td><td>Rerank</td></tr><tr><td>Azure AI</td><td><code>azure_ai/cohere-rerank-v4.0-fast</code></td><td>32K</td><td>$0.002/query</td><td>-</td><td>Rerank</td></tr><tr><td>OpenRouter</td><td><code>openrouter/openai/gpt-5.2</code></td><td>400K</td><td>$1.75</td><td>$14.00</td><td>Reasoning, vision, caching</td></tr><tr><td>OpenRouter</td><td><code>openrouter/openai/gpt-5.2-pro</code></td><td>400K</td><td>$21.00</td><td>$168.00</td><td>Reasoning, vision</td></tr><tr><td>OpenRouter</td><td><code>openrouter/mistralai/devstral-2512</code></td><td>262K</td><td>$0.15</td><td>$0.60</td><td>Function calling</td></tr><tr><td>OpenRouter</td><td><code>openrouter/mistralai/ministral-3b-2512</code></td><td>131K</td><td>$0.10</td><td>$0.10</td><td>Function calling, vision</td></tr><tr><td>OpenRouter</td><td><code>openrouter/mistralai/ministral-8b-2512</code></td><td>262K</td><td>$0.15</td><td>$0.15</td><td>Function calling, vision</td></tr><tr><td>OpenRouter</td><td><code>openrouter/mistralai/ministral-14b-2512</code></td><td>262K</td><td>$0.20</td><td>$0.20</td><td>Function calling, vision</td></tr><tr><td>OpenRouter</td><td><code>openrouter/mistralai/mistral-large-2512</code></td><td>262K</td><td>$0.50</td><td>$1.50</td><td>Function calling, vision</td></tr><tr><td>OpenAI</td><td><code>gpt-4o-transcribe-diarize</code></td><td>16K</td><td>$6.00/audio</td><td>-</td><td>Audio transcription with diarization</td></tr><tr><td>OpenAI</td><td><code>gpt-image-1.5-2025-12-16</code></td><td>-</td><td>Various</td><td>Various</td><td>Image generation</td></tr><tr><td>Stability</td><td><code>stability/sd3-large</code></td><td>-</td><td>-</td><td>$0.065/image</td><td>Image generation</td></tr><tr><td>Stability</td><td><code>stability/sd3.5-large</code></td><td>-</td><td>-</td><td>$0.065/image</td><td>Image generation</td></tr><tr><td>Stability</td><td><code>stability/stable-image-ultra</code></td><td>-</td><td>-</td><td>$0.08/image</td><td>Image generation</td></tr><tr><td>Stability</td><td><code>stability/inpaint</code></td><td>-</td><td>-</td><td>$0.005/image</td><td>Image editing</td></tr><tr><td>Stability</td><td><code>stability/outpaint</code></td><td>-</td><td>-</td><td>$0.004/image</td><td>Image editing</td></tr><tr><td>Bedrock</td><td><code>stability.stable-conservative-upscale-v1:0</code></td><td>-</td><td>-</td><td>$0.40/image</td><td>Image upscaling</td></tr><tr><td>Bedrock</td><td><code>stability.stable-creative-upscale-v1:0</code></td><td>-</td><td>-</td><td>$0.60/image</td><td>Image upscaling</td></tr><tr><td>Vertex AI</td><td><code>vertex_ai/deepseek-ai/deepseek-ocr-maas</code></td><td>-</td><td>$0.30</td><td>$1.20</td><td>OCR</td></tr><tr><td>LinkUp</td><td><code>linkup/search</code></td><td>-</td><td>$5.87/1K queries</td><td>-</td><td>Web search</td></tr><tr><td>LinkUp</td><td><code>linkup/search-deep</code></td><td>-</td><td>$58.67/1K queries</td><td>-</td><td>Deep web search</td></tr><tr><td>GitHub Copilot</td><td>20+ models</td><td>Various</td><td>-</td><td>-</td><td>Chat completions</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/gemini">Gemini</a></strong>
<ul>
<li>Add Gemini 3 Flash Preview day 0 support with reasoning - <a href="https://github.com/BerriAI/litellm/pull/18135" target="_blank" rel="noopener noreferrer">PR #18135</a></li>
<li>Support extra_headers in batch embeddings - <a href="https://github.com/BerriAI/litellm/pull/18004" target="_blank" rel="noopener noreferrer">PR #18004</a></li>
<li>Propagate token usage when generating images - <a href="https://github.com/BerriAI/litellm/pull/17987" target="_blank" rel="noopener noreferrer">PR #17987</a></li>
<li>Use JSON instead of form-data for image edit requests - <a href="https://github.com/BerriAI/litellm/pull/18012" target="_blank" rel="noopener noreferrer">PR #18012</a></li>
<li>Fix web search requests count - <a href="https://github.com/BerriAI/litellm/pull/17921" target="_blank" rel="noopener noreferrer">PR #17921</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Use dynamic max_tokens based on model - <a href="https://github.com/BerriAI/litellm/pull/17900" target="_blank" rel="noopener noreferrer">PR #17900</a></li>
<li>Fix claude-3-7-sonnet max_tokens to 64K default - <a href="https://github.com/BerriAI/litellm/pull/17979" target="_blank" rel="noopener noreferrer">PR #17979</a></li>
<li>Add OpenAI-compatible API with modify_params=True - <a href="https://github.com/BerriAI/litellm/pull/17106" target="_blank" rel="noopener noreferrer">PR #17106</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex">Vertex AI</a></strong>
<ul>
<li>Add Gemini 3 Flash Preview support - <a href="https://github.com/BerriAI/litellm/pull/18164" target="_blank" rel="noopener noreferrer">PR #18164</a></li>
<li>Add reasoning support for gemini-3-flash-preview - <a href="https://github.com/BerriAI/litellm/pull/18175" target="_blank" rel="noopener noreferrer">PR #18175</a></li>
<li>Fix image edit credential source - <a href="https://github.com/BerriAI/litellm/pull/18121" target="_blank" rel="noopener noreferrer">PR #18121</a></li>
<li>Pass credentials to PredictionServiceClient for custom endpoints - <a href="https://github.com/BerriAI/litellm/pull/17757" target="_blank" rel="noopener noreferrer">PR #17757</a></li>
<li>Fix multimodal embeddings for text + base64 image combinations - <a href="https://github.com/BerriAI/litellm/pull/18172" target="_blank" rel="noopener noreferrer">PR #18172</a></li>
<li>Add OCR support for DeepSeek model - <a href="https://github.com/BerriAI/litellm/pull/17971" target="_blank" rel="noopener noreferrer">PR #17971</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure_ai">Azure AI</a></strong>
<ul>
<li>Add Azure Cohere 4 reranking models - <a href="https://github.com/BerriAI/litellm/pull/17961" target="_blank" rel="noopener noreferrer">PR #17961</a></li>
<li>Add Azure DeepSeek V3.2 versions - <a href="https://github.com/BerriAI/litellm/pull/18019" target="_blank" rel="noopener noreferrer">PR #18019</a></li>
<li>Return AzureAnthropicConfig for Claude models in get_provider_chat_config - <a href="https://github.com/BerriAI/litellm/pull/18086" target="_blank" rel="noopener noreferrer">PR #18086</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/fireworks_ai">Fireworks AI</a></strong>
<ul>
<li>Add reasoning param support for Fireworks AI models - <a href="https://github.com/BerriAI/litellm/pull/17967" target="_blank" rel="noopener noreferrer">PR #17967</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>
<ul>
<li>Add Qwen 2 and Qwen 3 to get_bedrock_model_id - <a href="https://github.com/BerriAI/litellm/pull/18100" target="_blank" rel="noopener noreferrer">PR #18100</a></li>
<li>Remove ttl field when routing to bedrock - <a href="https://github.com/BerriAI/litellm/pull/18049" target="_blank" rel="noopener noreferrer">PR #18049</a></li>
<li>Add Bedrock Stability image edit models - <a href="https://github.com/BerriAI/litellm/pull/18254" target="_blank" rel="noopener noreferrer">PR #18254</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/perplexity">Perplexity</a></strong>
<ul>
<li>Use API-provided cost instead of manual calculation - <a href="https://github.com/BerriAI/litellm/pull/17887" target="_blank" rel="noopener noreferrer">PR #17887</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openai">OpenAI</a></strong>
<ul>
<li>Add diarize model for audio transcription - <a href="https://github.com/BerriAI/litellm/pull/18117" target="_blank" rel="noopener noreferrer">PR #18117</a></li>
<li>Add gpt-image-1.5-2025-12-16 in model cost map - <a href="https://github.com/BerriAI/litellm/pull/18107" target="_blank" rel="noopener noreferrer">PR #18107</a></li>
<li>Fix cost calculation of gpt-image-1 model - <a href="https://github.com/BerriAI/litellm/pull/17966" target="_blank" rel="noopener noreferrer">PR #17966</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/github_copilot">GitHub Copilot</a></strong>
<ul>
<li>Add github_copilot model info - <a href="https://github.com/BerriAI/litellm/pull/17858" target="_blank" rel="noopener noreferrer">PR #17858</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/custom_llm_server">Custom LLM</a></strong>
<ul>
<li>Add image_edit and aimage_edit support - <a href="https://github.com/BerriAI/litellm/pull/17999" target="_blank" rel="noopener noreferrer">PR #17999</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes">Bug Fixes<a href="#bug-fixes" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h3>
<ul>
<li><strong><a href="/docs/providers/gemini">Gemini</a></strong>
<ul>
<li>Fix pricing for Gemini 3 Flash on Vertex AI - <a href="https://github.com/BerriAI/litellm/pull/18202" target="_blank" rel="noopener noreferrer">PR #18202</a></li>
<li>Add output_cost_per_image_token for gemini-2.5-flash-image models - <a href="https://github.com/BerriAI/litellm/pull/18156" target="_blank" rel="noopener noreferrer">PR #18156</a></li>
<li>Fix properties should be non-empty for OBJECT type - <a href="https://github.com/BerriAI/litellm/pull/18237" target="_blank" rel="noopener noreferrer">PR #18237</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/fireworks_ai">Qwen</a></strong>
<ul>
<li>Add qwen3-embedding-8b input per token price - <a href="https://github.com/BerriAI/litellm/pull/18018" target="_blank" rel="noopener noreferrer">PR #18018</a></li>
</ul>
</li>
<li><strong>General</strong>
<ul>
<li>Fix image URL handling - <a href="https://github.com/BerriAI/litellm/pull/18139" target="_blank" rel="noopener noreferrer">PR #18139</a></li>
<li>Support Signed URLs with Query Parameters in Image Processing - <a href="https://github.com/BerriAI/litellm/pull/17976" target="_blank" rel="noopener noreferrer">PR #17976</a></li>
<li>Add none to encoding_format instead of omitting it - <a href="https://github.com/BerriAI/litellm/pull/18042" target="_blank" rel="noopener noreferrer">PR #18042</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/response_api">Responses API</a></strong>
<ul>
<li>Add provider specific tools support - <a href="https://github.com/BerriAI/litellm/pull/17980" target="_blank" rel="noopener noreferrer">PR #17980</a></li>
<li>Add custom headers support - <a href="https://github.com/BerriAI/litellm/pull/18036" target="_blank" rel="noopener noreferrer">PR #18036</a></li>
<li>Fix tool calls transformation in completion bridge - <a href="https://github.com/BerriAI/litellm/pull/18226" target="_blank" rel="noopener noreferrer">PR #18226</a></li>
<li>Use list format with input_text for tool results - <a href="https://github.com/BerriAI/litellm/pull/18257" target="_blank" rel="noopener noreferrer">PR #18257</a></li>
<li>Add cost tracking in background mode - <a href="https://github.com/BerriAI/litellm/pull/18236" target="_blank" rel="noopener noreferrer">PR #18236</a></li>
<li>Fix Claude code responses API bridge errors - <a href="https://github.com/BerriAI/litellm/pull/18194" target="_blank" rel="noopener noreferrer">PR #18194</a></li>
</ul>
</li>
<li><strong><a href="/docs/completion/input">Chat Completions API</a></strong>
<ul>
<li>Add support for agent skills - <a href="https://github.com/BerriAI/litellm/pull/18031" target="_blank" rel="noopener noreferrer">PR #18031</a></li>
</ul>
</li>
<li><strong><a href="/docs/skills">Skills API</a></strong>
<ul>
<li>Unified Skills API works across Anthropic, Vertex, Azure, Bedrock - <a href="https://github.com/BerriAI/litellm/pull/18232" target="_blank" rel="noopener noreferrer">PR #18232</a></li>
</ul>
</li>
<li><strong><a href="/docs/search/index">Search API</a></strong>
<ul>
<li>Add new RAG Search API with rerankers - <a href="https://github.com/BerriAI/litellm/pull/18217" target="_blank" rel="noopener noreferrer">PR #18217</a></li>
</ul>
</li>
<li><strong><a href="/docs/interactions">Interactions API</a></strong>
<ul>
<li>Add Google Interactions API on SDK and AI Gateway - <a href="https://github.com/BerriAI/litellm/pull/18079" target="_blank" rel="noopener noreferrer">PR #18079</a>, <a href="https://github.com/BerriAI/litellm/pull/18081" target="_blank" rel="noopener noreferrer">PR #18081</a></li>
</ul>
</li>
<li><strong><a href="/docs/image_edits">Image Edit API</a></strong>
<ul>
<li>Add drop_params support and fix Vertex AI config - <a href="https://github.com/BerriAI/litellm/pull/18077" target="_blank" rel="noopener noreferrer">PR #18077</a></li>
</ul>
</li>
<li><strong>General</strong>
<ul>
<li>Skip adding beta headers for Vertex AI as it is not supported - <a href="https://github.com/BerriAI/litellm/pull/18037" target="_blank" rel="noopener noreferrer">PR #18037</a></li>
<li>Fix managed files endpoint - <a href="https://github.com/BerriAI/litellm/pull/18046" target="_blank" rel="noopener noreferrer">PR #18046</a></li>
<li>Allow base_model for non-Azure providers in proxy - <a href="https://github.com/BerriAI/litellm/pull/18038" target="_blank" rel="noopener noreferrer">PR #18038</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>General</strong>
<ul>
<li>Fix basemodel import in guardrail translation - <a href="https://github.com/BerriAI/litellm/pull/17977" target="_blank" rel="noopener noreferrer">PR #17977</a></li>
<li>Fix No module named &#x27;fastapi&#x27; error - <a href="https://github.com/BerriAI/litellm/pull/18239" target="_blank" rel="noopener noreferrer">PR #18239</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Virtual Keys</strong>
<ul>
<li>Add master key rotation for credentials table - <a href="https://github.com/BerriAI/litellm/pull/17952" target="_blank" rel="noopener noreferrer">PR #17952</a></li>
<li>Fix tag management to preserve encrypted fields in litellm_params - <a href="https://github.com/BerriAI/litellm/pull/17484" target="_blank" rel="noopener noreferrer">PR #17484</a></li>
<li>Fix key delete and regenerate permissions - <a href="https://github.com/BerriAI/litellm/pull/18214" target="_blank" rel="noopener noreferrer">PR #18214</a></li>
</ul>
</li>
<li><strong>Models + Endpoints</strong>
<ul>
<li>Add Models Conditional Rendering in UI - <a href="https://github.com/BerriAI/litellm/pull/18071" target="_blank" rel="noopener noreferrer">PR #18071</a></li>
<li>Add Health Check Model for Wildcard Model in UI - <a href="https://github.com/BerriAI/litellm/pull/18269" target="_blank" rel="noopener noreferrer">PR #18269</a></li>
<li>Auto Resolve Vector Store Embedding Model Config - <a href="https://github.com/BerriAI/litellm/pull/18167" target="_blank" rel="noopener noreferrer">PR #18167</a></li>
</ul>
</li>
<li><strong>Vector Stores</strong>
<ul>
<li>Add Milvus Vector Store UI support - <a href="https://github.com/BerriAI/litellm/pull/18030" target="_blank" rel="noopener noreferrer">PR #18030</a></li>
<li>Persist Vector Store Settings in Team Update - <a href="https://github.com/BerriAI/litellm/pull/18274" target="_blank" rel="noopener noreferrer">PR #18274</a></li>
</ul>
</li>
<li><strong>Logs &amp; Spend</strong>
<ul>
<li>Add LiteLLM Overhead to Logs - <a href="https://github.com/BerriAI/litellm/pull/18033" target="_blank" rel="noopener noreferrer">PR #18033</a></li>
<li>Show LiteLLM Overhead in Logs UI - <a href="https://github.com/BerriAI/litellm/pull/18034" target="_blank" rel="noopener noreferrer">PR #18034</a></li>
<li>Resolve Team ID to Team Alias in Usage Page - <a href="https://github.com/BerriAI/litellm/pull/18275" target="_blank" rel="noopener noreferrer">PR #18275</a></li>
<li>Fix Usage Page Top Key View Button Visibility - <a href="https://github.com/BerriAI/litellm/pull/18203" target="_blank" rel="noopener noreferrer">PR #18203</a></li>
</ul>
</li>
<li><strong>SSO &amp; Health</strong>
<ul>
<li>Add SSO Readiness Health Check - <a href="https://github.com/BerriAI/litellm/pull/18078" target="_blank" rel="noopener noreferrer">PR #18078</a></li>
<li>Fix /health/test_connection to resolve env variables like /chat/completions - <a href="https://github.com/BerriAI/litellm/pull/17752" target="_blank" rel="noopener noreferrer">PR #17752</a></li>
</ul>
</li>
<li><strong>CloudZero</strong>
<ul>
<li>Add CloudZero Cost Tracking UI - <a href="https://github.com/BerriAI/litellm/pull/18163" target="_blank" rel="noopener noreferrer">PR #18163</a></li>
<li>Add Delete CloudZero Settings Route and UI - <a href="https://github.com/BerriAI/litellm/pull/18168" target="_blank" rel="noopener noreferrer">PR #18168</a>, <a href="https://github.com/BerriAI/litellm/pull/18170" target="_blank" rel="noopener noreferrer">PR #18170</a></li>
</ul>
</li>
<li><strong>General</strong>
<ul>
<li>Update UI path handling for non-root Docker - <a href="https://github.com/BerriAI/litellm/pull/17989" target="_blank" rel="noopener noreferrer">PR #17989</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>UI Fixes</strong>
<ul>
<li>Fix Login Page Failed To Parse JSON Error - <a href="https://github.com/BerriAI/litellm/pull/18159" target="_blank" rel="noopener noreferrer">PR #18159</a></li>
<li>Fix new user route user_id collision handling - <a href="https://github.com/BerriAI/litellm/pull/17559" target="_blank" rel="noopener noreferrer">PR #17559</a></li>
<li>Fix Callback Environment Variables Casing - <a href="https://github.com/BerriAI/litellm/pull/17912" target="_blank" rel="noopener noreferrer">PR #17912</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ai-integrations">AI Integrations<a href="#ai-integrations" class="hash-link" aria-label="AI Integrations" title="AI Integrations"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="logging">Logging<a href="#logging" class="hash-link" aria-label="Logging" title="Logging"></a></h3>
<ul>
<li><strong><a href="/docs/observability/azure_sentinel">Azure Sentinel</a></strong>
<ul>
<li>Add new Azure Sentinel Logger integration - <a href="https://github.com/BerriAI/litellm/pull/18146" target="_blank" rel="noopener noreferrer">PR #18146</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#prometheus">Prometheus</a></strong>
<ul>
<li>Add extraction of top level metadata for custom labels - <a href="https://github.com/BerriAI/litellm/pull/18087" target="_blank" rel="noopener noreferrer">PR #18087</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#langfuse">Langfuse</a></strong>
<ul>
<li>Fix not working log_failure_event - <a href="https://github.com/BerriAI/litellm/pull/18234" target="_blank" rel="noopener noreferrer">PR #18234</a></li>
</ul>
</li>
<li><strong><a href="/docs/observability/phoenix_integration">Arize Phoenix</a></strong>
<ul>
<li>Fix nested spans - <a href="https://github.com/BerriAI/litellm/pull/18102" target="_blank" rel="noopener noreferrer">PR #18102</a></li>
</ul>
</li>
<li><strong>General</strong>
<ul>
<li>Change extra_headers to additional_headers - <a href="https://github.com/BerriAI/litellm/pull/17950" target="_blank" rel="noopener noreferrer">PR #17950</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="guardrails">Guardrails<a href="#guardrails" class="hash-link" aria-label="Guardrails" title="Guardrails"></a></h3>
<ul>
<li><strong><a href="/docs/proxy/guardrails/litellm_content_filter">LiteLLM Content Filter</a></strong>
<ul>
<li>Add built-in guardrails for harmful content, bias, etc. - <a href="https://github.com/BerriAI/litellm/pull/18029" target="_blank" rel="noopener noreferrer">PR #18029</a></li>
<li>Add support for running content filters on images - <a href="https://github.com/BerriAI/litellm/pull/18044" target="_blank" rel="noopener noreferrer">PR #18044</a></li>
<li>Add support for Brazil PII field - <a href="https://github.com/BerriAI/litellm/pull/18076" target="_blank" rel="noopener noreferrer">PR #18076</a></li>
<li>Add configurable guardrail options for content filtering - <a href="https://github.com/BerriAI/litellm/pull/18007" target="_blank" rel="noopener noreferrer">PR #18007</a></li>
</ul>
</li>
<li><strong><a href="/docs/adding_provider/generic_guardrail_api">Guardrails API</a></strong>
<ul>
<li>Support LLM tool call response checks on <code>/chat/completions</code>, <code>/v1/responses</code>, <code>/v1/messages</code> - <a href="https://github.com/BerriAI/litellm/pull/17619" target="_blank" rel="noopener noreferrer">PR #17619</a></li>
<li>Add guardrails load balancing - <a href="https://github.com/BerriAI/litellm/pull/18181" target="_blank" rel="noopener noreferrer">PR #18181</a></li>
<li>Fix guardrails for passthrough endpoint - <a href="https://github.com/BerriAI/litellm/pull/18109" target="_blank" rel="noopener noreferrer">PR #18109</a></li>
<li>Add headers to metadata for guardrails on pass-through endpoints - <a href="https://github.com/BerriAI/litellm/pull/17992" target="_blank" rel="noopener noreferrer">PR #17992</a></li>
<li>Various fixes for guardrail on OpenRouter models - <a href="https://github.com/BerriAI/litellm/pull/18085" target="_blank" rel="noopener noreferrer">PR #18085</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/lakera_ai">Lakera</a></strong>
<ul>
<li>Add monitor mode for Lakera - <a href="https://github.com/BerriAI/litellm/pull/18084" target="_blank" rel="noopener noreferrer">PR #18084</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/pillar_security">Pillar Security</a></strong>
<ul>
<li>Add masking support and MCP call support - <a href="https://github.com/BerriAI/litellm/pull/17959" target="_blank" rel="noopener noreferrer">PR #17959</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/bedrock">Bedrock Guardrails</a></strong>
<ul>
<li>Add support for Bedrock image guardrails - <a href="https://github.com/BerriAI/litellm/pull/18115" target="_blank" rel="noopener noreferrer">PR #18115</a></li>
<li>Guardrails block action takes precedence over masking - <a href="https://github.com/BerriAI/litellm/pull/17968" target="_blank" rel="noopener noreferrer">PR #17968</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="secret-managers">Secret Managers<a href="#secret-managers" class="hash-link" aria-label="Secret Managers" title="Secret Managers"></a></h3>
<ul>
<li><strong><a href="/docs/secret_managers/hashicorp_vault">HashiCorp Vault</a></strong>
<ul>
<li>Add documentation for configurable Vault mount - <a href="https://github.com/BerriAI/litellm/pull/18082" target="_blank" rel="noopener noreferrer">PR #18082</a></li>
<li>Add per-team Vault configuration - <a href="https://github.com/BerriAI/litellm/pull/18150" target="_blank" rel="noopener noreferrer">PR #18150</a></li>
</ul>
</li>
<li><strong>UI</strong>
<ul>
<li>Add secret manager settings controls to team management UI - <a href="https://github.com/BerriAI/litellm/pull/18149" target="_blank" rel="noopener noreferrer">PR #18149</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-budgets-and-rate-limiting">Spend Tracking, Budgets and Rate Limiting<a href="#spend-tracking-budgets-and-rate-limiting" class="hash-link" aria-label="Spend Tracking, Budgets and Rate Limiting" title="Spend Tracking, Budgets and Rate Limiting"></a></h2>
<ul>
<li><strong>Email Budget Alerts</strong> - Send email notifications when budgets are reached - <a href="https://github.com/BerriAI/litellm/pull/17995" target="_blank" rel="noopener noreferrer">PR #17995</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway">MCP Gateway<a href="#mcp-gateway" class="hash-link" aria-label="MCP Gateway" title="MCP Gateway"></a></h2>
<ul>
<li><strong>Auth Header Propagation</strong> - Add MCP auth header propagation - <a href="https://github.com/BerriAI/litellm/pull/17963" target="_blank" rel="noopener noreferrer">PR #17963</a></li>
<li><strong>Fix deepcopy error</strong> - Fix MCP tool call deepcopy error when processing requests - <a href="https://github.com/BerriAI/litellm/pull/18010" target="_blank" rel="noopener noreferrer">PR #18010</a></li>
<li><strong>Fix list tool</strong> - Fix MCP list_tools not working without database connection - <a href="https://github.com/BerriAI/litellm/pull/18161" target="_blank" rel="noopener noreferrer">PR #18161</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="agent-gateway-a2a">Agent Gateway (A2A)<a href="#agent-gateway-a2a" class="hash-link" aria-label="Agent Gateway (A2A)" title="Agent Gateway (A2A)"></a></h2>
<ul>
<li><strong>New Provider: Agent Gateway</strong> - Add pydantic ai agents support - <a href="https://github.com/BerriAI/litellm/pull/18013" target="_blank" rel="noopener noreferrer">PR #18013</a></li>
<li><strong>VertexAI Agent Engine</strong> - Add Vertex AI Agent Engine provider - <a href="https://github.com/BerriAI/litellm/pull/18014" target="_blank" rel="noopener noreferrer">PR #18014</a></li>
<li><strong>Fix model extraction</strong> - Fix get_model_from_request() to extract model ID from Vertex AI passthrough URLs - <a href="https://github.com/BerriAI/litellm/pull/18097" target="_blank" rel="noopener noreferrer">PR #18097</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<ul>
<li><strong>Lazy Imports</strong> - Use per-attribute lazy imports and extract shared constants - <a href="https://github.com/BerriAI/litellm/pull/17994" target="_blank" rel="noopener noreferrer">PR #17994</a></li>
<li><strong>Lazy Load HTTP Handlers</strong> - Lazy load http handlers - <a href="https://github.com/BerriAI/litellm/pull/17997" target="_blank" rel="noopener noreferrer">PR #17997</a></li>
<li><strong>Lazy Load Caches</strong> - Lazy load caches - <a href="https://github.com/BerriAI/litellm/pull/18001" target="_blank" rel="noopener noreferrer">PR #18001</a></li>
<li><strong>Lazy Load Types</strong> - Lazy load bedrock types, .types.utils, GuardrailItem - <a href="https://github.com/BerriAI/litellm/pull/18053" target="_blank" rel="noopener noreferrer">PR #18053</a>, <a href="https://github.com/BerriAI/litellm/pull/18054" target="_blank" rel="noopener noreferrer">PR #18054</a>, <a href="https://github.com/BerriAI/litellm/pull/18072" target="_blank" rel="noopener noreferrer">PR #18072</a></li>
<li><strong>Lazy Load Configs</strong> - Lazy load 41 configuration classes - <a href="https://github.com/BerriAI/litellm/pull/18267" target="_blank" rel="noopener noreferrer">PR #18267</a></li>
<li><strong>Lazy Load Client Decorators</strong> - Lazy load heavy client decorator imports - <a href="https://github.com/BerriAI/litellm/pull/18064" target="_blank" rel="noopener noreferrer">PR #18064</a></li>
<li><strong>Prisma Build Time</strong> - Download Prisma binaries at build time instead of runtime for security restricted environments - <a href="https://github.com/BerriAI/litellm/pull/17695" target="_blank" rel="noopener noreferrer">PR #17695</a></li>
<li><strong>Docker Alpine</strong> - Add libsndfile to Alpine image for ARM64 audio processing - <a href="https://github.com/BerriAI/litellm/pull/18092" target="_blank" rel="noopener noreferrer">PR #18092</a></li>
<li><strong>Security</strong> - Prevent LiteLLM API key leakage on /health endpoint failures - <a href="https://github.com/BerriAI/litellm/pull/18133" target="_blank" rel="noopener noreferrer">PR #18133</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="documentation-updates">Documentation Updates<a href="#documentation-updates" class="hash-link" aria-label="Documentation Updates" title="Documentation Updates"></a></h2>
<ul>
<li><strong>SAP Docs</strong> - Update SAP documentation - <a href="https://github.com/BerriAI/litellm/pull/17974" target="_blank" rel="noopener noreferrer">PR #17974</a></li>
<li><strong>Pydantic AI Agents</strong> - Add docs on using pydantic ai agents with LiteLLM A2A gateway - <a href="https://github.com/BerriAI/litellm/pull/18026" target="_blank" rel="noopener noreferrer">PR #18026</a></li>
<li><strong>Vertex AI Agent Engine</strong> - Add Vertex AI Agent Engine documentation - <a href="https://github.com/BerriAI/litellm/pull/18027" target="_blank" rel="noopener noreferrer">PR #18027</a></li>
<li><strong>Router Order</strong> - Add router order parameter documentation - <a href="https://github.com/BerriAI/litellm/pull/18045" target="_blank" rel="noopener noreferrer">PR #18045</a></li>
<li><strong>Secret Manager Settings</strong> - Improve secret manager settings documentation - <a href="https://github.com/BerriAI/litellm/pull/18235" target="_blank" rel="noopener noreferrer">PR #18235</a></li>
<li><strong>Gemini 3 Flash</strong> - Add version requirement in Gemini 3 Flash blog - <a href="https://github.com/BerriAI/litellm/pull/18227" target="_blank" rel="noopener noreferrer">PR #18227</a></li>
<li><strong>README</strong> - Expand Responses API section and update endpoints - <a href="https://github.com/BerriAI/litellm/pull/17354" target="_blank" rel="noopener noreferrer">PR #17354</a></li>
<li><strong>Amazon Nova</strong> - Add Amazon Nova to sidebar and supported models - <a href="https://github.com/BerriAI/litellm/pull/18220" target="_blank" rel="noopener noreferrer">PR #18220</a></li>
<li><strong>Benchmarks</strong> - Add infrastructure recommendations to benchmarks documentation - <a href="https://github.com/BerriAI/litellm/pull/18264" target="_blank" rel="noopener noreferrer">PR #18264</a></li>
<li><strong>Broken Links</strong> - Fix broken link corrections - <a href="https://github.com/BerriAI/litellm/pull/18104" target="_blank" rel="noopener noreferrer">PR #18104</a></li>
<li><strong>README Fixes</strong> - Various README improvements - <a href="https://github.com/BerriAI/litellm/pull/18206" target="_blank" rel="noopener noreferrer">PR #18206</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="infrastructure--cicd">Infrastructure / CI/CD<a href="#infrastructure--cicd" class="hash-link" aria-label="Infrastructure / CI/CD" title="Infrastructure / CI/CD"></a></h2>
<ul>
<li><strong>PR Templates</strong> - Add LiteLLM team PR template and CI/CD rules - <a href="https://github.com/BerriAI/litellm/pull/17983" target="_blank" rel="noopener noreferrer">PR #17983</a>, <a href="https://github.com/BerriAI/litellm/pull/17985" target="_blank" rel="noopener noreferrer">PR #17985</a></li>
<li><strong>Issue Labeling</strong> - Improve issue labeling with component dropdown and more provider keywords - <a href="https://github.com/BerriAI/litellm/pull/17957" target="_blank" rel="noopener noreferrer">PR #17957</a></li>
<li><strong>PR Template Cleanup</strong> - Remove redundant fields from PR template - <a href="https://github.com/BerriAI/litellm/pull/17956" target="_blank" rel="noopener noreferrer">PR #17956</a></li>
<li><strong>Dependencies</strong> - Bump altcha-lib from 1.3.0 to 1.4.1 - <a href="https://github.com/BerriAI/litellm/pull/18017" target="_blank" rel="noopener noreferrer">PR #18017</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@dongbin-lunark made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17757" target="_blank" rel="noopener noreferrer">PR #17757</a></li>
<li>@qdrddr made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/18004" target="_blank" rel="noopener noreferrer">PR #18004</a></li>
<li>@donicrosby made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17962" target="_blank" rel="noopener noreferrer">PR #17962</a></li>
<li>@NicolaivdSmagt made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17992" target="_blank" rel="noopener noreferrer">PR #17992</a></li>
<li>@Reapor-Yurnero made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/18085" target="_blank" rel="noopener noreferrer">PR #18085</a></li>
<li>@jk-f5 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/18086" target="_blank" rel="noopener noreferrer">PR #18086</a></li>
<li>@castrapel made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/18077" target="_blank" rel="noopener noreferrer">PR #18077</a></li>
<li>@dtikhonov made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17484" target="_blank" rel="noopener noreferrer">PR #17484</a></li>
<li>@opleonnn made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/18175" target="_blank" rel="noopener noreferrer">PR #18175</a></li>
<li>@eurogig made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/18084" target="_blank" rel="noopener noreferrer">PR #18084</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog">Full Changelog<a href="#full-changelog" class="hash-link" aria-label="Full Changelog" title="Full Changelog"></a></h2>
<p><strong><a href="https://github.com/BerriAI/litellm/compare/v1.80.10-nightly...v1.80.11" target="_blank" rel="noopener noreferrer">View complete changelog on GitHub</a></strong></p></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-80-10">[Preview] v1.80.10.rc.1 - Agent Gateway: Azure Foundry &amp; Bedrock AgentCore</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-12-13T10:00:00.000Z">20251213</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaff"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaff</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.80.10.rc.1</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.80.10</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>Agent (A2A) Gateway with Cost Tracking</strong> - <a href="/docs/a2a_cost_tracking">Track agent costs per query, per token pricing, and view agent usage in the dashboard</a></li>
<li><strong>2 New Agent Providers</strong> - <a href="/docs/providers/langgraph">LangGraph Agents</a> and <a href="/docs/providers/azure_ai_agents">Azure AI Foundry Agents</a> for agentic workflows</li>
<li><strong>New Provider: SAP Gen AI Hub</strong> - <a href="/docs/providers/sap">Full support for SAP Generative AI Hub with chat completions</a></li>
<li><strong>New Bedrock Writer Models</strong> - Add Palmyra-X4 and Palmyra-X5 models on Bedrock</li>
<li><strong>OpenAI GPT-5.2 Models</strong> - Full support for GPT-5.2, GPT-5.2-pro, and Azure GPT-5.2 models with reasoning support</li>
<li><strong>227 New Fireworks AI Models</strong> - Comprehensive model coverage for Fireworks AI platform</li>
<li><strong>MCP Support on /chat/completions</strong> - <a href="/docs/mcp">Use MCP servers directly via chat completions endpoint</a></li>
<li><strong>Performance Improvements</strong> - Reduced memory leaks by 50%</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-gateway---4-new-agent-providers">Agent Gateway - 4 New Agent Providers<a href="#agent-gateway---4-new-agent-providers" class="hash-link" aria-label="Agent Gateway - 4 New Agent Providers" title="Agent Gateway - 4 New Agent Providers"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAtklEQVR4nAXB207CMACA4d6qiDpY18UVx1h3YCsnIdoQ5iRxMdGohEC88v3f4vf7hK9n+NGMS8+QFI5y0VKt38nXXwzyT2R9JqiPiCvPcOGl+HpBZhvi0jGavhCvftBPfwT2F1mfEP1wTk/NUaljbFuUcSizJaw6otWBYfGBZzrEXdxwO265L/ckyzeCokFmO3yzRaaOG/1MP9oghqZjUH4TZq8k+SN6siR8sMioQmrLtarpySn/dqVJj/uoxrMAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="331"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/a2a_gateway2.85fac86.640.png" srcset="/assets/ideal-img/a2a_gateway2.85fac86.640.png 640w,/assets/ideal-img/a2a_gateway2.59b6608.1920.png 1920w" width="640" height="331"></noscript></div>
<br>
<p>This release adds support for agents from the following providers:</p>
<ul>
<li><strong>LangGraph Agents</strong> - Deploy and manage LangGraph-based agents</li>
<li><strong>Azure AI Foundry Agents</strong> - Enterprise agent deployments on Azure</li>
<li><strong>Bedrock AgentCore</strong> - AWS Bedrock agent integration</li>
<li><strong>A2A Agents</strong> - Agent-to-Agent protocol support</li>
</ul>
<p>AI Gateway admins can now add agents from any of these providers, and developers can invoke them through a unified interface using the A2A protocol.</p>
<p>For all agent requests running through the AI Gateway, LiteLLM automatically tracks request/response logs, cost, and token usage.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-a2a-usage-ui">Agent (A2A) Usage UI<a href="#agent-a2a-usage-ui" class="hash-link" aria-label="Agent (A2A) Usage UI" title="Agent (A2A) Usage UI"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAi0lEQVR4nDWNUQrCQAxE9/7441U8gMdQ6ZetUvRnW6htN8lunmy1Aw8CM5MJMUYq0zQhIn8U1R8pCapCaNuWpmnougfzvJBqwAruTilOzsYwKMHM2FmWFc95M+u3YhlVQyQT9rkkAl64fxLXcQZ3Ui548e0OtSmqeDbeq3K89RwuPadn5PwaqVqt8AWD4MDKrwIwXAAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/agent_usage.fe62c3e.640.png" srcset="/assets/ideal-img/agent_usage.fe62c3e.640.png 640w,/assets/ideal-img/agent_usage.186e6c2.1920.png 1920w" width="640" height="334"></noscript></div>
<p>Users can now filter usage statistics by agents, providing the same granular filtering capabilities available for teams, organizations, and customers.</p>
<p><strong>Details:</strong></p>
<ul>
<li>Filter usage analytics, spend logs, and activity metrics by agent ID</li>
<li>View breakdowns on a per-agent basis</li>
<li>Consistent filtering experience across all usage and analytics views</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-providers-and-endpoints">New Providers and Endpoints<a href="#new-providers-and-endpoints" class="hash-link" aria-label="New Providers and Endpoints" title="New Providers and Endpoints"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="new-providers-5-new-providers">New Providers (5 new providers)<a href="#new-providers-5-new-providers" class="hash-link" aria-label="New Providers (5 new providers)" title="New Providers (5 new providers)"></a></h3>
<table><thead><tr><th>Provider</th><th>Supported LiteLLM Endpoints</th><th>Description</th></tr></thead><tbody><tr><td><a href="/docs/providers/sap">SAP Gen AI Hub</a></td><td><code>/chat/completions</code>, <code>/messages</code>, <code>/responses</code></td><td>SAP Generative AI Hub integration for enterprise AI</td></tr><tr><td><a href="/docs/providers/langgraph">LangGraph</a></td><td><code>/chat/completions</code>, <code>/messages</code>, <code>/responses</code>, <code>/a2a</code></td><td>LangGraph agents for agentic workflows</td></tr><tr><td><a href="/docs/providers/azure_ai_agents">Azure AI Foundry Agents</a></td><td><code>/chat/completions</code>, <code>/messages</code>, <code>/responses</code>, <code>/a2a</code></td><td>Azure AI Foundry Agents for enterprise agent deployments</td></tr><tr><td><a href="/docs/providers/voyage">Voyage AI Rerank</a></td><td><code>/rerank</code></td><td>Voyage AI rerank models support</td></tr><tr><td><a href="/docs/providers/fireworks_ai">Fireworks AI Rerank</a></td><td><code>/rerank</code></td><td>Fireworks AI rerank endpoint support</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="new-llm-api-endpoints-4-new-endpoints">New LLM API Endpoints (4 new endpoints)<a href="#new-llm-api-endpoints-4-new-endpoints" class="hash-link" aria-label="New LLM API Endpoints (4 new endpoints)" title="New LLM API Endpoints (4 new endpoints)"></a></h3>
<table><thead><tr><th>Endpoint</th><th>Method</th><th>Description</th><th>Documentation</th></tr></thead><tbody><tr><td><code>/containers/{id}/files</code></td><td>GET</td><td>List files in a container</td><td><a href="/docs/container_files">Docs</a></td></tr><tr><td><code>/containers/{id}/files/{file_id}</code></td><td>GET</td><td>Retrieve container file metadata</td><td><a href="/docs/container_files">Docs</a></td></tr><tr><td><code>/containers/{id}/files/{file_id}</code></td><td>DELETE</td><td>Delete a file from a container</td><td><a href="/docs/container_files">Docs</a></td></tr><tr><td><code>/containers/{id}/files/{file_id}/content</code></td><td>GET</td><td>Retrieve container file content</td><td><a href="/docs/container_files">Docs</a></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-model-support-270-new-models">New Model Support (270+ new models)<a href="#new-model-support-270-new-models" class="hash-link" aria-label="New Model Support (270+ new models)" title="New Model Support (270+ new models)"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Features</th></tr></thead><tbody><tr><td>OpenAI</td><td><code>gpt-5.2</code></td><td>400K</td><td>$1.75</td><td>$14.00</td><td>Reasoning, vision, PDF, caching</td></tr><tr><td>OpenAI</td><td><code>gpt-5.2-pro</code></td><td>400K</td><td>$21.00</td><td>$168.00</td><td>Reasoning, web search, vision</td></tr><tr><td>Azure</td><td><code>azure/gpt-5.2</code></td><td>400K</td><td>$1.75</td><td>$14.00</td><td>Reasoning, vision, PDF, caching</td></tr><tr><td>Azure</td><td><code>azure/gpt-5.2-pro</code></td><td>400K</td><td>$21.00</td><td>$168.00</td><td>Reasoning, web search</td></tr><tr><td>Bedrock</td><td><code>us.writer.palmyra-x4-v1:0</code></td><td>128K</td><td>$2.50</td><td>$10.00</td><td>Function calling, PDF input</td></tr><tr><td>Bedrock</td><td><code>us.writer.palmyra-x5-v1:0</code></td><td>1M</td><td>$0.60</td><td>$6.00</td><td>Function calling, PDF input</td></tr><tr><td>Bedrock</td><td><code>eu.anthropic.claude-opus-4-5-20251101-v1:0</code></td><td>200K</td><td>$5.00</td><td>$25.00</td><td>Reasoning, computer use, vision</td></tr><tr><td>Bedrock</td><td><code>google.gemma-3-12b-it</code></td><td>128K</td><td>$0.10</td><td>$0.30</td><td>Audio input</td></tr><tr><td>Bedrock</td><td><code>moonshot.kimi-k2-thinking</code></td><td>128K</td><td>$0.60</td><td>$2.50</td><td>Reasoning</td></tr><tr><td>Bedrock</td><td><code>nvidia.nemotron-nano-12b-v2</code></td><td>128K</td><td>$0.20</td><td>$0.60</td><td>Vision</td></tr><tr><td>Bedrock</td><td><code>qwen.qwen3-next-80b-a3b</code></td><td>128K</td><td>$0.15</td><td>$1.20</td><td>Function calling</td></tr><tr><td>Vertex AI</td><td><code>vertex_ai/deepseek-ai/deepseek-v3.2-maas</code></td><td>164K</td><td>$0.56</td><td>$1.68</td><td>Reasoning, caching</td></tr><tr><td>Mistral</td><td><code>mistral/codestral-2508</code></td><td>256K</td><td>$0.30</td><td>$0.90</td><td>Function calling</td></tr><tr><td>Mistral</td><td><code>mistral/devstral-2512</code></td><td>256K</td><td>$0.40</td><td>$2.00</td><td>Function calling</td></tr><tr><td>Mistral</td><td><code>mistral/labs-devstral-small-2512</code></td><td>256K</td><td>$0.10</td><td>$0.30</td><td>Function calling</td></tr><tr><td>Cerebras</td><td><code>cerebras/zai-glm-4.6</code></td><td>128K</td><td>-</td><td>-</td><td>Chat completions</td></tr><tr><td>NVIDIA NIM</td><td><code>nvidia_nim/ranking/nvidia/llama-3.2-nv-rerankqa-1b-v2</code></td><td>-</td><td>Free</td><td>Free</td><td>Rerank</td></tr><tr><td>Voyage</td><td><code>voyage/rerank-2.5</code></td><td>32K</td><td>$0.05/1K tokens</td><td>-</td><td>Rerank</td></tr><tr><td>Fireworks AI</td><td>227 new models</td><td>Various</td><td>Various</td><td>Various</td><td>Full model catalog</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/openai">OpenAI</a></strong>
<ul>
<li>Add support for OpenAI GPT-5.2 models with reasoning_effort=&#x27;xhigh&#x27; - <a href="https://github.com/BerriAI/litellm/pull/17836" target="_blank" rel="noopener noreferrer">PR #17836</a>, <a href="https://github.com/BerriAI/litellm/pull/17875" target="_blank" rel="noopener noreferrer">PR #17875</a></li>
<li>Include &#x27;user&#x27; param for responses API models - <a href="https://github.com/BerriAI/litellm/pull/17648" target="_blank" rel="noopener noreferrer">PR #17648</a></li>
<li>Use optimized async http client for text completions - <a href="https://github.com/BerriAI/litellm/pull/17831" target="_blank" rel="noopener noreferrer">PR #17831</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure">Azure</a></strong>
<ul>
<li>Add Azure GPT-5.2 models support - <a href="https://github.com/BerriAI/litellm/pull/17866" target="_blank" rel="noopener noreferrer">PR #17866</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure_ai">Azure AI</a></strong>
<ul>
<li>Fix Azure AI Anthropic api-key header and passthrough cost calculation - <a href="https://github.com/BerriAI/litellm/pull/17656" target="_blank" rel="noopener noreferrer">PR #17656</a></li>
<li>Remove unsupported params from Azure AI Anthropic requests - <a href="https://github.com/BerriAI/litellm/pull/17822" target="_blank" rel="noopener noreferrer">PR #17822</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Prevent duplicate tool_result blocks with same tool - <a href="https://github.com/BerriAI/litellm/pull/17632" target="_blank" rel="noopener noreferrer">PR #17632</a></li>
<li>Handle partial JSON chunks in streaming responses - <a href="https://github.com/BerriAI/litellm/pull/17493" target="_blank" rel="noopener noreferrer">PR #17493</a></li>
<li>Preserve server_tool_use and web_search_tool_result in multi-turn conversations - <a href="https://github.com/BerriAI/litellm/pull/17746" target="_blank" rel="noopener noreferrer">PR #17746</a></li>
<li>Capture web_search_tool_result in streaming for multi-turn conversations - <a href="https://github.com/BerriAI/litellm/pull/17798" target="_blank" rel="noopener noreferrer">PR #17798</a></li>
<li>Add retrieve batches and retrieve file content support - <a href="https://github.com/BerriAI/litellm/pull/17700" target="_blank" rel="noopener noreferrer">PR #17700</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>
<ul>
<li>Add new Bedrock OSS models to model list - <a href="https://github.com/BerriAI/litellm/pull/17638" target="_blank" rel="noopener noreferrer">PR #17638</a></li>
<li>Add Bedrock Writer models (Palmyra-X4, Palmyra-X5) - <a href="https://github.com/BerriAI/litellm/pull/17685" target="_blank" rel="noopener noreferrer">PR #17685</a></li>
<li>Add EU Claude Opus 4.5 model - <a href="https://github.com/BerriAI/litellm/pull/17897" target="_blank" rel="noopener noreferrer">PR #17897</a></li>
<li>Add serviceTier support for Converse API - <a href="https://github.com/BerriAI/litellm/pull/17810" target="_blank" rel="noopener noreferrer">PR #17810</a></li>
<li>Fix header forwarding with custom API for Bedrock embeddings - <a href="https://github.com/BerriAI/litellm/pull/17872" target="_blank" rel="noopener noreferrer">PR #17872</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/gemini">Gemini</a></strong>
<ul>
<li>Add support for computer use for Gemini - <a href="https://github.com/BerriAI/litellm/pull/17756" target="_blank" rel="noopener noreferrer">PR #17756</a></li>
<li>Handle context window errors - <a href="https://github.com/BerriAI/litellm/pull/17751" target="_blank" rel="noopener noreferrer">PR #17751</a></li>
<li>Add speechConfig to GenerationConfig for Gemini TTS - <a href="https://github.com/BerriAI/litellm/pull/17851" target="_blank" rel="noopener noreferrer">PR #17851</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex">Vertex AI</a></strong>
<ul>
<li>Add DeepSeek-V3.2 model support - <a href="https://github.com/BerriAI/litellm/pull/17770" target="_blank" rel="noopener noreferrer">PR #17770</a></li>
<li>Preserve systemInstructions for generate content request - <a href="https://github.com/BerriAI/litellm/pull/17803" target="_blank" rel="noopener noreferrer">PR #17803</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/mistral">Mistral</a></strong>
<ul>
<li>Add Codestral 2508, Devstral 2512 models - <a href="https://github.com/BerriAI/litellm/pull/17801" target="_blank" rel="noopener noreferrer">PR #17801</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/cerebras">Cerebras</a></strong>
<ul>
<li>Add zai-glm-4.6 model support - <a href="https://github.com/BerriAI/litellm/pull/17683" target="_blank" rel="noopener noreferrer">PR #17683</a></li>
<li>Fix context window errors not recognized - <a href="https://github.com/BerriAI/litellm/pull/17587" target="_blank" rel="noopener noreferrer">PR #17587</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/deepseek">DeepSeek</a></strong>
<ul>
<li>Add native support for thinking and reasoning_effort params - <a href="https://github.com/BerriAI/litellm/pull/17712" target="_blank" rel="noopener noreferrer">PR #17712</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/nvidia_nim_rerank">NVIDIA NIM Rerank</a></strong>
<ul>
<li>Add llama-3.2-nv-rerankqa-1b-v2 rerank model - <a href="https://github.com/BerriAI/litellm/pull/17670" target="_blank" rel="noopener noreferrer">PR #17670</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/fireworks_ai">Fireworks AI</a></strong>
<ul>
<li>Add 227 new Fireworks AI models - <a href="https://github.com/BerriAI/litellm/pull/17692" target="_blank" rel="noopener noreferrer">PR #17692</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/dashscope">Dashscope</a></strong>
<ul>
<li>Fix default base_url error - <a href="https://github.com/BerriAI/litellm/pull/17584" target="_blank" rel="noopener noreferrer">PR #17584</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes">Bug Fixes<a href="#bug-fixes" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h3>
<ul>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Fix missing content in Anthropic to OpenAI conversion - <a href="https://github.com/BerriAI/litellm/pull/17693" target="_blank" rel="noopener noreferrer">PR #17693</a></li>
<li>Avoid error when we have just the tool_calls in input - <a href="https://github.com/BerriAI/litellm/pull/17753" target="_blank" rel="noopener noreferrer">PR #17753</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure">Azure</a></strong>
<ul>
<li>Fix error about encoding video id for Azure - <a href="https://github.com/BerriAI/litellm/pull/17708" target="_blank" rel="noopener noreferrer">PR #17708</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure_ai">Azure AI</a></strong>
<ul>
<li>Fix LLM provider for azure_ai in model map - <a href="https://github.com/BerriAI/litellm/pull/17805" target="_blank" rel="noopener noreferrer">PR #17805</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/watsonx">Watsonx</a></strong>
<ul>
<li>Fix Watsonx Audio Transcription to only send supported params to API - <a href="https://github.com/BerriAI/litellm/pull/17840" target="_blank" rel="noopener noreferrer">PR #17840</a></li>
</ul>
</li>
<li><strong><a href="/docs/routing">Router</a></strong>
<ul>
<li>Handle tools=None in completion requests - <a href="https://github.com/BerriAI/litellm/pull/17684" target="_blank" rel="noopener noreferrer">PR #17684</a></li>
<li>Add minimum request threshold for error rate cooldown - <a href="https://github.com/BerriAI/litellm/pull/17464" target="_blank" rel="noopener noreferrer">PR #17464</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/response_api">Responses API</a></strong>
<ul>
<li>Add usage details in responses usage object - <a href="https://github.com/BerriAI/litellm/pull/17641" target="_blank" rel="noopener noreferrer">PR #17641</a></li>
<li>Fix error for response API polling - <a href="https://github.com/BerriAI/litellm/pull/17654" target="_blank" rel="noopener noreferrer">PR #17654</a></li>
<li>Fix streaming tool_calls being dropped when text + tool_calls - <a href="https://github.com/BerriAI/litellm/pull/17652" target="_blank" rel="noopener noreferrer">PR #17652</a></li>
<li>Transform image content in tool results for Responses API - <a href="https://github.com/BerriAI/litellm/pull/17799" target="_blank" rel="noopener noreferrer">PR #17799</a></li>
<li>Fix responses api not applying tpm rate limits on api keys - <a href="https://github.com/BerriAI/litellm/pull/17707" target="_blank" rel="noopener noreferrer">PR #17707</a></li>
</ul>
</li>
<li><strong><a href="/docs/containers">Containers API</a></strong>
<ul>
<li>Allow using LIST, Create Containers using custom-llm-provider - <a href="https://github.com/BerriAI/litellm/pull/17740" target="_blank" rel="noopener noreferrer">PR #17740</a></li>
<li>Add new container API file management + UI Interface - <a href="https://github.com/BerriAI/litellm/pull/17745" target="_blank" rel="noopener noreferrer">PR #17745</a></li>
</ul>
</li>
<li><strong><a href="/docs/rerank">Rerank API</a></strong>
<ul>
<li>Add support for forwarding client headers in /rerank endpoint - <a href="https://github.com/BerriAI/litellm/pull/17873" target="_blank" rel="noopener noreferrer">PR #17873</a></li>
</ul>
</li>
<li><strong><a href="/docs/files_endpoints">Files API</a></strong>
<ul>
<li>Add support for expires_after param in Files endpoint - <a href="https://github.com/BerriAI/litellm/pull/17860" target="_blank" rel="noopener noreferrer">PR #17860</a></li>
</ul>
</li>
<li><strong><a href="/docs/videos">Video API</a></strong>
<ul>
<li>Use litellm params for all videos APIs - <a href="https://github.com/BerriAI/litellm/pull/17732" target="_blank" rel="noopener noreferrer">PR #17732</a></li>
<li>Respect videos content db creds - <a href="https://github.com/BerriAI/litellm/pull/17771" target="_blank" rel="noopener noreferrer">PR #17771</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/embedding">Embeddings API</a></strong>
<ul>
<li>Fix handling token array input decoding for embeddings - <a href="https://github.com/BerriAI/litellm/pull/17468" target="_blank" rel="noopener noreferrer">PR #17468</a></li>
</ul>
</li>
<li><strong><a href="/docs/completion/input">Chat Completions API</a></strong>
<ul>
<li>Add v0 target storage support - store files in Azure AI storage and use with chat completions API - <a href="https://github.com/BerriAI/litellm/pull/17758" target="_blank" rel="noopener noreferrer">PR #17758</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/gemini">generateContent API</a></strong>
<ul>
<li>Support model names with slashes on Gemini generateContent endpoints - <a href="https://github.com/BerriAI/litellm/pull/17743" target="_blank" rel="noopener noreferrer">PR #17743</a></li>
</ul>
</li>
<li><strong>General</strong>
<ul>
<li>Use audio content for caching - <a href="https://github.com/BerriAI/litellm/pull/17651" target="_blank" rel="noopener noreferrer">PR #17651</a></li>
<li>Return 403 exception when calling GET responses API - <a href="https://github.com/BerriAI/litellm/pull/17629" target="_blank" rel="noopener noreferrer">PR #17629</a></li>
<li>Add nested field removal support to additional_drop_params - <a href="https://github.com/BerriAI/litellm/pull/17711" target="_blank" rel="noopener noreferrer">PR #17711</a></li>
<li>Async post_call_streaming_iterator_hook now properly iterates async generators - <a href="https://github.com/BerriAI/litellm/pull/17626" target="_blank" rel="noopener noreferrer">PR #17626</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>General</strong>
<ul>
<li>Fix handle string content in is_cached_message - <a href="https://github.com/BerriAI/litellm/pull/17853" target="_blank" rel="noopener noreferrer">PR #17853</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>UI Settings</strong>
<ul>
<li>Add Get and Update Backend Routes for UI Settings - <a href="https://github.com/BerriAI/litellm/pull/17689" target="_blank" rel="noopener noreferrer">PR #17689</a></li>
<li>UI Settings page implementation - <a href="https://github.com/BerriAI/litellm/pull/17697" target="_blank" rel="noopener noreferrer">PR #17697</a></li>
<li>Ensure Model Page honors UI Settings - <a href="https://github.com/BerriAI/litellm/pull/17804" target="_blank" rel="noopener noreferrer">PR #17804</a></li>
<li>Add All Proxy Models to Default User Settings - <a href="https://github.com/BerriAI/litellm/pull/17902" target="_blank" rel="noopener noreferrer">PR #17902</a></li>
</ul>
</li>
<li><strong>Agent &amp; Usage UI</strong>
<ul>
<li>Daily Agent Usage Backend - <a href="https://github.com/BerriAI/litellm/pull/17781" target="_blank" rel="noopener noreferrer">PR #17781</a></li>
<li>Agent Usage UI - <a href="https://github.com/BerriAI/litellm/pull/17797" target="_blank" rel="noopener noreferrer">PR #17797</a></li>
<li>Add agent cost tracking on UI - <a href="https://github.com/BerriAI/litellm/pull/17899" target="_blank" rel="noopener noreferrer">PR #17899</a></li>
<li>New Badge for Agent Usage - <a href="https://github.com/BerriAI/litellm/pull/17883" target="_blank" rel="noopener noreferrer">PR #17883</a></li>
<li>Usage Entity labels for filtering - <a href="https://github.com/BerriAI/litellm/pull/17896" target="_blank" rel="noopener noreferrer">PR #17896</a></li>
<li>Agent Usage Page minor fixes - <a href="https://github.com/BerriAI/litellm/pull/17901" target="_blank" rel="noopener noreferrer">PR #17901</a></li>
<li>Usage Page View Select component - <a href="https://github.com/BerriAI/litellm/pull/17854" target="_blank" rel="noopener noreferrer">PR #17854</a></li>
<li>Usage Page Components refactor - <a href="https://github.com/BerriAI/litellm/pull/17848" target="_blank" rel="noopener noreferrer">PR #17848</a></li>
</ul>
</li>
<li><strong>Logs &amp; Spend</strong>
<ul>
<li>Enhanced spend analytics in logs view - <a href="https://github.com/BerriAI/litellm/pull/17623" target="_blank" rel="noopener noreferrer">PR #17623</a></li>
<li>Add user info delete modal for user management - <a href="https://github.com/BerriAI/litellm/pull/17625" target="_blank" rel="noopener noreferrer">PR #17625</a></li>
<li>Show request and response details in logs view - <a href="https://github.com/BerriAI/litellm/pull/17928" target="_blank" rel="noopener noreferrer">PR #17928</a></li>
</ul>
</li>
<li><strong>Virtual Keys</strong>
<ul>
<li>Fix x-litellm-key-spend header update - <a href="https://github.com/BerriAI/litellm/pull/17864" target="_blank" rel="noopener noreferrer">PR #17864</a></li>
</ul>
</li>
<li><strong>Models &amp; Endpoints</strong>
<ul>
<li>Model Hub Useful Links Rearrange - <a href="https://github.com/BerriAI/litellm/pull/17859" target="_blank" rel="noopener noreferrer">PR #17859</a></li>
<li>Create Team Model Dropdown honors Organization&#x27;s Models - <a href="https://github.com/BerriAI/litellm/pull/17834" target="_blank" rel="noopener noreferrer">PR #17834</a></li>
</ul>
</li>
<li><strong>SSO &amp; Auth</strong>
<ul>
<li>Allow upserting user role when SSO provider role changes - <a href="https://github.com/BerriAI/litellm/pull/17754" target="_blank" rel="noopener noreferrer">PR #17754</a></li>
<li>Allow fetching role from generic SSO provider (Keycloak) - <a href="https://github.com/BerriAI/litellm/pull/17787" target="_blank" rel="noopener noreferrer">PR #17787</a></li>
<li>JWT Auth - allow selecting team_id from request header - <a href="https://github.com/BerriAI/litellm/pull/17884" target="_blank" rel="noopener noreferrer">PR #17884</a></li>
<li>Remove SSO Config Values from Config Table on SSO Update - <a href="https://github.com/BerriAI/litellm/pull/17668" target="_blank" rel="noopener noreferrer">PR #17668</a></li>
</ul>
</li>
<li><strong>Teams</strong>
<ul>
<li>Attach team to org table - <a href="https://github.com/BerriAI/litellm/pull/17832" target="_blank" rel="noopener noreferrer">PR #17832</a></li>
<li>Expose the team alias when authenticating - <a href="https://github.com/BerriAI/litellm/pull/17725" target="_blank" rel="noopener noreferrer">PR #17725</a></li>
</ul>
</li>
<li><strong>MCP Server Management</strong>
<ul>
<li>Add extra_headers and allowed_tools to UpdateMCPServerRequest - <a href="https://github.com/BerriAI/litellm/pull/17940" target="_blank" rel="noopener noreferrer">PR #17940</a></li>
</ul>
</li>
<li><strong>Notifications</strong>
<ul>
<li>Show progress and pause on hover for Notifications - <a href="https://github.com/BerriAI/litellm/pull/17942" target="_blank" rel="noopener noreferrer">PR #17942</a></li>
</ul>
</li>
<li><strong>General</strong>
<ul>
<li>Allow Root Path to Redirect when Docs not on Root Path - <a href="https://github.com/BerriAI/litellm/pull/16843" target="_blank" rel="noopener noreferrer">PR #16843</a></li>
<li>Show UI version number on top left near logo - <a href="https://github.com/BerriAI/litellm/pull/17891" target="_blank" rel="noopener noreferrer">PR #17891</a></li>
<li>Re-organize left navigation with correct categories and agents on root - <a href="https://github.com/BerriAI/litellm/pull/17890" target="_blank" rel="noopener noreferrer">PR #17890</a></li>
<li>UI Playground - allow custom model names in model selector dropdown - <a href="https://github.com/BerriAI/litellm/pull/17892" target="_blank" rel="noopener noreferrer">PR #17892</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>UI Fixes</strong>
<ul>
<li>Fix links + old login page deprecation message - <a href="https://github.com/BerriAI/litellm/pull/17624" target="_blank" rel="noopener noreferrer">PR #17624</a></li>
<li>Filtering for Chat UI Endpoint Selector - <a href="https://github.com/BerriAI/litellm/pull/17567" target="_blank" rel="noopener noreferrer">PR #17567</a></li>
<li>Race Condition Handling in SCIM v2 - <a href="https://github.com/BerriAI/litellm/pull/17513" target="_blank" rel="noopener noreferrer">PR #17513</a></li>
<li>Make /litellm_model_cost_map public - <a href="https://github.com/BerriAI/litellm/pull/16795" target="_blank" rel="noopener noreferrer">PR #16795</a></li>
<li>Custom Callback on UI - <a href="https://github.com/BerriAI/litellm/pull/17522" target="_blank" rel="noopener noreferrer">PR #17522</a></li>
<li>Add User Writable Directory to Non Root Docker for Logo - <a href="https://github.com/BerriAI/litellm/pull/17180" target="_blank" rel="noopener noreferrer">PR #17180</a></li>
<li>Swap URL Input and Display Name inputs - <a href="https://github.com/BerriAI/litellm/pull/17682" target="_blank" rel="noopener noreferrer">PR #17682</a></li>
<li>Change deprecation banner to only show on /sso/key/generate - <a href="https://github.com/BerriAI/litellm/pull/17681" target="_blank" rel="noopener noreferrer">PR #17681</a></li>
<li>Change credential encryption to only affect db credentials - <a href="https://github.com/BerriAI/litellm/pull/17741" target="_blank" rel="noopener noreferrer">PR #17741</a></li>
</ul>
</li>
<li><strong>Auth &amp; Routes</strong>
<ul>
<li>Return 403 instead of 503 for unauthorized routes - <a href="https://github.com/BerriAI/litellm/pull/17723" target="_blank" rel="noopener noreferrer">PR #17723</a></li>
<li>AI Gateway Auth - allow using wildcard patterns for public routes - <a href="https://github.com/BerriAI/litellm/pull/17686" target="_blank" rel="noopener noreferrer">PR #17686</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ai-integrations">AI Integrations<a href="#ai-integrations" class="hash-link" aria-label="AI Integrations" title="AI Integrations"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="new-integrations-4-new-integrations">New Integrations (4 new integrations)<a href="#new-integrations-4-new-integrations" class="hash-link" aria-label="New Integrations (4 new integrations)" title="New Integrations (4 new integrations)"></a></h3>
<table><thead><tr><th>Integration</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><a href="/docs/proxy/logging#sumologic">SumoLogic</a></td><td>Logging</td><td>Native webhook integration for SumoLogic - <a href="https://github.com/BerriAI/litellm/pull/17630" target="_blank" rel="noopener noreferrer">PR #17630</a></td></tr><tr><td><a href="/docs/proxy/arize_phoenix_prompts">Arize Phoenix</a></td><td>Prompt Management</td><td>Arize Phoenix OSS prompt management integration - <a href="https://github.com/BerriAI/litellm/pull/17750" target="_blank" rel="noopener noreferrer">PR #17750</a></td></tr><tr><td><a href="/docs/proxy/email">Sendgrid</a></td><td>Email</td><td>Sendgrid email notifications integration - <a href="https://github.com/BerriAI/litellm/pull/17775" target="_blank" rel="noopener noreferrer">PR #17775</a></td></tr><tr><td><a href="/docs/proxy/guardrails/onyx_security">Onyx</a></td><td>Guardrails</td><td>Onyx guardrail hooks integration - <a href="https://github.com/BerriAI/litellm/pull/16591" target="_blank" rel="noopener noreferrer">PR #16591</a></td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="logging">Logging<a href="#logging" class="hash-link" aria-label="Logging" title="Logging"></a></h3>
<ul>
<li><strong><a href="/docs/proxy/logging#langfuse">Langfuse</a></strong>
<ul>
<li>Propagate Langfuse trace_id - <a href="https://github.com/BerriAI/litellm/pull/17669" target="_blank" rel="noopener noreferrer">PR #17669</a></li>
<li>Prefer standard trace id for Langfuse logging - <a href="https://github.com/BerriAI/litellm/pull/17791" target="_blank" rel="noopener noreferrer">PR #17791</a></li>
<li>Move query params to create_pass_through_route call in Langfuse passthrough - <a href="https://github.com/BerriAI/litellm/pull/17660" target="_blank" rel="noopener noreferrer">PR #17660</a></li>
<li>Add support for custom masking function - <a href="https://github.com/BerriAI/litellm/pull/17826" target="_blank" rel="noopener noreferrer">PR #17826</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#prometheus">Prometheus</a></strong>
<ul>
<li>Add &#x27;exception_status&#x27; to prometheus logger - <a href="https://github.com/BerriAI/litellm/pull/17847" target="_blank" rel="noopener noreferrer">PR #17847</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#otel">OpenTelemetry</a></strong>
<ul>
<li>Add latency metrics (TTFT, TPOT, Total Generation Time) to OTEL payload - <a href="https://github.com/BerriAI/litellm/pull/17888" target="_blank" rel="noopener noreferrer">PR #17888</a></li>
</ul>
</li>
<li><strong>General</strong>
<ul>
<li>Add polling via cache feature for async logging - <a href="https://github.com/BerriAI/litellm/pull/16862" target="_blank" rel="noopener noreferrer">PR #16862</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="guardrails">Guardrails<a href="#guardrails" class="hash-link" aria-label="Guardrails" title="Guardrails"></a></h3>
<ul>
<li><strong><a href="/docs/proxy/guardrails/hiddenlayer">HiddenLayer</a></strong>
<ul>
<li>Add HiddenLayer Guardrail Hooks - <a href="https://github.com/BerriAI/litellm/pull/17728" target="_blank" rel="noopener noreferrer">PR #17728</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/pillar_security">Pillar Security</a></strong>
<ul>
<li>Add opt-in evidence results for Pillar Security guardrail during monitoring - <a href="https://github.com/BerriAI/litellm/pull/17812" target="_blank" rel="noopener noreferrer">PR #17812</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/panw_prisma_airs">PANW Prisma AIRS</a></strong>
<ul>
<li>Add configurable fail-open, timeout, and app_user tracking - <a href="https://github.com/BerriAI/litellm/pull/17785" target="_blank" rel="noopener noreferrer">PR #17785</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/pii_masking_v2">Presidio</a></strong>
<ul>
<li>Add support for configurable confidence score thresholds and scope in Presidio PII masking - <a href="https://github.com/BerriAI/litellm/pull/17817" target="_blank" rel="noopener noreferrer">PR #17817</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/litellm_content_filter">LiteLLM Content Filter</a></strong>
<ul>
<li>Mask all regex pattern matches, not just first - <a href="https://github.com/BerriAI/litellm/pull/17727" target="_blank" rel="noopener noreferrer">PR #17727</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/secret_detection">Regex Guardrails</a></strong>
<ul>
<li>Add enhanced regex pattern matching for guardrails - <a href="https://github.com/BerriAI/litellm/pull/17915" target="_blank" rel="noopener noreferrer">PR #17915</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/grayswan">Gray Swan Guardrail</a></strong>
<ul>
<li>Add passthrough mode for model response - <a href="https://github.com/BerriAI/litellm/pull/17102" target="_blank" rel="noopener noreferrer">PR #17102</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="prompt-management">Prompt Management<a href="#prompt-management" class="hash-link" aria-label="Prompt Management" title="Prompt Management"></a></h3>
<ul>
<li><strong>General</strong>
<ul>
<li>New API for integrating prompt management providers - <a href="https://github.com/BerriAI/litellm/pull/17829" target="_blank" rel="noopener noreferrer">PR #17829</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-budgets-and-rate-limiting">Spend Tracking, Budgets and Rate Limiting<a href="#spend-tracking-budgets-and-rate-limiting" class="hash-link" aria-label="Spend Tracking, Budgets and Rate Limiting" title="Spend Tracking, Budgets and Rate Limiting"></a></h2>
<ul>
<li><strong>Service Tier Pricing</strong> - Extract service_tier from response/usage for OpenAI flex pricing - <a href="https://github.com/BerriAI/litellm/pull/17748" target="_blank" rel="noopener noreferrer">PR #17748</a></li>
<li><strong>Agent Cost Tracking</strong> - Track agent_id in SpendLogs - <a href="https://github.com/BerriAI/litellm/pull/17795" target="_blank" rel="noopener noreferrer">PR #17795</a></li>
<li><strong>Tag Activity</strong> - Deduplicate /tag/daily/activity metadata - <a href="https://github.com/BerriAI/litellm/pull/16764" target="_blank" rel="noopener noreferrer">PR #16764</a></li>
<li><strong>Rate Limiting</strong> - Dynamic Rate Limiter - allow specifying ttl for in memory cache - <a href="https://github.com/BerriAI/litellm/pull/17679" target="_blank" rel="noopener noreferrer">PR #17679</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway">MCP Gateway<a href="#mcp-gateway" class="hash-link" aria-label="MCP Gateway" title="MCP Gateway"></a></h2>
<ul>
<li><strong>Chat Completions Integration</strong> - Add support for using MCPs on /chat/completions - <a href="https://github.com/BerriAI/litellm/pull/17747" target="_blank" rel="noopener noreferrer">PR #17747</a></li>
<li><strong>UI Session Permissions</strong> - Fix UI session MCP permissions across real teams - <a href="https://github.com/BerriAI/litellm/pull/17620" target="_blank" rel="noopener noreferrer">PR #17620</a></li>
<li><strong>OAuth Callback</strong> - Fix MCP OAuth callback routing and URL handling - <a href="https://github.com/BerriAI/litellm/pull/17789" target="_blank" rel="noopener noreferrer">PR #17789</a></li>
<li><strong>Tool Name Prefix</strong> - Fix MCP tool name prefix - <a href="https://github.com/BerriAI/litellm/pull/17908" target="_blank" rel="noopener noreferrer">PR #17908</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="agent-gateway-a2a">Agent Gateway (A2A)<a href="#agent-gateway-a2a" class="hash-link" aria-label="Agent Gateway (A2A)" title="Agent Gateway (A2A)"></a></h2>
<ul>
<li><strong>Cost Per Query</strong> - Add cost per query for agent invocations - <a href="https://github.com/BerriAI/litellm/pull/17774" target="_blank" rel="noopener noreferrer">PR #17774</a></li>
<li><strong>Token Counting</strong> - Add token counting non streaming + streaming - <a href="https://github.com/BerriAI/litellm/pull/17779" target="_blank" rel="noopener noreferrer">PR #17779</a></li>
<li><strong>Cost Per Token</strong> - Add cost per token pricing for A2A - <a href="https://github.com/BerriAI/litellm/pull/17780" target="_blank" rel="noopener noreferrer">PR #17780</a></li>
<li><strong>LangGraph Provider</strong> - Add LangGraph provider for Agent Gateway - <a href="https://github.com/BerriAI/litellm/pull/17783" target="_blank" rel="noopener noreferrer">PR #17783</a></li>
<li><strong>Bedrock &amp; LangGraph Agents</strong> - Allow using Bedrock AgentCore, LangGraph agents with A2A Gateway - <a href="https://github.com/BerriAI/litellm/pull/17786" target="_blank" rel="noopener noreferrer">PR #17786</a></li>
<li><strong>Agent Management</strong> - Allow adding LangGraph, Bedrock Agent Core agents - <a href="https://github.com/BerriAI/litellm/pull/17802" target="_blank" rel="noopener noreferrer">PR #17802</a></li>
<li><strong>Azure Foundry Agents</strong> - Add Azure AI Foundry Agents support - <a href="https://github.com/BerriAI/litellm/pull/17845" target="_blank" rel="noopener noreferrer">PR #17845</a></li>
<li><strong>Azure Foundry UI</strong> - Allow adding Azure Foundry Agents on UI - <a href="https://github.com/BerriAI/litellm/pull/17909" target="_blank" rel="noopener noreferrer">PR #17909</a></li>
<li><strong>Azure Foundry Fixes</strong> - Ensure Azure Foundry agents work correctly - <a href="https://github.com/BerriAI/litellm/pull/17943" target="_blank" rel="noopener noreferrer">PR #17943</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<ul>
<li><strong>Memory Leak Fix</strong> - Cut memory leak in half - <a href="https://github.com/BerriAI/litellm/pull/17784" target="_blank" rel="noopener noreferrer">PR #17784</a></li>
<li><strong>Spend Logs Memory</strong> - Reduce memory accumulation of spend_logs - <a href="https://github.com/BerriAI/litellm/pull/17742" target="_blank" rel="noopener noreferrer">PR #17742</a></li>
<li><strong>Router Optimization</strong> - Replace time.perf_counter() with time.time() - <a href="https://github.com/BerriAI/litellm/pull/17881" target="_blank" rel="noopener noreferrer">PR #17881</a></li>
<li><strong>Filter Internal Params</strong> - Filter internal params in fallback code - <a href="https://github.com/BerriAI/litellm/pull/17941" target="_blank" rel="noopener noreferrer">PR #17941</a></li>
<li><strong>Gunicorn Suggestion</strong> - Suggest Gunicorn instead of uvicorn when using max_requests_before_restart - <a href="https://github.com/BerriAI/litellm/pull/17788" target="_blank" rel="noopener noreferrer">PR #17788</a></li>
<li><strong>Pydantic Warnings</strong> - Mitigate PydanticDeprecatedSince20 warnings - <a href="https://github.com/BerriAI/litellm/pull/17657" target="_blank" rel="noopener noreferrer">PR #17657</a></li>
<li><strong>Python 3.14 Support</strong> - Add Python 3.14 support via grpcio version constraints - <a href="https://github.com/BerriAI/litellm/pull/17666" target="_blank" rel="noopener noreferrer">PR #17666</a></li>
<li><strong>OpenAI Package</strong> - Bump openai package to 2.9.0 - <a href="https://github.com/BerriAI/litellm/pull/17818" target="_blank" rel="noopener noreferrer">PR #17818</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="documentation-updates">Documentation Updates<a href="#documentation-updates" class="hash-link" aria-label="Documentation Updates" title="Documentation Updates"></a></h2>
<ul>
<li><strong>Contributing</strong> - Update clone instructions to recommend forking first - <a href="https://github.com/BerriAI/litellm/pull/17637" target="_blank" rel="noopener noreferrer">PR #17637</a></li>
<li><strong>Getting Started</strong> - Improve Getting Started page and SDK documentation structure - <a href="https://github.com/BerriAI/litellm/pull/17614" target="_blank" rel="noopener noreferrer">PR #17614</a></li>
<li><strong>JSON Mode</strong> - Make it clearer how to get Pydantic model output - <a href="https://github.com/BerriAI/litellm/pull/17671" target="_blank" rel="noopener noreferrer">PR #17671</a></li>
<li><strong>drop_params</strong> - Update litellm docs for drop_params - <a href="https://github.com/BerriAI/litellm/pull/17658" target="_blank" rel="noopener noreferrer">PR #17658</a></li>
<li><strong>Environment Variables</strong> - Document missing environment variables and fix incorrect types - <a href="https://github.com/BerriAI/litellm/pull/17649" target="_blank" rel="noopener noreferrer">PR #17649</a></li>
<li><strong>SumoLogic</strong> - Add SumoLogic integration documentation - <a href="https://github.com/BerriAI/litellm/pull/17647" target="_blank" rel="noopener noreferrer">PR #17647</a></li>
<li><strong>SAP Gen AI</strong> - Add SAP Gen AI provider documentation - <a href="https://github.com/BerriAI/litellm/pull/17667" target="_blank" rel="noopener noreferrer">PR #17667</a></li>
<li><strong>Authentication</strong> - Add Note for Authentication - <a href="https://github.com/BerriAI/litellm/pull/17733" target="_blank" rel="noopener noreferrer">PR #17733</a></li>
<li><strong>Known Issues</strong> - Adding known issues to 1.80.5-stable docs - <a href="https://github.com/BerriAI/litellm/pull/17738" target="_blank" rel="noopener noreferrer">PR #17738</a></li>
<li><strong>Supported Endpoints</strong> - Fix Supported Endpoints page - <a href="https://github.com/BerriAI/litellm/pull/17710" target="_blank" rel="noopener noreferrer">PR #17710</a></li>
<li><strong>Token Count</strong> - Document token count endpoint - <a href="https://github.com/BerriAI/litellm/pull/17772" target="_blank" rel="noopener noreferrer">PR #17772</a></li>
<li><strong>Overview</strong> - Made litellm proxy and SDK difference cleaner in overview with a table - <a href="https://github.com/BerriAI/litellm/pull/17790" target="_blank" rel="noopener noreferrer">PR #17790</a></li>
<li><strong>Containers API</strong> - Add docs for containers files API + code interpreter on LiteLLM - <a href="https://github.com/BerriAI/litellm/pull/17749" target="_blank" rel="noopener noreferrer">PR #17749</a></li>
<li><strong>Target Storage</strong> - Add documentation for target storage - <a href="https://github.com/BerriAI/litellm/pull/17882" target="_blank" rel="noopener noreferrer">PR #17882</a></li>
<li><strong>Agent Usage</strong> - Agent Usage documentation - <a href="https://github.com/BerriAI/litellm/pull/17931" target="_blank" rel="noopener noreferrer">PR #17931</a>, <a href="https://github.com/BerriAI/litellm/pull/17932" target="_blank" rel="noopener noreferrer">PR #17932</a>, <a href="https://github.com/BerriAI/litellm/pull/17934" target="_blank" rel="noopener noreferrer">PR #17934</a></li>
<li><strong>Cursor Integration</strong> - Cursor Integration documentation - <a href="https://github.com/BerriAI/litellm/pull/17855" target="_blank" rel="noopener noreferrer">PR #17855</a>, <a href="https://github.com/BerriAI/litellm/pull/17939" target="_blank" rel="noopener noreferrer">PR #17939</a></li>
<li><strong>A2A Cost Tracking</strong> - A2A cost tracking docs - <a href="https://github.com/BerriAI/litellm/pull/17913" target="_blank" rel="noopener noreferrer">PR #17913</a></li>
<li><strong>Azure Search</strong> - Update azure search docs - <a href="https://github.com/BerriAI/litellm/pull/17726" target="_blank" rel="noopener noreferrer">PR #17726</a></li>
<li><strong>Milvus Client</strong> - Fix milvus client docs - <a href="https://github.com/BerriAI/litellm/pull/17736" target="_blank" rel="noopener noreferrer">PR #17736</a></li>
<li><strong>Streaming Logging</strong> - Remove streaming logging doc - <a href="https://github.com/BerriAI/litellm/pull/17739" target="_blank" rel="noopener noreferrer">PR #17739</a></li>
<li><strong>Integration Docs</strong> - Update integration docs location - <a href="https://github.com/BerriAI/litellm/pull/17644" target="_blank" rel="noopener noreferrer">PR #17644</a></li>
<li><strong>Links</strong> - Updated docs links for mistral and anthropic - <a href="https://github.com/BerriAI/litellm/pull/17852" target="_blank" rel="noopener noreferrer">PR #17852</a></li>
<li><strong>Community</strong> - Add community doc link - <a href="https://github.com/BerriAI/litellm/pull/17734" target="_blank" rel="noopener noreferrer">PR #17734</a></li>
<li><strong>Pricing</strong> - Update pricing for global.anthropic.claude-haiku-4-5-20251001-v1:0 - <a href="https://github.com/BerriAI/litellm/pull/17703" target="_blank" rel="noopener noreferrer">PR #17703</a></li>
<li><strong>gpt-image-1-mini</strong> - Correct model type for gpt-image-1-mini - <a href="https://github.com/BerriAI/litellm/pull/17635" target="_blank" rel="noopener noreferrer">PR #17635</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="infrastructure--deployment">Infrastructure / Deployment<a href="#infrastructure--deployment" class="hash-link" aria-label="Infrastructure / Deployment" title="Infrastructure / Deployment"></a></h2>
<ul>
<li><strong>Docker</strong> - Use python instead of wget for healthcheck in docker-compose.yml - <a href="https://github.com/BerriAI/litellm/pull/17646" target="_blank" rel="noopener noreferrer">PR #17646</a></li>
<li><strong>Helm Chart</strong> - Add extraResources support for Helm chart deployments - <a href="https://github.com/BerriAI/litellm/pull/17627" target="_blank" rel="noopener noreferrer">PR #17627</a></li>
<li><strong>Helm Versioning</strong> - Add semver prerelease suffix to helm chart versions - <a href="https://github.com/BerriAI/litellm/pull/17678" target="_blank" rel="noopener noreferrer">PR #17678</a></li>
<li><strong>Database Schema</strong> - Add storage_backend and storage_url columns to schema.prisma for target storage feature - <a href="https://github.com/BerriAI/litellm/pull/17936" target="_blank" rel="noopener noreferrer">PR #17936</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@xianzongxie-stripe made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16862" target="_blank" rel="noopener noreferrer">PR #16862</a></li>
<li>@krisxia0506 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17637" target="_blank" rel="noopener noreferrer">PR #17637</a></li>
<li>@chetanchoudhary-sumo made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17630" target="_blank" rel="noopener noreferrer">PR #17630</a></li>
<li>@kevinmarx made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17632" target="_blank" rel="noopener noreferrer">PR #17632</a></li>
<li>@expruc made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17627" target="_blank" rel="noopener noreferrer">PR #17627</a></li>
<li>@rcII made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17626" target="_blank" rel="noopener noreferrer">PR #17626</a></li>
<li>@tamirkiviti13 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16591" target="_blank" rel="noopener noreferrer">PR #16591</a></li>
<li>@Eric84626 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17629" target="_blank" rel="noopener noreferrer">PR #17629</a></li>
<li>@vasilisazayka made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16053" target="_blank" rel="noopener noreferrer">PR #16053</a></li>
<li>@juliettech13 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17663" target="_blank" rel="noopener noreferrer">PR #17663</a></li>
<li>@jason-nance made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17660" target="_blank" rel="noopener noreferrer">PR #17660</a></li>
<li>@yisding made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17671" target="_blank" rel="noopener noreferrer">PR #17671</a></li>
<li>@emilsvennesson made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17656" target="_blank" rel="noopener noreferrer">PR #17656</a></li>
<li>@kumekay made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17646" target="_blank" rel="noopener noreferrer">PR #17646</a></li>
<li>@chenzhaofei01 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17584" target="_blank" rel="noopener noreferrer">PR #17584</a></li>
<li>@shivamrawat1 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17733" target="_blank" rel="noopener noreferrer">PR #17733</a></li>
<li>@ephrimstanley made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17723" target="_blank" rel="noopener noreferrer">PR #17723</a></li>
<li>@hwittenborn made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17743" target="_blank" rel="noopener noreferrer">PR #17743</a></li>
<li>@peterkc made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17727" target="_blank" rel="noopener noreferrer">PR #17727</a></li>
<li>@saisurya237 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17725" target="_blank" rel="noopener noreferrer">PR #17725</a></li>
<li>@Ashton-Sidhu made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17728" target="_blank" rel="noopener noreferrer">PR #17728</a></li>
<li>@CyrusTC made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17810" target="_blank" rel="noopener noreferrer">PR #17810</a></li>
<li>@jichmi made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17703" target="_blank" rel="noopener noreferrer">PR #17703</a></li>
<li>@ryan-crabbe made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17852" target="_blank" rel="noopener noreferrer">PR #17852</a></li>
<li>@nlineback made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17851" target="_blank" rel="noopener noreferrer">PR #17851</a></li>
<li>@butnarurazvan made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17468" target="_blank" rel="noopener noreferrer">PR #17468</a></li>
<li>@yoshi-p27 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17915" target="_blank" rel="noopener noreferrer">PR #17915</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog">Full Changelog<a href="#full-changelog" class="hash-link" aria-label="Full Changelog" title="Full Changelog"></a></h2>
<p><strong><a href="https://github.com/BerriAI/litellm/compare/v1.80.8.rc.1...v1.80.10" target="_blank" rel="noopener noreferrer">View complete changelog on GitHub</a></strong></p></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-80-8">v1.80.8-stable - Introducing A2A Agent Gateway</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-12-06T10:00:00.000Z">2025126</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaff"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaff</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.80.8-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.80.8</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>Agent Gateway (A2A)</strong> - <a href="/docs/a2a">Invoke agents through the AI Gateway with request/response logging and access controls</a></li>
<li><strong>Guardrails API v2</strong> - <a href="/docs/adding_provider/generic_guardrail_api">Generic Guardrail API with streaming support, structured messages, and tool call checks</a></li>
<li><strong>Customer (End User) Usage UI</strong> - <a href="/docs/proxy/customer_usage">Track and visualize end-user spend directly in the dashboard</a></li>
<li><strong>vLLM Batch + Files API</strong> - <a href="/docs/batches">Support for batch and files API with vLLM deployments</a></li>
<li><strong>Dynamic Rate Limiting on Teams</strong> - <a href="/docs/proxy/team_budgets">Enable dynamic rate limits and priority reservation on team-level</a></li>
<li><strong>Google Cloud Chirp3 HD</strong> - <a href="/docs/text_to_speech">New text-to-speech provider with Chirp3 HD voices</a></li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-gateway-a2a">Agent Gateway (A2A)<a href="#agent-gateway-a2a" class="hash-link" aria-label="Agent Gateway (A2A)" title="Agent Gateway (A2A)"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAGCAIAAAB1kpiRAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAtklEQVR4nB3Gyw7BQBQA0EHn3plOqUc7jyu0HkkRFoQgRCSs7ET4Kksbn2BlL/ENfklIzuIwKUgKQrDGZAn1U5cZnSE3kK8iNww8zQsxgg19cqO9mx9LPslSGvW2xajHYjeod6cqSARNFrfX4fEpz45BnA03J9tasGqtq91AopNuvLo+N/d3ZXlRxWZDp7Www8CzvGAEEOYiPdwl67PwjFRJWG4HqsEEkkQSfz7Xyot/BwvcItAXFv0VgL1fhlMAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="381"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/a2a_gateway.4b90229.640.png" srcset="/assets/ideal-img/a2a_gateway.4b90229.640.png 640w,/assets/ideal-img/a2a_gateway.9582a3f.1344.png 1344w" width="640" height="381"></noscript></div>
<br>
<p>This release introduces <strong>A2A Agent Gateway</strong> for LiteLLM, allowing you to invoke and manage A2A agents with the same controls you have for LLM APIs.</p>
<p>As a <strong>LiteLLM Gateway Admin</strong>, you can now do the following:</p>
<ul>
<li><strong>Request/Response Logging</strong> - Every agent invocation is logged to the Logs page with full request and response tracking.</li>
<li><strong>Access Control</strong> - Control which Team/Key can access which agents.</li>
</ul>
<p>As a developer, you can continue using the A2A SDK, all you need to do is point you <code>A2AClient</code> to the LiteLLM proxy URL and your API key.</p>
<p><strong>Works with the A2A SDK:</strong></p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> a2a</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">client </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> A2AClient</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">client </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> A2AClient</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    base_url</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;http://localhost:4000&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># Your LiteLLM proxy</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    api_key</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;sk-1234&quot;</span><span class="token plain">                   </span><span class="token comment" style="color:#999988;font-style:italic"># LiteLLM API key</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">response </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> client</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">send_message</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    agent_id</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;my-agent&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    message</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;What&#x27;s the status of my order?&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p>Get started with Agent Gateway here: <a href="/docs/a2a">Agent Gateway Documentation</a></p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="customer-end-user-usage-ui">Customer (End User) Usage UI<a href="#customer-end-user-usage-ui" class="hash-link" aria-label="Customer (End User) Usage UI" title="Customer (End User) Usage UI"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAkklEQVR4nD2OTQrCMBQGc0u9g1fwPl5GKLRuFFdKF4I/sbVJ895LRiLFxWw+huFzwzDivSfGSEqykP6IJMwU13UdbdsyTYEQZ0QM0UxSRVTJOfO8C67W3ksxhICpYWZkU6zKIoQgOFWlUocqnafE5tCz6188kkIplFJw9UecZ8jG3ge2xxur5sq6uXD6zD9p1MwXGsy+lh2zMRYAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/customer_usage.2937330.640.png" srcset="/assets/ideal-img/customer_usage.2937330.640.png 640w,/assets/ideal-img/customer_usage.e539153.1920.png 1920w" width="640" height="334"></noscript></div>
<p>Users can now filter usage statistics by customers, providing the same granular filtering capabilities available for teams and organizations.</p>
<p><strong>Details:</strong></p>
<ul>
<li>Filter usage analytics, spend logs, and activity metrics by customer ID</li>
<li>View customer-level breakdowns alongside existing team and user-level filters</li>
<li>Consistent filtering experience across all usage and analytics views</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-providers-and-endpoints">New Providers and Endpoints<a href="#new-providers-and-endpoints" class="hash-link" aria-label="New Providers and Endpoints" title="New Providers and Endpoints"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="new-providers-5-new-providers">New Providers (5 new providers)<a href="#new-providers-5-new-providers" class="hash-link" aria-label="New Providers (5 new providers)" title="New Providers (5 new providers)"></a></h3>
<table><thead><tr><th>Provider</th><th>Supported LiteLLM Endpoints</th><th>Description</th></tr></thead><tbody><tr><td><strong><a href="/docs/providers/zai">Z.AI (Zhipu AI)</a></strong></td><td><code>/v1/chat/completions</code>, <code>/v1/responses</code>, <code>/v1/messages</code></td><td>Built-in support for Zhipu AI GLM models</td></tr><tr><td><strong><a href="/docs/providers/ragflow">RAGFlow</a></strong></td><td><code>/v1/chat/completions</code>, <code>/v1/responses</code>, <code>/v1/messages</code>, <code>/v1/vector_stores</code></td><td>RAG-based chat completions with vector store support</td></tr><tr><td><strong><a href="/docs/providers/publicai">PublicAI</a></strong></td><td><code>/v1/chat/completions</code>, <code>/v1/responses</code>, <code>/v1/messages</code></td><td>OpenAI-compatible provider via JSON config</td></tr><tr><td><strong><a href="/docs/text_to_speech">Google Cloud Chirp3 HD</a></strong></td><td><code>/v1/audio/speech</code>, <code>/v1/audio/speech/stream</code></td><td>Text-to-speech with Google Cloud Chirp3 HD voices</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="new-llm-api-endpoints-2-new-endpoints">New LLM API Endpoints (2 new endpoints)<a href="#new-llm-api-endpoints-2-new-endpoints" class="hash-link" aria-label="New LLM API Endpoints (2 new endpoints)" title="New LLM API Endpoints (2 new endpoints)"></a></h3>
<table><thead><tr><th>Endpoint</th><th>Method</th><th>Description</th><th>Documentation</th></tr></thead><tbody><tr><td><code>/v1/agents/invoke</code></td><td>POST</td><td>Invoke A2A agents through the AI Gateway</td><td><a href="/docs/a2a">Agent Gateway</a></td></tr><tr><td><code>/cursor/chat/completions</code></td><td>POST</td><td>Cursor BYOK endpoint - accepts Responses API input, returns Chat Completions output</td><td><a href="/docs/tutorials/cursor_integration">Cursor Integration</a></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-model-support-33-new-models">New Model Support (33 new models)<a href="#new-model-support-33-new-models" class="hash-link" aria-label="New Model Support (33 new models)" title="New Model Support (33 new models)"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Features</th></tr></thead><tbody><tr><td>OpenAI</td><td><code>gpt-5.1-codex-max</code></td><td>400K</td><td>$1.25</td><td>$10.00</td><td>Reasoning, vision, PDF input, responses API</td></tr><tr><td>Azure</td><td><code>azure/gpt-5.1-codex-max</code></td><td>400K</td><td>$1.25</td><td>$10.00</td><td>Reasoning, vision, PDF input, responses API</td></tr><tr><td>Anthropic</td><td><code>claude-opus-4-5</code></td><td>200K</td><td>$5.00</td><td>$25.00</td><td>Computer use, reasoning, vision</td></tr><tr><td>Bedrock</td><td><code>global.anthropic.claude-opus-4-5-20251101-v1:0</code></td><td>200K</td><td>$5.00</td><td>$25.00</td><td>Computer use, reasoning, vision</td></tr><tr><td>Bedrock</td><td><code>amazon.nova-2-lite-v1:0</code></td><td>1M</td><td>$0.30</td><td>$2.50</td><td>Reasoning, vision, video, PDF input</td></tr><tr><td>Bedrock</td><td><code>amazon.titan-image-generator-v2:0</code></td><td>-</td><td>-</td><td>$0.008/image</td><td>Image generation</td></tr><tr><td>Fireworks</td><td><code>fireworks_ai/deepseek-v3p2</code></td><td>164K</td><td>$1.20</td><td>$1.20</td><td>Function calling, response schema</td></tr><tr><td>Fireworks</td><td><code>fireworks_ai/kimi-k2-instruct-0905</code></td><td>262K</td><td>$0.60</td><td>$2.50</td><td>Function calling, response schema</td></tr><tr><td>DeepSeek</td><td><code>deepseek/deepseek-v3.2</code></td><td>164K</td><td>$0.28</td><td>$0.40</td><td>Reasoning, function calling</td></tr><tr><td>Mistral</td><td><code>mistral/mistral-large-3</code></td><td>256K</td><td>$0.50</td><td>$1.50</td><td>Function calling, vision</td></tr><tr><td>Azure AI</td><td><code>azure_ai/mistral-large-3</code></td><td>256K</td><td>$0.50</td><td>$1.50</td><td>Function calling, vision</td></tr><tr><td>Moonshot</td><td><code>moonshot/kimi-k2-0905-preview</code></td><td>262K</td><td>$0.60</td><td>$2.50</td><td>Function calling, web search</td></tr><tr><td>Moonshot</td><td><code>moonshot/kimi-k2-turbo-preview</code></td><td>262K</td><td>$1.15</td><td>$8.00</td><td>Function calling, web search</td></tr><tr><td>Moonshot</td><td><code>moonshot/kimi-k2-thinking-turbo</code></td><td>262K</td><td>$1.15</td><td>$8.00</td><td>Function calling, web search</td></tr><tr><td>OpenRouter</td><td><code>openrouter/deepseek/deepseek-v3.2</code></td><td>164K</td><td>$0.28</td><td>$0.40</td><td>Reasoning, function calling</td></tr><tr><td>Databricks</td><td><code>databricks/databricks-claude-haiku-4-5</code></td><td>200K</td><td>$1.00</td><td>$5.00</td><td>Reasoning, function calling</td></tr><tr><td>Databricks</td><td><code>databricks/databricks-claude-opus-4</code></td><td>200K</td><td>$15.00</td><td>$75.00</td><td>Reasoning, function calling</td></tr><tr><td>Databricks</td><td><code>databricks/databricks-claude-opus-4-1</code></td><td>200K</td><td>$15.00</td><td>$75.00</td><td>Reasoning, function calling</td></tr><tr><td>Databricks</td><td><code>databricks/databricks-claude-opus-4-5</code></td><td>200K</td><td>$5.00</td><td>$25.00</td><td>Reasoning, function calling</td></tr><tr><td>Databricks</td><td><code>databricks/databricks-claude-sonnet-4</code></td><td>200K</td><td>$3.00</td><td>$15.00</td><td>Reasoning, function calling</td></tr><tr><td>Databricks</td><td><code>databricks/databricks-claude-sonnet-4-1</code></td><td>200K</td><td>$3.00</td><td>$15.00</td><td>Reasoning, function calling</td></tr><tr><td>Databricks</td><td><code>databricks/databricks-gemini-2-5-flash</code></td><td>1M</td><td>$0.30</td><td>$2.50</td><td>Function calling</td></tr><tr><td>Databricks</td><td><code>databricks/databricks-gemini-2-5-pro</code></td><td>1M</td><td>$1.25</td><td>$10.00</td><td>Function calling</td></tr><tr><td>Databricks</td><td><code>databricks/databricks-gpt-5</code></td><td>400K</td><td>$1.25</td><td>$10.00</td><td>Function calling</td></tr><tr><td>Databricks</td><td><code>databricks/databricks-gpt-5-1</code></td><td>400K</td><td>$1.25</td><td>$10.00</td><td>Function calling</td></tr><tr><td>Databricks</td><td><code>databricks/databricks-gpt-5-mini</code></td><td>400K</td><td>$0.25</td><td>$2.00</td><td>Function calling</td></tr><tr><td>Databricks</td><td><code>databricks/databricks-gpt-5-nano</code></td><td>400K</td><td>$0.05</td><td>$0.40</td><td>Function calling</td></tr><tr><td>Vertex AI</td><td><code>vertex_ai/chirp</code></td><td>-</td><td>$30.00/1M chars</td><td>-</td><td>Text-to-speech (Chirp3 HD)</td></tr><tr><td>Z.AI</td><td><code>zai/glm-4.6</code></td><td>200K</td><td>$0.60</td><td>$2.20</td><td>Function calling</td></tr><tr><td>Z.AI</td><td><code>zai/glm-4.5</code></td><td>128K</td><td>$0.60</td><td>$2.20</td><td>Function calling</td></tr><tr><td>Z.AI</td><td><code>zai/glm-4.5v</code></td><td>128K</td><td>$0.60</td><td>$1.80</td><td>Function calling, vision</td></tr><tr><td>Z.AI</td><td><code>zai/glm-4.5-flash</code></td><td>128K</td><td>Free</td><td>Free</td><td>Function calling</td></tr><tr><td>Vertex AI</td><td><code>vertex_ai/bge-large-en-v1.5</code></td><td>-</td><td>-</td><td>-</td><td>BGE Embeddings</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/providers/openai">OpenAI</a></strong></p>
<ul>
<li>Add <code>gpt-5.1-codex-max</code> model pricing and configuration - <a href="https://github.com/BerriAI/litellm/pull/17541" target="_blank" rel="noopener noreferrer">PR #17541</a></li>
<li>Add xhigh reasoning effort for gpt-5.1-codex-max - <a href="https://github.com/BerriAI/litellm/pull/17585" target="_blank" rel="noopener noreferrer">PR #17585</a></li>
<li>Add clear error message for empty LLM endpoint responses - <a href="https://github.com/BerriAI/litellm/pull/17445" target="_blank" rel="noopener noreferrer">PR #17445</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/azure/azure">Azure OpenAI</a></strong></p>
<ul>
<li>Allow reasoning_effort=&#x27;none&#x27; for Azure gpt-5.1 models - <a href="https://github.com/BerriAI/litellm/pull/17311" target="_blank" rel="noopener noreferrer">PR #17311</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/anthropic">Anthropic</a></strong></p>
<ul>
<li>Add <code>claude-opus-4-5</code> alias to pricing data - <a href="https://github.com/BerriAI/litellm/pull/17313" target="_blank" rel="noopener noreferrer">PR #17313</a></li>
<li>Parse <code>&lt;budget:thinking&gt;</code> blocks for opus 4.5 - <a href="https://github.com/BerriAI/litellm/pull/17534" target="_blank" rel="noopener noreferrer">PR #17534</a></li>
<li>Update new Anthropic features as reviewed - <a href="https://github.com/BerriAI/litellm/pull/17142" target="_blank" rel="noopener noreferrer">PR #17142</a></li>
<li>Skip empty text blocks in Anthropic system messages - <a href="https://github.com/BerriAI/litellm/pull/17442" target="_blank" rel="noopener noreferrer">PR #17442</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/bedrock">Bedrock</a></strong></p>
<ul>
<li>Add Nova embedding support - <a href="https://github.com/BerriAI/litellm/pull/17253" target="_blank" rel="noopener noreferrer">PR #17253</a></li>
<li>Add support for Bedrock Qwen 2 imported model - <a href="https://github.com/BerriAI/litellm/pull/17461" target="_blank" rel="noopener noreferrer">PR #17461</a></li>
<li>Bedrock OpenAI model support - <a href="https://github.com/BerriAI/litellm/pull/17368" target="_blank" rel="noopener noreferrer">PR #17368</a></li>
<li>Add support for file content download for Bedrock batches - <a href="https://github.com/BerriAI/litellm/pull/17470" target="_blank" rel="noopener noreferrer">PR #17470</a></li>
<li>Make streaming chunk size configurable in Bedrock API - <a href="https://github.com/BerriAI/litellm/pull/17357" target="_blank" rel="noopener noreferrer">PR #17357</a></li>
<li>Add experimental latest-user filtering for Bedrock - <a href="https://github.com/BerriAI/litellm/pull/17282" target="_blank" rel="noopener noreferrer">PR #17282</a></li>
<li>Handle Cohere v4 embed response dictionary format - <a href="https://github.com/BerriAI/litellm/pull/17220" target="_blank" rel="noopener noreferrer">PR #17220</a></li>
<li>Remove not compatible beta header from Bedrock - <a href="https://github.com/BerriAI/litellm/pull/17301" target="_blank" rel="noopener noreferrer">PR #17301</a></li>
<li>Add model price and details for Global Opus 4.5 Bedrock endpoint - <a href="https://github.com/BerriAI/litellm/pull/17380" target="_blank" rel="noopener noreferrer">PR #17380</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/gemini">Gemini (Google AI Studio + Vertex AI)</a></strong></p>
<ul>
<li>Add better handling in image generation for Gemini models - <a href="https://github.com/BerriAI/litellm/pull/17292" target="_blank" rel="noopener noreferrer">PR #17292</a></li>
<li>Fix reasoning_content showing duplicate content in streaming responses - <a href="https://github.com/BerriAI/litellm/pull/17266" target="_blank" rel="noopener noreferrer">PR #17266</a></li>
<li>Handle partial JSON chunks after first valid chunk - <a href="https://github.com/BerriAI/litellm/pull/17496" target="_blank" rel="noopener noreferrer">PR #17496</a></li>
<li>Fix Gemini 3 last chunk thinking block - <a href="https://github.com/BerriAI/litellm/pull/17403" target="_blank" rel="noopener noreferrer">PR #17403</a></li>
<li>Fix Gemini image_tokens treated as text tokens in cost calculation - <a href="https://github.com/BerriAI/litellm/pull/17554" target="_blank" rel="noopener noreferrer">PR #17554</a></li>
<li>Make sure that media resolution is only for Gemini 3 model - <a href="https://github.com/BerriAI/litellm/pull/17137" target="_blank" rel="noopener noreferrer">PR #17137</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/vertex">Vertex AI</a></strong></p>
<ul>
<li>Add Google Cloud Chirp3 HD support on /speech - <a href="https://github.com/BerriAI/litellm/pull/17391" target="_blank" rel="noopener noreferrer">PR #17391</a></li>
<li>Add BGE Embeddings support - <a href="https://github.com/BerriAI/litellm/pull/17362" target="_blank" rel="noopener noreferrer">PR #17362</a></li>
<li>Handle global location for Vertex AI image generation endpoint - <a href="https://github.com/BerriAI/litellm/pull/17255" target="_blank" rel="noopener noreferrer">PR #17255</a></li>
<li>Add Google Private API Endpoint to Vertex AI fields - <a href="https://github.com/BerriAI/litellm/pull/17382" target="_blank" rel="noopener noreferrer">PR #17382</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/zai">Z.AI (Zhipu AI)</a></strong></p>
<ul>
<li>Add Z.AI as built-in provider - <a href="https://github.com/BerriAI/litellm/pull/17307" target="_blank" rel="noopener noreferrer">PR #17307</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/github_copilot">GitHub Copilot</a></strong></p>
<ul>
<li>Add Embedding API support - <a href="https://github.com/BerriAI/litellm/pull/17278" target="_blank" rel="noopener noreferrer">PR #17278</a></li>
<li>Preserve encrypted_content in reasoning items for multi-turn conversations - <a href="https://github.com/BerriAI/litellm/pull/17130" target="_blank" rel="noopener noreferrer">PR #17130</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/databricks">Databricks</a></strong></p>
<ul>
<li>Update Databricks model pricing and add new models - <a href="https://github.com/BerriAI/litellm/pull/17277" target="_blank" rel="noopener noreferrer">PR #17277</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/ovhcloud">OVHcloud</a></strong></p>
<ul>
<li>Add support of audio transcription for OVHcloud - <a href="https://github.com/BerriAI/litellm/pull/17305" target="_blank" rel="noopener noreferrer">PR #17305</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/mistral">Mistral</a></strong></p>
<ul>
<li>Add Mistral Large 3 model support - <a href="https://github.com/BerriAI/litellm/pull/17547" target="_blank" rel="noopener noreferrer">PR #17547</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/moonshot">Moonshot</a></strong></p>
<ul>
<li>Fix missing Moonshot turbo models and fix incorrect pricing - <a href="https://github.com/BerriAI/litellm/pull/17432" target="_blank" rel="noopener noreferrer">PR #17432</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/togetherai">Together AI</a></strong></p>
<ul>
<li>Add context window exception mapping for Together AI - <a href="https://github.com/BerriAI/litellm/pull/17284" target="_blank" rel="noopener noreferrer">PR #17284</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/watsonx/index">WatsonX</a></strong></p>
<ul>
<li>Allow passing zen_api_key dynamically - <a href="https://github.com/BerriAI/litellm/pull/16655" target="_blank" rel="noopener noreferrer">PR #16655</a></li>
<li>Fix Watsonx Audio Transcription API - <a href="https://github.com/BerriAI/litellm/pull/17326" target="_blank" rel="noopener noreferrer">PR #17326</a></li>
<li>Fix audio transcriptions, don&#x27;t force content type in request headers - <a href="https://github.com/BerriAI/litellm/pull/17546" target="_blank" rel="noopener noreferrer">PR #17546</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/fireworks_ai">Fireworks AI</a></strong></p>
<ul>
<li>Add new model <code>fireworks_ai/kimi-k2-instruct-0905</code> - <a href="https://github.com/BerriAI/litellm/pull/17328" target="_blank" rel="noopener noreferrer">PR #17328</a></li>
<li>Add <code>fireworks/deepseek-v3p2</code> - <a href="https://github.com/BerriAI/litellm/pull/17395" target="_blank" rel="noopener noreferrer">PR #17395</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/deepseek">DeepSeek</a></strong></p>
<ul>
<li>Support Deepseek 3.2 with Reasoning - <a href="https://github.com/BerriAI/litellm/pull/17384" target="_blank" rel="noopener noreferrer">PR #17384</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/bedrock">Nova Lite 2</a></strong></p>
<ul>
<li>Add Nova Lite 2 reasoning support with reasoningConfig - <a href="https://github.com/BerriAI/litellm/pull/17371" target="_blank" rel="noopener noreferrer">PR #17371</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/ollama">Ollama</a></strong></p>
<ul>
<li>Fix auth not working with ollama.com - <a href="https://github.com/BerriAI/litellm/pull/17191" target="_blank" rel="noopener noreferrer">PR #17191</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/groq">Groq</a></strong></p>
<ul>
<li>Fix supports_response_schema before using json_tool_call workaround - <a href="https://github.com/BerriAI/litellm/pull/17438" target="_blank" rel="noopener noreferrer">PR #17438</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/vllm">vLLM</a></strong></p>
<ul>
<li>Fix empty response + vLLM streaming - <a href="https://github.com/BerriAI/litellm/pull/17516" target="_blank" rel="noopener noreferrer">PR #17516</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/azure_ai">Azure AI</a></strong></p>
<ul>
<li>Migrate Anthropic provider to Azure AI - <a href="https://github.com/BerriAI/litellm/pull/17202" target="_blank" rel="noopener noreferrer">PR #17202</a></li>
<li>Fix GA path for Azure OpenAI realtime models - <a href="https://github.com/BerriAI/litellm/pull/17260" target="_blank" rel="noopener noreferrer">PR #17260</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/bedrock#twelvelabs-pegasus---video-understanding">Bedrock TwelveLabs</a></strong></p>
<ul>
<li>Add support for TwelveLabs Pegasus video understanding - <a href="https://github.com/BerriAI/litellm/pull/17193" target="_blank" rel="noopener noreferrer">PR #17193</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes">Bug Fixes<a href="#bug-fixes" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h3>
<ul>
<li>
<p><strong><a href="/docs/providers/bedrock">Bedrock</a></strong></p>
<ul>
<li>Fix extra_headers in messages API bedrock invoke - <a href="https://github.com/BerriAI/litellm/pull/17271" target="_blank" rel="noopener noreferrer">PR #17271</a></li>
<li>Fix Bedrock models in model map - <a href="https://github.com/BerriAI/litellm/pull/17419" target="_blank" rel="noopener noreferrer">PR #17419</a></li>
<li>Make Bedrock converse messages respect modify_params as expected - <a href="https://github.com/BerriAI/litellm/pull/17427" target="_blank" rel="noopener noreferrer">PR #17427</a></li>
<li>Fix Anthropic beta headers for Bedrock imported Qwen models - <a href="https://github.com/BerriAI/litellm/pull/17467" target="_blank" rel="noopener noreferrer">PR #17467</a></li>
<li>Preserve usage from JSON response for OpenAI provider in Bedrock - <a href="https://github.com/BerriAI/litellm/pull/17589" target="_blank" rel="noopener noreferrer">PR #17589</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/sambanova">SambaNova</a></strong></p>
<ul>
<li>Fix acompletion throws error with SambaNova models - <a href="https://github.com/BerriAI/litellm/pull/17217" target="_blank" rel="noopener noreferrer">PR #17217</a></li>
</ul>
</li>
<li>
<p><strong>General</strong></p>
<ul>
<li>Fix AttributeError when metadata is null in request body - <a href="https://github.com/BerriAI/litellm/pull/17306" target="_blank" rel="noopener noreferrer">PR #17306</a></li>
<li>Fix 500 error for malformed request - <a href="https://github.com/BerriAI/litellm/pull/17291" target="_blank" rel="noopener noreferrer">PR #17291</a></li>
<li>Respect custom LLM provider in header - <a href="https://github.com/BerriAI/litellm/pull/17290" target="_blank" rel="noopener noreferrer">PR #17290</a></li>
<li>Replace deprecated .dict() with .model_dump() in streaming_handler - <a href="https://github.com/BerriAI/litellm/pull/17359" target="_blank" rel="noopener noreferrer">PR #17359</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/response_api">Responses API</a></strong></p>
<ul>
<li>Add cost tracking for responses API - <a href="https://github.com/BerriAI/litellm/pull/17258" target="_blank" rel="noopener noreferrer">PR #17258</a></li>
<li>Map output_tokens_details of responses API to completion_tokens_details - <a href="https://github.com/BerriAI/litellm/pull/17458" target="_blank" rel="noopener noreferrer">PR #17458</a></li>
<li>Add image generation support for Responses API - <a href="https://github.com/BerriAI/litellm/pull/16586" target="_blank" rel="noopener noreferrer">PR #16586</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/batches">Batch API</a></strong></p>
<ul>
<li>Add vLLM batch+files API support - <a href="https://github.com/BerriAI/litellm/pull/15823" target="_blank" rel="noopener noreferrer">PR #15823</a></li>
<li>Fix optional parameter default value - <a href="https://github.com/BerriAI/litellm/pull/17434" target="_blank" rel="noopener noreferrer">PR #17434</a></li>
<li>Add status parameter as optional for FileObject - <a href="https://github.com/BerriAI/litellm/pull/17431" target="_blank" rel="noopener noreferrer">PR #17431</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/videos">Video Generation API</a></strong></p>
<ul>
<li>Add passthrough cost tracking for Veo - <a href="https://github.com/BerriAI/litellm/pull/17296" target="_blank" rel="noopener noreferrer">PR #17296</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/ocr">OCR API</a></strong></p>
<ul>
<li>Add missing OCR and aOCR to CallTypes enum - <a href="https://github.com/BerriAI/litellm/pull/17435" target="_blank" rel="noopener noreferrer">PR #17435</a></li>
</ul>
</li>
<li>
<p><strong>General</strong></p>
<ul>
<li>Support routing to only websearch supported deployments - <a href="https://github.com/BerriAI/litellm/pull/17500" target="_blank" rel="noopener noreferrer">PR #17500</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>General</strong>
<ul>
<li>Fix streaming error validation - <a href="https://github.com/BerriAI/litellm/pull/17242" target="_blank" rel="noopener noreferrer">PR #17242</a></li>
<li>Add length validation for empty tool_calls in delta - <a href="https://github.com/BerriAI/litellm/pull/17523" target="_blank" rel="noopener noreferrer">PR #17523</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong>New Login Page</strong></p>
<ul>
<li>New Login Page UI - <a href="https://github.com/BerriAI/litellm/pull/17443" target="_blank" rel="noopener noreferrer">PR #17443</a></li>
<li>Refactor /login route - <a href="https://github.com/BerriAI/litellm/pull/17379" target="_blank" rel="noopener noreferrer">PR #17379</a></li>
<li>Add auto_redirect_to_sso to UI Config - <a href="https://github.com/BerriAI/litellm/pull/17399" target="_blank" rel="noopener noreferrer">PR #17399</a></li>
<li>Add Auto Redirect to SSO to New Login Page - <a href="https://github.com/BerriAI/litellm/pull/17451" target="_blank" rel="noopener noreferrer">PR #17451</a></li>
</ul>
</li>
<li>
<p><strong>Customer (End User) Usage</strong></p>
<ul>
<li>Customer (end user) Usage feature - <a href="https://github.com/BerriAI/litellm/pull/17498" target="_blank" rel="noopener noreferrer">PR #17498</a></li>
<li>Customer Usage UI - <a href="https://github.com/BerriAI/litellm/pull/17506" target="_blank" rel="noopener noreferrer">PR #17506</a></li>
<li>Add Info Banner for Customer Usage - <a href="https://github.com/BerriAI/litellm/pull/17598" target="_blank" rel="noopener noreferrer">PR #17598</a></li>
</ul>
</li>
<li>
<p><strong>Virtual Keys</strong></p>
<ul>
<li>Standardize API Key vs Virtual Key in UI - <a href="https://github.com/BerriAI/litellm/pull/17325" target="_blank" rel="noopener noreferrer">PR #17325</a></li>
<li>Add User Alias Column to Internal User Table - <a href="https://github.com/BerriAI/litellm/pull/17321" target="_blank" rel="noopener noreferrer">PR #17321</a></li>
<li>Delete Credential Enhancements - <a href="https://github.com/BerriAI/litellm/pull/17317" target="_blank" rel="noopener noreferrer">PR #17317</a></li>
</ul>
</li>
<li>
<p><strong>Models + Endpoints</strong></p>
<ul>
<li>Show all credential values on Edit Credential Modal - <a href="https://github.com/BerriAI/litellm/pull/17397" target="_blank" rel="noopener noreferrer">PR #17397</a></li>
<li>Change Edit Team Models Shown to Match Create Team - <a href="https://github.com/BerriAI/litellm/pull/17394" target="_blank" rel="noopener noreferrer">PR #17394</a></li>
<li>Support Images in Compare UI - <a href="https://github.com/BerriAI/litellm/pull/17562" target="_blank" rel="noopener noreferrer">PR #17562</a></li>
</ul>
</li>
<li>
<p><strong>Callbacks</strong></p>
<ul>
<li>Show all callbacks on UI - <a href="https://github.com/BerriAI/litellm/pull/16335" target="_blank" rel="noopener noreferrer">PR #16335</a></li>
<li>Credentials to use React Query - <a href="https://github.com/BerriAI/litellm/pull/17465" target="_blank" rel="noopener noreferrer">PR #17465</a></li>
</ul>
</li>
<li>
<p><strong>Management Routes</strong></p>
<ul>
<li>Allow admin viewer to access global tag usage - <a href="https://github.com/BerriAI/litellm/pull/17501" target="_blank" rel="noopener noreferrer">PR #17501</a></li>
<li>Allow wildcard routes for nonproxy admin (SCIM) - <a href="https://github.com/BerriAI/litellm/pull/17178" target="_blank" rel="noopener noreferrer">PR #17178</a></li>
<li>Return 404 when a user is not found on /user/info - <a href="https://github.com/BerriAI/litellm/pull/16850" target="_blank" rel="noopener noreferrer">PR #16850</a></li>
</ul>
</li>
<li>
<p><strong>OCI Configuration</strong></p>
<ul>
<li>Enable Oracle Cloud Infrastructure configuration via UI - <a href="https://github.com/BerriAI/litellm/pull/17159" target="_blank" rel="noopener noreferrer">PR #17159</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li>
<p><strong>UI Fixes</strong></p>
<ul>
<li>Fix Request and Response Panel JSONViewer - <a href="https://github.com/BerriAI/litellm/pull/17233" target="_blank" rel="noopener noreferrer">PR #17233</a></li>
<li>Adding Button Loading States to Edit Settings - <a href="https://github.com/BerriAI/litellm/pull/17236" target="_blank" rel="noopener noreferrer">PR #17236</a></li>
<li>Fix Various Text, button state, and test changes - <a href="https://github.com/BerriAI/litellm/pull/17237" target="_blank" rel="noopener noreferrer">PR #17237</a></li>
<li>Fix Fallbacks Immediately Deleting before API resolves - <a href="https://github.com/BerriAI/litellm/pull/17238" target="_blank" rel="noopener noreferrer">PR #17238</a></li>
<li>Remove Feature Flags - <a href="https://github.com/BerriAI/litellm/pull/17240" target="_blank" rel="noopener noreferrer">PR #17240</a></li>
<li>Fix metadata tags and model name display in UI for Azure passthrough - <a href="https://github.com/BerriAI/litellm/pull/17258" target="_blank" rel="noopener noreferrer">PR #17258</a></li>
<li>Change labeling around Vertex Fields - <a href="https://github.com/BerriAI/litellm/pull/17383" target="_blank" rel="noopener noreferrer">PR #17383</a></li>
<li>Remove second scrollbar when sidebar is expanded + tooltip z index - <a href="https://github.com/BerriAI/litellm/pull/17436" target="_blank" rel="noopener noreferrer">PR #17436</a></li>
<li>Fix Select in Edit Membership Modal - <a href="https://github.com/BerriAI/litellm/pull/17524" target="_blank" rel="noopener noreferrer">PR #17524</a></li>
<li>Change useAuthorized Hook to redirect to new Login Page - <a href="https://github.com/BerriAI/litellm/pull/17553" target="_blank" rel="noopener noreferrer">PR #17553</a></li>
</ul>
</li>
<li>
<p><strong>SSO</strong></p>
<ul>
<li>Fix the generic SSO provider - <a href="https://github.com/BerriAI/litellm/pull/17227" target="_blank" rel="noopener noreferrer">PR #17227</a></li>
<li>Clear SSO integration for all users - <a href="https://github.com/BerriAI/litellm/pull/17287" target="_blank" rel="noopener noreferrer">PR #17287</a></li>
<li>Fix SSO users not added to Entra synced team - <a href="https://github.com/BerriAI/litellm/pull/17331" target="_blank" rel="noopener noreferrer">PR #17331</a></li>
</ul>
</li>
<li>
<p><strong>Auth / JWT</strong></p>
<ul>
<li>JWT Auth - Allow using regular OIDC flow with user info endpoints - <a href="https://github.com/BerriAI/litellm/pull/17324" target="_blank" rel="noopener noreferrer">PR #17324</a></li>
<li>Fix litellm user auth not passing issue - <a href="https://github.com/BerriAI/litellm/pull/17342" target="_blank" rel="noopener noreferrer">PR #17342</a></li>
<li>Add other routes in JWT auth - <a href="https://github.com/BerriAI/litellm/pull/17345" target="_blank" rel="noopener noreferrer">PR #17345</a></li>
<li>Fix new org team validate against org - <a href="https://github.com/BerriAI/litellm/pull/17333" target="_blank" rel="noopener noreferrer">PR #17333</a></li>
<li>Fix litellm_enterprise ensure imported routes exist - <a href="https://github.com/BerriAI/litellm/pull/17337" target="_blank" rel="noopener noreferrer">PR #17337</a></li>
<li>Use organization.members instead of deprecated organization field - <a href="https://github.com/BerriAI/litellm/pull/17557" target="_blank" rel="noopener noreferrer">PR #17557</a></li>
</ul>
</li>
<li>
<p><strong>Organizations/Teams</strong></p>
<ul>
<li>Fix organization max budget not enforced - <a href="https://github.com/BerriAI/litellm/pull/17334" target="_blank" rel="noopener noreferrer">PR #17334</a></li>
<li>Fix budget update to allow null max_budget - <a href="https://github.com/BerriAI/litellm/pull/17545" target="_blank" rel="noopener noreferrer">PR #17545</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ai-integrations-2-new-integrations">AI Integrations (2 new integrations)<a href="#ai-integrations-2-new-integrations" class="hash-link" aria-label="AI Integrations (2 new integrations)" title="AI Integrations (2 new integrations)"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="logging-1-new-integration">Logging (1 new integration)<a href="#logging-1-new-integration" class="hash-link" aria-label="Logging (1 new integration)" title="Logging (1 new integration)"></a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-integration">New Integration<a href="#new-integration" class="hash-link" aria-label="New Integration" title="New Integration"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/logging">Weave</a></strong>
<ul>
<li>Basic Weave OTEL integration - <a href="https://github.com/BerriAI/litellm/pull/17439" target="_blank" rel="noopener noreferrer">PR #17439</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="improvements--fixes">Improvements &amp; Fixes<a href="#improvements--fixes" class="hash-link" aria-label="Improvements &amp; Fixes" title="Improvements &amp; Fixes"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/proxy/logging#datadog">DataDog</a></strong></p>
<ul>
<li>Fix Datadog callback regression when ddtrace is installed - <a href="https://github.com/BerriAI/litellm/pull/17393" target="_blank" rel="noopener noreferrer">PR #17393</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/observability/arize_integration">Arize Phoenix</a></strong></p>
<ul>
<li>Fix clean arize-phoenix traces - <a href="https://github.com/BerriAI/litellm/pull/16611" target="_blank" rel="noopener noreferrer">PR #16611</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/logging#mlflow">MLflow</a></strong></p>
<ul>
<li>Fix MLflow streaming spans for Anthropic passthrough - <a href="https://github.com/BerriAI/litellm/pull/17288" target="_blank" rel="noopener noreferrer">PR #17288</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/logging#langfuse">Langfuse</a></strong></p>
<ul>
<li>Fix Langfuse logger test mock setup - <a href="https://github.com/BerriAI/litellm/pull/17591" target="_blank" rel="noopener noreferrer">PR #17591</a></li>
</ul>
</li>
<li>
<p><strong>General</strong></p>
<ul>
<li>Improve PII anonymization handling in logging callbacks - <a href="https://github.com/BerriAI/litellm/pull/17207" target="_blank" rel="noopener noreferrer">PR #17207</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="guardrails-1-new-integration">Guardrails (1 new integration)<a href="#guardrails-1-new-integration" class="hash-link" aria-label="Guardrails (1 new integration)" title="Guardrails (1 new integration)"></a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-integration-1">New Integration<a href="#new-integration-1" class="hash-link" aria-label="New Integration" title="New Integration"></a></h4>
<ul>
<li><strong><a href="/docs/adding_provider/generic_guardrail_api">Generic Guardrail API</a></strong>
<ul>
<li>Generic Guardrail API - allows guardrail providers to add INSTANT support for LiteLLM w/out PR to repo - <a href="https://github.com/BerriAI/litellm/pull/17175" target="_blank" rel="noopener noreferrer">PR #17175</a></li>
<li>Guardrails API V2 - user api key metadata, session id, specify input type (request/response), image support - <a href="https://github.com/BerriAI/litellm/pull/17338" target="_blank" rel="noopener noreferrer">PR #17338</a></li>
<li>Guardrails API - add streaming support - <a href="https://github.com/BerriAI/litellm/pull/17400" target="_blank" rel="noopener noreferrer">PR #17400</a></li>
<li>Guardrails API - support tool call checks on OpenAI <code>/chat/completions</code>, OpenAI <code>/responses</code>, Anthropic <code>/v1/messages</code> - <a href="https://github.com/BerriAI/litellm/pull/17459" target="_blank" rel="noopener noreferrer">PR #17459</a></li>
<li>Guardrails API - new <code>structured_messages</code> param - <a href="https://github.com/BerriAI/litellm/pull/17518" target="_blank" rel="noopener noreferrer">PR #17518</a></li>
<li>Correctly map a v1/messages call to the anthropic unified guardrail - <a href="https://github.com/BerriAI/litellm/pull/17424" target="_blank" rel="noopener noreferrer">PR #17424</a></li>
<li>Support during_call event type for unified guardrails - <a href="https://github.com/BerriAI/litellm/pull/17514" target="_blank" rel="noopener noreferrer">PR #17514</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="improvements--fixes-1">Improvements &amp; Fixes<a href="#improvements--fixes-1" class="hash-link" aria-label="Improvements &amp; Fixes" title="Improvements &amp; Fixes"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/proxy/guardrails/noma_security">Noma Guardrail</a></strong></p>
<ul>
<li>Refactor Noma guardrail to use shared Responses transformation and include system instructions - <a href="https://github.com/BerriAI/litellm/pull/17315" target="_blank" rel="noopener noreferrer">PR #17315</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/guardrails/pii_masking_v2">Presidio</a></strong></p>
<ul>
<li>Handle empty content and error dict responses in guardrails - <a href="https://github.com/BerriAI/litellm/pull/17489" target="_blank" rel="noopener noreferrer">PR #17489</a></li>
<li>Fix Presidio guardrail test TypeError and license base64 decoding error - <a href="https://github.com/BerriAI/litellm/pull/17538" target="_blank" rel="noopener noreferrer">PR #17538</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/guardrails/tool_permission">Tool Permissions</a></strong></p>
<ul>
<li>Add regex-based tool_name/tool_type matching for tool-permission - <a href="https://github.com/BerriAI/litellm/pull/17164" target="_blank" rel="noopener noreferrer">PR #17164</a></li>
<li>Add images for tool permission guardrail documentation - <a href="https://github.com/BerriAI/litellm/pull/17322" target="_blank" rel="noopener noreferrer">PR #17322</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/guardrails/aim_security">AIM Guardrails</a></strong></p>
<ul>
<li>Fix AIM guardrail tests - <a href="https://github.com/BerriAI/litellm/pull/17499" target="_blank" rel="noopener noreferrer">PR #17499</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/guardrails/bedrock">Bedrock Guardrails</a></strong></p>
<ul>
<li>Fix Bedrock Guardrail indent and import - <a href="https://github.com/BerriAI/litellm/pull/17378" target="_blank" rel="noopener noreferrer">PR #17378</a></li>
</ul>
</li>
<li>
<p><strong>General Guardrails</strong></p>
<ul>
<li>Mask all matching keywords in content filter - <a href="https://github.com/BerriAI/litellm/pull/17521" target="_blank" rel="noopener noreferrer">PR #17521</a></li>
<li>Ensure guardrail metadata is preserved in request_data - <a href="https://github.com/BerriAI/litellm/pull/17593" target="_blank" rel="noopener noreferrer">PR #17593</a></li>
<li>Fix apply_guardrail method and improve test isolation - <a href="https://github.com/BerriAI/litellm/pull/17555" target="_blank" rel="noopener noreferrer">PR #17555</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="secret-managers">Secret Managers<a href="#secret-managers" class="hash-link" aria-label="Secret Managers" title="Secret Managers"></a></h3>
<ul>
<li>
<p><strong><a href="/docs/secret_managers/cyberark">CyberArk</a></strong></p>
<ul>
<li>Allow setting SSL verify to false - <a href="https://github.com/BerriAI/litellm/pull/17433" target="_blank" rel="noopener noreferrer">PR #17433</a></li>
</ul>
</li>
<li>
<p><strong>General</strong></p>
<ul>
<li>Make email and secret manager operations independent in key management hooks - <a href="https://github.com/BerriAI/litellm/pull/17551" target="_blank" rel="noopener noreferrer">PR #17551</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-budgets-and-rate-limiting">Spend Tracking, Budgets and Rate Limiting<a href="#spend-tracking-budgets-and-rate-limiting" class="hash-link" aria-label="Spend Tracking, Budgets and Rate Limiting" title="Spend Tracking, Budgets and Rate Limiting"></a></h2>
<ul>
<li>
<p><strong>Rate Limiting</strong></p>
<ul>
<li>Parallel Request Limiter with /messages - <a href="https://github.com/BerriAI/litellm/pull/17426" target="_blank" rel="noopener noreferrer">PR #17426</a></li>
<li>Allow using dynamic rate limit/priority reservation on teams - <a href="https://github.com/BerriAI/litellm/pull/17061" target="_blank" rel="noopener noreferrer">PR #17061</a></li>
<li>Dynamic Rate Limiter - Fix token count increases/decreases by 1 instead of actual count + Redis TTL - <a href="https://github.com/BerriAI/litellm/pull/17558" target="_blank" rel="noopener noreferrer">PR #17558</a></li>
</ul>
</li>
<li>
<p><strong>Spend Logs</strong></p>
<ul>
<li>Deprecate <code>spend/logs</code> &amp; add <code>spend/logs/v2</code> - <a href="https://github.com/BerriAI/litellm/pull/17167" target="_blank" rel="noopener noreferrer">PR #17167</a></li>
<li>Optimize SpendLogs queries to use timestamp filtering for index usage - <a href="https://github.com/BerriAI/litellm/pull/17504" target="_blank" rel="noopener noreferrer">PR #17504</a></li>
</ul>
</li>
<li>
<p><strong>Enforce User Param</strong></p>
<ul>
<li>Enforce support of enforce_user_param to OpenAI post endpoints - <a href="https://github.com/BerriAI/litellm/pull/17407" target="_blank" rel="noopener noreferrer">PR #17407</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway">MCP Gateway<a href="#mcp-gateway" class="hash-link" aria-label="MCP Gateway" title="MCP Gateway"></a></h2>
<ul>
<li>
<p><strong>MCP Configuration</strong></p>
<ul>
<li>Remove URL format validation for MCP server endpoints - <a href="https://github.com/BerriAI/litellm/pull/17270" target="_blank" rel="noopener noreferrer">PR #17270</a></li>
<li>Add stack trace to MCP error message - <a href="https://github.com/BerriAI/litellm/pull/17269" target="_blank" rel="noopener noreferrer">PR #17269</a></li>
</ul>
</li>
<li>
<p><strong>MCP Tool Results</strong></p>
<ul>
<li>Preserve tool metadata in CallToolResult - <a href="https://github.com/BerriAI/litellm/pull/17561" target="_blank" rel="noopener noreferrer">PR #17561</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="agent-gateway-a2a-1">Agent Gateway (A2A)<a href="#agent-gateway-a2a-1" class="hash-link" aria-label="Agent Gateway (A2A)" title="Agent Gateway (A2A)"></a></h2>
<ul>
<li>
<p><strong>Agent Invocation</strong></p>
<ul>
<li>Allow invoking agents through AI Gateway - <a href="https://github.com/BerriAI/litellm/pull/17440" target="_blank" rel="noopener noreferrer">PR #17440</a></li>
<li>Allow tracking request/response in &quot;Logs&quot; Page - <a href="https://github.com/BerriAI/litellm/pull/17449" target="_blank" rel="noopener noreferrer">PR #17449</a></li>
</ul>
</li>
<li>
<p><strong>Agent Access Control</strong></p>
<ul>
<li>Enforce Allowed agents by key, team + add agent access groups on backend - <a href="https://github.com/BerriAI/litellm/pull/17502" target="_blank" rel="noopener noreferrer">PR #17502</a></li>
</ul>
</li>
<li>
<p><strong>Agent Gateway UI</strong></p>
<ul>
<li>Allow testing agents on UI - <a href="https://github.com/BerriAI/litellm/pull/17455" target="_blank" rel="noopener noreferrer">PR #17455</a></li>
<li>Set allowed agents by key, team - <a href="https://github.com/BerriAI/litellm/pull/17511" target="_blank" rel="noopener noreferrer">PR #17511</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<ul>
<li>
<p><strong>Audio/Speech Performance</strong></p>
<ul>
<li>Fix <code>/audio/speech</code> performance by using <code>shared_sessions</code> - <a href="https://github.com/BerriAI/litellm/pull/16739" target="_blank" rel="noopener noreferrer">PR #16739</a></li>
</ul>
</li>
<li>
<p><strong>Memory Optimization</strong></p>
<ul>
<li>Prevent memory leak in aiohttp connection pooling - <a href="https://github.com/BerriAI/litellm/pull/17388" target="_blank" rel="noopener noreferrer">PR #17388</a></li>
<li>Lazy-load utils to reduce memory + import time - <a href="https://github.com/BerriAI/litellm/pull/17171" target="_blank" rel="noopener noreferrer">PR #17171</a></li>
</ul>
</li>
<li>
<p><strong>Database</strong></p>
<ul>
<li>Update default database connection number - <a href="https://github.com/BerriAI/litellm/pull/17353" target="_blank" rel="noopener noreferrer">PR #17353</a></li>
<li>Update default proxy_batch_write_at number - <a href="https://github.com/BerriAI/litellm/pull/17355" target="_blank" rel="noopener noreferrer">PR #17355</a></li>
<li>Add background health checks to db - <a href="https://github.com/BerriAI/litellm/pull/17528" target="_blank" rel="noopener noreferrer">PR #17528</a></li>
</ul>
</li>
<li>
<p><strong>Proxy Caching</strong></p>
<ul>
<li>Fix proxy caching between requests in aiohttp transport - <a href="https://github.com/BerriAI/litellm/pull/17122" target="_blank" rel="noopener noreferrer">PR #17122</a></li>
</ul>
</li>
<li>
<p><strong>Session Management</strong></p>
<ul>
<li>Fix session consistency, move Lasso API version away from source code - <a href="https://github.com/BerriAI/litellm/pull/17316" target="_blank" rel="noopener noreferrer">PR #17316</a></li>
<li>Conditionally pass enable_cleanup_closed to aiohttp TCPConnector - <a href="https://github.com/BerriAI/litellm/pull/17367" target="_blank" rel="noopener noreferrer">PR #17367</a></li>
</ul>
</li>
<li>
<p><strong>Vector Store</strong></p>
<ul>
<li>Fix vector store configuration synchronization failure - <a href="https://github.com/BerriAI/litellm/pull/17525" target="_blank" rel="noopener noreferrer">PR #17525</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="documentation-updates">Documentation Updates<a href="#documentation-updates" class="hash-link" aria-label="Documentation Updates" title="Documentation Updates"></a></h2>
<ul>
<li>
<p><strong>Provider Documentation</strong></p>
<ul>
<li>Add Azure AI Foundry documentation for Claude models - <a href="https://github.com/BerriAI/litellm/pull/17104" target="_blank" rel="noopener noreferrer">PR #17104</a></li>
<li>Document responses and embedding API for GitHub Copilot - <a href="https://github.com/BerriAI/litellm/pull/17456" target="_blank" rel="noopener noreferrer">PR #17456</a></li>
<li>Add gpt-5.1-codex-max to OpenAI provider documentation - <a href="https://github.com/BerriAI/litellm/pull/17602" target="_blank" rel="noopener noreferrer">PR #17602</a></li>
<li>Update Instructions For Phoenix Integration - <a href="https://github.com/BerriAI/litellm/pull/17373" target="_blank" rel="noopener noreferrer">PR #17373</a></li>
</ul>
</li>
<li>
<p><strong>Guides</strong></p>
<ul>
<li>Add guide on how to debug gateway error vs provider error - <a href="https://github.com/BerriAI/litellm/pull/17387" target="_blank" rel="noopener noreferrer">PR #17387</a></li>
<li>Agent Gateway documentation - <a href="https://github.com/BerriAI/litellm/pull/17454" target="_blank" rel="noopener noreferrer">PR #17454</a></li>
<li>A2A Permission management documentation - <a href="https://github.com/BerriAI/litellm/pull/17515" target="_blank" rel="noopener noreferrer">PR #17515</a></li>
<li>Update docs to link agent hub - <a href="https://github.com/BerriAI/litellm/pull/17462" target="_blank" rel="noopener noreferrer">PR #17462</a></li>
</ul>
</li>
<li>
<p><strong>Projects</strong></p>
<ul>
<li>Add Google ADK and Harbor to projects - <a href="https://github.com/BerriAI/litellm/pull/17352" target="_blank" rel="noopener noreferrer">PR #17352</a></li>
<li>Add Microsoft Agent Lightning to projects - <a href="https://github.com/BerriAI/litellm/pull/17422" target="_blank" rel="noopener noreferrer">PR #17422</a></li>
</ul>
</li>
<li>
<p><strong>Cleanup</strong></p>
<ul>
<li>Cleanup: Remove orphan docs pages and Docusaurus template files - <a href="https://github.com/BerriAI/litellm/pull/17356" target="_blank" rel="noopener noreferrer">PR #17356</a></li>
<li>Remove <code>source .env</code> from docs - <a href="https://github.com/BerriAI/litellm/pull/17466" target="_blank" rel="noopener noreferrer">PR #17466</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="infrastructure--cicd">Infrastructure / CI/CD<a href="#infrastructure--cicd" class="hash-link" aria-label="Infrastructure / CI/CD" title="Infrastructure / CI/CD"></a></h2>
<ul>
<li>
<p><strong>Helm Chart</strong></p>
<ul>
<li>Add ingress-only labels - <a href="https://github.com/BerriAI/litellm/pull/17348" target="_blank" rel="noopener noreferrer">PR #17348</a></li>
</ul>
</li>
<li>
<p><strong>Docker</strong></p>
<ul>
<li>Add retry logic to apk package installation in Dockerfile.non_root - <a href="https://github.com/BerriAI/litellm/pull/17596" target="_blank" rel="noopener noreferrer">PR #17596</a></li>
<li>Chainguard fixes - <a href="https://github.com/BerriAI/litellm/pull/17406" target="_blank" rel="noopener noreferrer">PR #17406</a></li>
</ul>
</li>
<li>
<p><strong>OpenAPI Schema</strong></p>
<ul>
<li>Refactor add_schema_to_components to move definitions to components/schemas - <a href="https://github.com/BerriAI/litellm/pull/17389" target="_blank" rel="noopener noreferrer">PR #17389</a></li>
</ul>
</li>
<li>
<p><strong>Security</strong></p>
<ul>
<li>Fix security vulnerability: update mdast-util-to-hast to 13.2.1 - <a href="https://github.com/BerriAI/litellm/pull/17601" target="_blank" rel="noopener noreferrer">PR #17601</a></li>
<li>Bump jws from 3.2.2 to 3.2.3 - <a href="https://github.com/BerriAI/litellm/pull/17494" target="_blank" rel="noopener noreferrer">PR #17494</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@weichiet made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17242" target="_blank" rel="noopener noreferrer">PR #17242</a></li>
<li>@AndyForest made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17220" target="_blank" rel="noopener noreferrer">PR #17220</a></li>
<li>@omkar806 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17217" target="_blank" rel="noopener noreferrer">PR #17217</a></li>
<li>@v0rtex20k made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17178" target="_blank" rel="noopener noreferrer">PR #17178</a></li>
<li>@hxomer made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17207" target="_blank" rel="noopener noreferrer">PR #17207</a></li>
<li>@orgersh92 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17316" target="_blank" rel="noopener noreferrer">PR #17316</a></li>
<li>@dannykopping made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17313" target="_blank" rel="noopener noreferrer">PR #17313</a></li>
<li>@rioiart made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17333" target="_blank" rel="noopener noreferrer">PR #17333</a></li>
<li>@codgician made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17278" target="_blank" rel="noopener noreferrer">PR #17278</a></li>
<li>@epistoteles made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17277" target="_blank" rel="noopener noreferrer">PR #17277</a></li>
<li>@kothamah made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17368" target="_blank" rel="noopener noreferrer">PR #17368</a></li>
<li>@flozonn made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17371" target="_blank" rel="noopener noreferrer">PR #17371</a></li>
<li>@richardmcsong made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17389" target="_blank" rel="noopener noreferrer">PR #17389</a></li>
<li>@matt-greathouse made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17384" target="_blank" rel="noopener noreferrer">PR #17384</a></li>
<li>@mossbanay made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17380" target="_blank" rel="noopener noreferrer">PR #17380</a></li>
<li>@mhielpos-asapp made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17376" target="_blank" rel="noopener noreferrer">PR #17376</a></li>
<li>@Joilence made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17367" target="_blank" rel="noopener noreferrer">PR #17367</a></li>
<li>@deepaktammali made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17357" target="_blank" rel="noopener noreferrer">PR #17357</a></li>
<li>@axiomofjoy made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16611" target="_blank" rel="noopener noreferrer">PR #16611</a></li>
<li>@DevajMody made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17445" target="_blank" rel="noopener noreferrer">PR #17445</a></li>
<li>@andrewtruong made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17439" target="_blank" rel="noopener noreferrer">PR #17439</a></li>
<li>@AnasAbdelR made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17490" target="_blank" rel="noopener noreferrer">PR #17490</a></li>
<li>@dominicfeliton made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17516" target="_blank" rel="noopener noreferrer">PR #17516</a></li>
<li>@kristianmitk made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17504" target="_blank" rel="noopener noreferrer">PR #17504</a></li>
<li>@rgshr made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17130" target="_blank" rel="noopener noreferrer">PR #17130</a></li>
<li>@dominicfallows made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17489" target="_blank" rel="noopener noreferrer">PR #17489</a></li>
<li>@irfansofyana made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17467" target="_blank" rel="noopener noreferrer">PR #17467</a></li>
<li>@GusBricker made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17191" target="_blank" rel="noopener noreferrer">PR #17191</a></li>
<li>@OlivverX made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17255" target="_blank" rel="noopener noreferrer">PR #17255</a></li>
<li>@withsmilo made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/17585" target="_blank" rel="noopener noreferrer">PR #17585</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog">Full Changelog<a href="#full-changelog" class="hash-link" aria-label="Full Changelog" title="Full Changelog"></a></h2>
<p><strong><a href="https://github.com/BerriAI/litellm/compare/v1.80.7-nightly...v1.80.8" target="_blank" rel="noopener noreferrer">View complete changelog on GitHub</a></strong></p></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-80-5">v1.80.5-stable - Gemini 3.0 Support</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-11-22T10:00:00.000Z">20251122</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaff"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaff</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.80.5-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.80.5</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>Gemini 3</strong> - <a href="/blog/gemini_3">Day-0 support for Gemini 3 models with thought signatures</a></li>
<li><strong>Prompt Management</strong> - <a href="/docs/proxy/litellm_prompt_management">Full prompt versioning support with UI for editing, testing, and version history</a></li>
<li><strong>MCP Hub</strong> - <a href="/docs/proxy/ai_hub#mcp-servers">Publish and discover MCP servers within your organization</a></li>
<li><strong>Model Compare UI</strong> - <a href="/docs/proxy/model_compare_ui">Side-by-side model comparison interface for testing</a></li>
<li><strong>Batch API Spend Tracking</strong> - <a href="/docs/proxy/cost_tracking#-custom-spend-log-metadata">Granular spend tracking with custom metadata for batch and file creation requests</a></li>
<li><strong>AWS IAM Secret Manager</strong> - <a href="/docs/secret_managers/aws_secret_manager#iam-role-assumption">IAM role authentication support for AWS Secret Manager</a></li>
<li><strong>Logging Callback Controls</strong> - <a href="/docs/proxy/dynamic_logging#disabling-dynamic-callback-management-enterprise">Admin-level controls to prevent callers from disabling logging callbacks in compliance environments</a></li>
<li><strong>Proxy CLI JWT Authentication</strong> - <a href="/docs/proxy/cli_sso">Enable developers to authenticate to LiteLLM AI Gateway using the Proxy CLI</a></li>
<li><strong>Batch API Routing</strong> - <a href="/docs/batches#multi-account--model-based-routing">Route batch operations to different provider accounts using model-specific credentials from your config.yaml</a></li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="prompt-management">Prompt Management<a href="#prompt-management" class="hash-link" aria-label="Prompt Management" title="Prompt Management"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAeklEQVR4nEWNywqDUAxE7/9/ohtXighakGqet6ekIg0cGDKZTFvXlWmaWJaF0iJChONeGGb2022eZ4ZhYBxHtm1HVYkIIhM1R9XovdMqUZRZyTIyO2rB8RZOUbJ/aHfFTdWqGc94JK9Dubz/D+ujXMJ5Xrjf1c++Z/IFPt/CP4TPQfAAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/prompt_history.0899e07.640.png" srcset="/assets/ideal-img/prompt_history.0899e07.640.png 640w,/assets/ideal-img/prompt_history.6f089cb.1920.png 1920w" width="640" height="334"></noscript></div>
<br>
<br>
<p>This release introduces <strong>LiteLLM Prompt Studio</strong> - a comprehensive prompt management solution built directly into the LiteLLM UI. Create, test, and version your prompts without leaving your browser.</p>
<p>You can now do the following on LiteLLM Prompt Studio:</p>
<ul>
<li><strong>Create &amp; Test Prompts</strong>: Build prompts with developer messages (system instructions) and test them in real-time with an interactive chat interface</li>
<li><strong>Dynamic Variables</strong>: Use <code>{{variable_name}}</code> syntax to create reusable prompt templates with automatic variable detection</li>
<li><strong>Version Control</strong>: Automatic versioning for every prompt update with complete version history tracking and rollback capabilities</li>
<li><strong>Prompt Studio</strong>: Edit prompts in a dedicated studio environment with live testing and preview</li>
</ul>
<p><strong>API Integration:</strong></p>
<p>Use your prompts in any application with simple API calls:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">response </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> client</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">chat</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">completions</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">create</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    model</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;gpt-4&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    extra_body</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token string" style="color:#e3116c">&quot;prompt_id&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;your-prompt-id&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token string" style="color:#e3116c">&quot;prompt_version&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># Optional: specify version</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token string" style="color:#e3116c">&quot;prompt_variables&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token string" style="color:#e3116c">&quot;name&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;value&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># Optional: pass variables</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p>Get started here: <a href="/docs/proxy/litellm_prompt_management">LiteLLM Prompt Management Documentation</a></p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="performance--realtime-182-lower-p99-latency">Performance  <code>/realtime</code> 182 Lower p99 Latency<a href="#performance--realtime-182-lower-p99-latency" class="hash-link" aria-label="performance--realtime-182-lower-p99-latency" title="performance--realtime-182-lower-p99-latency"></a></h3>
<p>This update reduces <code>/realtime</code> latency by removing redundant encodings on the hot path, reusing shared SSL contexts, and caching formatting strings that were being regenerated twice per request despite rarely changing.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="results">Results<a href="#results" class="hash-link" aria-label="Results" title="Results"></a></h4>
<table><thead><tr><th>Metric</th><th>Before</th><th>After</th><th>Improvement</th></tr></thead><tbody><tr><td>Median latency</td><td>2,200 ms</td><td><strong>59 ms</strong></td><td><strong>97% (~37 faster)</strong></td></tr><tr><td>p95 latency</td><td>8,500 ms</td><td><strong>67 ms</strong></td><td><strong>99% (~127 faster)</strong></td></tr><tr><td>p99 latency</td><td>18,000 ms</td><td><strong>99 ms</strong></td><td><strong>99% (~182 faster)</strong></td></tr><tr><td>Average latency</td><td>3,214 ms</td><td><strong>63 ms</strong></td><td><strong>98% (~51 faster)</strong></td></tr><tr><td>RPS</td><td>165</td><td><strong>1,207</strong></td><td><strong>+631% (~7.3 increase)</strong></td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="test-setup">Test Setup<a href="#test-setup" class="hash-link" aria-label="Test Setup" title="Test Setup"></a></h4>
<table><thead><tr><th>Category</th><th>Specification</th></tr></thead><tbody><tr><td><strong>Load Testing</strong></td><td>Locust: 1,000 concurrent users, 500 ramp-up</td></tr><tr><td><strong>System</strong></td><td>4 vCPUs, 8 GB RAM, 4 workers, 4 instances</td></tr><tr><td><strong>Database</strong></td><td>PostgreSQL (Redis unused)</td></tr><tr><td><strong>Configuration</strong></td><td><a href="https://gist.github.com/AlexsanderHamir/420fb44c31c00b4f17a99588637f01ec" target="_blank" rel="noopener noreferrer">config.yaml</a></td></tr><tr><td><strong>Load Script</strong></td><td><a href="https://gist.github.com/AlexsanderHamir/73b83ada21d9b84d4fe09665cf1745f5" target="_blank" rel="noopener noreferrer">no_cache_hits.py</a></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="model-compare-ui">Model Compare UI<a href="#model-compare-ui" class="hash-link" aria-label="Model Compare UI" title="Model Compare UI"></a></h3>
<p>New interactive playground UI enables side-by-side comparison of multiple LLM models, making it easy to evaluate and compare model responses.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Compare responses from multiple models in real-time</li>
<li>Side-by-side view with synchronized scrolling</li>
<li>Support for all LiteLLM-supported models</li>
<li>Cost tracking per model</li>
<li>Response time comparison</li>
<li>Pre-configured prompts for quick and easy testing</li>
</ul>
<p><strong>Details:</strong></p>
<ul>
<li>
<p><strong>Parameterization</strong>: Configure API keys, endpoints, models, and model parameters, as well as interaction types (chat completions, embeddings, etc.)</p>
</li>
<li>
<p><strong>Model Comparison</strong>: Compare up to 3 different models simultaneously with side-by-side response views</p>
</li>
<li>
<p><strong>Comparison Metrics</strong>: View detailed comparison information including:</p>
<ul>
<li>Time To First Token</li>
<li>Input / Output / Reasoning Tokens</li>
<li>Total Latency</li>
<li>Cost (if enabled in config)</li>
</ul>
</li>
<li>
<p><strong>Safety Filters</strong>: Configure and test guardrails (safety filters) directly in the playground interface</p>
</li>
</ul>
<p><a href="/docs/proxy/model_compare_ui">Get Started with Model Compare</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-providers-and-endpoints">New Providers and Endpoints<a href="#new-providers-and-endpoints" class="hash-link" aria-label="New Providers and Endpoints" title="New Providers and Endpoints"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="new-providers">New Providers<a href="#new-providers" class="hash-link" aria-label="New Providers" title="New Providers"></a></h3>
<table><thead><tr><th>Provider</th><th>Supported Endpoints</th><th>Description</th></tr></thead><tbody><tr><td><strong><a href="/docs/providers/docker_model_runner">Docker Model Runner</a></strong></td><td><code>/v1/chat/completions</code></td><td>Run LLM models in Docker containers</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-model-support">New Model Support<a href="#new-model-support" class="hash-link" aria-label="New Model Support" title="New Model Support"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Features</th></tr></thead><tbody><tr><td>Azure</td><td><code>azure/gpt-5.1</code></td><td>272K</td><td>$1.38</td><td>$11.00</td><td>Reasoning, vision, PDF input, responses API</td></tr><tr><td>Azure</td><td><code>azure/gpt-5.1-2025-11-13</code></td><td>272K</td><td>$1.38</td><td>$11.00</td><td>Reasoning, vision, PDF input, responses API</td></tr><tr><td>Azure</td><td><code>azure/gpt-5.1-codex</code></td><td>272K</td><td>$1.38</td><td>$11.00</td><td>Responses API, reasoning, vision</td></tr><tr><td>Azure</td><td><code>azure/gpt-5.1-codex-2025-11-13</code></td><td>272K</td><td>$1.38</td><td>$11.00</td><td>Responses API, reasoning, vision</td></tr><tr><td>Azure</td><td><code>azure/gpt-5.1-codex-mini</code></td><td>272K</td><td>$0.275</td><td>$2.20</td><td>Responses API, reasoning, vision</td></tr><tr><td>Azure</td><td><code>azure/gpt-5.1-codex-mini-2025-11-13</code></td><td>272K</td><td>$0.275</td><td>$2.20</td><td>Responses API, reasoning, vision</td></tr><tr><td>Azure EU</td><td><code>azure/eu/gpt-5-2025-08-07</code></td><td>272K</td><td>$1.375</td><td>$11.00</td><td>Reasoning, vision, PDF input</td></tr><tr><td>Azure EU</td><td><code>azure/eu/gpt-5-mini-2025-08-07</code></td><td>272K</td><td>$0.275</td><td>$2.20</td><td>Reasoning, vision, PDF input</td></tr><tr><td>Azure EU</td><td><code>azure/eu/gpt-5-nano-2025-08-07</code></td><td>272K</td><td>$0.055</td><td>$0.44</td><td>Reasoning, vision, PDF input</td></tr><tr><td>Azure EU</td><td><code>azure/eu/gpt-5.1</code></td><td>272K</td><td>$1.38</td><td>$11.00</td><td>Reasoning, vision, PDF input, responses API</td></tr><tr><td>Azure EU</td><td><code>azure/eu/gpt-5.1-codex</code></td><td>272K</td><td>$1.38</td><td>$11.00</td><td>Responses API, reasoning, vision</td></tr><tr><td>Azure EU</td><td><code>azure/eu/gpt-5.1-codex-mini</code></td><td>272K</td><td>$0.275</td><td>$2.20</td><td>Responses API, reasoning, vision</td></tr><tr><td>Gemini</td><td><code>gemini-3-pro-preview</code></td><td>2M</td><td>$1.25</td><td>$5.00</td><td>Reasoning, vision, function calling</td></tr><tr><td>Gemini</td><td><code>gemini-3-pro-image</code></td><td>2M</td><td>$1.25</td><td>$5.00</td><td>Image generation, reasoning</td></tr><tr><td>OpenRouter</td><td><code>openrouter/deepseek/deepseek-v3p1-terminus</code></td><td>164K</td><td>$0.20</td><td>$0.40</td><td>Function calling, reasoning</td></tr><tr><td>OpenRouter</td><td><code>openrouter/moonshot/kimi-k2-instruct</code></td><td>262K</td><td>$0.60</td><td>$2.50</td><td>Function calling, web search</td></tr><tr><td>OpenRouter</td><td><code>openrouter/gemini/gemini-3-pro-preview</code></td><td>2M</td><td>$1.25</td><td>$5.00</td><td>Reasoning, vision, function calling</td></tr><tr><td>XAI</td><td><code>xai/grok-4.1-fast</code></td><td>2M</td><td>$0.20</td><td>$0.50</td><td>Reasoning, function calling</td></tr><tr><td>Together AI</td><td><code>together_ai/z-ai/glm-4.6</code></td><td>203K</td><td>$0.40</td><td>$1.75</td><td>Function calling, reasoning</td></tr><tr><td>Cerebras</td><td><code>cerebras/gpt-oss-120b</code></td><td>131K</td><td>$0.60</td><td>$0.60</td><td>Function calling</td></tr><tr><td>Bedrock</td><td><code>anthropic.claude-sonnet-4-5-20250929-v1:0</code></td><td>200K</td><td>$3.00</td><td>$15.00</td><td>Computer use, reasoning, vision</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/providers/gemini">Gemini (Google AI Studio + Vertex AI)</a></strong></p>
<ul>
<li>Add Day 0 gemini-3-pro-preview support - <a href="https://github.com/BerriAI/litellm/pull/16719" target="_blank" rel="noopener noreferrer">PR #16719</a></li>
<li>Add support for Gemini 3 Pro Image model - <a href="https://github.com/BerriAI/litellm/pull/16938" target="_blank" rel="noopener noreferrer">PR #16938</a></li>
<li>Add reasoning_content to streaming responses with tools enabled - <a href="https://github.com/BerriAI/litellm/pull/16854" target="_blank" rel="noopener noreferrer">PR #16854</a></li>
<li>Add includeThoughts=True for Gemini 3 reasoning_effort - <a href="https://github.com/BerriAI/litellm/pull/16838" target="_blank" rel="noopener noreferrer">PR #16838</a></li>
<li>Support thought signatures for Gemini 3 in responses API - <a href="https://github.com/BerriAI/litellm/pull/16872" target="_blank" rel="noopener noreferrer">PR #16872</a></li>
<li>Correct wrong system message handling for gemma - <a href="https://github.com/BerriAI/litellm/pull/16767" target="_blank" rel="noopener noreferrer">PR #16767</a></li>
<li>Gemini 3 Pro Image: capture image_tokens and support cost_per_output_image - <a href="https://github.com/BerriAI/litellm/pull/16912" target="_blank" rel="noopener noreferrer">PR #16912</a></li>
<li>Fix missing costs for gemini-2.5-flash-image - <a href="https://github.com/BerriAI/litellm/pull/16882" target="_blank" rel="noopener noreferrer">PR #16882</a></li>
<li>Gemini 3 thought signatures in tool call id - <a href="https://github.com/BerriAI/litellm/pull/16895" target="_blank" rel="noopener noreferrer">PR #16895</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/azure">Azure</a></strong></p>
<ul>
<li>Add azure gpt-5.1 models - <a href="https://github.com/BerriAI/litellm/pull/16817" target="_blank" rel="noopener noreferrer">PR #16817</a></li>
<li>Add Azure models 2025 11 to cost maps - <a href="https://github.com/BerriAI/litellm/pull/16762" target="_blank" rel="noopener noreferrer">PR #16762</a></li>
<li>Update Azure Pricing - <a href="https://github.com/BerriAI/litellm/pull/16371" target="_blank" rel="noopener noreferrer">PR #16371</a></li>
<li>Add SSML Support for Azure Text-to-Speech (AVA) - <a href="https://github.com/BerriAI/litellm/pull/16747" target="_blank" rel="noopener noreferrer">PR #16747</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/openai">OpenAI</a></strong></p>
<ul>
<li>Support GPT-5.1 reasoning.effort=&#x27;none&#x27; in proxy - <a href="https://github.com/BerriAI/litellm/pull/16745" target="_blank" rel="noopener noreferrer">PR #16745</a></li>
<li>Add gpt-5.1-codex and gpt-5.1-codex-mini models to documentation - <a href="https://github.com/BerriAI/litellm/pull/16735" target="_blank" rel="noopener noreferrer">PR #16735</a></li>
<li>Inherit BaseVideoConfig to enable async content response for OpenAI video - <a href="https://github.com/BerriAI/litellm/pull/16708" target="_blank" rel="noopener noreferrer">PR #16708</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/anthropic">Anthropic</a></strong></p>
<ul>
<li>Add support for <code>strict</code> parameter in Anthropic tool schemas - <a href="https://github.com/BerriAI/litellm/pull/16725" target="_blank" rel="noopener noreferrer">PR #16725</a></li>
<li>Add image as url support to anthropic - <a href="https://github.com/BerriAI/litellm/pull/16868" target="_blank" rel="noopener noreferrer">PR #16868</a></li>
<li>Add thought signature support to v1/messages api - <a href="https://github.com/BerriAI/litellm/pull/16812" target="_blank" rel="noopener noreferrer">PR #16812</a></li>
<li>Anthropic - support Structured Outputs <code>output_format</code> for Claude 4.5 sonnet and Opus 4.1 - <a href="https://github.com/BerriAI/litellm/pull/16949" target="_blank" rel="noopener noreferrer">PR #16949</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/bedrock">Bedrock</a></strong></p>
<ul>
<li>Haiku 4.5 correct Bedrock configs - <a href="https://github.com/BerriAI/litellm/pull/16732" target="_blank" rel="noopener noreferrer">PR #16732</a></li>
<li>Ensure consistent chunk IDs in Bedrock streaming responses - <a href="https://github.com/BerriAI/litellm/pull/16596" target="_blank" rel="noopener noreferrer">PR #16596</a></li>
<li>Add Claude 4.5 to US Gov Cloud - <a href="https://github.com/BerriAI/litellm/pull/16957" target="_blank" rel="noopener noreferrer">PR #16957</a></li>
<li>Fix images being dropped from tool results for bedrock - <a href="https://github.com/BerriAI/litellm/pull/16492" target="_blank" rel="noopener noreferrer">PR #16492</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/vertex">Vertex AI</a></strong></p>
<ul>
<li>Add Vertex AI Image Edit Support - <a href="https://github.com/BerriAI/litellm/pull/16828" target="_blank" rel="noopener noreferrer">PR #16828</a></li>
<li>Update veo 3 pricing and add prod models - <a href="https://github.com/BerriAI/litellm/pull/16781" target="_blank" rel="noopener noreferrer">PR #16781</a></li>
<li>Fix Video download for veo3 - <a href="https://github.com/BerriAI/litellm/pull/16875" target="_blank" rel="noopener noreferrer">PR #16875</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/snowflake">Snowflake</a></strong></p>
<ul>
<li>Snowflake provider support: added embeddings, PAT, account_id - <a href="https://github.com/BerriAI/litellm/pull/15727" target="_blank" rel="noopener noreferrer">PR #15727</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/oci">OCI</a></strong></p>
<ul>
<li>Add oci_endpoint_id Parameter for OCI Dedicated Endpoints - <a href="https://github.com/BerriAI/litellm/pull/16723" target="_blank" rel="noopener noreferrer">PR #16723</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/xai">XAI</a></strong></p>
<ul>
<li>Add support for Grok 4.1 Fast models - <a href="https://github.com/BerriAI/litellm/pull/16936" target="_blank" rel="noopener noreferrer">PR #16936</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/togetherai">Together AI</a></strong></p>
<ul>
<li>Add GLM 4.6 from together.ai - <a href="https://github.com/BerriAI/litellm/pull/16942" target="_blank" rel="noopener noreferrer">PR #16942</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/cerebras">Cerebras</a></strong></p>
<ul>
<li>Fix Cerebras GPT-OSS-120B model name - <a href="https://github.com/BerriAI/litellm/pull/16939" target="_blank" rel="noopener noreferrer">PR #16939</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes">Bug Fixes<a href="#bug-fixes" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h3>
<ul>
<li>
<p><strong><a href="/docs/providers/openai">OpenAI</a></strong></p>
<ul>
<li>Fix for 16863 - openai conversion from responses to completions - <a href="https://github.com/BerriAI/litellm/pull/16864" target="_blank" rel="noopener noreferrer">PR #16864</a></li>
<li>Revert &quot;Make all gpt-5 and reasoning models to responses by default&quot; - <a href="https://github.com/BerriAI/litellm/pull/16849" target="_blank" rel="noopener noreferrer">PR #16849</a></li>
</ul>
</li>
<li>
<p><strong>General</strong></p>
<ul>
<li>Get custom_llm_provider from query param - <a href="https://github.com/BerriAI/litellm/pull/16731" target="_blank" rel="noopener noreferrer">PR #16731</a></li>
<li>Fix optional param mapping - <a href="https://github.com/BerriAI/litellm/pull/16852" target="_blank" rel="noopener noreferrer">PR #16852</a></li>
<li>Add None check for litellm_params - <a href="https://github.com/BerriAI/litellm/pull/16754" target="_blank" rel="noopener noreferrer">PR #16754</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/response_api">Responses API</a></strong></p>
<ul>
<li>Add Responses API support for gpt-5.1-codex model - <a href="https://github.com/BerriAI/litellm/pull/16845" target="_blank" rel="noopener noreferrer">PR #16845</a></li>
<li>Add managed files support for responses API - <a href="https://github.com/BerriAI/litellm/pull/16733" target="_blank" rel="noopener noreferrer">PR #16733</a></li>
<li>Add extra_body support for response supported api params from chat completion - <a href="https://github.com/BerriAI/litellm/pull/16765" target="_blank" rel="noopener noreferrer">PR #16765</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/batches">Batch API</a></strong></p>
<ul>
<li>Support /delete for files + support /cancel for batches - <a href="https://github.com/BerriAI/litellm/pull/16387" target="_blank" rel="noopener noreferrer">PR #16387</a></li>
<li>Add config based routing support for batches and files - <a href="https://github.com/BerriAI/litellm/pull/16872" target="_blank" rel="noopener noreferrer">PR #16872</a></li>
<li>Populate spend_logs_metadata in batch and files endpoints - <a href="https://github.com/BerriAI/litellm/pull/16921" target="_blank" rel="noopener noreferrer">PR #16921</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/search">Search APIs</a></strong></p>
<ul>
<li>Search APIs - error in firecrawl-search &quot;Invalid request body&quot; - <a href="https://github.com/BerriAI/litellm/pull/16943" target="_blank" rel="noopener noreferrer">PR #16943</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/vector_stores">Vector Stores</a></strong></p>
<ul>
<li>Fix vector store create issue - <a href="https://github.com/BerriAI/litellm/pull/16804" target="_blank" rel="noopener noreferrer">PR #16804</a></li>
<li>Team vector-store permissions now respected for key access - <a href="https://github.com/BerriAI/litellm/pull/16639" target="_blank" rel="noopener noreferrer">PR #16639</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/audio_transcription">Audio Transcription</a></strong></p>
<ul>
<li>Fix audio transcription cost tracking - <a href="https://github.com/BerriAI/litellm/pull/16478" target="_blank" rel="noopener noreferrer">PR #16478</a></li>
<li>Add missing shared_sessions to audio/transcriptions - <a href="https://github.com/BerriAI/litellm/pull/16858" target="_blank" rel="noopener noreferrer">PR #16858</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/video_generation">Video Generation API</a></strong></p>
<ul>
<li>Fix videos tagging - <a href="https://github.com/BerriAI/litellm/pull/16770" target="_blank" rel="noopener noreferrer">PR #16770</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>General</strong>
<ul>
<li>Responses API cost tracking with custom deployment names - <a href="https://github.com/BerriAI/litellm/pull/16778" target="_blank" rel="noopener noreferrer">PR #16778</a></li>
<li>Trim logged response strings in spend-logs - <a href="https://github.com/BerriAI/litellm/pull/16654" target="_blank" rel="noopener noreferrer">PR #16654</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong>Proxy CLI Auth</strong></p>
<ul>
<li>Allow using JWTs for signing in with Proxy CLI - <a href="https://github.com/BerriAI/litellm/pull/16756" target="_blank" rel="noopener noreferrer">PR #16756</a></li>
</ul>
</li>
<li>
<p><strong>Virtual Keys</strong></p>
<ul>
<li>Fix Key Model Alias Not Working - <a href="https://github.com/BerriAI/litellm/pull/16896" target="_blank" rel="noopener noreferrer">PR #16896</a></li>
</ul>
</li>
<li>
<p><strong>Models + Endpoints</strong></p>
<ul>
<li>Add additional model settings to chat models in test key - <a href="https://github.com/BerriAI/litellm/pull/16793" target="_blank" rel="noopener noreferrer">PR #16793</a></li>
<li>Deactivate delete button on model table for config models - <a href="https://github.com/BerriAI/litellm/pull/16787" target="_blank" rel="noopener noreferrer">PR #16787</a></li>
<li>Change Public Model Hub to use proxyBaseUrl - <a href="https://github.com/BerriAI/litellm/pull/16892" target="_blank" rel="noopener noreferrer">PR #16892</a></li>
<li>Add JSON Viewer to request/response panel - <a href="https://github.com/BerriAI/litellm/pull/16687" target="_blank" rel="noopener noreferrer">PR #16687</a></li>
<li>Standarize icon images - <a href="https://github.com/BerriAI/litellm/pull/16837" target="_blank" rel="noopener noreferrer">PR #16837</a></li>
</ul>
</li>
<li>
<p><strong>Teams</strong></p>
<ul>
<li>Teams table empty state - <a href="https://github.com/BerriAI/litellm/pull/16738" target="_blank" rel="noopener noreferrer">PR #16738</a></li>
</ul>
</li>
<li>
<p><strong>Fallbacks</strong></p>
<ul>
<li>Fallbacks icon button tooltips and delete with friction - <a href="https://github.com/BerriAI/litellm/pull/16737" target="_blank" rel="noopener noreferrer">PR #16737</a></li>
</ul>
</li>
<li>
<p><strong>MCP Servers</strong></p>
<ul>
<li>Delete user and MCP Server Modal, MCP Table Tooltips - <a href="https://github.com/BerriAI/litellm/pull/16751" target="_blank" rel="noopener noreferrer">PR #16751</a></li>
</ul>
</li>
<li>
<p><strong>Callbacks</strong></p>
<ul>
<li>Expose backend endpoint for callbacks settings - <a href="https://github.com/BerriAI/litellm/pull/16698" target="_blank" rel="noopener noreferrer">PR #16698</a></li>
<li>Edit add callbacks route to use data from backend - <a href="https://github.com/BerriAI/litellm/pull/16699" target="_blank" rel="noopener noreferrer">PR #16699</a></li>
</ul>
</li>
<li>
<p><strong>Usage &amp; Analytics</strong></p>
<ul>
<li>Allow partial matches for user ID in User Table - <a href="https://github.com/BerriAI/litellm/pull/16952" target="_blank" rel="noopener noreferrer">PR #16952</a></li>
</ul>
</li>
<li>
<p><strong>General UI</strong></p>
<ul>
<li>Allow setting base_url in API reference docs - <a href="https://github.com/BerriAI/litellm/pull/16674" target="_blank" rel="noopener noreferrer">PR #16674</a></li>
<li>Change /public fields to honor server root path - <a href="https://github.com/BerriAI/litellm/pull/16930" target="_blank" rel="noopener noreferrer">PR #16930</a></li>
<li>Correct ui build - <a href="https://github.com/BerriAI/litellm/pull/16702" target="_blank" rel="noopener noreferrer">PR #16702</a></li>
<li>Enable automatic dark/light mode based on system preference - <a href="https://github.com/BerriAI/litellm/pull/16748" target="_blank" rel="noopener noreferrer">PR #16748</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li>
<p><strong>UI Fixes</strong></p>
<ul>
<li>Fix flaky tests due to antd Notification Manager - <a href="https://github.com/BerriAI/litellm/pull/16740" target="_blank" rel="noopener noreferrer">PR #16740</a></li>
<li>Fix UI MCP Tool Test Regression - <a href="https://github.com/BerriAI/litellm/pull/16695" target="_blank" rel="noopener noreferrer">PR #16695</a></li>
<li>Fix edit logging settings not appearing - <a href="https://github.com/BerriAI/litellm/pull/16798" target="_blank" rel="noopener noreferrer">PR #16798</a></li>
<li>Add css to truncate long request ids in request viewer - <a href="https://github.com/BerriAI/litellm/pull/16665" target="_blank" rel="noopener noreferrer">PR #16665</a></li>
<li>Remove azure/ prefix in Placeholder for Azure in Add Model - <a href="https://github.com/BerriAI/litellm/pull/16597" target="_blank" rel="noopener noreferrer">PR #16597</a></li>
<li>Remove UI Session Token from user/info return - <a href="https://github.com/BerriAI/litellm/pull/16851" target="_blank" rel="noopener noreferrer">PR #16851</a></li>
<li>Remove console logs and errors from model tab - <a href="https://github.com/BerriAI/litellm/pull/16455" target="_blank" rel="noopener noreferrer">PR #16455</a></li>
<li>Change Bulk Invite User Roles to Match Backend - <a href="https://github.com/BerriAI/litellm/pull/16906" target="_blank" rel="noopener noreferrer">PR #16906</a></li>
<li>Mock Tremor&#x27;s Tooltip to Fix Flaky UI Tests - <a href="https://github.com/BerriAI/litellm/pull/16786" target="_blank" rel="noopener noreferrer">PR #16786</a></li>
<li>Fix e2e ui playwright test - <a href="https://github.com/BerriAI/litellm/pull/16799" target="_blank" rel="noopener noreferrer">PR #16799</a></li>
<li>Fix Tests in CI/CD - <a href="https://github.com/BerriAI/litellm/pull/16972" target="_blank" rel="noopener noreferrer">PR #16972</a></li>
</ul>
</li>
<li>
<p><strong>SSO</strong></p>
<ul>
<li>Ensure <code>role</code> from SSO provider is used when a user is inserted onto LiteLLM - <a href="https://github.com/BerriAI/litellm/pull/16794" target="_blank" rel="noopener noreferrer">PR #16794</a></li>
<li>Docs - SSO - Manage User Roles via Azure App Roles - <a href="https://github.com/BerriAI/litellm/pull/16796" target="_blank" rel="noopener noreferrer">PR #16796</a></li>
</ul>
</li>
<li>
<p><strong>Auth</strong></p>
<ul>
<li>Ensure Team Tags works when using JWT Auth - <a href="https://github.com/BerriAI/litellm/pull/16797" target="_blank" rel="noopener noreferrer">PR #16797</a></li>
<li>Fix key never expires - <a href="https://github.com/BerriAI/litellm/pull/16692" target="_blank" rel="noopener noreferrer">PR #16692</a></li>
</ul>
</li>
<li>
<p><strong>Swagger UI</strong></p>
<ul>
<li>Fixes Swagger UI resolver errors for chat completion endpoints caused by Pydantic v2 <code>$defs</code> not being properly exposed in the OpenAPI schema - <a href="https://github.com/BerriAI/litellm/pull/16784" target="_blank" rel="noopener noreferrer">PR #16784</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ai-integrations">AI Integrations<a href="#ai-integrations" class="hash-link" aria-label="AI Integrations" title="AI Integrations"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="logging">Logging<a href="#logging" class="hash-link" aria-label="Logging" title="Logging"></a></h3>
<ul>
<li>
<p><strong><a href="/docs/observability/arize_phoenix">Arize Phoenix</a></strong></p>
<ul>
<li>Fix arize phoenix logging - <a href="https://github.com/BerriAI/litellm/pull/16301" target="_blank" rel="noopener noreferrer">PR #16301</a></li>
<li>Arize Phoenix - root span logging - <a href="https://github.com/BerriAI/litellm/pull/16949" target="_blank" rel="noopener noreferrer">PR #16949</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/logging#langfuse">Langfuse</a></strong></p>
<ul>
<li>Filter secret fields form Langfuse - <a href="https://github.com/BerriAI/litellm/pull/16842" target="_blank" rel="noopener noreferrer">PR #16842</a></li>
</ul>
</li>
<li>
<p><strong>General</strong></p>
<ul>
<li>Exclude litellm_credential_name from Sensitive Data Masker (Updated) - <a href="https://github.com/BerriAI/litellm/pull/16958" target="_blank" rel="noopener noreferrer">PR #16958</a></li>
<li>Allow admins to disable, dynamic callback controls - <a href="https://github.com/BerriAI/litellm/pull/16750" target="_blank" rel="noopener noreferrer">PR #16750</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="guardrails">Guardrails<a href="#guardrails" class="hash-link" aria-label="Guardrails" title="Guardrails"></a></h3>
<ul>
<li>
<p><strong><a href="/docs/proxy/guardrails">IBM Guardrails</a></strong></p>
<ul>
<li>Fix IBM Guardrails optional params, add extra_headers field - <a href="https://github.com/BerriAI/litellm/pull/16771" target="_blank" rel="noopener noreferrer">PR #16771</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/guardrails">Noma Guardrail</a></strong></p>
<ul>
<li>Use LiteLLM key alias as fallback Noma applicationId in NomaGuardrail - <a href="https://github.com/BerriAI/litellm/pull/16832" target="_blank" rel="noopener noreferrer">PR #16832</a></li>
<li>Allow custom violation message for tool-permission guardrail - <a href="https://github.com/BerriAI/litellm/pull/16916" target="_blank" rel="noopener noreferrer">PR #16916</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/guardrails">Grayswan Guardrail</a></strong></p>
<ul>
<li>Grayswan guardrail passthrough on flagged - <a href="https://github.com/BerriAI/litellm/pull/16891" target="_blank" rel="noopener noreferrer">PR #16891</a></li>
</ul>
</li>
<li>
<p><strong>General Guardrails</strong></p>
<ul>
<li>Fix prompt injection not working - <a href="https://github.com/BerriAI/litellm/pull/16701" target="_blank" rel="noopener noreferrer">PR #16701</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="prompt-management-1">Prompt Management<a href="#prompt-management-1" class="hash-link" aria-label="Prompt Management" title="Prompt Management"></a></h3>
<ul>
<li><strong><a href="/docs/proxy/prompt_management">Prompt Management</a></strong>
<ul>
<li>Allow specifying just prompt_id in a request to a model - <a href="https://github.com/BerriAI/litellm/pull/16834" target="_blank" rel="noopener noreferrer">PR #16834</a></li>
<li>Add support for versioning prompts - <a href="https://github.com/BerriAI/litellm/pull/16836" target="_blank" rel="noopener noreferrer">PR #16836</a></li>
<li>Allow storing prompt version in DB - <a href="https://github.com/BerriAI/litellm/pull/16848" target="_blank" rel="noopener noreferrer">PR #16848</a></li>
<li>Add UI for editing the prompts - <a href="https://github.com/BerriAI/litellm/pull/16853" target="_blank" rel="noopener noreferrer">PR #16853</a></li>
<li>Allow testing prompts with Chat UI - <a href="https://github.com/BerriAI/litellm/pull/16898" target="_blank" rel="noopener noreferrer">PR #16898</a></li>
<li>Allow viewing version history - <a href="https://github.com/BerriAI/litellm/pull/16901" target="_blank" rel="noopener noreferrer">PR #16901</a></li>
<li>Allow specifying prompt version in code - <a href="https://github.com/BerriAI/litellm/pull/16929" target="_blank" rel="noopener noreferrer">PR #16929</a></li>
<li>UI, allow seeing model, prompt id for Prompt - <a href="https://github.com/BerriAI/litellm/pull/16932" target="_blank" rel="noopener noreferrer">PR #16932</a></li>
<li>Show &quot;get code&quot; section for prompt management + minor polish of showing version history - <a href="https://github.com/BerriAI/litellm/pull/16941" target="_blank" rel="noopener noreferrer">PR #16941</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="secret-managers">Secret Managers<a href="#secret-managers" class="hash-link" aria-label="Secret Managers" title="Secret Managers"></a></h3>
<ul>
<li><strong><a href="/docs/secret_managers">AWS Secrets Manager</a></strong>
<ul>
<li>Adds IAM role assumption support for AWS Secret Manager - <a href="https://github.com/BerriAI/litellm/pull/16887" target="_blank" rel="noopener noreferrer">PR #16887</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway">MCP Gateway<a href="#mcp-gateway" class="hash-link" aria-label="MCP Gateway" title="MCP Gateway"></a></h2>
<ul>
<li><strong>MCP Hub</strong> - Publish/discover MCP Servers within a company - <a href="https://github.com/BerriAI/litellm/pull/16857" target="_blank" rel="noopener noreferrer">PR #16857</a></li>
<li><strong>MCP Resources</strong> - MCP resources support - <a href="https://github.com/BerriAI/litellm/pull/16800" target="_blank" rel="noopener noreferrer">PR #16800</a></li>
<li><strong>MCP OAuth</strong> - Docs - mcp oauth flow details - <a href="https://github.com/BerriAI/litellm/pull/16742" target="_blank" rel="noopener noreferrer">PR #16742</a></li>
<li><strong>MCP Lifecycle</strong> - Drop MCPClient.connect and use run_with_session lifecycle - <a href="https://github.com/BerriAI/litellm/pull/16696" target="_blank" rel="noopener noreferrer">PR #16696</a></li>
<li><strong>MCP Server IDs</strong> - Add mcp server ids - <a href="https://github.com/BerriAI/litellm/pull/16904" target="_blank" rel="noopener noreferrer">PR #16904</a></li>
<li><strong>MCP URL Format</strong> - Fix mcp url format - <a href="https://github.com/BerriAI/litellm/pull/16940" target="_blank" rel="noopener noreferrer">PR #16940</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<ul>
<li><strong>Realtime Endpoint Performance</strong> - Fix bottlenecks degrading realtime endpoint performance - <a href="https://github.com/BerriAI/litellm/pull/16670" target="_blank" rel="noopener noreferrer">PR #16670</a></li>
<li><strong>SSL Context Caching</strong> - Cache SSL contexts to prevent excessive memory allocation - <a href="https://github.com/BerriAI/litellm/pull/16955" target="_blank" rel="noopener noreferrer">PR #16955</a></li>
<li><strong>Cache Optimization</strong> - Fix cache cooldown key generation - <a href="https://github.com/BerriAI/litellm/pull/16954" target="_blank" rel="noopener noreferrer">PR #16954</a></li>
<li><strong>Router Cache</strong> - Fix routing for requests with same cacheable prefix but different user messages - <a href="https://github.com/BerriAI/litellm/pull/16951" target="_blank" rel="noopener noreferrer">PR #16951</a></li>
<li><strong>Redis Event Loop</strong> - Fix redis event loop closed at first call - <a href="https://github.com/BerriAI/litellm/pull/16913" target="_blank" rel="noopener noreferrer">PR #16913</a></li>
<li><strong>Dependency Management</strong> - Upgrade pydantic to version 2.11.0 - <a href="https://github.com/BerriAI/litellm/pull/16909" target="_blank" rel="noopener noreferrer">PR #16909</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="documentation-updates">Documentation Updates<a href="#documentation-updates" class="hash-link" aria-label="Documentation Updates" title="Documentation Updates"></a></h2>
<ul>
<li>
<p><strong>Provider Documentation</strong></p>
<ul>
<li>Add missing details to benchmark comparison - <a href="https://github.com/BerriAI/litellm/pull/16690" target="_blank" rel="noopener noreferrer">PR #16690</a></li>
<li>Fix anthropic pass-through endpoint - <a href="https://github.com/BerriAI/litellm/pull/16883" target="_blank" rel="noopener noreferrer">PR #16883</a></li>
<li>Cleanup repo and improve AI docs - <a href="https://github.com/BerriAI/litellm/pull/16775" target="_blank" rel="noopener noreferrer">PR #16775</a></li>
</ul>
</li>
<li>
<p><strong>API Documentation</strong></p>
<ul>
<li>Add docs related to openai metadata - <a href="https://github.com/BerriAI/litellm/pull/16872" target="_blank" rel="noopener noreferrer">PR #16872</a></li>
<li>Update docs with all supported endpoints and cost tracking - <a href="https://github.com/BerriAI/litellm/pull/16872" target="_blank" rel="noopener noreferrer">PR #16872</a></li>
</ul>
</li>
<li>
<p><strong>General Documentation</strong></p>
<ul>
<li>Add mini-swe-agent to Projects built on LiteLLM - <a href="https://github.com/BerriAI/litellm/pull/16971" target="_blank" rel="noopener noreferrer">PR #16971</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="infrastructure--cicd">Infrastructure / CI/CD<a href="#infrastructure--cicd" class="hash-link" aria-label="Infrastructure / CI/CD" title="Infrastructure / CI/CD"></a></h2>
<ul>
<li>
<p><strong>UI Testing</strong></p>
<ul>
<li>Break e2e_ui_testing into build, unit, and e2e steps - <a href="https://github.com/BerriAI/litellm/pull/16783" target="_blank" rel="noopener noreferrer">PR #16783</a></li>
<li>Building UI for Testing - <a href="https://github.com/BerriAI/litellm/pull/16968" target="_blank" rel="noopener noreferrer">PR #16968</a></li>
<li>CI/CD Fixes - <a href="https://github.com/BerriAI/litellm/pull/16937" target="_blank" rel="noopener noreferrer">PR #16937</a></li>
</ul>
</li>
<li>
<p><strong>Dependency Management</strong></p>
<ul>
<li>Bump js-yaml from 3.14.1 to 3.14.2 in /tests/proxy_admin_ui_tests/ui_unit_tests - <a href="https://github.com/BerriAI/litellm/pull/16755" target="_blank" rel="noopener noreferrer">PR #16755</a></li>
<li>Bump js-yaml from 3.14.1 to 3.14.2 - <a href="https://github.com/BerriAI/litellm/pull/16802" target="_blank" rel="noopener noreferrer">PR #16802</a></li>
</ul>
</li>
<li>
<p><strong>Migration</strong></p>
<ul>
<li>Migration job labels - <a href="https://github.com/BerriAI/litellm/pull/16831" target="_blank" rel="noopener noreferrer">PR #16831</a></li>
</ul>
</li>
<li>
<p><strong>Config</strong></p>
<ul>
<li>This yaml actually works - <a href="https://github.com/BerriAI/litellm/pull/16757" target="_blank" rel="noopener noreferrer">PR #16757</a></li>
</ul>
</li>
<li>
<p><strong>Release Notes</strong></p>
<ul>
<li>Add perf improvements on embeddings to release notes - <a href="https://github.com/BerriAI/litellm/pull/16697" target="_blank" rel="noopener noreferrer">PR #16697</a></li>
<li>Docs - v1.80.0 - <a href="https://github.com/BerriAI/litellm/pull/16694" target="_blank" rel="noopener noreferrer">PR #16694</a></li>
</ul>
</li>
<li>
<p><strong>Investigation</strong></p>
<ul>
<li>Investigate issue root cause - <a href="https://github.com/BerriAI/litellm/pull/16859" target="_blank" rel="noopener noreferrer">PR #16859</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@mattmorgis made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16371" target="_blank" rel="noopener noreferrer">PR #16371</a></li>
<li>@mmandic-coatue made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16732" target="_blank" rel="noopener noreferrer">PR #16732</a></li>
<li>@Bradley-Butcher made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16725" target="_blank" rel="noopener noreferrer">PR #16725</a></li>
<li>@BenjaminLevy made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16757" target="_blank" rel="noopener noreferrer">PR #16757</a></li>
<li>@CatBraaain made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16767" target="_blank" rel="noopener noreferrer">PR #16767</a></li>
<li>@tushar8408 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16831" target="_blank" rel="noopener noreferrer">PR #16831</a></li>
<li>@nbsp1221 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16845" target="_blank" rel="noopener noreferrer">PR #16845</a></li>
<li>@idola9 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16832" target="_blank" rel="noopener noreferrer">PR #16832</a></li>
<li>@nkukard made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16864" target="_blank" rel="noopener noreferrer">PR #16864</a></li>
<li>@alhuang10 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16852" target="_blank" rel="noopener noreferrer">PR #16852</a></li>
<li>@sebslight made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16838" target="_blank" rel="noopener noreferrer">PR #16838</a></li>
<li>@TsurumaruTsuyoshi made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16905" target="_blank" rel="noopener noreferrer">PR #16905</a></li>
<li>@cyberjunk made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16492" target="_blank" rel="noopener noreferrer">PR #16492</a></li>
<li>@colinlin-stripe made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16895" target="_blank" rel="noopener noreferrer">PR #16895</a></li>
<li>@sureshdsk made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16883" target="_blank" rel="noopener noreferrer">PR #16883</a></li>
<li>@eiliyaabedini made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16875" target="_blank" rel="noopener noreferrer">PR #16875</a></li>
<li>@justin-tahara made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16957" target="_blank" rel="noopener noreferrer">PR #16957</a></li>
<li>@wangsoft made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16913" target="_blank" rel="noopener noreferrer">PR #16913</a></li>
<li>@dsduenas made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16891" target="_blank" rel="noopener noreferrer">PR #16891</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="known-issues">Known Issues<a href="#known-issues" class="hash-link" aria-label="Known Issues" title="Known Issues"></a></h2>
<ul>
<li><code>/audit</code> and <code>/user/available_users</code> routes return 404. Fixed in <a href="https://github.com/BerriAI/litellm/pull/17337" target="_blank" rel="noopener noreferrer">PR #17337</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog">Full Changelog<a href="#full-changelog" class="hash-link" aria-label="Full Changelog" title="Full Changelog"></a></h2>
<p><strong><a href="https://github.com/BerriAI/litellm/compare/v1.80.0-nightly...v1.80.5.rc.2" target="_blank" rel="noopener noreferrer">View complete changelog on GitHub</a></strong></p></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-80-0">v1.80.0-stable - Introducing Agent Hub: Register, Publish, and Share Agents</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-11-15T10:00:00.000Z">20251115</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaff"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaff</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.80.0-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.80.0</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong> Agent Hub Support</strong> - Register and make agents public for your organization</li>
<li><strong>RunwayML Provider</strong> - Complete video generation, image generation, and text-to-speech support</li>
<li><strong>GPT-5.1 Family Support</strong> - Day-0 support for OpenAI&#x27;s latest GPT-5.1 and GPT-5.1-Codex models</li>
<li><strong>Prometheus OSS</strong> - Prometheus metrics now available in open-source version</li>
<li><strong>Vector Store Files API</strong> - Complete OpenAI-compatible Vector Store Files API with full CRUD operations</li>
<li><strong>Embeddings Performance</strong> - O(1) lookup optimization for router embeddings with shared sessions</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="agent-hub">Agent Hub<a href="#agent-hub" class="hash-link" aria-label="Agent Hub" title="Agent Hub"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAbklEQVR4nE2N4QoAIQiDe//XDIKgH0GoWdqOujtI+BDd2ELOGTFGEBFEBCyCMQbcDHYRSilIKaHWitYaiBi9K9wd/6y1ELZ7P/eec55U6f2IN0FV8bMrmRmiA74A/0zmjrDFG5YOYoWZn3vOt/EB/L7DnNBwMoAAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/agent_hub_clean.26a5a7f.640.png" srcset="/assets/ideal-img/agent_hub_clean.26a5a7f.640.png 640w,/assets/ideal-img/agent_hub_clean.2a6b3b3.1920.png 1920w" width="640" height="334"></noscript></div>
<p>This release adds support for registering and making agents public for your organization. This is great for <strong>Proxy Admins</strong> who want a central place to make agents built in their organization, discoverable to their users.</p>
<p>Here&#x27;s the flow:</p>
<ol>
<li>Add agent to litellm.</li>
<li>Make it public.</li>
<li>Allow anyone to discover it on the public AI Hub page.</li>
</ol>
<p><a href="/docs/proxy/ai_hub"><strong>Get Started with Agent Hub</strong></a></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="performance--embeddings-13-lower-p95-latency">Performance  <code>/embeddings</code> 13 Lower p95 Latency<a href="#performance--embeddings-13-lower-p95-latency" class="hash-link" aria-label="performance--embeddings-13-lower-p95-latency" title="performance--embeddings-13-lower-p95-latency"></a></h3>
<p>This update significantly improves <code>/embeddings</code> latency by routing it through the same optimized pipeline as <code>/chat/completions</code>, benefiting from all previously applied networking optimizations.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="results">Results<a href="#results" class="hash-link" aria-label="Results" title="Results"></a></h3>
<table><thead><tr><th>Metric</th><th>Before</th><th>After</th><th>Improvement</th></tr></thead><tbody><tr><td>p95 latency</td><td>5,700 ms</td><td><strong>430 ms</strong></td><td>92% (~13 faster)**</td></tr><tr><td>p99 latency</td><td>7,200 ms</td><td><strong>780 ms</strong></td><td>89%</td></tr><tr><td>Average latency</td><td>844 ms</td><td><strong>262 ms</strong></td><td>69%</td></tr><tr><td>Median latency</td><td>290 ms</td><td><strong>230 ms</strong></td><td>21%</td></tr><tr><td>RPS</td><td>1,216.7</td><td><strong>1,219.7</strong></td><td><strong>+0.25%</strong></td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="test-setup">Test Setup<a href="#test-setup" class="hash-link" aria-label="Test Setup" title="Test Setup"></a></h3>
<table><thead><tr><th>Category</th><th>Specification</th></tr></thead><tbody><tr><td><strong>Load Testing</strong></td><td>Locust: 1,000 concurrent users, 500 ramp-up</td></tr><tr><td><strong>System</strong></td><td>4 vCPUs, 8 GB RAM, 4 workers, 4 instances</td></tr><tr><td><strong>Database</strong></td><td>PostgreSQL (Redis unused)</td></tr><tr><td><strong>Configuration</strong></td><td><a href="https://gist.github.com/AlexsanderHamir/550791675fd752befcac6a9e44024652" target="_blank" rel="noopener noreferrer">config.yaml</a></td></tr><tr><td><strong>Load Script</strong></td><td><a href="https://gist.github.com/AlexsanderHamir/99d673bf74cdd81fd39f59fa9048f2e8" target="_blank" rel="noopener noreferrer">no_cache_hits.py</a></td></tr></tbody></table>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="-runwayml"> RunwayML<a href="#-runwayml" class="hash-link" aria-label=" RunwayML" title=" RunwayML"></a></h3>
<p>Complete integration for RunwayML&#x27;s Gen-4 family of models, supporting video generation, image generation, and text-to-speech.</p>
<p><strong>Supported Endpoints:</strong></p>
<ul>
<li><code>/v1/videos</code> - Video generation (Gen-4 Turbo, Gen-4 Aleph, Gen-3A Turbo)</li>
<li><code>/v1/images/generations</code> - Image generation (Gen-4 Image, Gen-4 Image Turbo)</li>
<li><code>/v1/audio/speech</code> - Text-to-speech (ElevenLabs Multilingual v2)</li>
</ul>
<p><strong>Quick Start:</strong></p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">Generate Video with RunwayML</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv codeBlockLinesWithNumbering_o6Pm" style="counter-reset:line-count 0"><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">curl --location &#x27;http://localhost:4000/v1/videos&#x27; \</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">--header &#x27;Content-Type: application/json&#x27; \</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">--header &#x27;Authorization: Bearer sk-1234&#x27; \</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">--data &#x27;{</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    &quot;model&quot;: &quot;runwayml/gen4_turbo&quot;,</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    &quot;prompt&quot;: &quot;A high quality demo video of litellm ai gateway&quot;,</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    &quot;input_reference&quot;: &quot;https://example.com/image.jpg&quot;,</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    &quot;seconds&quot;: 5,</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    &quot;size&quot;: &quot;1280x720&quot;</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">}&#x27;</span></span><br></span></code></pre></div></div>
<p><a href="/docs/providers/runwayml/videos">Get Started with RunwayML</a></p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="prometheus-metrics---open-source">Prometheus Metrics - Open Source<a href="#prometheus-metrics---open-source" class="hash-link" aria-label="Prometheus Metrics - Open Source" title="Prometheus Metrics - Open Source"></a></h3>
<p>Prometheus metrics are now available in the open-source version of LiteLLM, providing comprehensive observability for your AI Gateway without requiring an enterprise license.</p>
<p><strong>Quick Start:</strong></p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">litellm_settings</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">success_callback</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&quot;prometheus&quot;</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">failure_callback</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&quot;prometheus&quot;</span><span class="token punctuation" style="color:#393A34">]</span><br></span></code></pre></div></div>
<p><a href="/docs/proxy/logging#prometheus">Get Started with Prometheus</a></p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="vector-store-files-api">Vector Store Files API<a href="#vector-store-files-api" class="hash-link" aria-label="Vector Store Files API" title="Vector Store Files API"></a></h3>
<p>Complete OpenAI-compatible Vector Store Files API now stable, enabling full file lifecycle management within vector stores.</p>
<p><strong>Supported Endpoints:</strong></p>
<ul>
<li><code>POST /v1/vector_stores/{vector_store_id}/files</code> - Create vector store file</li>
<li><code>GET /v1/vector_stores/{vector_store_id}/files</code> - List vector store files</li>
<li><code>GET /v1/vector_stores/{vector_store_id}/files/{file_id}</code> - Retrieve vector store file</li>
<li><code>GET /v1/vector_stores/{vector_store_id}/files/{file_id}/content</code> - Retrieve file content</li>
<li><code>DELETE /v1/vector_stores/{vector_store_id}/files/{file_id}</code> - Delete vector store file</li>
<li><code>DELETE /v1/vector_stores/{vector_store_id}</code> - Delete vector store</li>
</ul>
<p><strong>Quick Start:</strong></p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">Create Vector Store File</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv codeBlockLinesWithNumbering_o6Pm" style="counter-reset:line-count 0"><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">curl --location &#x27;http://localhost:4000/v1/vector_stores/vs_123/files&#x27; \</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">--header &#x27;Content-Type: application/json&#x27; \</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">--header &#x27;Authorization: Bearer sk-1234&#x27; \</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">--data &#x27;{</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    &quot;file_id&quot;: &quot;file_abc&quot;</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">}&#x27;</span></span><br></span></code></pre></div></div>
<p><a href="/docs/vector_store_files">Get Started with Vector Stores</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-providers-and-endpoints">New Providers and Endpoints<a href="#new-providers-and-endpoints" class="hash-link" aria-label="New Providers and Endpoints" title="New Providers and Endpoints"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="new-providers">New Providers<a href="#new-providers" class="hash-link" aria-label="New Providers" title="New Providers"></a></h3>
<table><thead><tr><th>Provider</th><th>Supported Endpoints</th><th>Description</th></tr></thead><tbody><tr><td><strong><a href="/docs/providers/runwayml/videos">RunwayML</a></strong></td><td><code>/v1/videos</code>, <code>/v1/images/generations</code>, <code>/v1/audio/speech</code></td><td>Gen-4 video generation, image generation, and text-to-speech</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="new-llm-api-endpoints">New LLM API Endpoints<a href="#new-llm-api-endpoints" class="hash-link" aria-label="New LLM API Endpoints" title="New LLM API Endpoints"></a></h3>
<table><thead><tr><th>Endpoint</th><th>Method</th><th>Description</th><th>Documentation</th></tr></thead><tbody><tr><td><code>/v1/vector_stores/{vector_store_id}/files</code></td><td>POST</td><td>Create vector store file</td><td><a href="/docs/vector_store_files">Docs</a></td></tr><tr><td><code>/v1/vector_stores/{vector_store_id}/files</code></td><td>GET</td><td>List vector store files</td><td><a href="/docs/vector_store_files">Docs</a></td></tr><tr><td><code>/v1/vector_stores/{vector_store_id}/files/{file_id}</code></td><td>GET</td><td>Retrieve vector store file</td><td><a href="/docs/vector_store_files">Docs</a></td></tr><tr><td><code>/v1/vector_stores/{vector_store_id}/files/{file_id}/content</code></td><td>GET</td><td>Retrieve file content</td><td><a href="/docs/vector_store_files">Docs</a></td></tr><tr><td><code>/v1/vector_stores/{vector_store_id}/files/{file_id}</code></td><td>DELETE</td><td>Delete vector store file</td><td><a href="/docs/vector_store_files">Docs</a></td></tr><tr><td><code>/v1/vector_stores/{vector_store_id}</code></td><td>DELETE</td><td>Delete vector store</td><td><a href="/docs/vector_store_files">Docs</a></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-model-support">New Model Support<a href="#new-model-support" class="hash-link" aria-label="New Model Support" title="New Model Support"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Features</th></tr></thead><tbody><tr><td>OpenAI</td><td><code>gpt-5.1</code></td><td>272K</td><td>$1.25</td><td>$10.00</td><td>Reasoning, vision, PDF input, responses API</td></tr><tr><td>OpenAI</td><td><code>gpt-5.1-2025-11-13</code></td><td>272K</td><td>$1.25</td><td>$10.00</td><td>Reasoning, vision, PDF input, responses API</td></tr><tr><td>OpenAI</td><td><code>gpt-5.1-chat-latest</code></td><td>128K</td><td>$1.25</td><td>$10.00</td><td>Reasoning, vision, PDF input</td></tr><tr><td>OpenAI</td><td><code>gpt-5.1-codex</code></td><td>272K</td><td>$1.25</td><td>$10.00</td><td>Responses API, reasoning, vision</td></tr><tr><td>OpenAI</td><td><code>gpt-5.1-codex-mini</code></td><td>272K</td><td>$0.25</td><td>$2.00</td><td>Responses API, reasoning, vision</td></tr><tr><td>Moonshot</td><td><code>moonshot/kimi-k2-thinking</code></td><td>262K</td><td>$0.60</td><td>$2.50</td><td>Function calling, web search, reasoning</td></tr><tr><td>Mistral</td><td><code>mistral/magistral-medium-2509</code></td><td>40K</td><td>$2.00</td><td>$5.00</td><td>Reasoning, function calling</td></tr><tr><td>Vertex AI</td><td><code>vertex_ai/moonshotai/kimi-k2-thinking-maas</code></td><td>256K</td><td>$0.60</td><td>$2.50</td><td>Function calling, web search</td></tr><tr><td>OpenRouter</td><td><code>openrouter/deepseek/deepseek-v3.2-exp</code></td><td>164K</td><td>$0.20</td><td>$0.40</td><td>Function calling, prompt caching</td></tr><tr><td>OpenRouter</td><td><code>openrouter/minimax/minimax-m2</code></td><td>205K</td><td>$0.26</td><td>$1.02</td><td>Function calling, reasoning</td></tr><tr><td>OpenRouter</td><td><code>openrouter/z-ai/glm-4.6</code></td><td>203K</td><td>$0.40</td><td>$1.75</td><td>Function calling, reasoning</td></tr><tr><td>OpenRouter</td><td><code>openrouter/z-ai/glm-4.6:exacto</code></td><td>203K</td><td>$0.45</td><td>$1.90</td><td>Function calling, reasoning</td></tr><tr><td>Voyage</td><td><code>voyage/voyage-3.5</code></td><td>32K</td><td>$0.06</td><td>-</td><td>Embeddings</td></tr><tr><td>Voyage</td><td><code>voyage/voyage-3.5-lite</code></td><td>32K</td><td>$0.02</td><td>-</td><td>Embeddings</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="video-generation-models">Video Generation Models<a href="#video-generation-models" class="hash-link" aria-label="Video Generation Models" title="Video Generation Models"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Cost Per Second</th><th>Resolutions</th><th>Features</th></tr></thead><tbody><tr><td>RunwayML</td><td><code>runwayml/gen4_turbo</code></td><td>$0.05</td><td>1280x720, 720x1280</td><td>Text + image to video</td></tr><tr><td>RunwayML</td><td><code>runwayml/gen4_aleph</code></td><td>$0.15</td><td>1280x720, 720x1280</td><td>Text + image to video</td></tr><tr><td>RunwayML</td><td><code>runwayml/gen3a_turbo</code></td><td>$0.05</td><td>1280x720, 720x1280</td><td>Text + image to video</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="image-generation-models">Image Generation Models<a href="#image-generation-models" class="hash-link" aria-label="Image Generation Models" title="Image Generation Models"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Cost Per Image</th><th>Resolutions</th><th>Features</th></tr></thead><tbody><tr><td>RunwayML</td><td><code>runwayml/gen4_image</code></td><td>$0.05</td><td>1280x720, 1920x1080</td><td>Text + image to image</td></tr><tr><td>RunwayML</td><td><code>runwayml/gen4_image_turbo</code></td><td>$0.02</td><td>1280x720, 1920x1080</td><td>Text + image to image</td></tr><tr><td>Fal.ai</td><td><code>fal_ai/fal-ai/flux-pro/v1.1</code></td><td>$0.04/image</td><td>-</td><td>Image generation</td></tr><tr><td>Fal.ai</td><td><code>fal_ai/fal-ai/flux/schnell</code></td><td>$0.003/image</td><td>-</td><td>Fast image generation</td></tr><tr><td>Fal.ai</td><td><code>fal_ai/fal-ai/bytedance/seedream/v3/text-to-image</code></td><td>$0.03/image</td><td>-</td><td>Image generation</td></tr><tr><td>Fal.ai</td><td><code>fal_ai/fal-ai/bytedance/dreamina/v3.1/text-to-image</code></td><td>$0.03/image</td><td>-</td><td>Image generation</td></tr><tr><td>Fal.ai</td><td><code>fal_ai/fal-ai/ideogram/v3</code></td><td>$0.06/image</td><td>-</td><td>Image generation</td></tr><tr><td>Fal.ai</td><td><code>fal_ai/fal-ai/imagen4/preview/fast</code></td><td>$0.02/image</td><td>-</td><td>Fast image generation</td></tr><tr><td>Fal.ai</td><td><code>fal_ai/fal-ai/imagen4/preview/ultra</code></td><td>$0.06/image</td><td>-</td><td>High-quality image generation</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="audio-models">Audio Models<a href="#audio-models" class="hash-link" aria-label="Audio Models" title="Audio Models"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Cost</th><th>Features</th></tr></thead><tbody><tr><td>RunwayML</td><td><code>runwayml/eleven_multilingual_v2</code></td><td>$0.0003/char</td><td>Text-to-speech</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/providers/openai">OpenAI</a></strong></p>
<ul>
<li>Add GPT-5.1 family support with reasoning capabilities - <a href="https://github.com/BerriAI/litellm/pull/16598" target="_blank" rel="noopener noreferrer">PR #16598</a></li>
<li>Add support for <code>reasoning_effort=&#x27;none&#x27;</code> for GPT-5.1 - <a href="https://github.com/BerriAI/litellm/pull/16658" target="_blank" rel="noopener noreferrer">PR #16658</a></li>
<li>Add <code>verbosity</code> parameter support for GPT-5 family models - <a href="https://github.com/BerriAI/litellm/pull/16660" target="_blank" rel="noopener noreferrer">PR #16660</a></li>
<li>Fix forward OpenAI organization for image generation - <a href="https://github.com/BerriAI/litellm/pull/16607" target="_blank" rel="noopener noreferrer">PR #16607</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/gemini">Gemini (Google AI Studio + Vertex AI)</a></strong></p>
<ul>
<li>Add support for <code>reasoning_effort=&#x27;none&#x27;</code> for Gemini models - <a href="https://github.com/BerriAI/litellm/pull/16548" target="_blank" rel="noopener noreferrer">PR #16548</a></li>
<li>Add all Gemini image models support in image generation - <a href="https://github.com/BerriAI/litellm/pull/16526" target="_blank" rel="noopener noreferrer">PR #16526</a></li>
<li>Add Gemini image edit support - <a href="https://github.com/BerriAI/litellm/pull/16430" target="_blank" rel="noopener noreferrer">PR #16430</a></li>
<li>Fix preserve non-ASCII characters in function call arguments - <a href="https://github.com/BerriAI/litellm/pull/16550" target="_blank" rel="noopener noreferrer">PR #16550</a></li>
<li>Fix Gemini conversation format issue with MCP auto-execution - <a href="https://github.com/BerriAI/litellm/pull/16592" target="_blank" rel="noopener noreferrer">PR #16592</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/bedrock">Bedrock</a></strong></p>
<ul>
<li>Add support for filtering knowledge base queries - <a href="https://github.com/BerriAI/litellm/pull/16543" target="_blank" rel="noopener noreferrer">PR #16543</a></li>
<li>Ensure correct <code>aws_region</code> is used when provided dynamically for embeddings - <a href="https://github.com/BerriAI/litellm/pull/16547" target="_blank" rel="noopener noreferrer">PR #16547</a></li>
<li>Add support for custom KMS encryption keys in Bedrock Batch operations - <a href="https://github.com/BerriAI/litellm/pull/16662" target="_blank" rel="noopener noreferrer">PR #16662</a></li>
<li>Add bearer token authentication support for AgentCore - <a href="https://github.com/BerriAI/litellm/pull/16556" target="_blank" rel="noopener noreferrer">PR #16556</a></li>
<li>Fix AgentCore SSE stream iterator to async for proper streaming support - <a href="https://github.com/BerriAI/litellm/pull/16293" target="_blank" rel="noopener noreferrer">PR #16293</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/anthropic">Anthropic</a></strong></p>
<ul>
<li>Add context management param support - <a href="https://github.com/BerriAI/litellm/pull/16528" target="_blank" rel="noopener noreferrer">PR #16528</a></li>
<li>Fix preserve <code>$defs</code> for Anthropic tools input schema - <a href="https://github.com/BerriAI/litellm/pull/16648" target="_blank" rel="noopener noreferrer">PR #16648</a></li>
<li>Fix support Anthropic tool_use and tool_result in token counter - <a href="https://github.com/BerriAI/litellm/pull/16351" target="_blank" rel="noopener noreferrer">PR #16351</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/vertex_ai">Vertex AI</a></strong></p>
<ul>
<li>Add Vertex Kimi-K2-Thinking support - <a href="https://github.com/BerriAI/litellm/pull/16671" target="_blank" rel="noopener noreferrer">PR #16671</a></li>
<li>Add <code>vertex_credentials</code> support to <code>litellm.rerank()</code> - <a href="https://github.com/BerriAI/litellm/pull/16479" target="_blank" rel="noopener noreferrer">PR #16479</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/mistral">Mistral</a></strong></p>
<ul>
<li>Fix Magistral streaming to emit reasoning chunks - <a href="https://github.com/BerriAI/litellm/pull/16434" target="_blank" rel="noopener noreferrer">PR #16434</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/moonshot">Moonshot (Kimi)</a></strong></p>
<ul>
<li>Add Kimi K2 thinking model support - <a href="https://github.com/BerriAI/litellm/pull/16445" target="_blank" rel="noopener noreferrer">PR #16445</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/sambanova">SambaNova</a></strong></p>
<ul>
<li>Fix SambaNova API rejecting requests when message content is passed as a list format - <a href="https://github.com/BerriAI/litellm/pull/16612" target="_blank" rel="noopener noreferrer">PR #16612</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/vllm">VLLM</a></strong></p>
<ul>
<li>Fix use vllm passthrough config for hosted vllm provider instead of raising error - <a href="https://github.com/BerriAI/litellm/pull/16537" target="_blank" rel="noopener noreferrer">PR #16537</a></li>
<li>Add headers to VLLM Passthrough requests with success event logging - <a href="https://github.com/BerriAI/litellm/pull/16532" target="_blank" rel="noopener noreferrer">PR #16532</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/azure">Azure</a></strong></p>
<ul>
<li>Fix improve Azure auth parameter handling for None values - <a href="https://github.com/BerriAI/litellm/pull/14436" target="_blank" rel="noopener noreferrer">PR #14436</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/groq">Groq</a></strong></p>
<ul>
<li>Fix parse failed chunks for Groq - <a href="https://github.com/BerriAI/litellm/pull/16595" target="_blank" rel="noopener noreferrer">PR #16595</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/voyage">Voyage</a></strong></p>
<ul>
<li>Add Voyage 3.5 and 3.5-lite embeddings pricing and doc update - <a href="https://github.com/BerriAI/litellm/pull/16641" target="_blank" rel="noopener noreferrer">PR #16641</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/image_generation">Fal.ai</a></strong></p>
<ul>
<li>Add fal-ai/flux/schnell support - <a href="https://github.com/BerriAI/litellm/pull/16580" target="_blank" rel="noopener noreferrer">PR #16580</a></li>
<li>Add all Imagen4 variants of fal ai in model map - <a href="https://github.com/BerriAI/litellm/pull/16579" target="_blank" rel="noopener noreferrer">PR #16579</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes">Bug Fixes<a href="#bug-fixes" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h3>
<ul>
<li><strong>General</strong>
<ul>
<li>Fix sanitize null token usage in OpenAI-compatible responses - <a href="https://github.com/BerriAI/litellm/pull/16493" target="_blank" rel="noopener noreferrer">PR #16493</a></li>
<li>Fix apply provided timeout value to ClientTimeout.total - <a href="https://github.com/BerriAI/litellm/pull/16395" target="_blank" rel="noopener noreferrer">PR #16395</a></li>
<li>Fix raising wrong 429 error on wrong exception - <a href="https://github.com/BerriAI/litellm/pull/16482" target="_blank" rel="noopener noreferrer">PR #16482</a></li>
<li>Add new models, delete repeat models, update pricing - <a href="https://github.com/BerriAI/litellm/pull/16491" target="_blank" rel="noopener noreferrer">PR #16491</a></li>
<li>Update model logging format for custom LLM provider - <a href="https://github.com/BerriAI/litellm/pull/16485" target="_blank" rel="noopener noreferrer">PR #16485</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-endpoints">New Endpoints<a href="#new-endpoints" class="hash-link" aria-label="New Endpoints" title="New Endpoints"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/management_endpoints">GET /providers</a></strong>
<ul>
<li>Add GET list of providers endpoint - <a href="https://github.com/BerriAI/litellm/pull/16432" target="_blank" rel="noopener noreferrer">PR #16432</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/video_generation">Video Generation API</a></strong></p>
<ul>
<li>Allow internal users to access video generation routes - <a href="https://github.com/BerriAI/litellm/pull/16472" target="_blank" rel="noopener noreferrer">PR #16472</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/vector_stores">Vector Stores API</a></strong></p>
<ul>
<li>Vector store files stable release with complete CRUD operations - <a href="https://github.com/BerriAI/litellm/pull/16643" target="_blank" rel="noopener noreferrer">PR #16643</a>
<ul>
<li><code>POST /v1/vector_stores/{vector_store_id}/files</code> - Create vector store file</li>
<li><code>GET /v1/vector_stores/{vector_store_id}/files</code> - List vector store files</li>
<li><code>GET /v1/vector_stores/{vector_store_id}/files/{file_id}</code> - Retrieve vector store file</li>
<li><code>GET /v1/vector_stores/{vector_store_id}/files/{file_id}/content</code> - Retrieve file content</li>
<li><code>DELETE /v1/vector_stores/{vector_store_id}/files/{file_id}</code> - Delete vector store file</li>
<li><code>DELETE /v1/vector_stores/{vector_store_id}</code> - Delete vector store</li>
</ul>
</li>
<li>Ensure users can access <code>search_results</code> for both stream + non-stream response - <a href="https://github.com/BerriAI/litellm/pull/16459" target="_blank" rel="noopener noreferrer">PR #16459</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/video_generation">Video Generation API</a></strong></p>
<ul>
<li>Fix use GET for <code>/v1/videos/{video_id}/content</code> - <a href="https://github.com/BerriAI/litellm/pull/16672" target="_blank" rel="noopener noreferrer">PR #16672</a></li>
</ul>
</li>
<li>
<p><strong>General</strong></p>
<ul>
<li>Fix remove generic exception handling - <a href="https://github.com/BerriAI/litellm/pull/16599" target="_blank" rel="noopener noreferrer">PR #16599</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong>Proxy CLI Auth</strong></p>
<ul>
<li>Fix remove strict master_key check in add_deployment - <a href="https://github.com/BerriAI/litellm/pull/16453" target="_blank" rel="noopener noreferrer">PR #16453</a></li>
</ul>
</li>
<li>
<p><strong>Virtual Keys</strong></p>
<ul>
<li>UI - Add Tags To Edit Key Flow - <a href="https://github.com/BerriAI/litellm/pull/16500" target="_blank" rel="noopener noreferrer">PR #16500</a></li>
<li>UI - Test Key Page show models based on selected endpoint - <a href="https://github.com/BerriAI/litellm/pull/16452" target="_blank" rel="noopener noreferrer">PR #16452</a></li>
<li>UI - Expose user_alias in view and update path - <a href="https://github.com/BerriAI/litellm/pull/16669" target="_blank" rel="noopener noreferrer">PR #16669</a></li>
</ul>
</li>
<li>
<p><strong>Models + Endpoints</strong></p>
<ul>
<li>UI - Add LiteLLM Params to Edit Model - <a href="https://github.com/BerriAI/litellm/pull/16496" target="_blank" rel="noopener noreferrer">PR #16496</a></li>
<li>UI - Add Model use backend data - <a href="https://github.com/BerriAI/litellm/pull/16664" target="_blank" rel="noopener noreferrer">PR #16664</a></li>
<li>UI - Remove Description Field from LLM Credentials - <a href="https://github.com/BerriAI/litellm/pull/16608" target="_blank" rel="noopener noreferrer">PR #16608</a></li>
<li>UI - Add RunwayML on Admin UI supported models/providers - <a href="https://github.com/BerriAI/litellm/pull/16606" target="_blank" rel="noopener noreferrer">PR #16606</a></li>
<li>Infra - Migrate Add Model Fields to Backend - <a href="https://github.com/BerriAI/litellm/pull/16620" target="_blank" rel="noopener noreferrer">PR #16620</a></li>
<li>Add API Endpoint for creating model access group - <a href="https://github.com/BerriAI/litellm/pull/16663" target="_blank" rel="noopener noreferrer">PR #16663</a></li>
</ul>
</li>
<li>
<p><strong>Teams</strong></p>
<ul>
<li>UI - Invite User Searchable Team Select - <a href="https://github.com/BerriAI/litellm/pull/16454" target="_blank" rel="noopener noreferrer">PR #16454</a></li>
<li>Fix use user budget instead of key budget when creating new team - <a href="https://github.com/BerriAI/litellm/pull/16074" target="_blank" rel="noopener noreferrer">PR #16074</a></li>
</ul>
</li>
<li>
<p><strong>Budgets</strong></p>
<ul>
<li>UI - Move Budgets out of Experimental - <a href="https://github.com/BerriAI/litellm/pull/16544" target="_blank" rel="noopener noreferrer">PR #16544</a></li>
</ul>
</li>
<li>
<p><strong>Guardrails</strong></p>
<ul>
<li>UI - Config Guardrails should not be deletable from table - <a href="https://github.com/BerriAI/litellm/pull/16540" target="_blank" rel="noopener noreferrer">PR #16540</a></li>
<li>Fix remove enterprise restriction from guardrails list endpoint - <a href="https://github.com/BerriAI/litellm/pull/15333" target="_blank" rel="noopener noreferrer">PR #15333</a></li>
</ul>
</li>
<li>
<p><strong>Callbacks</strong></p>
<ul>
<li>UI - New Callbacks table - <a href="https://github.com/BerriAI/litellm/pull/16512" target="_blank" rel="noopener noreferrer">PR #16512</a></li>
<li>Fix delete callbacks failing - <a href="https://github.com/BerriAI/litellm/pull/16473" target="_blank" rel="noopener noreferrer">PR #16473</a></li>
</ul>
</li>
<li>
<p><strong>Usage &amp; Analytics</strong></p>
<ul>
<li>UI - Improve Usage Indicator - <a href="https://github.com/BerriAI/litellm/pull/16504" target="_blank" rel="noopener noreferrer">PR #16504</a></li>
<li>UI - Model Info Page Health Check - <a href="https://github.com/BerriAI/litellm/pull/16416" target="_blank" rel="noopener noreferrer">PR #16416</a></li>
<li>Infra - Show Deprecation Warning for Model Analytics Tab - <a href="https://github.com/BerriAI/litellm/pull/16417" target="_blank" rel="noopener noreferrer">PR #16417</a></li>
<li>Fix Litellm tags usage add request_id - <a href="https://github.com/BerriAI/litellm/pull/16111" target="_blank" rel="noopener noreferrer">PR #16111</a></li>
</ul>
</li>
<li>
<p><strong>Health Check</strong></p>
<ul>
<li>Add Langfuse OTEL and SQS to Health Check - <a href="https://github.com/BerriAI/litellm/pull/16514" target="_blank" rel="noopener noreferrer">PR #16514</a></li>
</ul>
</li>
<li>
<p><strong>General UI</strong></p>
<ul>
<li>UI - Normalize table action columns appearance - <a href="https://github.com/BerriAI/litellm/pull/16657" target="_blank" rel="noopener noreferrer">PR #16657</a></li>
<li>UI - Button Styles and Sizing in Settings Pages - <a href="https://github.com/BerriAI/litellm/pull/16600" target="_blank" rel="noopener noreferrer">PR #16600</a></li>
<li>UI - SSO Modal Cosmetic Changes - <a href="https://github.com/BerriAI/litellm/pull/16554" target="_blank" rel="noopener noreferrer">PR #16554</a></li>
<li>Fix UI logos loading with SERVER_ROOT_PATH - <a href="https://github.com/BerriAI/litellm/pull/16618" target="_blank" rel="noopener noreferrer">PR #16618</a></li>
<li>Fix remove misleading &#x27;Custom&#x27; option mention from OpenAI endpoint tooltips - <a href="https://github.com/BerriAI/litellm/pull/16622" target="_blank" rel="noopener noreferrer">PR #16622</a></li>
</ul>
</li>
<li>
<p><strong>SSO</strong></p>
<ul>
<li>Ensure <code>role</code> from SSO provider is used when a user is inserted onto LiteLLM - <a href="https://github.com/BerriAI/litellm/pull/16794" target="_blank" rel="noopener noreferrer">PR #16794</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Management Endpoints</strong>
<ul>
<li>Fix inconsistent error responses in customer management endpoints - <a href="https://github.com/BerriAI/litellm/pull/16450" target="_blank" rel="noopener noreferrer">PR #16450</a></li>
<li>Fix correct date range filtering in /spend/logs endpoint - <a href="https://github.com/BerriAI/litellm/pull/16443" target="_blank" rel="noopener noreferrer">PR #16443</a></li>
<li>Fix /spend/logs/ui Access Control - <a href="https://github.com/BerriAI/litellm/pull/16446" target="_blank" rel="noopener noreferrer">PR #16446</a></li>
<li>Add pagination for /spend/logs/session/ui endpoint - <a href="https://github.com/BerriAI/litellm/pull/16603" target="_blank" rel="noopener noreferrer">PR #16603</a></li>
<li>Fix LiteLLM Usage shows key_hash - <a href="https://github.com/BerriAI/litellm/pull/16471" target="_blank" rel="noopener noreferrer">PR #16471</a></li>
<li>Fix app_roles missing from jwt payload - <a href="https://github.com/BerriAI/litellm/pull/16448" target="_blank" rel="noopener noreferrer">PR #16448</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail--prompt-management-integrations">Logging / Guardrail / Prompt Management Integrations<a href="#logging--guardrail--prompt-management-integrations" class="hash-link" aria-label="Logging / Guardrail / Prompt Management Integrations" title="Logging / Guardrail / Prompt Management Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-integration">New Integration<a href="#new-integration" class="hash-link" aria-label="New Integration" title="New Integration"></a></h4>
<ul>
<li><strong> <a href="/docs/proxy/guardrails/zscaler_ai_guard">Zscaler AI Guard</a></strong>
<ul>
<li>Add Zscaler AI Guard hook for security policy enforcement - <a href="https://github.com/BerriAI/litellm/pull/15691" target="_blank" rel="noopener noreferrer">PR #15691</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="logging">Logging<a href="#logging" class="hash-link" aria-label="Logging" title="Logging"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/proxy/logging#langfuse">Langfuse</a></strong></p>
<ul>
<li>Fix handle null usage values to prevent validation errors - <a href="https://github.com/BerriAI/litellm/pull/16396" target="_blank" rel="noopener noreferrer">PR #16396</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/logging">CloudZero</a></strong></p>
<ul>
<li>Fix updated spend would not be sent to CloudZero - <a href="https://github.com/BerriAI/litellm/pull/16201" target="_blank" rel="noopener noreferrer">PR #16201</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="guardrails">Guardrails<a href="#guardrails" class="hash-link" aria-label="Guardrails" title="Guardrails"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/guardrails">IBM Detector</a></strong>
<ul>
<li>Ensure detector-id is passed as header to IBM detector server - <a href="https://github.com/BerriAI/litellm/pull/16649" target="_blank" rel="noopener noreferrer">PR #16649</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="prompt-management">Prompt Management<a href="#prompt-management" class="hash-link" aria-label="Prompt Management" title="Prompt Management"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/prompt_management">Custom Prompt Management</a></strong>
<ul>
<li>Add SDK focused examples for custom prompt management - <a href="https://github.com/BerriAI/litellm/pull/16441" target="_blank" rel="noopener noreferrer">PR #16441</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-budgets-and-rate-limiting">Spend Tracking, Budgets and Rate Limiting<a href="#spend-tracking-budgets-and-rate-limiting" class="hash-link" aria-label="Spend Tracking, Budgets and Rate Limiting" title="Spend Tracking, Budgets and Rate Limiting"></a></h2>
<ul>
<li><strong>End User Budgets</strong>
<ul>
<li>Allow pointing max_end_user budget to an id, so the default ID applies to all end users - <a href="https://github.com/BerriAI/litellm/pull/16456" target="_blank" rel="noopener noreferrer">PR #16456</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway">MCP Gateway<a href="#mcp-gateway" class="hash-link" aria-label="MCP Gateway" title="MCP Gateway"></a></h2>
<ul>
<li><strong>Configuration</strong>
<ul>
<li>Add dynamic OAuth2 metadata discovery for MCP servers - <a href="https://github.com/BerriAI/litellm/pull/16676" target="_blank" rel="noopener noreferrer">PR #16676</a></li>
<li>Fix allow tool call even when server name prefix is missing - <a href="https://github.com/BerriAI/litellm/pull/16425" target="_blank" rel="noopener noreferrer">PR #16425</a></li>
<li>Fix exclude unauthorized MCP servers from allowed server list - <a href="https://github.com/BerriAI/litellm/pull/16551" target="_blank" rel="noopener noreferrer">PR #16551</a></li>
<li>Fix unable to delete MCP server from permission settings - <a href="https://github.com/BerriAI/litellm/pull/16407" target="_blank" rel="noopener noreferrer">PR #16407</a></li>
<li>Fix avoid crashing when MCP server record lacks credentials - <a href="https://github.com/BerriAI/litellm/pull/16601" target="_blank" rel="noopener noreferrer">PR #16601</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="agents">Agents<a href="#agents" class="hash-link" aria-label="Agents" title="Agents"></a></h2>
<ul>
<li><strong><a href="/docs/agents">Agent Registration (A2A Spec)</a></strong>
<ul>
<li>Support agent registration + discovery following Agent-to-Agent specification - <a href="https://github.com/BerriAI/litellm/pull/16615" target="_blank" rel="noopener noreferrer">PR #16615</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<ul>
<li>
<p><strong>Embeddings Performance</strong></p>
<ul>
<li>Use router&#x27;s O(1) lookup and shared sessions for embeddings - <a href="https://github.com/BerriAI/litellm/pull/16344" target="_blank" rel="noopener noreferrer">PR #16344</a></li>
</ul>
</li>
<li>
<p><strong>Router Reliability</strong></p>
<ul>
<li>Support default fallbacks for unknown models - <a href="https://github.com/BerriAI/litellm/pull/16419" target="_blank" rel="noopener noreferrer">PR #16419</a></li>
</ul>
</li>
<li>
<p><strong>Callback Management</strong></p>
<ul>
<li>Add atexit handlers to flush callbacks for async completions - <a href="https://github.com/BerriAI/litellm/pull/16487" target="_blank" rel="noopener noreferrer">PR #16487</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<ul>
<li><strong>Configuration Management</strong>
<ul>
<li>Fix update model_cost_map_url to use environment variable - <a href="https://github.com/BerriAI/litellm/pull/16429" target="_blank" rel="noopener noreferrer">PR #16429</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="documentation-updates">Documentation Updates<a href="#documentation-updates" class="hash-link" aria-label="Documentation Updates" title="Documentation Updates"></a></h2>
<ul>
<li>
<p><strong>Provider Documentation</strong></p>
<ul>
<li>Fix streaming example in README - <a href="https://github.com/BerriAI/litellm/pull/16461" target="_blank" rel="noopener noreferrer">PR #16461</a></li>
<li>Update broken Slack invite links to support page - <a href="https://github.com/BerriAI/litellm/pull/16546" target="_blank" rel="noopener noreferrer">PR #16546</a></li>
<li>Fix code block indentation for fallbacks page - <a href="https://github.com/BerriAI/litellm/pull/16542" target="_blank" rel="noopener noreferrer">PR #16542</a></li>
<li>Documentation code example corrections - <a href="https://github.com/BerriAI/litellm/pull/16502" target="_blank" rel="noopener noreferrer">PR #16502</a></li>
<li>Document <code>reasoning_effort</code> summary field options - <a href="https://github.com/BerriAI/litellm/pull/16549" target="_blank" rel="noopener noreferrer">PR #16549</a></li>
</ul>
</li>
<li>
<p><strong>API Documentation</strong></p>
<ul>
<li>Add docs on APIs for model access management - <a href="https://github.com/BerriAI/litellm/pull/16673" target="_blank" rel="noopener noreferrer">PR #16673</a></li>
<li>Add docs for showing how to auto reload new pricing data - <a href="https://github.com/BerriAI/litellm/pull/16675" target="_blank" rel="noopener noreferrer">PR #16675</a></li>
<li>LiteLLM Quick start - show how model resolution works - <a href="https://github.com/BerriAI/litellm/pull/16602" target="_blank" rel="noopener noreferrer">PR #16602</a></li>
<li>Add docs for tracking callback failure - <a href="https://github.com/BerriAI/litellm/pull/16474" target="_blank" rel="noopener noreferrer">PR #16474</a></li>
</ul>
</li>
<li>
<p><strong>General Documentation</strong></p>
<ul>
<li>Fix container api link in release page - <a href="https://github.com/BerriAI/litellm/pull/16440" target="_blank" rel="noopener noreferrer">PR #16440</a></li>
<li>Add softgen to projects that are using litellm - <a href="https://github.com/BerriAI/litellm/pull/16423" target="_blank" rel="noopener noreferrer">PR #16423</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@artplan1 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16423" target="_blank" rel="noopener noreferrer">PR #16423</a></li>
<li>@JehandadK made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16472" target="_blank" rel="noopener noreferrer">PR #16472</a></li>
<li>@vmiscenko made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16453" target="_blank" rel="noopener noreferrer">PR #16453</a></li>
<li>@mcowger made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16429" target="_blank" rel="noopener noreferrer">PR #16429</a></li>
<li>@yellowsubmarine372 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16395" target="_blank" rel="noopener noreferrer">PR #16395</a></li>
<li>@Hebruwu made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16201" target="_blank" rel="noopener noreferrer">PR #16201</a></li>
<li>@jwang-gif made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15691" target="_blank" rel="noopener noreferrer">PR #15691</a></li>
<li>@AnthonyMonaco made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16502" target="_blank" rel="noopener noreferrer">PR #16502</a></li>
<li>@andrewm4894 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16487" target="_blank" rel="noopener noreferrer">PR #16487</a></li>
<li>@f14-bertolotti made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16485" target="_blank" rel="noopener noreferrer">PR #16485</a></li>
<li>@busla made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16293" target="_blank" rel="noopener noreferrer">PR #16293</a></li>
<li>@MightyGoldenOctopus made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16537" target="_blank" rel="noopener noreferrer">PR #16537</a></li>
<li>@ultmaster made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14436" target="_blank" rel="noopener noreferrer">PR #14436</a></li>
<li>@bchrobot made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16542" target="_blank" rel="noopener noreferrer">PR #16542</a></li>
<li>@sep-grindr made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16622" target="_blank" rel="noopener noreferrer">PR #16622</a></li>
<li>@pnookala-godaddy made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16607" target="_blank" rel="noopener noreferrer">PR #16607</a></li>
<li>@dtunikov made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16592" target="_blank" rel="noopener noreferrer">PR #16592</a></li>
<li>@lukapecnik made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16648" target="_blank" rel="noopener noreferrer">PR #16648</a></li>
<li>@jyeros made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16618" target="_blank" rel="noopener noreferrer">PR #16618</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog">Full Changelog<a href="#full-changelog" class="hash-link" aria-label="Full Changelog" title="Full Changelog"></a></h2>
<p><strong><a href="https://github.com/BerriAI/litellm/compare/v1.79.3.rc.1...v1.80.0.rc.1" target="_blank" rel="noopener noreferrer">View complete changelog on GitHub</a></strong></p>
<hr></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-79-3">v1.79.3-stable - Built-in Guardrails on AI Gateway</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-11-08T10:00:00.000Z">2025118</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaff"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaff</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.79.3-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.79.3.rc.1</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>LiteLLM Custom Guardrail</strong> - Built-in guardrail with UI configuration support</li>
<li><strong>Performance Improvements</strong> - <code>/responses</code> API 19 Lower Median Latency</li>
<li><strong>Veo3 Video Generation (Vertex AI + Google AI Studio)</strong> - Use OpenAI Video API to generate videos with Vertex AI and Google AI Studio Veo3 models</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="built-in-guardrails-on-ai-gateway">Built-in Guardrails on AI Gateway<a href="#built-in-guardrails-on-ai-gateway" class="hash-link" aria-label="Built-in Guardrails on AI Gateway" title="Built-in Guardrails on AI Gateway"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAhklEQVR4nE2NUQrCMBAFc/+D6S0UpKRVWtA2MbubTUdSEfyYv3lvwjRNxBgZx5GUEiIFr0LRShGlVsPMCMuy0OVhGMg5cX+sxPhkXhuv7X1Iokpwd7409t25XIXTeWOZMznLIbbWCKrKj2pGskouGbOe7fnCXo3QF/+oG9pfvNGaI+IMN+MDAF/BZMhZpgYAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/built_in_guard.e6c1ea5.640.png" srcset="/assets/ideal-img/built_in_guard.e6c1ea5.640.png 640w,/assets/ideal-img/built_in_guard.105707e.1920.png 1920w" width="640" height="334"></noscript></div>
<br>
<p>This release introduces built-in guardrails for LiteLLM AI Gateway, allowing you to enforce protections without depending on an external guardrail API.</p>
<ul>
<li><strong>Blocking Keywords</strong> - Block known sensitive keywords like &quot;litellm&quot;, &quot;python&quot;, etc.</li>
<li><strong>Pattern Detection</strong> - Block known sensitive patterns like emails, Social Security Numbers, API keys, etc.</li>
<li><strong>Custom Regex Patterns</strong> - Define custom regex patterns for your specific use case.</li>
</ul>
<p>Get started with the built-in guardrails on AI Gateway <a href="https://docs.litellm.ai/docs/proxy/guardrails/litellm_content_filter" target="_blank" rel="noopener noreferrer">here</a>.</p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="performance--responses-19-lower-median-latency">Performance  <code>/responses</code> 19 Lower Median Latency<a href="#performance--responses-19-lower-median-latency" class="hash-link" aria-label="performance--responses-19-lower-median-latency" title="performance--responses-19-lower-median-latency"></a></h3>
<p>This update significantly improves <code>/responses</code> latency by integrating our internal network management for connection handling, eliminating per-request setup overhead.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="results">Results<a href="#results" class="hash-link" aria-label="Results" title="Results"></a></h4>
<table><thead><tr><th>Metric</th><th>Before</th><th>After</th><th>Improvement</th></tr></thead><tbody><tr><td>Median latency</td><td>3,600 ms</td><td><strong>190 ms</strong></td><td><strong>95% (~19 faster)</strong></td></tr><tr><td>p95 latency</td><td>4,300 ms</td><td><strong>280 ms</strong></td><td>93%</td></tr><tr><td>p99 latency</td><td>4,600 ms</td><td><strong>590 ms</strong></td><td>87%</td></tr><tr><td>Average latency</td><td>3,571 ms</td><td><strong>208 ms</strong></td><td>94%</td></tr><tr><td>RPS</td><td>231</td><td><strong>1,059</strong></td><td>+358%</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="test-setup">Test Setup<a href="#test-setup" class="hash-link" aria-label="Test Setup" title="Test Setup"></a></h4>
<table><thead><tr><th>Category</th><th>Specification</th></tr></thead><tbody><tr><td><strong>Load Testing</strong></td><td>Locust: 1,000 concurrent users, 500 ramp-up</td></tr><tr><td><strong>System</strong></td><td>4 vCPUs, 8 GB RAM, 4 workers, 4 instances</td></tr><tr><td><strong>Database</strong></td><td>PostgreSQL (Redis unused)</td></tr><tr><td><strong>Configuration</strong></td><td><a href="https://gist.github.com/AlexsanderHamir/550791675fd752befcac6a9e44024652" target="_blank" rel="noopener noreferrer">config.yaml</a></td></tr><tr><td><strong>Load Script</strong></td><td><a href="https://gist.github.com/AlexsanderHamir/99d673bf74cdd81fd39f59fa9048f2e8" target="_blank" rel="noopener noreferrer">no_cache_hits.py</a></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-model-support">New Model Support<a href="#new-model-support" class="hash-link" aria-label="New Model Support" title="New Model Support"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Features</th></tr></thead><tbody><tr><td>Azure</td><td><code>azure/gpt-5-pro</code></td><td>272K</td><td>$15.00</td><td>$120.00</td><td>Responses API, reasoning, vision, PDF input</td></tr><tr><td>Azure</td><td><code>azure/gpt-image-1-mini</code></td><td>-</td><td>-</td><td>-</td><td>Image generation - per pixel pricing</td></tr><tr><td>Azure</td><td><code>azure/container</code></td><td>-</td><td>-</td><td>-</td><td>Container API - $0.03/session</td></tr><tr><td>OpenAI</td><td><code>openai/container</code></td><td>-</td><td>-</td><td>-</td><td>Container API - $0.03/session</td></tr><tr><td>Cohere</td><td><code>cohere/embed-v4.0</code></td><td>128K</td><td>$0.12</td><td>-</td><td>Embeddings with image input support</td></tr><tr><td>Gemini</td><td><code>gemini/gemini-live-2.5-flash-preview-native-audio-09-2025</code></td><td>1M</td><td>$0.30</td><td>$2.00</td><td>Native audio, vision, web search</td></tr><tr><td>Vertex AI</td><td><code>vertex_ai/minimaxai/minimax-m2-maas</code></td><td>196K</td><td>$0.30</td><td>$1.20</td><td>Function calling, tool choice</td></tr><tr><td>NVIDIA</td><td><code>nvidia/nemotron-nano-9b-v2</code></td><td>-</td><td>-</td><td>-</td><td>Chat completions</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="ocr-models">OCR Models<a href="#ocr-models" class="hash-link" aria-label="OCR Models" title="OCR Models"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Cost Per Page</th><th>Features</th></tr></thead><tbody><tr><td>Azure AI</td><td><code>azure_ai/doc-intelligence/prebuilt-read</code></td><td>$0.0015</td><td>Document reading</td></tr><tr><td>Azure AI</td><td><code>azure_ai/doc-intelligence/prebuilt-layout</code></td><td>$0.01</td><td>Layout analysis</td></tr><tr><td>Azure AI</td><td><code>azure_ai/doc-intelligence/prebuilt-document</code></td><td>$0.01</td><td>Document processing</td></tr><tr><td>Vertex AI</td><td><code>vertex_ai/mistral-ocr-2505</code></td><td>$0.0005</td><td>OCR processing</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="search-models">Search Models<a href="#search-models" class="hash-link" aria-label="Search Models" title="Search Models"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Pricing</th><th>Features</th></tr></thead><tbody><tr><td>Firecrawl</td><td><code>firecrawl/search</code></td><td>Tiered: $0.00166-$0.0166/query</td><td>10-100 results per query</td></tr><tr><td>SearXNG</td><td><code>searxng/search</code></td><td>Free</td><td>Open-source metasearch</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/providers/azure">Azure</a></strong></p>
<ul>
<li>Add Azure GPT-5-Pro Responses API support with reasoning capabilities - <a href="https://github.com/BerriAI/litellm/pull/16235" target="_blank" rel="noopener noreferrer">PR #16235</a></li>
<li>Add gpt-image-1-mini pricing for Azure with quality tiers (low/medium/high) - <a href="https://github.com/BerriAI/litellm/pull/16182" target="_blank" rel="noopener noreferrer">PR #16182</a></li>
<li>Add support for returning Azure Content Policy error information when exceptions from Azure OpenAI occur - <a href="https://github.com/BerriAI/litellm/pull/16231" target="_blank" rel="noopener noreferrer">PR #16231</a></li>
<li>Fix Azure GPT-5 incorrectly routed to O-series config (temperature parameter unsupported) - <a href="https://github.com/BerriAI/litellm/pull/16246" target="_blank" rel="noopener noreferrer">PR #16246</a></li>
<li>Fix Azure doesn&#x27;t accept extra body param - <a href="https://github.com/BerriAI/litellm/pull/16116" target="_blank" rel="noopener noreferrer">PR #16116</a></li>
<li>Fix Azure DALL-E-3 health check content policy violation by using safe default prompt - <a href="https://github.com/BerriAI/litellm/pull/16329" target="_blank" rel="noopener noreferrer">PR #16329</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/bedrock">Bedrock</a></strong></p>
<ul>
<li>Fix empty assistant message handling in AWS Bedrock Converse API to prevent 400 Bad Request errors - <a href="https://github.com/BerriAI/litellm/pull/15850" target="_blank" rel="noopener noreferrer">PR #15850</a></li>
<li>Fix: Filter AWS authentication params from Bedrock InvokeModel request body - <a href="https://github.com/BerriAI/litellm/pull/16315" target="_blank" rel="noopener noreferrer">PR #16315</a></li>
<li>Fix Bedrock proxy adding name to file content, breaks when cache_control in use - <a href="https://github.com/BerriAI/litellm/pull/16275" target="_blank" rel="noopener noreferrer">PR #16275</a></li>
<li>Fix global.anthropic.claude-haiku-4-5-20251001-v1:0 supports_reasoning flag and update pricing - <a href="https://github.com/BerriAI/litellm/pull/16263" target="_blank" rel="noopener noreferrer">PR #16263</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/gemini">Gemini (Google AI Studio + Vertex AI)</a></strong></p>
<ul>
<li>Add gemini live audio model cost in model map - <a href="https://github.com/BerriAI/litellm/pull/16183" target="_blank" rel="noopener noreferrer">PR #16183</a></li>
<li>Fix translation problem with Gemini parallel tool calls - <a href="https://github.com/BerriAI/litellm/pull/16194" target="_blank" rel="noopener noreferrer">PR #16194</a></li>
<li>Fix: Send Gemini API key via x-goog-api-key header with custom api_base - <a href="https://github.com/BerriAI/litellm/pull/16085" target="_blank" rel="noopener noreferrer">PR #16085</a></li>
<li>Fix image_config.aspect_ratio not working for gemini-2.5-flash-image - <a href="https://github.com/BerriAI/litellm/pull/15999" target="_blank" rel="noopener noreferrer">PR #15999</a></li>
<li>Fix Gemini minimal reasoning env overrides disabling thoughts - <a href="https://github.com/BerriAI/litellm/pull/16347" target="_blank" rel="noopener noreferrer">PR #16347</a></li>
<li>Fix cache_read_input_token_cost for gemini-2.5-flash - <a href="https://github.com/BerriAI/litellm/pull/16354" target="_blank" rel="noopener noreferrer">PR #16354</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/anthropic">Anthropic</a></strong></p>
<ul>
<li>Fix Anthropic token counting for VertexAI - <a href="https://github.com/BerriAI/litellm/pull/16171" target="_blank" rel="noopener noreferrer">PR #16171</a></li>
<li>Fix anthropic-adapter: properly translate Anthropic image format to OpenAI - <a href="https://github.com/BerriAI/litellm/pull/16202" target="_blank" rel="noopener noreferrer">PR #16202</a></li>
<li>Enable automated prompt caching message format for Claude on Databricks - <a href="https://github.com/BerriAI/litellm/pull/16200" target="_blank" rel="noopener noreferrer">PR #16200</a></li>
<li>Add support for Anthropic Memory Tool - <a href="https://github.com/BerriAI/litellm/pull/16115" target="_blank" rel="noopener noreferrer">PR #16115</a></li>
<li>Propagate cache creation/read token costs for model info to fix Anthropic long context cost calculations - <a href="https://github.com/BerriAI/litellm/pull/16376" target="_blank" rel="noopener noreferrer">PR #16376</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/vertex_ai">Vertex AI</a></strong></p>
<ul>
<li>Add Vertex MiniMAX m2 model support - <a href="https://github.com/BerriAI/litellm/pull/16373" target="_blank" rel="noopener noreferrer">PR #16373</a></li>
<li>Correctly map 429 Resource Exhausted to RateLimitError - <a href="https://github.com/BerriAI/litellm/pull/16363" target="_blank" rel="noopener noreferrer">PR #16363</a></li>
<li>Add <code>vertex_credentials</code> support to <code>litellm.rerank()</code> for Vertex AI - <a href="https://github.com/BerriAI/litellm/pull/16266" target="_blank" rel="noopener noreferrer">PR #16266</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/databricks">Databricks</a></strong></p>
<ul>
<li>Fix databricks streaming - <a href="https://github.com/BerriAI/litellm/pull/16368" target="_blank" rel="noopener noreferrer">PR #16368</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/deepgram">Deepgram</a></strong></p>
<ul>
<li>Return the diarized transcript when it&#x27;s required in the request - <a href="https://github.com/BerriAI/litellm/pull/16133" target="_blank" rel="noopener noreferrer">PR #16133</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/fireworks_ai">Fireworks</a></strong></p>
<ul>
<li>Update Fireworks audio endpoints to new <code>api.fireworks.ai</code> domains - <a href="https://github.com/BerriAI/litellm/pull/16346" target="_blank" rel="noopener noreferrer">PR #16346</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/cohere">Cohere</a></strong></p>
<ul>
<li>Add cohere embed-v4.0 model support - <a href="https://github.com/BerriAI/litellm/pull/16358" target="_blank" rel="noopener noreferrer">PR #16358</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/watsonx">Watsonx</a></strong></p>
<ul>
<li>Support <code>reasoning_effort</code> for watsonx chat models - <a href="https://github.com/BerriAI/litellm/pull/16261" target="_blank" rel="noopener noreferrer">PR #16261</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/openai">OpenAI</a></strong></p>
<ul>
<li>Remove automatic summary from reasoning_effort transformation - <a href="https://github.com/BerriAI/litellm/pull/16210" target="_blank" rel="noopener noreferrer">PR #16210</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/xai">XAI</a></strong></p>
<ul>
<li>Remove Grok 4 Models Reasoning Effort Parameter - <a href="https://github.com/BerriAI/litellm/pull/16265" target="_blank" rel="noopener noreferrer">PR #16265</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/vllm">Hosted VLLM</a></strong></p>
<ul>
<li>Fix HostedVLLMRerankConfig will not be used - <a href="https://github.com/BerriAI/litellm/pull/16352" target="_blank" rel="noopener noreferrer">PR #16352</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-provider-support">New Provider Support<a href="#new-provider-support" class="hash-link" aria-label="New Provider Support" title="New Provider Support"></a></h4>
<ul>
<li><strong><a href="/docs/providers/bedrock">Bedrock Agentcore</a></strong>
<ul>
<li>Add Bedrock Agentcore as a provider on LiteLLM Python SDK and LiteLLM AI Gateway - <a href="https://github.com/BerriAI/litellm/pull/16252" target="_blank" rel="noopener noreferrer">PR #16252</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/ocr">OCR API</a></strong></p>
<ul>
<li>Add VertexAI OCR provider support + cost tracking - <a href="https://github.com/BerriAI/litellm/pull/16216" target="_blank" rel="noopener noreferrer">PR #16216</a></li>
<li>Add Azure AI Doc Intelligence OCR support - <a href="https://github.com/BerriAI/litellm/pull/16219" target="_blank" rel="noopener noreferrer">PR #16219</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/search">Search API</a></strong></p>
<ul>
<li>Add firecrawl search API support with tiered pricing - <a href="https://github.com/BerriAI/litellm/pull/16257" target="_blank" rel="noopener noreferrer">PR #16257</a></li>
<li>Add searxng search API provider - <a href="https://github.com/BerriAI/litellm/pull/16259" target="_blank" rel="noopener noreferrer">PR #16259</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/response_api">Responses API</a></strong></p>
<ul>
<li>Support responses API streaming in langfuse otel - <a href="https://github.com/BerriAI/litellm/pull/16153" target="_blank" rel="noopener noreferrer">PR #16153</a></li>
<li>Pass extra_body parameters to provider in Responses API requests - <a href="https://github.com/BerriAI/litellm/pull/16320" target="_blank" rel="noopener noreferrer">PR #16320</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/container_api">Container API</a></strong></p>
<ul>
<li>Add E2E Container API Support - <a href="https://github.com/BerriAI/litellm/pull/16136" target="_blank" rel="noopener noreferrer">PR #16136</a></li>
<li>Update container documentation to be similar to others - <a href="https://github.com/BerriAI/litellm/pull/16327" target="_blank" rel="noopener noreferrer">PR #16327</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/video_generation">Video Generation API</a></strong></p>
<ul>
<li>Add Vertex and Gemini Videos API with Cost Tracking + UI support - <a href="https://github.com/BerriAI/litellm/pull/16323" target="_blank" rel="noopener noreferrer">PR #16323</a></li>
<li>Add <code>custom_llm_provider</code> support for video endpoints (non-generation) - <a href="https://github.com/BerriAI/litellm/pull/16121" target="_blank" rel="noopener noreferrer">PR #16121</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/audio">Audio API</a></strong></p>
<ul>
<li>Add gpt-4o-transcribe cost tracking - <a href="https://github.com/BerriAI/litellm/pull/16412" target="_blank" rel="noopener noreferrer">PR #16412</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/vector_stores">Vector Stores</a></strong></p>
<ul>
<li>Milvus - search vector store support + support multi-part form data on passthrough - <a href="https://github.com/BerriAI/litellm/pull/16035" target="_blank" rel="noopener noreferrer">PR #16035</a></li>
<li>Azure AI Vector Stores - support &quot;virtual&quot; indexes + create vector store on passthrough API - <a href="https://github.com/BerriAI/litellm/pull/16160" target="_blank" rel="noopener noreferrer">PR #16160</a></li>
<li>Milvus - Passthrough API support - adds create + read vector store support via passthrough API&#x27;s - <a href="https://github.com/BerriAI/litellm/pull/16170" target="_blank" rel="noopener noreferrer">PR #16170</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/embedding/supported_embedding">Embeddings API</a></strong></p>
<ul>
<li>Use valid CallTypes enum value in embeddings endpoint - <a href="https://github.com/BerriAI/litellm/pull/16328" target="_blank" rel="noopener noreferrer">PR #16328</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/rerank">Rerank API</a></strong></p>
<ul>
<li>Generalize tiered pricing in generic cost calculator - <a href="https://github.com/BerriAI/litellm/pull/16150" target="_blank" rel="noopener noreferrer">PR #16150</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>General</strong>
<ul>
<li>Fix index field not populated in streaming mode with n&gt;1 and tool calls - <a href="https://github.com/BerriAI/litellm/pull/15962" target="_blank" rel="noopener noreferrer">PR #15962</a></li>
<li>Pass aws_region_name in litellm_params - <a href="https://github.com/BerriAI/litellm/pull/16321" target="_blank" rel="noopener noreferrer">PR #16321</a></li>
<li>Add <code>retry-after</code> header support for errors <code>502</code>, <code>503</code>, <code>504</code> - <a href="https://github.com/BerriAI/litellm/pull/16288" target="_blank" rel="noopener noreferrer">PR #16288</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong>Virtual Keys</strong></p>
<ul>
<li>UI - Delete Team Member with friction - <a href="https://github.com/BerriAI/litellm/pull/16167" target="_blank" rel="noopener noreferrer">PR #16167</a></li>
<li>UI - Litellm test key audio support - <a href="https://github.com/BerriAI/litellm/pull/16251" target="_blank" rel="noopener noreferrer">PR #16251</a></li>
<li>UI - Test Key Page Revert Model To Single Select - <a href="https://github.com/BerriAI/litellm/pull/16390" target="_blank" rel="noopener noreferrer">PR #16390</a></li>
</ul>
</li>
<li>
<p><strong>Models + Endpoints</strong></p>
<ul>
<li>UI - Add Model Existing Credentials Improvement - <a href="https://github.com/BerriAI/litellm/pull/16166" target="_blank" rel="noopener noreferrer">PR #16166</a></li>
<li>UI - Add Azure AD Token field and Azure API Key optional - <a href="https://github.com/BerriAI/litellm/pull/16331" target="_blank" rel="noopener noreferrer">PR #16331</a></li>
<li>UI - Fixed Label for vLLM in Model Create Flow - <a href="https://github.com/BerriAI/litellm/pull/16285" target="_blank" rel="noopener noreferrer">PR #16285</a></li>
<li>UI - Include Model Access Group Models on Team Models Table - <a href="https://github.com/BerriAI/litellm/pull/16298" target="_blank" rel="noopener noreferrer">PR #16298</a></li>
<li>Fix /model_group/info Returning Entire Model List for SSO Users - <a href="https://github.com/BerriAI/litellm/pull/16296" target="_blank" rel="noopener noreferrer">PR #16296</a></li>
<li>Litellm non root docker Model Hub Table fix - <a href="https://github.com/BerriAI/litellm/pull/16282" target="_blank" rel="noopener noreferrer">PR #16282</a></li>
</ul>
</li>
<li>
<p><strong>Guardrails</strong></p>
<ul>
<li>UI - Fix regression where Guardrail Entity Could not be selected and entity was not displayed - <a href="https://github.com/BerriAI/litellm/pull/16165" target="_blank" rel="noopener noreferrer">PR #16165</a></li>
<li>UI - Guardrail Info Page Show PII Config - <a href="https://github.com/BerriAI/litellm/pull/16164" target="_blank" rel="noopener noreferrer">PR #16164</a></li>
<li>Change guardrail_information to list type - <a href="https://github.com/BerriAI/litellm/pull/16127" target="_blank" rel="noopener noreferrer">PR #16127</a></li>
<li>UI - LiteLLM Guardrail - ensure you can see UI Friendly name for PII Patterns - <a href="https://github.com/BerriAI/litellm/pull/16382" target="_blank" rel="noopener noreferrer">PR #16382</a></li>
<li>UI - Guardrails - LiteLLM Content Filter, Allow Viewing/Editing Content Filter Settings - <a href="https://github.com/BerriAI/litellm/pull/16383" target="_blank" rel="noopener noreferrer">PR #16383</a></li>
<li>UI - Guardrails - allow updating guardrails through UI. Ensure litellm_params actually get updated in memory - <a href="https://github.com/BerriAI/litellm/pull/16384" target="_blank" rel="noopener noreferrer">PR #16384</a></li>
</ul>
</li>
<li>
<p><strong>SSO Settings</strong></p>
<ul>
<li>Support dot notation on ui sso - <a href="https://github.com/BerriAI/litellm/pull/16135" target="_blank" rel="noopener noreferrer">PR #16135</a></li>
<li>UI - Prevent trailing slash in sso proxy base url input - <a href="https://github.com/BerriAI/litellm/pull/16244" target="_blank" rel="noopener noreferrer">PR #16244</a></li>
<li>UI - SSO Proxy Base URL input validation and remove normalizing / - <a href="https://github.com/BerriAI/litellm/pull/16332" target="_blank" rel="noopener noreferrer">PR #16332</a></li>
<li>UI - Surface SSO Create errors on create flow - <a href="https://github.com/BerriAI/litellm/pull/16369" target="_blank" rel="noopener noreferrer">PR #16369</a></li>
</ul>
</li>
<li>
<p><strong>Usage &amp; Analytics</strong></p>
<ul>
<li>UI - Tag Usage Top Model Table View and Label Fix - <a href="https://github.com/BerriAI/litellm/pull/16249" target="_blank" rel="noopener noreferrer">PR #16249</a></li>
<li>UI - Litellm usage date picker - <a href="https://github.com/BerriAI/litellm/pull/16264" target="_blank" rel="noopener noreferrer">PR #16264</a></li>
</ul>
</li>
<li>
<p><strong>Cache Settings</strong></p>
<ul>
<li>UI - Cache Settings Redis Add Semantic Cache Settings - <a href="https://github.com/BerriAI/litellm/pull/16398" target="_blank" rel="noopener noreferrer">PR #16398</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>General</strong>
<ul>
<li>UI - Remove encoding_format in request for embedding models - <a href="https://github.com/BerriAI/litellm/pull/16367" target="_blank" rel="noopener noreferrer">PR #16367</a></li>
<li>UI - Revert Changes for Test Key Multiple Model Select - <a href="https://github.com/BerriAI/litellm/pull/16372" target="_blank" rel="noopener noreferrer">PR #16372</a></li>
<li>UI - Various Small Issues - <a href="https://github.com/BerriAI/litellm/pull/16406" target="_blank" rel="noopener noreferrer">PR #16406</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ai-integrations">AI Integrations<a href="#ai-integrations" class="hash-link" aria-label="AI Integrations" title="AI Integrations"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="logging">Logging<a href="#logging" class="hash-link" aria-label="Logging" title="Logging"></a></h3>
<ul>
<li>
<p><strong><a href="/docs/proxy/logging#langfuse">Langfuse</a></strong></p>
<ul>
<li>Fix langfuse input tokens logic for cached tokens - <a href="https://github.com/BerriAI/litellm/pull/16203" target="_blank" rel="noopener noreferrer">PR #16203</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/logging#opik">Opik</a></strong></p>
<ul>
<li>Fix the bug with not incorrect attachment to existing trace &amp; refactor - <a href="https://github.com/BerriAI/litellm/pull/15529" target="_blank" rel="noopener noreferrer">PR #15529</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/logging#s3">S3</a></strong></p>
<ul>
<li>S3 logger, add support for ssl_verify when using minio logger - <a href="https://github.com/BerriAI/litellm/pull/16211" target="_blank" rel="noopener noreferrer">PR #16211</a></li>
<li>Strip base64 in s3 - <a href="https://github.com/BerriAI/litellm/pull/16157" target="_blank" rel="noopener noreferrer">PR #16157</a></li>
<li>Add allowing Key based prefix to s3 path - <a href="https://github.com/BerriAI/litellm/pull/16237" target="_blank" rel="noopener noreferrer">PR #16237</a></li>
<li>Add Prometheus metric to track callback logging failures in S3 - <a href="https://github.com/BerriAI/litellm/pull/16209" target="_blank" rel="noopener noreferrer">PR #16209</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/logging#opentelemetry">OpenTelemetry</a></strong></p>
<ul>
<li>OTEL - Log Cost Breakdown on OTEL Logger - <a href="https://github.com/BerriAI/litellm/pull/16334" target="_blank" rel="noopener noreferrer">PR #16334</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/logging#datadog">DataDog</a></strong></p>
<ul>
<li>Add DD Agent Host support for <code>datadog</code> callback - <a href="https://github.com/BerriAI/litellm/pull/16379" target="_blank" rel="noopener noreferrer">PR #16379</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="guardrails">Guardrails<a href="#guardrails" class="hash-link" aria-label="Guardrails" title="Guardrails"></a></h3>
<ul>
<li>
<p><strong><a href="/docs/proxy/guardrails">Noma</a></strong></p>
<ul>
<li>Revert Noma Apply Guardrail implementation - <a href="https://github.com/BerriAI/litellm/pull/16214" target="_blank" rel="noopener noreferrer">PR #16214</a></li>
<li>Litellm noma guardrail support images - <a href="https://github.com/BerriAI/litellm/pull/16199" target="_blank" rel="noopener noreferrer">PR #16199</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/guardrails">PANW Prisma AIRS</a></strong></p>
<ul>
<li>PANW prisma airs guardrail deduplication and enhanced session tracking - <a href="https://github.com/BerriAI/litellm/pull/16273" target="_blank" rel="noopener noreferrer">PR #16273</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/guardrails">LiteLLM Custom Guardrail</a></strong></p>
<ul>
<li>Add LiteLLM Gateway built in guardrail - <a href="https://github.com/BerriAI/litellm/pull/16338" target="_blank" rel="noopener noreferrer">PR #16338</a></li>
<li>UI - Allow configuring LiteLLM Custom Guardrail - <a href="https://github.com/BerriAI/litellm/pull/16339" target="_blank" rel="noopener noreferrer">PR #16339</a></li>
<li>Bug Fix: Content Filter Guard - <a href="https://github.com/BerriAI/litellm/pull/16414" target="_blank" rel="noopener noreferrer">PR #16414</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="secret-managers">Secret Managers<a href="#secret-managers" class="hash-link" aria-label="Secret Managers" title="Secret Managers"></a></h3>
<ul>
<li>
<p><strong><a href="/docs/secret_managers">CyberArk</a></strong></p>
<ul>
<li>Add CyberArk Secrets Manager Integration - <a href="https://github.com/BerriAI/litellm/pull/16278" target="_blank" rel="noopener noreferrer">PR #16278</a></li>
<li>Cyber Ark - Add Key Rotations support - <a href="https://github.com/BerriAI/litellm/pull/16289" target="_blank" rel="noopener noreferrer">PR #16289</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/secret_managers">HashiCorp Vault</a></strong></p>
<ul>
<li>Add configurable mount name and path prefix for HashiCorp Vault - <a href="https://github.com/BerriAI/litellm/pull/16253" target="_blank" rel="noopener noreferrer">PR #16253</a></li>
<li>Secret Manager - Hashicorp, add auth via approle - <a href="https://github.com/BerriAI/litellm/pull/16374" target="_blank" rel="noopener noreferrer">PR #16374</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/secret_managers">AWS Secrets Manager</a></strong></p>
<ul>
<li>Add tags and descriptions support to aws secrets manager - <a href="https://github.com/BerriAI/litellm/pull/16224" target="_blank" rel="noopener noreferrer">PR #16224</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/secret_managers">Custom Secret Manager</a></strong></p>
<ul>
<li>Add Custom Secret Manager - Allow users to define and write a custom secret manager - <a href="https://github.com/BerriAI/litellm/pull/16297" target="_blank" rel="noopener noreferrer">PR #16297</a></li>
</ul>
</li>
<li>
<p><strong>General</strong></p>
<ul>
<li>Email Notifications - Ensure Users get Key Rotated Email - <a href="https://github.com/BerriAI/litellm/pull/16292" target="_blank" rel="noopener noreferrer">PR #16292</a></li>
<li>Fix verify ssl on sts boto3 - <a href="https://github.com/BerriAI/litellm/pull/16313" target="_blank" rel="noopener noreferrer">PR #16313</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-budgets-and-rate-limiting">Spend Tracking, Budgets and Rate Limiting<a href="#spend-tracking-budgets-and-rate-limiting" class="hash-link" aria-label="Spend Tracking, Budgets and Rate Limiting" title="Spend Tracking, Budgets and Rate Limiting"></a></h2>
<ul>
<li><strong>Cost Tracking</strong>
<ul>
<li>Fix OpenAI Responses API streaming tests usage field names and cost calculation - <a href="https://github.com/BerriAI/litellm/pull/16236" target="_blank" rel="noopener noreferrer">PR #16236</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway">MCP Gateway<a href="#mcp-gateway" class="hash-link" aria-label="MCP Gateway" title="MCP Gateway"></a></h2>
<ul>
<li><strong>Configuration</strong>
<ul>
<li>Configure static mcp header - <a href="https://github.com/BerriAI/litellm/pull/16179" target="_blank" rel="noopener noreferrer">PR #16179</a></li>
<li>Persist mcp credentials in db - <a href="https://github.com/BerriAI/litellm/pull/16308" target="_blank" rel="noopener noreferrer">PR #16308</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<ul>
<li>
<p><strong>Memory Leak Fixes</strong></p>
<ul>
<li>Resolve memory accumulation caused by Pydantic 2.11+ deprecation warnings - <a href="https://github.com/BerriAI/litellm/pull/16110" target="_blank" rel="noopener noreferrer">PR #16110</a></li>
</ul>
</li>
<li>
<p><strong>Session Management</strong></p>
<ul>
<li>Add shared_session support to responses API - <a href="https://github.com/BerriAI/litellm/pull/16260" target="_blank" rel="noopener noreferrer">PR #16260</a></li>
</ul>
</li>
<li>
<p><strong>Error Handling</strong></p>
<ul>
<li>Gracefully handle connection closed errors during streaming - <a href="https://github.com/BerriAI/litellm/pull/16294" target="_blank" rel="noopener noreferrer">PR #16294</a></li>
<li>Handle None values in daily spend sort key - <a href="https://github.com/BerriAI/litellm/pull/16245" target="_blank" rel="noopener noreferrer">PR #16245</a></li>
</ul>
</li>
<li>
<p><strong>Configuration</strong></p>
<ul>
<li>Remove minimum validation for cache control injection index - <a href="https://github.com/BerriAI/litellm/pull/16149" target="_blank" rel="noopener noreferrer">PR #16149</a></li>
<li>Improve clearing logic - only remove unvisited endpoints - <a href="https://github.com/BerriAI/litellm/pull/16400" target="_blank" rel="noopener noreferrer">PR #16400</a></li>
</ul>
</li>
<li>
<p><strong>Redis</strong></p>
<ul>
<li>Handle float redis_version from AWS ElastiCache Valkey - <a href="https://github.com/BerriAI/litellm/pull/16207" target="_blank" rel="noopener noreferrer">PR #16207</a></li>
</ul>
</li>
<li>
<p><strong>Hooks</strong></p>
<ul>
<li>Add parallel execution handling in during_call_hook - <a href="https://github.com/BerriAI/litellm/pull/16279" target="_blank" rel="noopener noreferrer">PR #16279</a></li>
</ul>
</li>
<li>
<p><strong>Infrastructure</strong></p>
<ul>
<li>Install runtime node for prisma - <a href="https://github.com/BerriAI/litellm/pull/16410" target="_blank" rel="noopener noreferrer">PR #16410</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="documentation-updates">Documentation Updates<a href="#documentation-updates" class="hash-link" aria-label="Documentation Updates" title="Documentation Updates"></a></h2>
<ul>
<li>
<p><strong>Provider Documentation</strong></p>
<ul>
<li>Docs - v1.79.1 - <a href="https://github.com/BerriAI/litellm/pull/16163" target="_blank" rel="noopener noreferrer">PR #16163</a></li>
<li>Fix broken link on model_management.md - <a href="https://github.com/BerriAI/litellm/pull/16217" target="_blank" rel="noopener noreferrer">PR #16217</a></li>
<li>Fix image generation response format - use &#x27;images&#x27; array instead of &#x27;image&#x27; object - <a href="https://github.com/BerriAI/litellm/pull/16378" target="_blank" rel="noopener noreferrer">PR #16378</a></li>
</ul>
</li>
<li>
<p><strong>General Documentation</strong></p>
<ul>
<li>Add minimum resource requirement for production - <a href="https://github.com/BerriAI/litellm/pull/16146" target="_blank" rel="noopener noreferrer">PR #16146</a></li>
<li>Add benchmark comparison with other AI gateways - <a href="https://github.com/BerriAI/litellm/pull/16248" target="_blank" rel="noopener noreferrer">PR #16248</a></li>
<li>LiteLLM content filter guard documentation - <a href="https://github.com/BerriAI/litellm/pull/16413" target="_blank" rel="noopener noreferrer">PR #16413</a></li>
<li>Fix typo of the word orginal - <a href="https://github.com/BerriAI/litellm/pull/16255" target="_blank" rel="noopener noreferrer">PR #16255</a></li>
</ul>
</li>
<li>
<p><strong>Security</strong></p>
<ul>
<li>Remove tornado test files (including test.key), fixes Python 3.13 security issues - <a href="https://github.com/BerriAI/litellm/pull/16342" target="_blank" rel="noopener noreferrer">PR #16342</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@steve-gore-snapdocs made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16149" target="_blank" rel="noopener noreferrer">PR #16149</a></li>
<li>@timbmg made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16120" target="_blank" rel="noopener noreferrer">PR #16120</a></li>
<li>@Nivg made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16202" target="_blank" rel="noopener noreferrer">PR #16202</a></li>
<li>@pablobgar made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16194" target="_blank" rel="noopener noreferrer">PR #16194</a></li>
<li>@AlanPonnachan made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16150" target="_blank" rel="noopener noreferrer">PR #16150</a></li>
<li>@Chesars made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16236" target="_blank" rel="noopener noreferrer">PR #16236</a></li>
<li>@bowenliang123 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16255" target="_blank" rel="noopener noreferrer">PR #16255</a></li>
<li>@dean-zavad made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16199" target="_blank" rel="noopener noreferrer">PR #16199</a></li>
<li>@alexkuzmik made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15529" target="_blank" rel="noopener noreferrer">PR #15529</a></li>
<li>@Granine made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16281" target="_blank" rel="noopener noreferrer">PR #16281</a></li>
<li>@Oodapow made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16279" target="_blank" rel="noopener noreferrer">PR #16279</a></li>
<li>@jgoodyear made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16275" target="_blank" rel="noopener noreferrer">PR #16275</a></li>
<li>@Qanpi made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16321" target="_blank" rel="noopener noreferrer">PR #16321</a></li>
<li>@ShimonMimoun made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16313" target="_blank" rel="noopener noreferrer">PR #16313</a></li>
<li>@andriykislitsyn made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16288" target="_blank" rel="noopener noreferrer">PR #16288</a></li>
<li>@reckless-huang made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16263" target="_blank" rel="noopener noreferrer">PR #16263</a></li>
<li>@chenmoneygithub made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16368" target="_blank" rel="noopener noreferrer">PR #16368</a></li>
<li>@stembe-digitalex made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16354" target="_blank" rel="noopener noreferrer">PR #16354</a></li>
<li>@jfcherng made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16352" target="_blank" rel="noopener noreferrer">PR #16352</a></li>
<li>@xingyaoww made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16246" target="_blank" rel="noopener noreferrer">PR #16246</a></li>
<li>@emerzon made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16373" target="_blank" rel="noopener noreferrer">PR #16373</a></li>
<li>@wwwillchen made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16376" target="_blank" rel="noopener noreferrer">PR #16376</a></li>
<li>@fabriciojoc made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16203" target="_blank" rel="noopener noreferrer">PR #16203</a></li>
<li>@jroberts2600 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16273" target="_blank" rel="noopener noreferrer">PR #16273</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog">Full Changelog<a href="#full-changelog" class="hash-link" aria-label="Full Changelog" title="Full Changelog"></a></h2>
<p><strong><a href="https://github.com/BerriAI/litellm/compare/v1.79.1-nightly...v1.79.2.rc.1" target="_blank" rel="noopener noreferrer">View complete changelog on GitHub</a></strong></p></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-79-1">v1.79.1-stable - Guardrail Playground</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-11-01T10:00:00.000Z">2025111</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaff"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaff</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.79.1-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.80.0</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>Container API Support</strong> - End-to-end OpenAI Container API support with proxy integration, logging, and cost tracking</li>
<li><strong>FAL AI Image Generation</strong> - Native support for FAL AI image generation models with cost tracking</li>
<li><strong>UI Enhancements</strong> - Guardrail Playground, Cache Settings, Tag Routing, SSO Settings</li>
<li><strong>Batch API Rate Limiting</strong> - Input-based rate limits support for Batch API requests</li>
<li><strong>Vector Store Expansion</strong> - Milvus vector store support and Azure AI virtual indexes</li>
<li><strong>Memory Leak Fixes</strong> - Resolved issues accounting for 90% of memory leaks on Python SDK &amp; AI Gateway</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="dependency-upgrades">Dependency Upgrades<a href="#dependency-upgrades" class="hash-link" aria-label="Dependency Upgrades" title="Dependency Upgrades"></a></h2>
<ul>
<li><strong>Dependencies</strong>
<ul>
<li>Build(deps): bump starlette from 0.47.2 to 0.49.1 - <a href="https://github.com/BerriAI/litellm/pull/16027" target="_blank" rel="noopener noreferrer">PR #16027</a></li>
<li>Build(deps): bump fastapi from 0.116.1 to 0.120.1 - <a href="https://github.com/BerriAI/litellm/pull/16054" target="_blank" rel="noopener noreferrer">PR #16054</a></li>
<li>Build(deps): bump hono from 4.9.7 to 4.10.3 in /litellm-js/spend-logs - <a href="https://github.com/BerriAI/litellm/pull/15915" target="_blank" rel="noopener noreferrer">PR #15915</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-model-support">New Model Support<a href="#new-model-support" class="hash-link" aria-label="New Model Support" title="New Model Support"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Features</th></tr></thead><tbody><tr><td>Mistral</td><td><code>mistral/codestral-embed</code></td><td>8K</td><td>$0.15</td><td>-</td><td>Embeddings</td></tr><tr><td>Mistral</td><td><code>mistral/codestral-embed-2505</code></td><td>8K</td><td>$0.15</td><td>-</td><td>Embeddings</td></tr><tr><td>Gemini</td><td><code>gemini/gemini-embedding-001</code></td><td>2K</td><td>$0.15</td><td>-</td><td>Embeddings</td></tr><tr><td>FAL AI</td><td><code>fal_ai/fal-ai/flux-pro/v1.1-ultra</code></td><td>-</td><td>-</td><td>-</td><td>Image generation - $0.0398/image</td></tr><tr><td>FAL AI</td><td><code>fal_ai/fal-ai/imagen4/preview</code></td><td>-</td><td>-</td><td>-</td><td>Image generation - $0.0398/image</td></tr><tr><td>FAL AI</td><td><code>fal_ai/fal-ai/recraft/v3/text-to-image</code></td><td>-</td><td>-</td><td>-</td><td>Image generation - $0.0398/image</td></tr><tr><td>FAL AI</td><td><code>fal_ai/fal-ai/stable-diffusion-v35-medium</code></td><td>-</td><td>-</td><td>-</td><td>Image generation - $0.0398/image</td></tr><tr><td>FAL AI</td><td><code>fal_ai/bria/text-to-image/3.2</code></td><td>-</td><td>-</td><td>-</td><td>Image generation - $0.0398/image</td></tr><tr><td>OpenAI</td><td><code>openai/sora-2-pro</code></td><td>-</td><td>-</td><td>-</td><td>Video generation - $0.30/video/second</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/providers/anthropic">Anthropic</a></strong></p>
<ul>
<li>Extended Claude 3-7 Sonnet deprecation date from 2026-02-01 to 2026-02-19 - <a href="https://github.com/BerriAI/litellm/pull/15976" target="_blank" rel="noopener noreferrer">PR #15976</a></li>
<li>Extended Claude Opus 4-0 deprecation date from 2025-03-01 to 2026-05-01 - <a href="https://github.com/BerriAI/litellm/pull/15976" target="_blank" rel="noopener noreferrer">PR #15976</a></li>
<li>Removed Claude Haiku 3-5 deprecation date (previously 2025-03-01) - <a href="https://github.com/BerriAI/litellm/pull/15976" target="_blank" rel="noopener noreferrer">PR #15976</a></li>
<li>Added Claude Opus 4-1, Claude Opus 4-0 20250513, Claude Sonnet 4 20250514 deprecation dates - <a href="https://github.com/BerriAI/litellm/pull/15976" target="_blank" rel="noopener noreferrer">PR #15976</a></li>
<li>Added web search support for Claude Opus 4-1 - <a href="https://github.com/BerriAI/litellm/pull/15976" target="_blank" rel="noopener noreferrer">PR #15976</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/bedrock">Bedrock</a></strong></p>
<ul>
<li>Fix empty assistant message handling in AWS Bedrock Converse API to prevent 400 Bad Request errors - <a href="https://github.com/BerriAI/litellm/pull/15850" target="_blank" rel="noopener noreferrer">PR #15850</a></li>
<li>Allow using ARNs when generating images via Bedrock - <a href="https://github.com/BerriAI/litellm/pull/15789" target="_blank" rel="noopener noreferrer">PR #15789</a></li>
<li>Add per model group header forwarding for Bedrock Invoke API - <a href="https://github.com/BerriAI/litellm/pull/16042" target="_blank" rel="noopener noreferrer">PR #16042</a></li>
<li>Preserve Bedrock inference profile IDs in health checks - <a href="https://github.com/BerriAI/litellm/pull/15947" target="_blank" rel="noopener noreferrer">PR #15947</a></li>
<li>Added fallback logic for detecting file content-type when S3 returns generic type - When using Bedrock with S3-hosted files, if the S3 object&#x27;s Content-Type is not correctly set (e.g., binary/octet-stream instead of image/png), Bedrock can now handle it correctly - <a href="https://github.com/BerriAI/litellm/pull/15635" target="_blank" rel="noopener noreferrer">PR #15635</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/azure">Azure</a></strong></p>
<ul>
<li>Add deprecation dates for Azure OpenAI models (gpt-4o-2024-08-06, gpt-4o-2024-11-20, gpt-4.1 series, o3-2025-04-16, text-embedding-3-small) - <a href="https://github.com/BerriAI/litellm/pull/15976" target="_blank" rel="noopener noreferrer">PR #15976</a></li>
<li>Fix Azure OpenAI ContextWindowExceededError mapping from Azure errors - <a href="https://github.com/BerriAI/litellm/pull/15981" target="_blank" rel="noopener noreferrer">PR #15981</a></li>
<li>Add handling for <code>v1</code> under Azure API versions - <a href="https://github.com/BerriAI/litellm/pull/15984" target="_blank" rel="noopener noreferrer">PR #15984</a></li>
<li>Fix azure doesn&#x27;t accept extra body param - <a href="https://github.com/BerriAI/litellm/pull/16116" target="_blank" rel="noopener noreferrer">PR #16116</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/openai">OpenAI</a></strong></p>
<ul>
<li>Add deprecation dates for gpt-3.5-turbo-1106, gpt-4-0125-preview, gpt-4-1106-preview, o1-mini-2024-09-12 - <a href="https://github.com/BerriAI/litellm/pull/15976" target="_blank" rel="noopener noreferrer">PR #15976</a></li>
<li>Add extended Sora-2 modality support (text + image inputs) - <a href="https://github.com/BerriAI/litellm/pull/15976" target="_blank" rel="noopener noreferrer">PR #15976</a></li>
<li>Updated OpenAI Sora-2-Pro pricing to $0.30/video/second - <a href="https://github.com/BerriAI/litellm/pull/15976" target="_blank" rel="noopener noreferrer">PR #15976</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/openrouter">OpenRouter</a></strong></p>
<ul>
<li>Add Claude Haiku 4.5 pricing for OpenRouter - <a href="https://github.com/BerriAI/litellm/pull/15909" target="_blank" rel="noopener noreferrer">PR #15909</a></li>
<li>Add base_url config with environment variables documentation - <a href="https://github.com/BerriAI/litellm/pull/15946" target="_blank" rel="noopener noreferrer">PR #15946</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/mistral">Mistral</a></strong></p>
<ul>
<li>Add codestral-embed-2505 embedding model - <a href="https://github.com/BerriAI/litellm/pull/16071" target="_blank" rel="noopener noreferrer">PR #16071</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/gemini">Gemini (Google AI Studio + Vertex AI)</a></strong></p>
<ul>
<li>Fix gemini request mutation for tool use - <a href="https://github.com/BerriAI/litellm/pull/16002" target="_blank" rel="noopener noreferrer">PR #16002</a></li>
<li>Add gemini-embedding-001 pricing entry for Google GenAI API - <a href="https://github.com/BerriAI/litellm/pull/16078" target="_blank" rel="noopener noreferrer">PR #16078</a></li>
<li>Changes to fix frequency_penalty and presence_penalty issue for gemini-2.5-pro model - <a href="https://github.com/BerriAI/litellm/pull/16041" target="_blank" rel="noopener noreferrer">PR #16041</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/deepinfra">DeepInfra</a></strong></p>
<ul>
<li>Add vision support for Qwen/Qwen3-chat-32b model - <a href="https://github.com/BerriAI/litellm/pull/15976" target="_blank" rel="noopener noreferrer">PR #15976</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/vercel_ai_gateway">Vercel AI Gateway</a></strong></p>
<ul>
<li>Fix vercel_ai_gateway entry for glm-4.6 (moved from vercel_ai_gateway/glm-4.6 to vercel_ai_gateway/zai/glm-4.6) - <a href="https://github.com/BerriAI/litellm/pull/16084" target="_blank" rel="noopener noreferrer">PR #16084</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/fireworks_ai">Fireworks</a></strong></p>
<ul>
<li>Don&#x27;t add &quot;accounts/fireworks/models&quot; prefix for Fireworks Provider - <a href="https://github.com/BerriAI/litellm/pull/15938" target="_blank" rel="noopener noreferrer">PR #15938</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/cohere">Cohere</a></strong></p>
<ul>
<li>Add OpenAI-compatible annotations support for Cohere v2 citations - <a href="https://github.com/BerriAI/litellm/pull/16038" target="_blank" rel="noopener noreferrer">PR #16038</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/deepgram">Deepgram</a></strong></p>
<ul>
<li>Handle Deepgram detected language when available - <a href="https://github.com/BerriAI/litellm/pull/16093" target="_blank" rel="noopener noreferrer">PR #16093</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes">Bug Fixes<a href="#bug-fixes" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h3>
<ul>
<li><strong><a href="/docs/providers/xai">Xai</a></strong>
<ul>
<li>Add Xai websearch cost tracking - <a href="https://github.com/BerriAI/litellm/pull/16001" target="_blank" rel="noopener noreferrer">PR #16001</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-provider-support">New Provider Support<a href="#new-provider-support" class="hash-link" aria-label="New Provider Support" title="New Provider Support"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/image_generation">FAL AI</a></strong></p>
<ul>
<li>Add FAL AI Image Generation support - <a href="https://github.com/BerriAI/litellm/pull/16067" target="_blank" rel="noopener noreferrer">PR #16067</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/oci">OCI (Oracle Cloud Infrastructure)</a></strong></p>
<ul>
<li>Add OCI Signer Authentication support - <a href="https://github.com/BerriAI/litellm/pull/16064" target="_blank" rel="noopener noreferrer">PR #16064</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/containers">Container API</a></strong></p>
<ul>
<li>Add end-to-end OpenAI Container API support to LiteLLM SDK - <a href="https://github.com/BerriAI/litellm/pull/16136" target="_blank" rel="noopener noreferrer">PR #16136</a></li>
<li>Add proxy support for container APIs - <a href="https://github.com/BerriAI/litellm/pull/16049" target="_blank" rel="noopener noreferrer">PR #16049</a></li>
<li>Add logging support for Container API - <a href="https://github.com/BerriAI/litellm/pull/16049" target="_blank" rel="noopener noreferrer">PR #16049</a></li>
<li>Add cost tracking support for containers with documentation - <a href="https://github.com/BerriAI/litellm/pull/16117" target="_blank" rel="noopener noreferrer">PR #16117</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/response_api">Responses API</a></strong></p>
<ul>
<li>Respect <code>LiteLLM-Disable-Message-Redaction</code> header for Responses API - <a href="https://github.com/BerriAI/litellm/pull/15966" target="_blank" rel="noopener noreferrer">PR #15966</a></li>
<li>Add /openai routes for responses API (Azure OpenAI SDK Compatibility) - <a href="https://github.com/BerriAI/litellm/pull/15988" target="_blank" rel="noopener noreferrer">PR #15988</a></li>
<li>Redact reasoning summaries in ResponsesAPI output when message logging is disabled - <a href="https://github.com/BerriAI/litellm/pull/15965" target="_blank" rel="noopener noreferrer">PR #15965</a></li>
<li>Support text.format parameter in Responses API for providers without native ResponsesAPIConfig - <a href="https://github.com/BerriAI/litellm/pull/16023" target="_blank" rel="noopener noreferrer">PR #16023</a></li>
<li>Add LLM provider response headers to Responses API - <a href="https://github.com/BerriAI/litellm/pull/16091" target="_blank" rel="noopener noreferrer">PR #16091</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/video_generation">Video Generation API</a></strong></p>
<ul>
<li>Add <code>custom_llm_provider</code> support for video endpoints (non-generation) - <a href="https://github.com/BerriAI/litellm/pull/16121" target="_blank" rel="noopener noreferrer">PR #16121</a></li>
<li>Fix documentation for videos - <a href="https://github.com/BerriAI/litellm/pull/15937" target="_blank" rel="noopener noreferrer">PR #15937</a></li>
<li>Add OpenAI client usage documentation for videos and fix navigation visibility - <a href="https://github.com/BerriAI/litellm/pull/15996" target="_blank" rel="noopener noreferrer">PR #15996</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/moderations">Moderations API</a></strong></p>
<ul>
<li>Moderations endpoint now respects <code>api_base</code> configuration parameter - <a href="https://github.com/BerriAI/litellm/pull/16087" target="_blank" rel="noopener noreferrer">PR #16087</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/vector_stores">Vector Stores</a></strong></p>
<ul>
<li>Milvus - search vector store support - <a href="https://github.com/BerriAI/litellm/pull/16035" target="_blank" rel="noopener noreferrer">PR #16035</a></li>
<li>Azure AI Vector Stores - support &quot;virtual&quot; indexes + create vector store on passthrough API - <a href="https://github.com/BerriAI/litellm/pull/16160" target="_blank" rel="noopener noreferrer">PR #16160</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/pass_through/vertex_ai">Passthrough Endpoints</a></strong></p>
<ul>
<li>Support multi-part form data on passthrough - <a href="https://github.com/BerriAI/litellm/pull/16035" target="_blank" rel="noopener noreferrer">PR #16035</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong>Virtual Keys</strong></p>
<ul>
<li>Validation for Proxy Base URL in SSO Settings - <a href="https://github.com/BerriAI/litellm/pull/16082" target="_blank" rel="noopener noreferrer">PR #16082</a></li>
<li>Test Key UI Embeddings support - <a href="https://github.com/BerriAI/litellm/pull/16065" target="_blank" rel="noopener noreferrer">PR #16065</a></li>
<li>Add Key Type Select in Key Settings - <a href="https://github.com/BerriAI/litellm/pull/16034" target="_blank" rel="noopener noreferrer">PR #16034</a></li>
<li>Key Already Exist Error Notification - <a href="https://github.com/BerriAI/litellm/pull/15993" target="_blank" rel="noopener noreferrer">PR #15993</a></li>
</ul>
</li>
<li>
<p><strong>Models + Endpoints</strong></p>
<ul>
<li>Changed API Base from Select to Input in New LLM Credentials - <a href="https://github.com/BerriAI/litellm/pull/15987" target="_blank" rel="noopener noreferrer">PR #15987</a></li>
<li>Remove limit from admin UI numerical input - <a href="https://github.com/BerriAI/litellm/pull/15991" target="_blank" rel="noopener noreferrer">PR #15991</a></li>
<li>Config Models should not be editable - <a href="https://github.com/BerriAI/litellm/pull/16020" target="_blank" rel="noopener noreferrer">PR #16020</a></li>
<li>Add tags in model creation - <a href="https://github.com/BerriAI/litellm/pull/16138" target="_blank" rel="noopener noreferrer">PR #16138</a></li>
<li>Add Tags to update model - <a href="https://github.com/BerriAI/litellm/pull/16140" target="_blank" rel="noopener noreferrer">PR #16140</a></li>
</ul>
</li>
<li>
<p><strong>Guardrails</strong></p>
<ul>
<li>Add Apply Guardrail Testing Playground - <a href="https://github.com/BerriAI/litellm/pull/16030" target="_blank" rel="noopener noreferrer">PR #16030</a></li>
<li>Config Guardrails should not be editable and guardrail info fix - <a href="https://github.com/BerriAI/litellm/pull/16142" target="_blank" rel="noopener noreferrer">PR #16142</a></li>
</ul>
</li>
<li>
<p><strong>Cache Settings</strong></p>
<ul>
<li>Allow setting cache settings on UI - <a href="https://github.com/BerriAI/litellm/pull/16143" target="_blank" rel="noopener noreferrer">PR #16143</a></li>
</ul>
</li>
<li>
<p><strong>Routing</strong></p>
<ul>
<li>Allow setting all routing strategies, tag filtering on UI - <a href="https://github.com/BerriAI/litellm/pull/16139" target="_blank" rel="noopener noreferrer">PR #16139</a></li>
</ul>
</li>
<li>
<p><strong>Admin Settings</strong></p>
<ul>
<li>Add license metadata to health/readiness endpoint - <a href="https://github.com/BerriAI/litellm/pull/15997" target="_blank" rel="noopener noreferrer">PR #15997</a></li>
<li>Litellm Backend SSO Changes - <a href="https://github.com/BerriAI/litellm/pull/16029" target="_blank" rel="noopener noreferrer">PR #16029</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail--prompt-management-integrations">Logging / Guardrail / Prompt Management Integrations<a href="#logging--guardrail--prompt-management-integrations" class="hash-link" aria-label="Logging / Guardrail / Prompt Management Integrations" title="Logging / Guardrail / Prompt Management Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/proxy/logging#opentelemetry">OpenTelemetry</a></strong></p>
<ul>
<li>Enable OpenTelemetry context propagation by external tracers - <a href="https://github.com/BerriAI/litellm/pull/15940" target="_blank" rel="noopener noreferrer">PR #15940</a></li>
<li>Ensure error information is logged on OTEL - <a href="https://github.com/BerriAI/litellm/pull/15978" target="_blank" rel="noopener noreferrer">PR #15978</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/logging#langfuse">Langfuse</a></strong></p>
<ul>
<li>Fix duplicate trace in langfuse_otel - <a href="https://github.com/BerriAI/litellm/pull/15931" target="_blank" rel="noopener noreferrer">PR #15931</a></li>
<li>Support tool usage messages with Langfuse OTEL integration - <a href="https://github.com/BerriAI/litellm/pull/15932" target="_blank" rel="noopener noreferrer">PR #15932</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/logging#datadog">DataDog</a></strong></p>
<ul>
<li>Ensure key&#x27;s metadata + guardrail is logged on DD - <a href="https://github.com/BerriAI/litellm/pull/15980" target="_blank" rel="noopener noreferrer">PR #15980</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/logging#opik">Opik</a></strong></p>
<ul>
<li>Enhance requester metadata retrieval from API key auth - <a href="https://github.com/BerriAI/litellm/pull/15897" target="_blank" rel="noopener noreferrer">PR #15897</a></li>
<li>User auth key metadata Documentation - <a href="https://github.com/BerriAI/litellm/pull/16004" target="_blank" rel="noopener noreferrer">PR #16004</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/logging#sqs">SQS</a></strong></p>
<ul>
<li>Add Base64 handling for SQS Logger - <a href="https://github.com/BerriAI/litellm/pull/16028" target="_blank" rel="noopener noreferrer">PR #16028</a></li>
</ul>
</li>
<li>
<p><strong>General</strong></p>
<ul>
<li>Fix: User API key and team id and user id missing from custom callback is not misfiring - <a href="https://github.com/BerriAI/litellm/pull/15982" target="_blank" rel="noopener noreferrer">PR #15982</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="guardrails">Guardrails<a href="#guardrails" class="hash-link" aria-label="Guardrails" title="Guardrails"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/proxy/guardrails">IBM Guardrails</a></strong></p>
<ul>
<li>Update IBM Guardrails to correctly use SSL Verify argument - <a href="https://github.com/BerriAI/litellm/pull/15975" target="_blank" rel="noopener noreferrer">PR #15975</a></li>
<li>Add additional detail to ibm_guardrails.md documentation - <a href="https://github.com/BerriAI/litellm/pull/15971" target="_blank" rel="noopener noreferrer">PR #15971</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/guardrails">Model Armor</a></strong></p>
<ul>
<li>Support during_call for model armor guardrails - <a href="https://github.com/BerriAI/litellm/pull/15970" target="_blank" rel="noopener noreferrer">PR #15970</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/guardrails">Lasso Security</a></strong></p>
<ul>
<li>Upgrade to Lasso API v3 and fix ULID generation - <a href="https://github.com/BerriAI/litellm/pull/15941" target="_blank" rel="noopener noreferrer">PR #15941</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/guardrails">PANW Prisma AIRS</a></strong></p>
<ul>
<li>Add per-request profile overrides to PANW Prisma AIRS - <a href="https://github.com/BerriAI/litellm/pull/16069" target="_blank" rel="noopener noreferrer">PR #16069</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/guardrails">Grayswan</a></strong></p>
<ul>
<li>Improve Grayswan guardrail documentation - <a href="https://github.com/BerriAI/litellm/pull/15875" target="_blank" rel="noopener noreferrer">PR #15875</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/guardrails">Pillar AI</a></strong></p>
<ul>
<li>Graceful degradation for pillar service when using litellm - <a href="https://github.com/BerriAI/litellm/pull/15857" target="_blank" rel="noopener noreferrer">PR #15857</a></li>
</ul>
</li>
<li>
<p><strong>General</strong></p>
<ul>
<li>Ensure Key Guardrails are applied - <a href="https://github.com/BerriAI/litellm/pull/16025" target="_blank" rel="noopener noreferrer">PR #16025</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="prompt-management">Prompt Management<a href="#prompt-management" class="hash-link" aria-label="Prompt Management" title="Prompt Management"></a></h4>
<ul>
<li><strong><a href="/docs/prompt_management">GitLab</a></strong>
<ul>
<li>Add GitlabPromptCache and enable subfolder access - <a href="https://github.com/BerriAI/litellm/pull/15712" target="_blank" rel="noopener noreferrer">PR #15712</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-budgets-and-rate-limiting">Spend Tracking, Budgets and Rate Limiting<a href="#spend-tracking-budgets-and-rate-limiting" class="hash-link" aria-label="Spend Tracking, Budgets and Rate Limiting" title="Spend Tracking, Budgets and Rate Limiting"></a></h2>
<ul>
<li>
<p><strong>Cost Tracking</strong></p>
<ul>
<li>Fix spend tracking for OCR/aOCR requests (log <code>pages_processed</code> + recognize <code>OCRResponse</code>) - <a href="https://github.com/BerriAI/litellm/pull/16070" target="_blank" rel="noopener noreferrer">PR #16070</a></li>
</ul>
</li>
<li>
<p><strong>Rate Limiting</strong></p>
<ul>
<li>Add support for Batch API Rate limiting - PR1 adds support for input based rate limits - <a href="https://github.com/BerriAI/litellm/pull/16075" target="_blank" rel="noopener noreferrer">PR #16075</a></li>
<li>Handle multiple rate limit types per descriptor and prevent IndexError - <a href="https://github.com/BerriAI/litellm/pull/16039" target="_blank" rel="noopener noreferrer">PR #16039</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway">MCP Gateway<a href="#mcp-gateway" class="hash-link" aria-label="MCP Gateway" title="MCP Gateway"></a></h2>
<ul>
<li><strong>OAuth</strong>
<ul>
<li>Add support for dynamic client registration - <a href="https://github.com/BerriAI/litellm/pull/15921" target="_blank" rel="noopener noreferrer">PR #15921</a></li>
<li>Respect X-Forwarded- headers in OAuth endpoints - <a href="https://github.com/BerriAI/litellm/pull/16036" target="_blank" rel="noopener noreferrer">PR #16036</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<ul>
<li>
<p><strong>Memory Leak Fixes</strong></p>
<ul>
<li>Fix: prevent httpx DeprecationWarning memory leak in AsyncHTTPHandler - <a href="https://github.com/BerriAI/litellm/pull/16024" target="_blank" rel="noopener noreferrer">PR #16024</a></li>
<li>Fix: resolve memory accumulation caused by Pydantic 2.11+ deprecation warnings - <a href="https://github.com/BerriAI/litellm/pull/16110" target="_blank" rel="noopener noreferrer">PR #16110</a></li>
<li>Fix(apscheduler): prevent memory leaks from jitter and frequent job intervals - <a href="https://github.com/BerriAI/litellm/pull/15846" target="_blank" rel="noopener noreferrer">PR #15846</a></li>
</ul>
</li>
<li>
<p><strong>Configuration</strong></p>
<ul>
<li>Remove minimum validation for cache control injection index - <a href="https://github.com/BerriAI/litellm/pull/16149" target="_blank" rel="noopener noreferrer">PR #16149</a></li>
<li>Fix prompt_caching.md: wrong prompt_tokens definition - <a href="https://github.com/BerriAI/litellm/pull/16044" target="_blank" rel="noopener noreferrer">PR #16044</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="documentation-updates">Documentation Updates<a href="#documentation-updates" class="hash-link" aria-label="Documentation Updates" title="Documentation Updates"></a></h2>
<ul>
<li>
<p><strong>Provider Documentation</strong></p>
<ul>
<li>Use custom-llm-provider header in examples - <a href="https://github.com/BerriAI/litellm/pull/16055" target="_blank" rel="noopener noreferrer">PR #16055</a></li>
<li>Litellm docs readme fixes - <a href="https://github.com/BerriAI/litellm/pull/16107" target="_blank" rel="noopener noreferrer">PR #16107</a></li>
<li>Readme fixes add supported providers - <a href="https://github.com/BerriAI/litellm/pull/16109" target="_blank" rel="noopener noreferrer">PR #16109</a></li>
</ul>
</li>
<li>
<p><strong>Model References</strong></p>
<ul>
<li>Add supports vision field to qwen-vl models in model_prices_and_context_window.json - <a href="https://github.com/BerriAI/litellm/pull/16106" target="_blank" rel="noopener noreferrer">PR #16106</a></li>
</ul>
</li>
<li>
<p><strong>General Documentation</strong></p>
<ul>
<li>1-79-0 docs - <a href="https://github.com/BerriAI/litellm/pull/15936" target="_blank" rel="noopener noreferrer">PR #15936</a></li>
<li>Add minimum resource requirement for production - <a href="https://github.com/BerriAI/litellm/pull/16146" target="_blank" rel="noopener noreferrer">PR #16146</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@RobGeada made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15975" target="_blank" rel="noopener noreferrer">PR #15975</a></li>
<li>@shanto12 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15946" target="_blank" rel="noopener noreferrer">PR #15946</a></li>
<li>@dima-hx430 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15976" target="_blank" rel="noopener noreferrer">PR #15976</a></li>
<li>@m-misiura made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15971" target="_blank" rel="noopener noreferrer">PR #15971</a></li>
<li>@ylgibby made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15947" target="_blank" rel="noopener noreferrer">PR #15947</a></li>
<li>@Somtom made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15909" target="_blank" rel="noopener noreferrer">PR #15909</a></li>
<li>@rodolfo-nobrega made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16023" target="_blank" rel="noopener noreferrer">PR #16023</a></li>
<li>@bernata made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15997" target="_blank" rel="noopener noreferrer">PR #15997</a></li>
<li>@AlbertDeFusco made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15881" target="_blank" rel="noopener noreferrer">PR #15881</a></li>
<li>@komarovd95 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15789" target="_blank" rel="noopener noreferrer">PR #15789</a></li>
<li>@langpingxue made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15635" target="_blank" rel="noopener noreferrer">PR #15635</a></li>
<li>@OrionCodeDev made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16070" target="_blank" rel="noopener noreferrer">PR #16070</a></li>
<li>@sbinnee made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16078" target="_blank" rel="noopener noreferrer">PR #16078</a></li>
<li>@JetoPistola made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16106" target="_blank" rel="noopener noreferrer">PR #16106</a></li>
<li>@gvioss made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16093" target="_blank" rel="noopener noreferrer">PR #16093</a></li>
<li>@pale-aura made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16084" target="_blank" rel="noopener noreferrer">PR #16084</a></li>
<li>@tanvithakur94 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16041" target="_blank" rel="noopener noreferrer">PR #16041</a></li>
<li>@li-boxuan made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16044" target="_blank" rel="noopener noreferrer">PR #16044</a></li>
<li>@1stprinciple made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15938" target="_blank" rel="noopener noreferrer">PR #15938</a></li>
<li>@raghav-stripe made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16137" target="_blank" rel="noopener noreferrer">PR #16137</a></li>
<li>@steve-gore-snapdocs made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/16149" target="_blank" rel="noopener noreferrer">PR #16149</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog">Full Changelog<a href="#full-changelog" class="hash-link" aria-label="Full Changelog" title="Full Changelog"></a></h2>
<p><strong><a href="https://github.com/BerriAI/litellm/compare/v1.79.0-stable...v1.80.0-stable" target="_blank" rel="noopener noreferrer">View complete changelog on GitHub</a></strong></p></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-79-0">v1.79.0-stable - Search APIs</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-10-26T10:00:00.000Z">20251026</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaff"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaff</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.79.0-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.79.0</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="major-changes">Major Changes<a href="#major-changes" class="hash-link" aria-label="Major Changes" title="Major Changes"></a></h2>
<ul>
<li><strong>Cohere models will now be routed to Cohere v2 API by default</strong> - <a href="https://github.com/BerriAI/litellm/pull/15722" target="_blank" rel="noopener noreferrer">PR #15722</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>Search APIs</strong> - Native <code>/v1/search</code> endpoint with support for Perplexity, Tavily, Parallel AI, Exa AI, DataforSEO, and Google PSE with cost tracking</li>
<li><strong>Vector Stores</strong> - Vertex AI Search API integration as vector store through LiteLLM with passthrough endpoint support</li>
<li><strong>Guardrails Expansion</strong> - Apply guardrails across Responses API, Image Gen, Text completions, Audio transcriptions, Audio Speech, Rerank, and Anthropic Messages API via unified <code>apply_guardrails</code> function</li>
<li><strong>New Guardrail Providers</strong> - Gray Swan, Dynamo AI, IBM Guardrails, Lasso Security v3, and Bedrock Guardrail apply_guardrail endpoint support</li>
<li><strong>Video Generation API</strong> - Native support for OpenAI Sora-2 and Azure Sora-2 (Pro, Pro-High-Res) with cost tracking and logging support</li>
<li><strong>Azure AI Speech (TTS)</strong> - Native Azure AI Speech integration with cost tracking for standard and HD voices</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-model-support">New Model Support<a href="#new-model-support" class="hash-link" aria-label="New Model Support" title="New Model Support"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Features</th></tr></thead><tbody><tr><td>Bedrock</td><td><code>anthropic.claude-3-7-sonnet-20240620-v1:0</code></td><td>200K</td><td>$3.60</td><td>$18.00</td><td>Chat, reasoning, vision, function calling, prompt caching, computer use</td></tr><tr><td>Bedrock GovCloud</td><td><code>us-gov-west-1/anthropic.claude-3-7-sonnet-20250219-v1:0</code></td><td>200K</td><td>$3.60</td><td>$18.00</td><td>Chat, reasoning, vision, function calling, prompt caching, computer use</td></tr><tr><td>Vertex AI</td><td><code>mistral-medium-3</code></td><td>128K</td><td>$0.40</td><td>$2.00</td><td>Chat, function calling, tool choice</td></tr><tr><td>Vertex AI</td><td><code>codestral-2</code></td><td>128K</td><td>$0.30</td><td>$0.90</td><td>Chat, function calling, tool choice</td></tr><tr><td>Bedrock</td><td><code>amazon.titan-image-generator-v1</code></td><td>-</td><td>-</td><td>-</td><td>Image generation - $0.008/image, $0.01/premium image</td></tr><tr><td>Bedrock</td><td><code>amazon.titan-image-generator-v2</code></td><td>-</td><td>-</td><td>-</td><td>Image generation - $0.008/image, $0.01/premium image</td></tr><tr><td>OpenAI</td><td><code>sora-2</code></td><td>-</td><td>-</td><td>-</td><td>Video generation - $0.10/video/second</td></tr><tr><td>Azure</td><td><code>sora-2</code></td><td>-</td><td>-</td><td>-</td><td>Video generation - $0.10/video/second</td></tr><tr><td>Azure</td><td><code>sora-2-pro</code></td><td>-</td><td>-</td><td>-</td><td>Video generation - $0.30/video/second</td></tr><tr><td>Azure</td><td><code>sora-2-pro-high-res</code></td><td>-</td><td>-</td><td>-</td><td>Video generation - $0.50/video/second</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/providers/anthropic">Anthropic</a></strong></p>
<ul>
<li>Fix cache_control incorrectly applied to all content items instead of last item only - <a href="https://github.com/BerriAI/litellm/pull/15699" target="_blank" rel="noopener noreferrer">PR #15699</a></li>
<li>Forward anthropic-beta headers to Bedrock, VertexAI - <a href="https://github.com/BerriAI/litellm/pull/15700" target="_blank" rel="noopener noreferrer">PR #15700</a></li>
<li>Change max_tokens value to match max_output_tokens for claude sonnet - <a href="https://github.com/BerriAI/litellm/pull/15715" target="_blank" rel="noopener noreferrer">PR #15715</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/bedrock">Bedrock</a></strong></p>
<ul>
<li>Add AWS us-gov-west-1 Claude 3.7 Sonnet costs - <a href="https://github.com/BerriAI/litellm/pull/15775" target="_blank" rel="noopener noreferrer">PR #15775</a></li>
<li>Fix the date for sonnet 3.7 in govcloud - <a href="https://github.com/BerriAI/litellm/pull/15800" target="_blank" rel="noopener noreferrer">PR #15800</a></li>
<li>Use proper bedrock model name in health check - <a href="https://github.com/BerriAI/litellm/pull/15808" target="_blank" rel="noopener noreferrer">PR #15808</a></li>
<li>Support for embeddings_by_type Response Format in Bedrock Cohere Embed v1 - <a href="https://github.com/BerriAI/litellm/pull/15707" target="_blank" rel="noopener noreferrer">PR #15707</a></li>
<li>Add titan image generations with cost tracking - <a href="https://github.com/BerriAI/litellm/pull/15916" target="_blank" rel="noopener noreferrer">PR #15916</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/gemini">Gemini</a></strong></p>
<ul>
<li>Add imageConfig parameter for gemini-2.5-flash-image - <a href="https://github.com/BerriAI/litellm/pull/15530" target="_blank" rel="noopener noreferrer">PR #15530</a></li>
<li>Replace deprecated gemini-1.5-pro-preview-0514 - <a href="https://github.com/BerriAI/litellm/pull/15852" target="_blank" rel="noopener noreferrer">PR #15852</a></li>
<li>Update vertex ai gemini costs - <a href="https://github.com/BerriAI/litellm/pull/15911" target="_blank" rel="noopener noreferrer">PR #15911</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/ollama">Ollama</a></strong></p>
<ul>
<li>Set &#x27;think&#x27; to False when reasoning effort is minimal/none/disable - <a href="https://github.com/BerriAI/litellm/pull/15763" target="_blank" rel="noopener noreferrer">PR #15763</a></li>
<li>Handle parsing ollama chunk error - <a href="https://github.com/BerriAI/litellm/pull/15717" target="_blank" rel="noopener noreferrer">PR #15717</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/vertex">Vertex AI</a></strong></p>
<ul>
<li>Add mistral medium 3 and Codestral 2 on vertex - <a href="https://github.com/BerriAI/litellm/pull/15887" target="_blank" rel="noopener noreferrer">PR #15887</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/databricks">Databricks</a></strong></p>
<ul>
<li>Allow prompt caching to be used for Anthropic Claude on Databricks - <a href="https://github.com/BerriAI/litellm/pull/15801" target="_blank" rel="noopener noreferrer">PR #15801</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/azure">Azure</a></strong></p>
<ul>
<li>Add Azure AVA TTS integration - <a href="https://github.com/BerriAI/litellm/pull/15749" target="_blank" rel="noopener noreferrer">PR #15749</a></li>
<li>Add Azure AVA (Speech AI) Cost Tracking - <a href="https://github.com/BerriAI/litellm/pull/15754" target="_blank" rel="noopener noreferrer">PR #15754</a></li>
<li>Azure AI Speech - Ensure <code>voice</code> is mapped from request body to SSML body, allow sending <code>role</code> and <code>style</code> - <a href="https://github.com/BerriAI/litellm/pull/15810" target="_blank" rel="noopener noreferrer">PR #15810</a></li>
<li>Add Azure support for video generation functionality (Sora-2) - <a href="https://github.com/BerriAI/litellm/pull/15901" target="_blank" rel="noopener noreferrer">PR #15901</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/openai">OpenAI</a></strong></p>
<ul>
<li>OpenAI videos refactoring - <a href="https://github.com/BerriAI/litellm/pull/15900" target="_blank" rel="noopener noreferrer">PR #15900</a></li>
</ul>
</li>
<li>
<p><strong>General</strong></p>
<ul>
<li>Read from custom-llm-provider header - <a href="https://github.com/BerriAI/litellm/pull/15528" target="_blank" rel="noopener noreferrer">PR #15528</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/response_api">Responses API</a></strong></p>
<ul>
<li>Add gpt 4.1 pricing for response endpoint - <a href="https://github.com/BerriAI/litellm/pull/15593" target="_blank" rel="noopener noreferrer">PR #15593</a></li>
<li>Fix Incorrect status value in responses api with gemini - <a href="https://github.com/BerriAI/litellm/pull/15753" target="_blank" rel="noopener noreferrer">PR #15753</a></li>
<li>Simplify reasoning item handling for gpt-5-codex - <a href="https://github.com/BerriAI/litellm/pull/15815" target="_blank" rel="noopener noreferrer">PR #15815</a></li>
<li>ErrorEvent ValidationError when OpenAI Responses API returns nested error structure - <a href="https://github.com/BerriAI/litellm/pull/15804" target="_blank" rel="noopener noreferrer">PR #15804</a></li>
<li>Fix reasoning item ID auto-generation causing encrypted content verification errors - <a href="https://github.com/BerriAI/litellm/pull/15782" target="_blank" rel="noopener noreferrer">PR #15782</a></li>
<li>Support tags in metadata - <a href="https://github.com/BerriAI/litellm/pull/15867" target="_blank" rel="noopener noreferrer">PR #15867</a></li>
<li>Security: prevent User A from retrieving User B&#x27;s response, if response.id is leaked - <a href="https://github.com/BerriAI/litellm/pull/15757" target="_blank" rel="noopener noreferrer">PR #15757</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/batch_api">Batch API</a></strong></p>
<ul>
<li>Add pre and post call for list batches - <a href="https://github.com/BerriAI/litellm/pull/15673" target="_blank" rel="noopener noreferrer">PR #15673</a></li>
<li>Add function responsible to call precall - <a href="https://github.com/BerriAI/litellm/pull/15636" target="_blank" rel="noopener noreferrer">PR #15636</a></li>
<li>Fix &quot;User default_user_id does not have access to the object&quot; when object not in db - <a href="https://github.com/BerriAI/litellm/pull/15873" target="_blank" rel="noopener noreferrer">PR #15873</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/ocr">OCR API</a></strong></p>
<ul>
<li>Add Azure AI - OCR to docs - <a href="https://github.com/BerriAI/litellm/pull/15768" target="_blank" rel="noopener noreferrer">PR #15768</a></li>
<li>Add mode + Health check support for OCR models - <a href="https://github.com/BerriAI/litellm/pull/15767" target="_blank" rel="noopener noreferrer">PR #15767</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/search_api">Search API</a></strong></p>
<ul>
<li>Add def search() APIs for Web Search - Perplexity API - <a href="https://github.com/BerriAI/litellm/pull/15769" target="_blank" rel="noopener noreferrer">PR #15769</a></li>
<li>Add Tavily Search API - <a href="https://github.com/BerriAI/litellm/pull/15770" target="_blank" rel="noopener noreferrer">PR #15770</a></li>
<li>Add Parallel AI - Search API - <a href="https://github.com/BerriAI/litellm/pull/15772" target="_blank" rel="noopener noreferrer">PR #15772</a></li>
<li>Add EXA AI Search API to LiteLLM - <a href="https://github.com/BerriAI/litellm/pull/15774" target="_blank" rel="noopener noreferrer">PR #15774</a></li>
<li>Add /search endpoint on LiteLLM Gateway - <a href="https://github.com/BerriAI/litellm/pull/15780" target="_blank" rel="noopener noreferrer">PR #15780</a></li>
<li>Add DataforSEO Search API - <a href="https://github.com/BerriAI/litellm/pull/15817" target="_blank" rel="noopener noreferrer">PR #15817</a></li>
<li>Add Google PSE Search Provider - <a href="https://github.com/BerriAI/litellm/pull/15816" target="_blank" rel="noopener noreferrer">PR #15816</a></li>
<li>Add cost tracking for Search API requests - Google PSE, Tavily, Parallel AI, Exa AI - <a href="https://github.com/BerriAI/litellm/pull/15821" target="_blank" rel="noopener noreferrer">PR #15821</a></li>
<li>Backend: Allow storing configured Search APIs in DB - <a href="https://github.com/BerriAI/litellm/pull/15862" target="_blank" rel="noopener noreferrer">PR #15862</a></li>
<li>Exa Search API - ensure request params are sent to Exa AI - <a href="https://github.com/BerriAI/litellm/pull/15855" target="_blank" rel="noopener noreferrer">PR #15855</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/vector_stores">Vector Stores</a></strong></p>
<ul>
<li>Support Vertex AI Search API as vector store through LiteLLM - <a href="https://github.com/BerriAI/litellm/pull/15781" target="_blank" rel="noopener noreferrer">PR #15781</a></li>
<li>Azure AI - Search Vector Stores - <a href="https://github.com/BerriAI/litellm/pull/15873" target="_blank" rel="noopener noreferrer">PR #15873</a></li>
<li>VertexAI Search Vector Store - Passthrough endpoint support + Vector store search Cost tracking support - <a href="https://github.com/BerriAI/litellm/pull/15824" target="_blank" rel="noopener noreferrer">PR #15824</a></li>
<li>Don&#x27;t raise error if managed object is not found - <a href="https://github.com/BerriAI/litellm/pull/15873" target="_blank" rel="noopener noreferrer">PR #15873</a></li>
<li>Show config.yaml vector stores on UI - <a href="https://github.com/BerriAI/litellm/pull/15873" target="_blank" rel="noopener noreferrer">PR #15873</a></li>
<li>Cost tracking for search spend - <a href="https://github.com/BerriAI/litellm/pull/15859" target="_blank" rel="noopener noreferrer">PR #15859</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/image_generation">Images API</a></strong></p>
<ul>
<li>Pass user-defined headers and extra_headers to image-edit calls - <a href="https://github.com/BerriAI/litellm/pull/15811" target="_blank" rel="noopener noreferrer">PR #15811</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/video_generation">Video Generation API</a></strong></p>
<ul>
<li>Add Azure support for video generation functionality (Sora-2, Sora-2-Pro, Sora-2-Pro-High-Res) - <a href="https://github.com/BerriAI/litellm/pull/15901" target="_blank" rel="noopener noreferrer">PR #15901</a></li>
<li>OpenAI video generation refactoring (Sora-2) - <a href="https://github.com/BerriAI/litellm/pull/15900" target="_blank" rel="noopener noreferrer">PR #15900</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/bedrock_invoke">Bedrock /invoke</a></strong></p>
<ul>
<li>Fix: Hooks broken on /bedrock passthrough due to missing metadata - <a href="https://github.com/BerriAI/litellm/pull/15849" target="_blank" rel="noopener noreferrer">PR #15849</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/realtime_api">Realtime API</a></strong></p>
<ul>
<li>Fix: OpenAI Realtime API integration fails due to websockets.exceptions.PayloadTooBig error - <a href="https://github.com/BerriAI/litellm/pull/15751" target="_blank" rel="noopener noreferrer">PR #15751</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong>Passthrough</strong></p>
<ul>
<li>Set auth on passthrough endpoints, on the UI - <a href="https://github.com/BerriAI/litellm/pull/15778" target="_blank" rel="noopener noreferrer">PR #15778</a></li>
<li>Fix pass-through endpoint budget enforcement bug - <a href="https://github.com/BerriAI/litellm/pull/15805" target="_blank" rel="noopener noreferrer">PR #15805</a></li>
</ul>
</li>
<li>
<p><strong>Organizations</strong></p>
<ul>
<li>Allow org admins to create teams on UI - <a href="https://github.com/BerriAI/litellm/pull/15924" target="_blank" rel="noopener noreferrer">PR #15924</a></li>
</ul>
</li>
<li>
<p><strong>Search Tools</strong></p>
<ul>
<li>UI - Search Tools, allow adding search tools on UI + testing search - <a href="https://github.com/BerriAI/litellm/pull/15871" target="_blank" rel="noopener noreferrer">PR #15871</a></li>
<li>UI - Add logos for search providers - <a href="https://github.com/BerriAI/litellm/pull/15872" target="_blank" rel="noopener noreferrer">PR #15872</a></li>
</ul>
</li>
<li>
<p><strong>General</strong></p>
<ul>
<li>Fix routing for custom server root path - <a href="https://github.com/BerriAI/litellm/pull/15701" target="_blank" rel="noopener noreferrer">PR #15701</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail--prompt-management-integrations">Logging / Guardrail / Prompt Management Integrations<a href="#logging--guardrail--prompt-management-integrations" class="hash-link" aria-label="Logging / Guardrail / Prompt Management Integrations" title="Logging / Guardrail / Prompt Management Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/proxy/logging#opentelemetry">OpenTelemetry</a></strong></p>
<ul>
<li>Fix OpenTelemetry Logging functionality - <a href="https://github.com/BerriAI/litellm/pull/15645" target="_blank" rel="noopener noreferrer">PR #15645</a></li>
<li>Fix issue where headers were not being split correctly - <a href="https://github.com/BerriAI/litellm/pull/15916" target="_blank" rel="noopener noreferrer">PR #15916</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/logging#sentry">Sentry</a></strong></p>
<ul>
<li>Add SENTRY_ENVIRONMENT configuration for Sentry integration - <a href="https://github.com/BerriAI/litellm/pull/15760" target="_blank" rel="noopener noreferrer">PR #15760</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/logging#helicone">Helicone</a></strong></p>
<ul>
<li>Fix JSON serialization error in Helicone logging by removing OpenTelemetry span from metadata - <a href="https://github.com/BerriAI/litellm/pull/15728" target="_blank" rel="noopener noreferrer">PR #15728</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/logging#mlflow">MLFlow</a></strong></p>
<ul>
<li>Fix MLFlow tags - split request_tags into (key, val) if request_tag has colon - <a href="https://github.com/BerriAI/litellm/pull/15914" target="_blank" rel="noopener noreferrer">PR #15914</a></li>
</ul>
</li>
<li>
<p><strong>General</strong></p>
<ul>
<li>Rename configured_cold_storage_logger to cold_storage_custom_logger - <a href="https://github.com/BerriAI/litellm/pull/15798" target="_blank" rel="noopener noreferrer">PR #15798</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="guardrails">Guardrails<a href="#guardrails" class="hash-link" aria-label="Guardrails" title="Guardrails"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/proxy/guardrails">Gray Swan</a></strong></p>
<ul>
<li>Add GraySwan Guardrails support - <a href="https://github.com/BerriAI/litellm/pull/15756" target="_blank" rel="noopener noreferrer">PR #15756</a></li>
<li>Rename GraySwan to Gray Swan - <a href="https://github.com/BerriAI/litellm/pull/15771" target="_blank" rel="noopener noreferrer">PR #15771</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/guardrails">Dynamo AI</a></strong></p>
<ul>
<li>New Guardrail - Dynamo AI Guardrail - <a href="https://github.com/BerriAI/litellm/pull/15920" target="_blank" rel="noopener noreferrer">PR #15920</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/guardrails">IBM Guardrails</a></strong></p>
<ul>
<li>IBM Guardrails integration - <a href="https://github.com/BerriAI/litellm/pull/15924" target="_blank" rel="noopener noreferrer">PR #15924</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/guardrails">Lasso Security</a></strong></p>
<ul>
<li>Add v3 API Support - <a href="https://github.com/BerriAI/litellm/pull/12452" target="_blank" rel="noopener noreferrer">PR #12452</a></li>
<li>Fixed lasso import config, redis cluster hash tags for test keys - <a href="https://github.com/BerriAI/litellm/pull/15917" target="_blank" rel="noopener noreferrer">PR #15917</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/guardrails">Bedrock Guardrails</a></strong></p>
<ul>
<li>Implement Bedrock Guardrail apply_guardrail endpoint support - <a href="https://github.com/BerriAI/litellm/pull/15892" target="_blank" rel="noopener noreferrer">PR #15892</a></li>
</ul>
</li>
<li>
<p><strong>General</strong></p>
<ul>
<li>Guardrails - Responses API, Image Gen, Text completions, Audio transcriptions, Audio Speech, Rerank, Anthropic Messages API support via the unified <code>apply_guardrails</code> function - <a href="https://github.com/BerriAI/litellm/pull/15706" target="_blank" rel="noopener noreferrer">PR #15706</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-budgets-and-rate-limiting">Spend Tracking, Budgets and Rate Limiting<a href="#spend-tracking-budgets-and-rate-limiting" class="hash-link" aria-label="Spend Tracking, Budgets and Rate Limiting" title="Spend Tracking, Budgets and Rate Limiting"></a></h2>
<ul>
<li><strong>Rate Limiting</strong>
<ul>
<li>Support absolute RPM/TPM in priority_reservation - <a href="https://github.com/BerriAI/litellm/pull/15813" target="_blank" rel="noopener noreferrer">PR #15813</a></li>
<li>Org level tpm/rpm limits + Team tpm/rpm validation when assigned to org - <a href="https://github.com/BerriAI/litellm/pull/15549" target="_blank" rel="noopener noreferrer">PR #15549</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway">MCP Gateway<a href="#mcp-gateway" class="hash-link" aria-label="MCP Gateway" title="MCP Gateway"></a></h2>
<ul>
<li><strong>OAuth</strong>
<ul>
<li>Auth Header Fix for MCP Tool Call - <a href="https://github.com/BerriAI/litellm/pull/15736" target="_blank" rel="noopener noreferrer">PR #15736</a></li>
<li>Add response_type + PKCE parameters to OAuth authorization endpoint - <a href="https://github.com/BerriAI/litellm/pull/15720" target="_blank" rel="noopener noreferrer">PR #15720</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<ul>
<li>
<p><strong>Database</strong></p>
<ul>
<li>Minimize the occurrence of deadlocks - <a href="https://github.com/BerriAI/litellm/pull/15281" target="_blank" rel="noopener noreferrer">PR #15281</a></li>
</ul>
</li>
<li>
<p><strong>Redis</strong></p>
<ul>
<li>Apply max_connections configuration to Redis async client - <a href="https://github.com/BerriAI/litellm/pull/15797" target="_blank" rel="noopener noreferrer">PR #15797</a></li>
</ul>
</li>
<li>
<p><strong>Caching</strong></p>
<ul>
<li>Add documentation for <code>enable_caching_on_provider_specific_optional_params</code> setting - <a href="https://github.com/BerriAI/litellm/pull/15885" target="_blank" rel="noopener noreferrer">PR #15885</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="documentation-updates">Documentation Updates<a href="#documentation-updates" class="hash-link" aria-label="Documentation Updates" title="Documentation Updates"></a></h2>
<ul>
<li><strong>Provider Documentation</strong>
<ul>
<li>Update worker recommendation - <a href="https://github.com/BerriAI/litellm/pull/15702" target="_blank" rel="noopener noreferrer">PR #15702</a></li>
<li>Fix the wrong request body in json mode doc - <a href="https://github.com/BerriAI/litellm/pull/15729" target="_blank" rel="noopener noreferrer">PR #15729</a></li>
<li>Add details in docs - <a href="https://github.com/BerriAI/litellm/pull/15721" target="_blank" rel="noopener noreferrer">PR #15721</a></li>
<li>Add responses api on openai docs - <a href="https://github.com/BerriAI/litellm/pull/15866" target="_blank" rel="noopener noreferrer">PR #15866</a></li>
<li>Add OpenAI responses api - <a href="https://github.com/BerriAI/litellm/pull/15868" target="_blank" rel="noopener noreferrer">PR #15868</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@tlecomte made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15528" target="_blank" rel="noopener noreferrer">PR #15528</a></li>
<li>@tomhaynes made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15645" target="_blank" rel="noopener noreferrer">PR #15645</a></li>
<li>@talalryz made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15720" target="_blank" rel="noopener noreferrer">PR #15720</a></li>
<li>@1vinodsingh1 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15736" target="_blank" rel="noopener noreferrer">PR #15736</a></li>
<li>@nuernber made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15775" target="_blank" rel="noopener noreferrer">PR #15775</a></li>
<li>@Thomas-Mildner made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15760" target="_blank" rel="noopener noreferrer">PR #15760</a></li>
<li>@javiergarciapleo made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15721" target="_blank" rel="noopener noreferrer">PR #15721</a></li>
<li>@lshgdut made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15717" target="_blank" rel="noopener noreferrer">PR #15717</a></li>
<li>@kk-wangjifeng made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15530" target="_blank" rel="noopener noreferrer">PR #15530</a></li>
<li>@anthonyivn2 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15801" target="_blank" rel="noopener noreferrer">PR #15801</a></li>
<li>@romanglo made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15707" target="_blank" rel="noopener noreferrer">PR #15707</a></li>
<li>@mythral made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15859" target="_blank" rel="noopener noreferrer">PR #15859</a></li>
<li>@mubashirosmani made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15866" target="_blank" rel="noopener noreferrer">PR #15866</a></li>
<li>@CAFxX made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15281" target="_blank" rel="noopener noreferrer">PR #15281</a></li>
<li>@reflection made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15914" target="_blank" rel="noopener noreferrer">PR #15914</a></li>
<li>@shadielfares made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15917" target="_blank" rel="noopener noreferrer">PR #15917</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="pr-count-summary">PR Count Summary<a href="#pr-count-summary" class="hash-link" aria-label="PR Count Summary" title="PR Count Summary"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="10262025">10/26/2025<a href="#10262025" class="hash-link" aria-label="10/26/2025" title="10/26/2025"></a></h3>
<ul>
<li>New Models / Updated Models: 20</li>
<li>LLM API Endpoints: 29</li>
<li>Management Endpoints / UI: 5</li>
<li>Logging / Guardrail / Prompt Management Integrations: 10</li>
<li>Spend Tracking, Budgets and Rate Limiting: 2</li>
<li>MCP Gateway: 2</li>
<li>Performance / Loadbalancing / Reliability improvements: 3</li>
<li>Documentation Updates: 5</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog">Full Changelog<a href="#full-changelog" class="hash-link" aria-label="Full Changelog" title="Full Changelog"></a></h2>
<p><strong><a href="https://github.com/BerriAI/litellm/compare/v1.78.5-stable...v1.79.0-stable" target="_blank" rel="noopener noreferrer">View complete changelog on GitHub</a></strong></p></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-78-5">v1.78.5-stable - Native OCR Support</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-10-18T10:00:00.000Z">20251018</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaff"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaff</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.78.5-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.78.5</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>Native OCR Endpoints</strong> - Native <code>/v1/ocr</code> endpoint support with cost tracking for Mistral OCR and Azure AI OCR</li>
<li><strong>Global Vendor Discounts</strong> - Specify global vendor discount percentages for accurate cost tracking and reporting</li>
<li><strong>Team Spending Reports</strong> - Team admins can now export detailed spending reports for their teams</li>
<li><strong>Claude Haiku 4.5</strong> - Day 0 support for Claude Haiku 4.5 across Bedrock, Vertex AI, and OpenRouter with 200K context window</li>
<li><strong>GPT-5-Codex</strong> - Support for GPT-5-Codex via Responses API on OpenAI and Azure</li>
<li><strong>Performance Improvements</strong> - Major router optimizations: O(1) model lookups, 10-100x faster shallow copy, 30-40% faster timing calls, and O(n) to O(1) hash generation</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-model-support">New Model Support<a href="#new-model-support" class="hash-link" aria-label="New Model Support" title="New Model Support"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Features</th></tr></thead><tbody><tr><td>Anthropic</td><td><code>claude-haiku-4-5</code></td><td>200K</td><td>$1.00</td><td>$5.00</td><td>Chat, reasoning, vision, function calling, prompt caching, computer use</td></tr><tr><td>Anthropic</td><td><code>claude-haiku-4-5-20251001</code></td><td>200K</td><td>$1.00</td><td>$5.00</td><td>Chat, reasoning, vision, function calling, prompt caching, computer use</td></tr><tr><td>Bedrock</td><td><code>anthropic.claude-haiku-4-5-20251001-v1:0</code></td><td>200K</td><td>$1.00</td><td>$5.00</td><td>Chat, reasoning, vision, function calling, prompt caching</td></tr><tr><td>Bedrock</td><td><code>global.anthropic.claude-haiku-4-5-20251001-v1:0</code></td><td>200K</td><td>$1.00</td><td>$5.00</td><td>Chat, reasoning, vision, function calling, prompt caching</td></tr><tr><td>Bedrock</td><td><code>jp.anthropic.claude-haiku-4-5-20251001-v1:0</code></td><td>200K</td><td>$1.10</td><td>$5.50</td><td>Chat, reasoning, vision, function calling, prompt caching (JP Cross-Region)</td></tr><tr><td>Bedrock</td><td><code>us.anthropic.claude-haiku-4-5-20251001-v1:0</code></td><td>200K</td><td>$1.10</td><td>$5.50</td><td>Chat, reasoning, vision, function calling, prompt caching (US region)</td></tr><tr><td>Bedrock</td><td><code>eu.anthropic.claude-haiku-4-5-20251001-v1:0</code></td><td>200K</td><td>$1.10</td><td>$5.50</td><td>Chat, reasoning, vision, function calling, prompt caching (EU region)</td></tr><tr><td>Bedrock</td><td><code>apac.anthropic.claude-haiku-4-5-20251001-v1:0</code></td><td>200K</td><td>$1.10</td><td>$5.50</td><td>Chat, reasoning, vision, function calling, prompt caching (APAC region)</td></tr><tr><td>Bedrock</td><td><code>au.anthropic.claude-haiku-4-5-20251001-v1:0</code></td><td>200K</td><td>$1.10</td><td>$5.50</td><td>Chat, reasoning, vision, function calling, prompt caching (AU region)</td></tr><tr><td>Vertex AI</td><td><code>vertex_ai/claude-haiku-4-5@20251001</code></td><td>200K</td><td>$1.00</td><td>$5.00</td><td>Chat, reasoning, vision, function calling, prompt caching</td></tr><tr><td>OpenAI</td><td><code>gpt-5</code></td><td>272K</td><td>$1.25</td><td>$10.00</td><td>Chat, responses API, reasoning, vision, function calling, prompt caching</td></tr><tr><td>OpenAI</td><td><code>gpt-5-codex</code></td><td>272K</td><td>$1.25</td><td>$10.00</td><td>Responses API mode</td></tr><tr><td>Azure</td><td><code>azure/gpt-5-codex</code></td><td>272K</td><td>$1.25</td><td>$10.00</td><td>Responses API mode</td></tr><tr><td>Gemini</td><td><code>gemini-2.5-flash-image</code></td><td>32K</td><td>$0.30</td><td>$2.50</td><td>Image generation (GA - Nano Banana) - $0.039/image</td></tr><tr><td>ZhipuAI</td><td><code>glm-4.6</code></td><td>-</td><td>-</td><td>-</td><td>Chat completions</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/providers/openai">OpenAI</a></strong></p>
<ul>
<li>GPT-5 return reasoning content via /chat/completions + GPT-5-Codex working on Claude Code - <a href="https://github.com/BerriAI/litellm/pull/15441" target="_blank" rel="noopener noreferrer">PR #15441</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/anthropic">Anthropic</a></strong></p>
<ul>
<li>Reduce claude-4-sonnet max_output_tokens to 64k - <a href="https://github.com/BerriAI/litellm/pull/15409" target="_blank" rel="noopener noreferrer">PR #15409</a></li>
<li>Added claude-haiku-4.5 - <a href="https://github.com/BerriAI/litellm/pull/15579" target="_blank" rel="noopener noreferrer">PR #15579</a></li>
<li>Add support for thinking blocks and redacted thinking blocks in Anthropic v1/messages API - <a href="https://github.com/BerriAI/litellm/pull/15501" target="_blank" rel="noopener noreferrer">PR #15501</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/bedrock">Bedrock</a></strong></p>
<ul>
<li>Add anthropic.claude-haiku-4-5-20251001-v1:0 on Bedrock, VertexAI - <a href="https://github.com/BerriAI/litellm/pull/15581" target="_blank" rel="noopener noreferrer">PR #15581</a></li>
<li>Add Claude Haiku 4.5 support for Bedrock global and US regions - <a href="https://github.com/BerriAI/litellm/pull/15650" target="_blank" rel="noopener noreferrer">PR #15650</a></li>
<li>Add Claude Haiku 4.5 support for Bedrock Other regions - <a href="https://github.com/BerriAI/litellm/pull/15653" target="_blank" rel="noopener noreferrer">PR #15653</a></li>
<li>Add JP Cross-Region Inference jp.anthropic.claude-haiku-4-5-20251001 - <a href="https://github.com/BerriAI/litellm/pull/15598" target="_blank" rel="noopener noreferrer">PR #15598</a></li>
<li>Fix: bedrock-pricing-geo-inregion-cross-region / add Global Cross-Region Inference - <a href="https://github.com/BerriAI/litellm/pull/15685" target="_blank" rel="noopener noreferrer">PR #15685</a></li>
<li>Fix: Support us-gov prefix for AWS GovCloud Bedrock models - <a href="https://github.com/BerriAI/litellm/pull/15626" target="_blank" rel="noopener noreferrer">PR #15626</a></li>
<li>Fix GPT-OSS in Bedrock now supports streaming. Revert fake streaming - <a href="https://github.com/BerriAI/litellm/pull/15668" target="_blank" rel="noopener noreferrer">PR #15668</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/gemini">Gemini</a></strong></p>
<ul>
<li>Feat(pricing): Add Gemini 2.5 Flash Image (Nano Banana) in GA - <a href="https://github.com/BerriAI/litellm/pull/15557" target="_blank" rel="noopener noreferrer">PR #15557</a></li>
<li>Fix: Gemini 2.5 Flash Image should not have supports_web_search=true - <a href="https://github.com/BerriAI/litellm/pull/15642" target="_blank" rel="noopener noreferrer">PR #15642</a></li>
<li>Remove penalty params as supported params for gemini preview model - <a href="https://github.com/BerriAI/litellm/pull/15503" target="_blank" rel="noopener noreferrer">PR #15503</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/ollama">Ollama</a></strong></p>
<ul>
<li>Fix(ollama/chat): correctly map reasoning_effort to think in requests - <a href="https://github.com/BerriAI/litellm/pull/15465" target="_blank" rel="noopener noreferrer">PR #15465</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/openrouter">OpenRouter</a></strong></p>
<ul>
<li>Add anthropic/claude-sonnet-4.5 to OpenRouter cost map - <a href="https://github.com/BerriAI/litellm/pull/15472" target="_blank" rel="noopener noreferrer">PR #15472</a></li>
<li>Prompt caching for anthropic models with OpenRouter - <a href="https://github.com/BerriAI/litellm/pull/15535" target="_blank" rel="noopener noreferrer">PR #15535</a></li>
<li>Get completion cost directly from OpenRouter - <a href="https://github.com/BerriAI/litellm/pull/15448" target="_blank" rel="noopener noreferrer">PR #15448</a></li>
<li>Fix OpenRouter Claude Opus 4 model naming - <a href="https://github.com/BerriAI/litellm/pull/15495" target="_blank" rel="noopener noreferrer">PR #15495</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/comet">CometAPI</a></strong></p>
<ul>
<li>Fix(cometapi): improve CometAPI provider support (embeddings, image generation, docs) - <a href="https://github.com/BerriAI/litellm/pull/15591" target="_blank" rel="noopener noreferrer">PR #15591</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/lemonade">Lemonade</a></strong></p>
<ul>
<li>Adding new models to the lemonade provider - <a href="https://github.com/BerriAI/litellm/pull/15554" target="_blank" rel="noopener noreferrer">PR #15554</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/watsonx">Watson X</a></strong></p>
<ul>
<li>Fix (pricing): Fix pricing for watsonx model family for various models - <a href="https://github.com/BerriAI/litellm/pull/15670" target="_blank" rel="noopener noreferrer">PR #15670</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/vercel_ai_gateway">Vercel AI Gateway</a></strong></p>
<ul>
<li>Add glm-4.6 model to pricing configuration - <a href="https://github.com/BerriAI/litellm/pull/15679" target="_blank" rel="noopener noreferrer">PR #15679</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/vertex">Vertex AI</a></strong></p>
<ul>
<li>Add Vertex AI Discovery Engine Rerank Support - <a href="https://github.com/BerriAI/litellm/pull/15532" target="_blank" rel="noopener noreferrer">PR #15532</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes">Bug Fixes<a href="#bug-fixes" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h3>
<ul>
<li>
<p><strong><a href="/docs/providers/anthropic">Anthropic</a></strong></p>
<ul>
<li>Fix: Pricing for Claude Sonnet 4.5 in US regions is 10x too high - <a href="https://github.com/BerriAI/litellm/pull/15374" target="_blank" rel="noopener noreferrer">PR #15374</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/openrouter">OpenRouter</a></strong></p>
<ul>
<li>Change gpt-5-codex support in model_price json - <a href="https://github.com/BerriAI/litellm/pull/15540" target="_blank" rel="noopener noreferrer">PR #15540</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/bedrock">Bedrock</a></strong></p>
<ul>
<li>Fix filtering headers for signature calcs - <a href="https://github.com/BerriAI/litellm/pull/15590" target="_blank" rel="noopener noreferrer">PR #15590</a></li>
</ul>
</li>
<li>
<p><strong>General</strong></p>
<ul>
<li>Add native reasoning and streaming support flag for gpt-5-codex - <a href="https://github.com/BerriAI/litellm/pull/15569" target="_blank" rel="noopener noreferrer">PR #15569</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/response_api">Responses API</a></strong></p>
<ul>
<li>Responses API - enable calling anthropic/gemini models in Responses API streaming in openai ruby sdk + DB - sanity check pending migrations before startup - <a href="https://github.com/BerriAI/litellm/pull/15432" target="_blank" rel="noopener noreferrer">PR #15432</a></li>
<li>Add support for responses mode in health check - <a href="https://github.com/BerriAI/litellm/pull/15658" target="_blank" rel="noopener noreferrer">PR #15658</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/ocr">OCR API</a></strong></p>
<ul>
<li>Feat: Add native litellm.ocr() functions - <a href="https://github.com/BerriAI/litellm/pull/15567" target="_blank" rel="noopener noreferrer">PR #15567</a></li>
<li>Feat: Add /ocr route on LiteLLM AI Gateway - Adds support for native Mistral OCR calling - <a href="https://github.com/BerriAI/litellm/pull/15571" target="_blank" rel="noopener noreferrer">PR #15571</a></li>
<li>Feat: Add Azure AI Mistral OCR Integration - <a href="https://github.com/BerriAI/litellm/pull/15572" target="_blank" rel="noopener noreferrer">PR #15572</a></li>
<li>Feat: Native /ocr endpoint support - <a href="https://github.com/BerriAI/litellm/pull/15573" target="_blank" rel="noopener noreferrer">PR #15573</a></li>
<li>Feat: Add Cost Tracking for /ocr endpoints - <a href="https://github.com/BerriAI/litellm/pull/15678" target="_blank" rel="noopener noreferrer">PR #15678</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/gemini">/generateContent</a></strong></p>
<ul>
<li>Fix: GEMINI - CLI - add google_routes to llm_api_routes - <a href="https://github.com/BerriAI/litellm/pull/15500" target="_blank" rel="noopener noreferrer">PR #15500</a></li>
<li>Fix Pydantic validation error for citationMetadata.citationSources in Google GenAI responses - <a href="https://github.com/BerriAI/litellm/pull/15592" target="_blank" rel="noopener noreferrer">PR #15592</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/image_generation">Images API</a></strong></p>
<ul>
<li>Fix: Dall-e-2 for Image Edits API - <a href="https://github.com/BerriAI/litellm/pull/15604" target="_blank" rel="noopener noreferrer">PR #15604</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/pass_through/bedrock">Bedrock Passthrough</a></strong></p>
<ul>
<li>Feat: Allow calling /invoke, /converse routes through AI Gateway + models on config.yaml - <a href="https://github.com/BerriAI/litellm/pull/15618" target="_blank" rel="noopener noreferrer">PR #15618</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>General</strong>
<ul>
<li>Fix: Convert object to a correct type - <a href="https://github.com/BerriAI/litellm/pull/15634" target="_blank" rel="noopener noreferrer">PR #15634</a></li>
<li>Bug Fix: Tags as metadata dicts were raising exceptions - <a href="https://github.com/BerriAI/litellm/pull/15625" target="_blank" rel="noopener noreferrer">PR #15625</a></li>
<li>Add type hint to function_to_dict and fix typo - <a href="https://github.com/BerriAI/litellm/pull/15580" target="_blank" rel="noopener noreferrer">PR #15580</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong>Virtual Keys</strong></p>
<ul>
<li>Docs: Key Rotations - <a href="https://github.com/BerriAI/litellm/pull/15455" target="_blank" rel="noopener noreferrer">PR #15455</a></li>
<li>Fix: UI - Key Max Budget Removal Error Fix - <a href="https://github.com/BerriAI/litellm/pull/15672" target="_blank" rel="noopener noreferrer">PR #15672</a></li>
<li>litellm_Key Settings Max Budget Removal Error Fix - <a href="https://github.com/BerriAI/litellm/pull/15669" target="_blank" rel="noopener noreferrer">PR #15669</a></li>
</ul>
</li>
<li>
<p><strong>Teams</strong></p>
<ul>
<li>Feat: Allow Team Admins to export a report of the team spending - <a href="https://github.com/BerriAI/litellm/pull/15542" target="_blank" rel="noopener noreferrer">PR #15542</a></li>
</ul>
</li>
<li>
<p><strong>Passthrough</strong></p>
<ul>
<li>Feat: Passthrough - allow admin to give access to specific passthrough endpoints - <a href="https://github.com/BerriAI/litellm/pull/15401" target="_blank" rel="noopener noreferrer">PR #15401</a></li>
</ul>
</li>
<li>
<p><strong>SCIM v2</strong></p>
<ul>
<li>Feat(scim_v2.py): if group.id doesn&#x27;t exist, use external id + Passthrough - ensure updates and deletions persist across instances - <a href="https://github.com/BerriAI/litellm/pull/15276" target="_blank" rel="noopener noreferrer">PR #15276</a></li>
</ul>
</li>
<li>
<p><strong>SSO</strong></p>
<ul>
<li>Feat: UI SSO - Add PKCE for OKTA SSO - <a href="https://github.com/BerriAI/litellm/pull/15608" target="_blank" rel="noopener noreferrer">PR #15608</a></li>
<li>Fix: Separate OAuth M2M authentication from UI SSO + Handle Introspection endpoint for Oauth2 - <a href="https://github.com/BerriAI/litellm/pull/15667" target="_blank" rel="noopener noreferrer">PR #15667</a></li>
<li>Fix/entraid app roles jwt claim clean - <a href="https://github.com/BerriAI/litellm/pull/15583" target="_blank" rel="noopener noreferrer">PR #15583</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail--prompt-management-integrations">Logging / Guardrail / Prompt Management Integrations<a href="#logging--guardrail--prompt-management-integrations" class="hash-link" aria-label="Logging / Guardrail / Prompt Management Integrations" title="Logging / Guardrail / Prompt Management Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="guardrails">Guardrails<a href="#guardrails" class="hash-link" aria-label="Guardrails" title="Guardrails"></a></h4>
<ul>
<li>
<p><strong>General</strong></p>
<ul>
<li>Fix apply_guardrail endpoint returning raw string instead of ApplyGuardrailResponse - <a href="https://github.com/BerriAI/litellm/pull/15436" target="_blank" rel="noopener noreferrer">PR #15436</a></li>
<li>Fix: Ensure guardrail memory sync after database updates - <a href="https://github.com/BerriAI/litellm/pull/15633" target="_blank" rel="noopener noreferrer">PR #15633</a></li>
<li>Feat: add guardrail for image generation - <a href="https://github.com/BerriAI/litellm/pull/15619" target="_blank" rel="noopener noreferrer">PR #15619</a></li>
<li>Feat: Add Guardrails for /v1/messages and /v1/responses API - <a href="https://github.com/BerriAI/litellm/pull/15686" target="_blank" rel="noopener noreferrer">PR #15686</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/proxy/guardrails">Pillar Security</a></strong></p>
<ul>
<li>Feature: update pillar security integration to support no persistence mode in litellm proxy - <a href="https://github.com/BerriAI/litellm/pull/15599" target="_blank" rel="noopener noreferrer">PR #15599</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="prompt-management">Prompt Management<a href="#prompt-management" class="hash-link" aria-label="Prompt Management" title="Prompt Management"></a></h4>
<ul>
<li><strong>General</strong>
<ul>
<li>Small fix code snippet custom_prompt_management.md - <a href="https://github.com/BerriAI/litellm/pull/15544" target="_blank" rel="noopener noreferrer">PR #15544</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-budgets-and-rate-limiting">Spend Tracking, Budgets and Rate Limiting<a href="#spend-tracking-budgets-and-rate-limiting" class="hash-link" aria-label="Spend Tracking, Budgets and Rate Limiting" title="Spend Tracking, Budgets and Rate Limiting"></a></h2>
<ul>
<li>
<p><strong>Cost Tracking</strong></p>
<ul>
<li>Feat: Cost Tracking - specify a global vendor discount for costs - <a href="https://github.com/BerriAI/litellm/pull/15546" target="_blank" rel="noopener noreferrer">PR #15546</a></li>
<li>Feat: UI - Allow setting Provider Discounts on UI - <a href="https://github.com/BerriAI/litellm/pull/15550" target="_blank" rel="noopener noreferrer">PR #15550</a></li>
</ul>
</li>
<li>
<p><strong>Budgets</strong></p>
<ul>
<li>Fix: improve budget clarity - <a href="https://github.com/BerriAI/litellm/pull/15682" target="_blank" rel="noopener noreferrer">PR #15682</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<ul>
<li>
<p><strong>Router Optimizations</strong></p>
<ul>
<li>Perf(router): use shallow copy instead of deepcopy for model aliases - 10-100x faster than deepcopy on nested dict structures - <a href="https://github.com/BerriAI/litellm/pull/15576" target="_blank" rel="noopener noreferrer">PR #15576</a></li>
<li>Perf(router): optimize string concatenation in hash generation - Improves time complexity from O(n) to O(n) - <a href="https://github.com/BerriAI/litellm/pull/15575" target="_blank" rel="noopener noreferrer">PR #15575</a></li>
<li>Perf(router): optimize model lookups with O(1) data structures - Replace O(n) scans with index map lookups - <a href="https://github.com/BerriAI/litellm/pull/15578" target="_blank" rel="noopener noreferrer">PR #15578</a></li>
<li>Perf(router): optimize model lookups with O(1) index maps - Use model_id_to_deployment_index_map and model_name_to_deployment_indices for instant lookups - <a href="https://github.com/BerriAI/litellm/pull/15574" target="_blank" rel="noopener noreferrer">PR #15574</a></li>
<li>Perf(router): optimize timing functions in completion hot path - Use time.perf_counter() for duration measurements and time.monotonic() for timeout calculations, providing 30-40% faster timing calls - <a href="https://github.com/BerriAI/litellm/pull/15617" target="_blank" rel="noopener noreferrer">PR #15617</a></li>
</ul>
</li>
<li>
<p><strong>SSL/TLS Performance</strong></p>
<ul>
<li>Feat(ssl): add configurable ECDH curve for TLS performance - Configure via ssl_ecdh_curve setting to disable PQC on OpenSSL 3.x for better performance - <a href="https://github.com/BerriAI/litellm/pull/15617" target="_blank" rel="noopener noreferrer">PR #15617</a></li>
</ul>
</li>
<li>
<p><strong>Token Counter</strong></p>
<ul>
<li>Fix(token-counter): extract model_info from deployment for custom_tokenizer - <a href="https://github.com/BerriAI/litellm/pull/15680" target="_blank" rel="noopener noreferrer">PR #15680</a></li>
</ul>
</li>
<li>
<p><strong>Performance Metrics</strong></p>
<ul>
<li>Add: perf summary - <a href="https://github.com/BerriAI/litellm/pull/15458" target="_blank" rel="noopener noreferrer">PR #15458</a></li>
</ul>
</li>
<li>
<p><strong>CI/CD</strong></p>
<ul>
<li>Fix: CI/CD - Missing env key &amp; Linter type error - <a href="https://github.com/BerriAI/litellm/pull/15606" target="_blank" rel="noopener noreferrer">PR #15606</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="documentation-updates">Documentation Updates<a href="#documentation-updates" class="hash-link" aria-label="Documentation Updates" title="Documentation Updates"></a></h2>
<ul>
<li>
<p><strong>Provider Documentation</strong></p>
<ul>
<li>Litellm docs 10 11 2025 - <a href="https://github.com/BerriAI/litellm/pull/15457" target="_blank" rel="noopener noreferrer">PR #15457</a></li>
<li>Docs: add ecs deployment guide - <a href="https://github.com/BerriAI/litellm/pull/15468" target="_blank" rel="noopener noreferrer">PR #15468</a></li>
<li>Docs: Update benchmark results - <a href="https://github.com/BerriAI/litellm/pull/15461" target="_blank" rel="noopener noreferrer">PR #15461</a></li>
<li>Fix: add missing context to benchmark docs - <a href="https://github.com/BerriAI/litellm/pull/15688" target="_blank" rel="noopener noreferrer">PR #15688</a></li>
</ul>
</li>
<li>
<p><strong>General</strong></p>
<ul>
<li>Fixed a few typos - <a href="https://github.com/BerriAI/litellm/pull/15267" target="_blank" rel="noopener noreferrer">PR #15267</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@jlan-nl made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15374" target="_blank" rel="noopener noreferrer">PR #15374</a></li>
<li>@ImadSaddik made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15267" target="_blank" rel="noopener noreferrer">PR #15267</a></li>
<li>@huangyafei made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15472" target="_blank" rel="noopener noreferrer">PR #15472</a></li>
<li>@mubashir1osmani made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15468" target="_blank" rel="noopener noreferrer">PR #15468</a></li>
<li>@kowyo made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15465" target="_blank" rel="noopener noreferrer">PR #15465</a></li>
<li>@dhruvyad made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15448" target="_blank" rel="noopener noreferrer">PR #15448</a></li>
<li>@davizucon made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15544" target="_blank" rel="noopener noreferrer">PR #15544</a></li>
<li>@FelipeRodriguesGare made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15540" target="_blank" rel="noopener noreferrer">PR #15540</a></li>
<li>@ndrsfel made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15557" target="_blank" rel="noopener noreferrer">PR #15557</a></li>
<li>@shinharaguchi made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15598" target="_blank" rel="noopener noreferrer">PR #15598</a></li>
<li>@TensorNull made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15591" target="_blank" rel="noopener noreferrer">PR #15591</a></li>
<li>@TeddyAmkie made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15583" target="_blank" rel="noopener noreferrer">PR #15583</a></li>
<li>@aniketmaurya made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15580" target="_blank" rel="noopener noreferrer">PR #15580</a></li>
<li>@eddierichter-amd made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15554" target="_blank" rel="noopener noreferrer">PR #15554</a></li>
<li>@konekohana made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15535" target="_blank" rel="noopener noreferrer">PR #15535</a></li>
<li>@Classic298 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15495" target="_blank" rel="noopener noreferrer">PR #15495</a></li>
<li>@afogel made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15599" target="_blank" rel="noopener noreferrer">PR #15599</a></li>
<li>@orolega made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15633" target="_blank" rel="noopener noreferrer">PR #15633</a></li>
<li>@LucasSugi made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15634" target="_blank" rel="noopener noreferrer">PR #15634</a></li>
<li>@uc4w6c made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15619" target="_blank" rel="noopener noreferrer">PR #15619</a></li>
<li>@Sameerlite made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15658" target="_blank" rel="noopener noreferrer">PR #15658</a></li>
<li>@yuneng-jiang made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15672" target="_blank" rel="noopener noreferrer">PR #15672</a></li>
<li>@Nikro made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15680" target="_blank" rel="noopener noreferrer">PR #15680</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog">Full Changelog<a href="#full-changelog" class="hash-link" aria-label="Full Changelog" title="Full Changelog"></a></h2>
<p><strong><a href="https://github.com/BerriAI/litellm/compare/v1.78.0-stable...v1.78.4-stable" target="_blank" rel="noopener noreferrer">View complete changelog on GitHub</a></strong></p></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-78-0">v1.78.0-stable - MCP Gateway: Control Tool Access by Team, Key</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-10-11T10:00:00.000Z">20251011</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaff"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaff</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.78.0-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.78.0.post1</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>MCP Gateway - Control Tool Access by Team, Key</strong> - Control MCP tool access by team/key.</li>
<li><strong>Performance Improvements</strong> - 70% Lower p99 Latency</li>
<li><strong>GPT-5 Pro &amp; GPT-Image-1-Mini</strong> - Day 0 support for OpenAI&#x27;s GPT-5 Pro (400K context) and gpt-image-1-mini image generation</li>
<li><strong>EnkryptAI Guardrails</strong> - New guardrail integration for content moderation</li>
<li><strong>Tag-Based Budgets</strong> - Support for setting budgets based on request tags</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway---control-tool-access-by-team-key">MCP Gateway - Control Tool Access by Team, Key<a href="#mcp-gateway---control-tool-access-by-team-key" class="hash-link" aria-label="MCP Gateway - Control Tool Access by Team, Key" title="MCP Gateway - Control Tool Access by Team, Key"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAeUlEQVR4nD2N2wrEIAxE/f/fFME+dLVbreZSZ0lKN3AggZOZkLcNMUaUUsDMmHOCiHwXkT/BhJQScs4u1aNjjEdWvWGz1kIwW1UdIkbrA/0aYBaIKhaA79kQrMK+37rWL5fPPrHX5on7pz7ii8n3sgyARVGO6Tcx4weoYcKyMzLPngAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/tool_control.ac192bc.640.png" srcset="/assets/ideal-img/tool_control.ac192bc.640.png 640w,/assets/ideal-img/tool_control.e24c881.1920.png 1920w" width="640" height="334"></noscript></div>
<br>
<p>Proxy admins can now control MCP tool access by team or key. This makes it easy to grant different teams selective access to tools from the same MCP server.</p>
<p>For example, you can now give your Engineering team access to <code>list_repositories</code>, <code>create_issue</code>, and <code>search_code</code> tools, while Sales only gets <code>search_code</code> and <code>close_issue</code> tools.</p>
<p>This makes it easier for Proxy Admins to govern MCP Tool Access.</p>
<p><a href="/docs/mcp_control#set-allowed-tools-for-a-key-team-or-organization">Get Started</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance---70-lower-p99-latency">Performance - 70% Lower p99 Latency<a href="#performance---70-lower-p99-latency" class="hash-link" aria-label="Performance - 70% Lower p99 Latency" title="Performance - 70% Lower p99 Latency"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAECAYAAAC3OK7NAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAgUlEQVR4nG2Muw6CQBQF+Xgr/2AJBpFCe6OhdNfnH9AaE+3o9hHgXseIrZOcak4m67qOGCOjCP9QVYZhIEsp0WwbNtWaelWT5znGGMplSVEUWGuncxZCYHGomO3m7G3D7XzleDlNc87Rtu2v2Pc9T//iHh/EMfJWRURQ0an09d57PqvQjw8/qE4/AAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="251"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/1_78_0_perf.6b236b5.640.png" srcset="/assets/ideal-img/1_78_0_perf.6b236b5.640.png 640w,/assets/ideal-img/1_78_0_perf.c68642f.1920.png 1920w" width="640" height="251"></noscript></div>
<br>
<p>This release cuts p99 latency by 70% on LiteLLM AI Gateway, making it even better for low-latency use cases.</p>
<p>These gains come from two key enhancements:</p>
<p><strong>Reliable Sessions</strong></p>
<p>Added support for shared sessions with aiohttp. The shared_session parameter is now consistently used across all calls, enabling connection pooling.</p>
<p><strong>Faster Routing</strong></p>
<p>A new <code>model_name_to_deployment_indices</code> hash map replaces O(n) list scans in <code>_get_all_deployments()</code> with O(1) hash lookups, boosting routing performance and scalability.</p>
<p>As a result, performance improved across all latency percentiles:</p>
<ul>
<li><strong>Median latency:</strong> 110 ms  <strong>100 ms</strong> (9.1%)</li>
<li><strong>p95 latency:</strong> 440 ms  <strong>150 ms</strong> (65.9%)</li>
<li><strong>p99 latency:</strong> 810 ms  <strong>240 ms</strong> (70.4%)</li>
<li><strong>Average latency:</strong> 310 ms  <strong>111.73 ms</strong> (64.0%)</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="test-setup"><strong>Test Setup</strong><a href="#test-setup" class="hash-link" aria-label="test-setup" title="test-setup"></a></h3>
<p><strong>Locust</strong></p>
<ul>
<li><strong>Concurrent users:</strong>1,000</li>
<li><strong>Ramp-up:</strong>500</li>
</ul>
<p><strong>System Specs</strong></p>
<ul>
<li><strong>Database was used</strong></li>
<li><strong>CPU:</strong>4 vCPUs</li>
<li><strong>Memory:</strong>8 GB RAM</li>
<li><strong>LiteLLM Workers:</strong>4</li>
<li><strong>Instances</strong>: 4</li>
</ul>
<p><strong>Configuration (config.yaml)</strong></p>
<p>View the complete configuration:<a href="https://gist.github.com/AlexsanderHamir/53f7d554a5d2afcf2c4edb5b6be68ff4" target="_blank" rel="noopener noreferrer">gist.github.com/AlexsanderHamir/config.yaml</a></p>
<p><strong>Load Script (no_cache_hits.py)</strong></p>
<p>View the complete load testing script:<a href="https://gist.github.com/AlexsanderHamir/42c33d7a4dc7a57f56a78b560dee3a42" target="_blank" rel="noopener noreferrer">gist.github.com/AlexsanderHamir/no_cache_hits.py</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-model-support">New Model Support<a href="#new-model-support" class="hash-link" aria-label="New Model Support" title="New Model Support"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Features</th></tr></thead><tbody><tr><td>OpenAI</td><td><code>gpt-5-pro</code></td><td>400K</td><td>$15.00</td><td>$120.00</td><td>Responses API, reasoning, vision, function calling, prompt caching, web search</td></tr><tr><td>OpenAI</td><td><code>gpt-5-pro-2025-10-06</code></td><td>400K</td><td>$15.00</td><td>$120.00</td><td>Responses API, reasoning, vision, function calling, prompt caching, web search</td></tr><tr><td>OpenAI</td><td><code>gpt-image-1-mini</code></td><td>-</td><td>$2.00/img</td><td>-</td><td>Image generation and editing</td></tr><tr><td>OpenAI</td><td><code>gpt-realtime-mini</code></td><td>128K</td><td>$0.60</td><td>$2.40</td><td>Realtime audio, function calling</td></tr><tr><td>Azure AI</td><td><code>azure_ai/Phi-4-mini-reasoning</code></td><td>131K</td><td>$0.08</td><td>$0.32</td><td>Function calling</td></tr><tr><td>Azure AI</td><td><code>azure_ai/Phi-4-reasoning</code></td><td>32K</td><td>$0.125</td><td>$0.50</td><td>Function calling, reasoning</td></tr><tr><td>Azure AI</td><td><code>azure_ai/MAI-DS-R1</code></td><td>128K</td><td>$1.35</td><td>$5.40</td><td>Reasoning, function calling</td></tr><tr><td>Bedrock</td><td><code>au.anthropic.claude-sonnet-4-5-20250929-v1:0</code></td><td>200K</td><td>$3.30</td><td>$16.50</td><td>Chat, reasoning, vision, function calling, prompt caching</td></tr><tr><td>Bedrock</td><td><code>global.anthropic.claude-sonnet-4-5-20250929-v1:0</code></td><td>200K</td><td>$3.00</td><td>$15.00</td><td>Chat, reasoning, vision, function calling, prompt caching</td></tr><tr><td>Bedrock</td><td><code>global.anthropic.claude-sonnet-4-20250514-v1:0</code></td><td>1M</td><td>$3.00</td><td>$15.00</td><td>Chat, reasoning, vision, function calling, prompt caching</td></tr><tr><td>Bedrock</td><td><code>cohere.embed-v4:0</code></td><td>128K</td><td>$0.12</td><td>-</td><td>Embeddings, image input support</td></tr><tr><td>OCI</td><td><code>oci/cohere.command-latest</code></td><td>128K</td><td>$1.56</td><td>$1.56</td><td>Function calling</td></tr><tr><td>OCI</td><td><code>oci/cohere.command-a-03-2025</code></td><td>256K</td><td>$1.56</td><td>$1.56</td><td>Function calling</td></tr><tr><td>OCI</td><td><code>oci/cohere.command-plus-latest</code></td><td>128K</td><td>$1.56</td><td>$1.56</td><td>Function calling</td></tr><tr><td>Together AI</td><td><code>together_ai/moonshotai/Kimi-K2-Instruct-0905</code></td><td>262K</td><td>$1.00</td><td>$3.00</td><td>Function calling</td></tr><tr><td>Together AI</td><td><code>together_ai/Qwen/Qwen3-Next-80B-A3B-Instruct</code></td><td>262K</td><td>$0.15</td><td>$1.50</td><td>Function calling</td></tr><tr><td>Together AI</td><td><code>together_ai/Qwen/Qwen3-Next-80B-A3B-Thinking</code></td><td>262K</td><td>$0.15</td><td>$1.50</td><td>Function calling</td></tr><tr><td>Vertex AI</td><td>MedGemma models</td><td>Varies</td><td>Varies</td><td>Varies</td><td>Medical-focused Gemma models on custom endpoints</td></tr><tr><td>Watson X</td><td>27 new foundation models</td><td>Varies</td><td>Varies</td><td>Varies</td><td>Granite, Llama, Mistral families</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/providers/openai">OpenAI</a></strong></p>
<ul>
<li>Add GPT-5 Pro model configuration and documentation - <a href="https://github.com/BerriAI/litellm/pull/15258" target="_blank" rel="noopener noreferrer">PR #15258</a></li>
<li>Add stop parameter to non-supported params for GPT-5 - <a href="https://github.com/BerriAI/litellm/pull/15244" target="_blank" rel="noopener noreferrer">PR #15244</a></li>
<li>Day 0 Support, Add gpt-image-1-mini - <a href="https://github.com/BerriAI/litellm/pull/15259" target="_blank" rel="noopener noreferrer">PR #15259</a></li>
<li>Add gpt-realtime-mini support - <a href="https://github.com/BerriAI/litellm/pull/15283" target="_blank" rel="noopener noreferrer">PR #15283</a></li>
<li>Add gpt-5-pro-2025-10-06 to model costs - <a href="https://github.com/BerriAI/litellm/pull/15344" target="_blank" rel="noopener noreferrer">PR #15344</a></li>
<li>Minimal fix: gpt5 models should not go on cooldown when called with temperature!=1 - <a href="https://github.com/BerriAI/litellm/pull/15330" target="_blank" rel="noopener noreferrer">PR #15330</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/snowflake">Snowflake Cortex</a></strong></p>
<ul>
<li>Add function calling support for Snowflake Cortex REST API - <a href="https://github.com/BerriAI/litellm/pull/15221" target="_blank" rel="noopener noreferrer">PR #15221</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/gemini">Gemini</a></strong></p>
<ul>
<li>Fix header forwarding for Gemini/Vertex AI providers in proxy mode - <a href="https://github.com/BerriAI/litellm/pull/15231" target="_blank" rel="noopener noreferrer">PR #15231</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/azure">Azure</a></strong></p>
<ul>
<li>Removed stop param from unsupported azure models - <a href="https://github.com/BerriAI/litellm/pull/15229" target="_blank" rel="noopener noreferrer">PR #15229</a></li>
<li>Fix(azure/responses): remove invalid status param from azure call - <a href="https://github.com/BerriAI/litellm/pull/15253" target="_blank" rel="noopener noreferrer">PR #15253</a></li>
<li>Add new Azure AI models with pricing details - <a href="https://github.com/BerriAI/litellm/pull/15387" target="_blank" rel="noopener noreferrer">PR #15387</a></li>
<li>AzureAD Default credentials - select credential type based on environment - <a href="https://github.com/BerriAI/litellm/pull/14470" target="_blank" rel="noopener noreferrer">PR #14470</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/bedrock">Bedrock</a></strong></p>
<ul>
<li>Add Global Cross-Region Inference - <a href="https://github.com/BerriAI/litellm/pull/15210" target="_blank" rel="noopener noreferrer">PR #15210</a></li>
<li>Add Cohere Embed v4 support for AWS Bedrock - <a href="https://github.com/BerriAI/litellm/pull/15298" target="_blank" rel="noopener noreferrer">PR #15298</a></li>
<li>Fix(bedrock): include cacheWriteInputTokens in prompt_tokens calculation - <a href="https://github.com/BerriAI/litellm/pull/15292" target="_blank" rel="noopener noreferrer">PR #15292</a></li>
<li>Add Bedrock AU Cross-Region Inference for Claude Sonnet 4.5 - <a href="https://github.com/BerriAI/litellm/pull/15402" target="_blank" rel="noopener noreferrer">PR #15402</a></li>
<li>Converse  /v1/messages streaming doesn&#x27;t handle parallel tool calls with Claude models - <a href="https://github.com/BerriAI/litellm/pull/15315" target="_blank" rel="noopener noreferrer">PR #15315</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/vertex">Vertex AI</a></strong></p>
<ul>
<li>Implement Context Caching for Vertex AI provider - <a href="https://github.com/BerriAI/litellm/pull/15226" target="_blank" rel="noopener noreferrer">PR #15226</a></li>
<li>Support for Vertex AI Gemma Models on Custom Endpoints - <a href="https://github.com/BerriAI/litellm/pull/15397" target="_blank" rel="noopener noreferrer">PR #15397</a></li>
<li>VertexAI - gemma model family support (custom endpoints) - <a href="https://github.com/BerriAI/litellm/pull/15419" target="_blank" rel="noopener noreferrer">PR #15419</a></li>
<li>VertexAI Gemma model family streaming support + Added MedGemma - <a href="https://github.com/BerriAI/litellm/pull/15427" target="_blank" rel="noopener noreferrer">PR #15427</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/oci">OCI</a></strong></p>
<ul>
<li>Add OCI Cohere support with tool calling and streaming capabilities - <a href="https://github.com/BerriAI/litellm/pull/15365" target="_blank" rel="noopener noreferrer">PR #15365</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/watsonx">Watson X</a></strong></p>
<ul>
<li>Add Watson X foundation model definitions to model_prices_and_context_window.json - <a href="https://github.com/BerriAI/litellm/pull/15219" target="_blank" rel="noopener noreferrer">PR #15219</a></li>
<li>Watsonx - Apply correct prompt templates for openai/gpt-oss model family - <a href="https://github.com/BerriAI/litellm/pull/15341" target="_blank" rel="noopener noreferrer">PR #15341</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/openrouter">OpenRouter</a></strong></p>
<ul>
<li>Fix - (openrouter): move cache_control to content blocks for claude/gemini - <a href="https://github.com/BerriAI/litellm/pull/15345" target="_blank" rel="noopener noreferrer">PR #15345</a></li>
<li>Fix - OpenRouter cache_control to only apply to last content block - <a href="https://github.com/BerriAI/litellm/pull/15395" target="_blank" rel="noopener noreferrer">PR #15395</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/togetherai">Together AI</a></strong></p>
<ul>
<li>Add new together models - <a href="https://github.com/BerriAI/litellm/pull/15383" target="_blank" rel="noopener noreferrer">PR #15383</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes">Bug Fixes<a href="#bug-fixes" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h3>
<ul>
<li><strong>General</strong>
<ul>
<li>Bug fix: gpt-5-chat-latest has incorrect max_input_tokens value - <a href="https://github.com/BerriAI/litellm/pull/15116" target="_blank" rel="noopener noreferrer">PR #15116</a></li>
<li>Fix reasoning response ID - <a href="https://github.com/BerriAI/litellm/pull/15265" target="_blank" rel="noopener noreferrer">PR #15265</a></li>
<li>Fix issue with parsing assistant messages - <a href="https://github.com/BerriAI/litellm/pull/15320" target="_blank" rel="noopener noreferrer">PR #15320</a></li>
<li>Fix litellm_param based costing - <a href="https://github.com/BerriAI/litellm/pull/15336" target="_blank" rel="noopener noreferrer">PR #15336</a></li>
<li>Fix lint errors - <a href="https://github.com/BerriAI/litellm/pull/15406" target="_blank" rel="noopener noreferrer">PR #15406</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/response_api">Responses API</a></strong></p>
<ul>
<li>Added streaming support for response api streaming image generation - <a href="https://github.com/BerriAI/litellm/pull/15269" target="_blank" rel="noopener noreferrer">PR #15269</a></li>
<li>Add native Responses API support for litellm_proxy provider - <a href="https://github.com/BerriAI/litellm/pull/15347" target="_blank" rel="noopener noreferrer">PR #15347</a></li>
<li>Temporarily relax ResponsesAPIResponse parsing to support custom backends (e.g., vLLM) - <a href="https://github.com/BerriAI/litellm/pull/15362" target="_blank" rel="noopener noreferrer">PR #15362</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/files_api">Files API</a></strong></p>
<ul>
<li>Feat(files): add @client decorator to file operations - <a href="https://github.com/BerriAI/litellm/pull/15339" target="_blank" rel="noopener noreferrer">PR #15339</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/gemini">/generateContent</a></strong></p>
<ul>
<li>Fix gemini cli by actually streaming the response - <a href="https://github.com/BerriAI/litellm/pull/15264" target="_blank" rel="noopener noreferrer">PR #15264</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/pass_through/azure">Azure Passthrough</a></strong></p>
<ul>
<li>Azure - passthrough support with router models - <a href="https://github.com/BerriAI/litellm/pull/15240" target="_blank" rel="noopener noreferrer">PR #15240</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>General</strong>
<ul>
<li>Fix x-litellm-cache-key header not being returned on cache hit - <a href="https://github.com/BerriAI/litellm/pull/15348" target="_blank" rel="noopener noreferrer">PR #15348</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong>Proxy CLI Auth</strong></p>
<ul>
<li>Proxy CLI - dont store existing key in the URL, store it in the state param - <a href="https://github.com/BerriAI/litellm/pull/15290" target="_blank" rel="noopener noreferrer">PR #15290</a></li>
</ul>
</li>
<li>
<p><strong>Models + Endpoints</strong></p>
<ul>
<li>Make PATCH <code>/model/{model_id}/update</code> handle <code>team_id</code> consistently with POST <code>/model/new</code> - <a href="https://github.com/BerriAI/litellm/pull/15297" target="_blank" rel="noopener noreferrer">PR #15297</a></li>
<li>Feature: adds Infinity as a provider in the UI - <a href="https://github.com/BerriAI/litellm/pull/15285" target="_blank" rel="noopener noreferrer">PR #15285</a></li>
<li>Fix: model + endpoints page crash when config file contains router_settings.model_group_alias - <a href="https://github.com/BerriAI/litellm/pull/15308" target="_blank" rel="noopener noreferrer">PR #15308</a></li>
<li>Models &amp; Endpoints Initial Refactor - <a href="https://github.com/BerriAI/litellm/pull/15435" target="_blank" rel="noopener noreferrer">PR #15435</a></li>
<li>Litellm UI API Reference page updates - <a href="https://github.com/BerriAI/litellm/pull/15438" target="_blank" rel="noopener noreferrer">PR #15438</a></li>
</ul>
</li>
<li>
<p><strong>Teams</strong></p>
<ul>
<li>Teams page: new column &quot;Your Role&quot; on the teams table - <a href="https://github.com/BerriAI/litellm/pull/15384" target="_blank" rel="noopener noreferrer">PR #15384</a></li>
<li>LiteLLM Dashboard Teams UI refactor - <a href="https://github.com/BerriAI/litellm/pull/15418" target="_blank" rel="noopener noreferrer">PR #15418</a></li>
</ul>
</li>
<li>
<p><strong>UI Infrastructure</strong></p>
<ul>
<li>Added prettier to autoformat frontend - <a href="https://github.com/BerriAI/litellm/pull/15215" target="_blank" rel="noopener noreferrer">PR #15215</a></li>
<li>Adds turbopack to the npm run dev command in UI to build faster during development - <a href="https://github.com/BerriAI/litellm/pull/15250" target="_blank" rel="noopener noreferrer">PR #15250</a></li>
<li>(perf) fix: Replaces bloated key list calls with lean key aliases endpoint - <a href="https://github.com/BerriAI/litellm/pull/15252" target="_blank" rel="noopener noreferrer">PR #15252</a></li>
<li>Potentially fixes a UI spasm issue with an expired cookie - <a href="https://github.com/BerriAI/litellm/pull/15309" target="_blank" rel="noopener noreferrer">PR #15309</a></li>
<li>LiteLLM UI Refactor Infrastructure - <a href="https://github.com/BerriAI/litellm/pull/15236" target="_blank" rel="noopener noreferrer">PR #15236</a></li>
<li>Enforces removal of unused imports from UI - <a href="https://github.com/BerriAI/litellm/pull/15416" target="_blank" rel="noopener noreferrer">PR #15416</a></li>
<li>Fix: usage page &gt;&gt; Model Activity &gt;&gt; spend per day graph: y-axis clipping on large spend values - <a href="https://github.com/BerriAI/litellm/pull/15389" target="_blank" rel="noopener noreferrer">PR #15389</a></li>
<li>Updates guardrail provider logos - <a href="https://github.com/BerriAI/litellm/pull/15421" target="_blank" rel="noopener noreferrer">PR #15421</a></li>
</ul>
</li>
<li>
<p><strong>Admin Settings</strong></p>
<ul>
<li>Fix: Router settings do not update despite success message - <a href="https://github.com/BerriAI/litellm/pull/15249" target="_blank" rel="noopener noreferrer">PR #15249</a></li>
<li>Fix: Prevents DB from accidentally overriding config file values if they are empty in DB - <a href="https://github.com/BerriAI/litellm/pull/15340" target="_blank" rel="noopener noreferrer">PR #15340</a></li>
</ul>
</li>
<li>
<p><strong>SSO</strong></p>
<ul>
<li>SSO - support EntraID app roles - <a href="https://github.com/BerriAI/litellm/pull/15351" target="_blank" rel="noopener noreferrer">PR #15351</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail--prompt-management-integrations">Logging / Guardrail / Prompt Management Integrations<a href="#logging--guardrail--prompt-management-integrations" class="hash-link" aria-label="Logging / Guardrail / Prompt Management Integrations" title="Logging / Guardrail / Prompt Management Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/observability/posthog">PostHog</a></strong>
<ul>
<li>Feat: posthog per request api key - <a href="https://github.com/BerriAI/litellm/pull/15379" target="_blank" rel="noopener noreferrer">PR #15379</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="guardrails">Guardrails<a href="#guardrails" class="hash-link" aria-label="Guardrails" title="Guardrails"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/guardrails">EnkryptAI</a></strong>
<ul>
<li>Add EnkryptAI Guardrails on LiteLLM - <a href="https://github.com/BerriAI/litellm/pull/15390" target="_blank" rel="noopener noreferrer">PR #15390</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-budgets-and-rate-limiting">Spend Tracking, Budgets and Rate Limiting<a href="#spend-tracking-budgets-and-rate-limiting" class="hash-link" aria-label="Spend Tracking, Budgets and Rate Limiting" title="Spend Tracking, Budgets and Rate Limiting"></a></h2>
<ul>
<li>
<p><strong>Tag Management</strong></p>
<ul>
<li>Tag Management - Add support for setting tag based budgets - <a href="https://github.com/BerriAI/litellm/pull/15433" target="_blank" rel="noopener noreferrer">PR #15433</a></li>
</ul>
</li>
<li>
<p><strong>Dynamic Rate Limiter v3</strong></p>
<ul>
<li>QA/Fixes - Dynamic Rate Limiter v3 - final QA - <a href="https://github.com/BerriAI/litellm/pull/15311" target="_blank" rel="noopener noreferrer">PR #15311</a></li>
<li>Fix dynamic Rate limiter v3 - inserting litellm_model_saturation - <a href="https://github.com/BerriAI/litellm/pull/15394" target="_blank" rel="noopener noreferrer">PR #15394</a></li>
</ul>
</li>
<li>
<p><strong>Shared Health Check</strong></p>
<ul>
<li>Implement Shared Health Check State Across Pods - <a href="https://github.com/BerriAI/litellm/pull/15380" target="_blank" rel="noopener noreferrer">PR #15380</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway">MCP Gateway<a href="#mcp-gateway" class="hash-link" aria-label="MCP Gateway" title="MCP Gateway"></a></h2>
<ul>
<li>
<p><strong>Tool Control</strong></p>
<ul>
<li>MCP Gateway - UI - Select allowed tools for Key, Teams - <a href="https://github.com/BerriAI/litellm/pull/15241" target="_blank" rel="noopener noreferrer">PR #15241</a></li>
<li>MCP Gateway - Backend - Allow storing allowed tools by team/key - <a href="https://github.com/BerriAI/litellm/pull/15243" target="_blank" rel="noopener noreferrer">PR #15243</a></li>
<li>MCP Gateway - Fine-grained Database Object Storage Control - <a href="https://github.com/BerriAI/litellm/pull/15255" target="_blank" rel="noopener noreferrer">PR #15255</a></li>
<li>MCP Gateway - Litellm mcp fixes team control - <a href="https://github.com/BerriAI/litellm/pull/15304" target="_blank" rel="noopener noreferrer">PR #15304</a></li>
<li>MCP Gateway - QA/Fixes - Ensure Team/Key level enforcement works for MCPs - <a href="https://github.com/BerriAI/litellm/pull/15305" target="_blank" rel="noopener noreferrer">PR #15305</a></li>
<li>Feature: Include server_name in /v1/mcp/server/health endpoint response - <a href="https://github.com/BerriAI/litellm/pull/15431" target="_blank" rel="noopener noreferrer">PR #15431</a></li>
</ul>
</li>
<li>
<p><strong>OpenAPI Integration</strong></p>
<ul>
<li>MCP - support converting OpenAPI specs to MCP servers - <a href="https://github.com/BerriAI/litellm/pull/15343" target="_blank" rel="noopener noreferrer">PR #15343</a></li>
<li>MCP - specify allowed params per tool - <a href="https://github.com/BerriAI/litellm/pull/15346" target="_blank" rel="noopener noreferrer">PR #15346</a></li>
</ul>
</li>
<li>
<p><strong>Configuration</strong></p>
<ul>
<li>MCP - support setting CA_BUNDLE_PATH - <a href="https://github.com/BerriAI/litellm/pull/15253" target="_blank" rel="noopener noreferrer">PR #15253</a></li>
<li>Fix: Ensure MCP client stays open during tool call - <a href="https://github.com/BerriAI/litellm/pull/15391" target="_blank" rel="noopener noreferrer">PR #15391</a></li>
<li>Remove hardcoded &quot;public&quot; schema in migration.sql - <a href="https://github.com/BerriAI/litellm/pull/15363" target="_blank" rel="noopener noreferrer">PR #15363</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<ul>
<li>
<p><strong>Router Optimizations</strong></p>
<ul>
<li>Fix - Router: add model_name index for O(1) deployment lookups - <a href="https://github.com/BerriAI/litellm/pull/15113" target="_blank" rel="noopener noreferrer">PR #15113</a></li>
<li>Refactor Utils: extract inner function from client - <a href="https://github.com/BerriAI/litellm/pull/15234" target="_blank" rel="noopener noreferrer">PR #15234</a></li>
<li>Fix Networking: remove limitations - <a href="https://github.com/BerriAI/litellm/pull/15302" target="_blank" rel="noopener noreferrer">PR #15302</a></li>
</ul>
</li>
<li>
<p><strong>Session Management</strong></p>
<ul>
<li>Fix - Sessions not being shared - <a href="https://github.com/BerriAI/litellm/pull/15388" target="_blank" rel="noopener noreferrer">PR #15388</a></li>
<li>Fix: remove panic from hot path - <a href="https://github.com/BerriAI/litellm/pull/15396" target="_blank" rel="noopener noreferrer">PR #15396</a></li>
<li>Fix - shared session parsing and usage issue - <a href="https://github.com/BerriAI/litellm/pull/15440" target="_blank" rel="noopener noreferrer">PR #15440</a></li>
<li>Fix: handle closed aiohttp sessions - <a href="https://github.com/BerriAI/litellm/pull/15442" target="_blank" rel="noopener noreferrer">PR #15442</a></li>
<li>Fix: prevent session leaks when recreating aiohttp sessions - <a href="https://github.com/BerriAI/litellm/pull/15443" target="_blank" rel="noopener noreferrer">PR #15443</a></li>
</ul>
</li>
<li>
<p><strong>SSL/TLS Performance</strong></p>
<ul>
<li>Perf: optimize SSL/TLS handshake performance with prioritized cipher - <a href="https://github.com/BerriAI/litellm/pull/15398" target="_blank" rel="noopener noreferrer">PR #15398</a></li>
</ul>
</li>
<li>
<p><strong>Dependencies</strong></p>
<ul>
<li>Upgrades tenacity version to 8.5.0 - <a href="https://github.com/BerriAI/litellm/pull/15303" target="_blank" rel="noopener noreferrer">PR #15303</a></li>
</ul>
</li>
<li>
<p><strong>Data Masking</strong></p>
<ul>
<li>Fix - SensitiveDataMasker converts lists to string - <a href="https://github.com/BerriAI/litellm/pull/15420" target="_blank" rel="noopener noreferrer">PR #15420</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-ai-gateway-improvements">General AI Gateway Improvements<a href="#general-ai-gateway-improvements" class="hash-link" aria-label="General AI Gateway Improvements" title="General AI Gateway Improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="security">Security<a href="#security" class="hash-link" aria-label="Security" title="Security"></a></h4>
<ul>
<li><strong>General</strong>
<ul>
<li>Fix: redact AWS credentials when redact_user_api_key_info enabled - <a href="https://github.com/BerriAI/litellm/pull/15321" target="_blank" rel="noopener noreferrer">PR #15321</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="documentation-updates">Documentation Updates<a href="#documentation-updates" class="hash-link" aria-label="Documentation Updates" title="Documentation Updates"></a></h2>
<ul>
<li>
<p><strong>Provider Documentation</strong></p>
<ul>
<li>Update doc: perf update - <a href="https://github.com/BerriAI/litellm/pull/15211" target="_blank" rel="noopener noreferrer">PR #15211</a></li>
<li>Add W&amp;B Inference documentation - <a href="https://github.com/BerriAI/litellm/pull/15278" target="_blank" rel="noopener noreferrer">PR #15278</a></li>
</ul>
</li>
<li>
<p><strong>Deployment</strong></p>
<ul>
<li>Deletion of docker-compose buggy comment that cause <code>config.yaml</code> based startup fail - <a href="https://github.com/BerriAI/litellm/pull/15425" target="_blank" rel="noopener noreferrer">PR #15425</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@Gal-bloch made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15219" target="_blank" rel="noopener noreferrer">PR #15219</a></li>
<li>@lcfyi made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15315" target="_blank" rel="noopener noreferrer">PR #15315</a></li>
<li>@ashengstd made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15362" target="_blank" rel="noopener noreferrer">PR #15362</a></li>
<li>@vkolehmainen made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15363" target="_blank" rel="noopener noreferrer">PR #15363</a></li>
<li>@jlan-nl made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15330" target="_blank" rel="noopener noreferrer">PR #15330</a></li>
<li>@BCook98 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15402" target="_blank" rel="noopener noreferrer">PR #15402</a></li>
<li>@PabloGmz96 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15425" target="_blank" rel="noopener noreferrer">PR #15425</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog"><strong><a href="https://github.com/BerriAI/litellm/compare/v1.77.7.rc.1...v1.78.0.rc.1" target="_blank" rel="noopener noreferrer">Full Changelog</a></strong><a href="#full-changelog" class="hash-link" aria-label="full-changelog" title="full-changelog"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-77-7">v1.77.7-stable - 2.9x Lower Median Latency</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-10-04T10:00:00.000Z">2025104</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaff"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaff</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.77.7.rc.1</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.77.7.rc.1</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>Dynamic Rate Limiter v3</strong> - Automatically maximizes throughput when capacity is available (&lt; 80% saturation) by allowing lower-priority requests to use unused capacity, then switches to fair priority-based allocation under high load ( 80%) to prevent blocking</li>
<li><strong>Major Performance Improvements</strong> - 2.9x lower median latency at 1,000 concurrent users.</li>
<li><strong>Claude Sonnet 4.5</strong> - Support for Anthropic&#x27;s new Claude Sonnet 4.5 model family with 200K+ context and tiered pricing</li>
<li><strong>MCP Gateway Enhancements</strong> - Fine-grained tool control, server permissions, and forwardable headers</li>
<li><strong>AMD Lemonade &amp; Nvidia NIM</strong> - New provider support for AMD Lemonade and Nvidia NIM Rerank</li>
<li><strong>GitLab Prompt Management</strong> - GitLab-based prompt management integration</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="performance---29x-lower-median-latency">Performance - 2.9x Lower Median Latency<a href="#performance---29x-lower-median-latency" class="hash-link" aria-label="Performance - 2.9x Lower Median Latency" title="Performance - 2.9x Lower Median Latency"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAICAYAAADA+m62AAAACXBIWXMAACxLAAAsSwGlPZapAAAA50lEQVR4nEVQTWuEMBD1hxUWvGz9G/6dQtljf0QPhb13L7X1JD2vF1GikdWQRTQxib4yU0ohE97MvHnzEVlrIYRA13Vo2xZN06Cua8ZCCGitEUJARMRhGDCOI6qqQlmWuJZX9tWoME0TvPeI6HPOQd81HGMP735j67oyiXC0LAucdzg9n3B4OCBJEhwfj4jjGGmagvKsSFXWWHx953i7nPGRZ3jPL8g+MxRFwaqsaFYDezd4rc94ki8IewB2eju2bftv/TfLbbihlS2klLx53/eMlVKcZyKZpwJreSY6yTzPMMaArkKKP4pcI8ne+ugVAAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="488"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/perf_77_7.683a8b8.640.png" srcset="/assets/ideal-img/perf_77_7.683a8b8.640.png 640w,/assets/ideal-img/perf_77_7.4719f17.1920.png 1920w" width="640" height="488"></noscript></div>
<br>
<p>This update removes LiteLLM router inefficiencies, reducing complexity from O(MN) to O(1). Previously, it built a new array and ran repeated checks like data[&quot;model&quot;] in llm_router.get_model_ids(). Now, a direct ID-to-deployment map eliminates redundant allocations and scans.</p>
<p>As a result, performance improved across all latency percentiles:</p>
<ul>
<li><strong>Median latency:</strong> 320 ms  <strong>110 ms</strong> (65.6%)</li>
<li><strong>p95 latency:</strong> 850 ms  <strong>440 ms</strong> (48.2%)</li>
<li><strong>p99 latency:</strong> 1,400 ms  <strong>810 ms</strong> (42.1%)</li>
<li><strong>Average latency:</strong> 864 ms  <strong>310 ms</strong> (64%)</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="test-setup">Test Setup<a href="#test-setup" class="hash-link" aria-label="Test Setup" title="Test Setup"></a></h4>
<p><strong>Locust</strong></p>
<ul>
<li><strong>Concurrent users:</strong> 1,000</li>
<li><strong>Ramp-up:</strong> 500</li>
</ul>
<p><strong>System Specs</strong></p>
<ul>
<li><strong>CPU:</strong> 4 vCPUs</li>
<li><strong>Memory:</strong> 8 GB RAM</li>
<li><strong>LiteLLM Workers:</strong> 4</li>
<li><strong>Instances</strong>: 4</li>
</ul>
<p><strong>Configuration (config.yaml)</strong></p>
<p>View the complete configuration: <a href="https://gist.github.com/AlexsanderHamir/53f7d554a5d2afcf2c4edb5b6be68ff4" target="_blank" rel="noopener noreferrer">gist.github.com/AlexsanderHamir/config.yaml</a></p>
<p><strong>Load Script (no_cache_hits.py)</strong></p>
<p>View the complete load testing script: <a href="https://gist.github.com/AlexsanderHamir/42c33d7a4dc7a57f56a78b560dee3a42" target="_blank" rel="noopener noreferrer">gist.github.com/AlexsanderHamir/no_cache_hits.py</a></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-oauth-20-support">MCP OAuth 2.0 Support<a href="#mcp-oauth-20-support" class="hash-link" aria-label="MCP OAuth 2.0 Support" title="MCP OAuth 2.0 Support"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAFAAoDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAQI/8QAHhAAAQQCAwEAAAAAAAAAAAAAAQACAwQRMSEjQWH/xAAVAQEBAAAAAAAAAAAAAAAAAAACBf/EABQRAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhEDEQA/AM4i50ztlr15ZZCXGZ7SXgkec4+62oiMEhEVUX//2Q==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/mcp_updates.17fe936.640.jpg" srcset="/assets/ideal-img/mcp_updates.17fe936.640.jpg 640w,/assets/ideal-img/mcp_updates.6af33b8.1920.jpg 1920w" width="640" height="334"></noscript></div>
<br>
<p>This release adds support for OAuth 2.0 Client Credentials for MCP servers. This is great for <strong>Internal Dev Tools</strong> use-cases, as it enables your users to call MCP servers, with their own credentials. E.g. Allowing your developers to call the Github MCP, with their own credentials.</p>
<p><a href="/docs/tutorials/claude_responses_api#connecting-mcp-servers">Set it up today on Claude Code</a></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="scheduled-key-rotations">Scheduled Key Rotations<a href="#scheduled-key-rotations" class="hash-link" aria-label="Scheduled Key Rotations" title="Scheduled Key Rotations"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAe0lEQVR4nD3OSw7DIAwEUO5/xC6yRURFoQ0E/2AqE6WL2Yyexg4pJez7DiICM6O2BmYBi4CIBwDvXyHnjBgjSikL19bRO0PVIKILqtoWVBVmXspKrQRiwZzTzYJmtgU/92TBduH9OXBeDar34oLPkkfl/u17VkcYY/zhD7F8wqEMj8vpAAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="331"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/schedule_key_rotations.6a2e4cd.640.png" srcset="/assets/ideal-img/schedule_key_rotations.6a2e4cd.640.png 640w,/assets/ideal-img/schedule_key_rotations.8f24277.1920.png 1920w" width="640" height="331"></noscript></div>
<br>
<p>This release brings support for scheduling virtual key rotations on LiteLLM AI Gateway.</p>
<p>From this release you can enforce Virtual Keys to rotate on a schedule of your choice e.g every 15 days/30 days/60 days etc.</p>
<p>This is great for Proxy Admins who need to enforce security policies for production workloads.</p>
<p><a href="/docs/proxy/virtual_keys#scheduled-key-rotations">Get Started</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-model-support">New Model Support<a href="#new-model-support" class="hash-link" aria-label="New Model Support" title="New Model Support"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Features</th></tr></thead><tbody><tr><td>Anthropic</td><td><code>claude-sonnet-4-5</code></td><td>200K</td><td>$3.00</td><td>$15.00</td><td>Chat, reasoning, vision, function calling, prompt caching</td></tr><tr><td>Anthropic</td><td><code>claude-sonnet-4-5-20250929</code></td><td>200K</td><td>$3.00</td><td>$15.00</td><td>Chat, reasoning, vision, function calling, prompt caching</td></tr><tr><td>Bedrock</td><td><code>eu.anthropic.claude-sonnet-4-5-20250929-v1:0</code></td><td>200K</td><td>$3.00</td><td>$15.00</td><td>Chat, reasoning, vision, function calling, prompt caching</td></tr><tr><td>Azure AI</td><td><code>azure_ai/grok-4</code></td><td>131K</td><td>$5.50</td><td>$27.50</td><td>Chat, reasoning, function calling, web search</td></tr><tr><td>Azure AI</td><td><code>azure_ai/grok-4-fast-reasoning</code></td><td>131K</td><td>$0.43</td><td>$1.73</td><td>Chat, reasoning, function calling, web search</td></tr><tr><td>Azure AI</td><td><code>azure_ai/grok-4-fast-non-reasoning</code></td><td>131K</td><td>$0.43</td><td>$1.73</td><td>Chat, function calling, web search</td></tr><tr><td>Azure AI</td><td><code>azure_ai/grok-code-fast-1</code></td><td>131K</td><td>$3.50</td><td>$17.50</td><td>Chat, function calling, web search</td></tr><tr><td>Groq</td><td><code>groq/moonshotai/kimi-k2-instruct-0905</code></td><td>Context varies</td><td>Pricing varies</td><td>Pricing varies</td><td>Chat, function calling</td></tr><tr><td>Ollama</td><td>Ollama Cloud models</td><td>Varies</td><td>Free</td><td>Free</td><td>Self-hosted models via Ollama Cloud</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Add new claude-sonnet-4-5 model family with tiered pricing above 200K tokens - <a href="https://github.com/BerriAI/litellm/pull/15041" target="_blank" rel="noopener noreferrer">PR #15041</a></li>
<li>Add anthropic/claude-sonnet-4-5 to model price json with prompt caching support - <a href="https://github.com/BerriAI/litellm/pull/15049" target="_blank" rel="noopener noreferrer">PR #15049</a></li>
<li>Add 200K prices for Sonnet 4.5 - <a href="https://github.com/BerriAI/litellm/pull/15140" target="_blank" rel="noopener noreferrer">PR #15140</a></li>
<li>Add cost tracking for /v1/messages in streaming response - <a href="https://github.com/BerriAI/litellm/pull/15102" target="_blank" rel="noopener noreferrer">PR #15102</a></li>
<li>Add /v1/messages/count_tokens to Anthropic routes for non-admin user access - <a href="https://github.com/BerriAI/litellm/pull/15034" target="_blank" rel="noopener noreferrer">PR #15034</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/gemini">Gemini</a></strong>
<ul>
<li>Ignore type param for gemini tools - <a href="https://github.com/BerriAI/litellm/pull/15022" target="_blank" rel="noopener noreferrer">PR #15022</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex">Vertex AI</a></strong>
<ul>
<li>Add LiteLLM Overhead metric for VertexAI - <a href="https://github.com/BerriAI/litellm/pull/15040" target="_blank" rel="noopener noreferrer">PR #15040</a></li>
<li>Support googlemap grounding in vertex ai - <a href="https://github.com/BerriAI/litellm/pull/15179" target="_blank" rel="noopener noreferrer">PR #15179</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure">Azure</a></strong>
<ul>
<li>Add azure_ai grok-4 model family - <a href="https://github.com/BerriAI/litellm/pull/15137" target="_blank" rel="noopener noreferrer">PR #15137</a></li>
<li>Use the <code>extra_query</code> parameter for GET requests in Azure Batch - <a href="https://github.com/BerriAI/litellm/pull/14997" target="_blank" rel="noopener noreferrer">PR #14997</a></li>
<li>Use extra_query for download results (Batch API) - <a href="https://github.com/BerriAI/litellm/pull/15025" target="_blank" rel="noopener noreferrer">PR #15025</a></li>
<li>Add support for Azure AD token-based authorization - <a href="https://github.com/BerriAI/litellm/pull/14813" target="_blank" rel="noopener noreferrer">PR #14813</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/ollama">Ollama</a></strong>
<ul>
<li>Add ollama cloud models - <a href="https://github.com/BerriAI/litellm/pull/15008" target="_blank" rel="noopener noreferrer">PR #15008</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/groq">Groq</a></strong>
<ul>
<li>Add groq/moonshotai/kimi-k2-instruct-0905 - <a href="https://github.com/BerriAI/litellm/pull/15079" target="_blank" rel="noopener noreferrer">PR #15079</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openai">OpenAI</a></strong>
<ul>
<li>Add support for GPT 5 codex models - <a href="https://github.com/BerriAI/litellm/pull/14841" target="_blank" rel="noopener noreferrer">PR #14841</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/deepinfra">DeepInfra</a></strong>
<ul>
<li>Update DeepInfra model data refresh with latest pricing - <a href="https://github.com/BerriAI/litellm/pull/14939" target="_blank" rel="noopener noreferrer">PR #14939</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>
<ul>
<li>Add JP Cross-Region Inference - <a href="https://github.com/BerriAI/litellm/pull/15188" target="_blank" rel="noopener noreferrer">PR #15188</a></li>
<li>Add &quot;eu.anthropic.claude-sonnet-4-5-20250929-v1:0&quot; - <a href="https://github.com/BerriAI/litellm/pull/15181" target="_blank" rel="noopener noreferrer">PR #15181</a></li>
<li>Add twelvelabs bedrock Async Invoke Support - <a href="https://github.com/BerriAI/litellm/pull/14871" target="_blank" rel="noopener noreferrer">PR #14871</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/nvidia_nim">Nvidia NIM</a></strong>
<ul>
<li>Add Nvidia NIM Rerank Support - <a href="https://github.com/BerriAI/litellm/pull/15152" target="_blank" rel="noopener noreferrer">PR #15152</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes">Bug Fixes<a href="#bug-fixes" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h3>
<ul>
<li><strong><a href="/docs/providers/vllm">VLLM</a></strong>
<ul>
<li>Fix response_format bug in hosted vllm audio_transcription - <a href="https://github.com/BerriAI/litellm/pull/15010" target="_blank" rel="noopener noreferrer">PR #15010</a></li>
<li>Fix passthrough of atranscription into kwargs going to upstream provider - <a href="https://github.com/BerriAI/litellm/pull/15005" target="_blank" rel="noopener noreferrer">PR #15005</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/oci">OCI</a></strong>
<ul>
<li>Fix OCI Generative AI Integration when using Proxy - <a href="https://github.com/BerriAI/litellm/pull/15072" target="_blank" rel="noopener noreferrer">PR #15072</a></li>
</ul>
</li>
<li><strong>General</strong>
<ul>
<li>Fix: Authorization header to use correct &quot;Bearer&quot; capitalization - <a href="https://github.com/BerriAI/litellm/pull/14764" target="_blank" rel="noopener noreferrer">PR #14764</a></li>
<li>Bug fix: gpt-5-chat-latest has incorrect max_input_tokens value - <a href="https://github.com/BerriAI/litellm/pull/15116" target="_blank" rel="noopener noreferrer">PR #15116</a></li>
<li>Update request handling for original exceptions - <a href="https://github.com/BerriAI/litellm/pull/15013" target="_blank" rel="noopener noreferrer">PR #15013</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-provider-support">New Provider Support<a href="#new-provider-support" class="hash-link" aria-label="New Provider Support" title="New Provider Support"></a></h4>
<ul>
<li><strong><a href="/docs/providers/lemonade">AMD Lemonade</a></strong>
<ul>
<li>Add AMD Lemonade provider support - <a href="https://github.com/BerriAI/litellm/pull/14840" target="_blank" rel="noopener noreferrer">PR #14840</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong><a href="/docs/response_api">Responses API</a></strong></p>
<ul>
<li>Return Cost for Responses API Streaming requests - <a href="https://github.com/BerriAI/litellm/pull/15053" target="_blank" rel="noopener noreferrer">PR #15053</a></li>
</ul>
</li>
<li>
<p><strong><a href="/docs/providers/gemini">/generateContent</a></strong></p>
<ul>
<li>Add full support for native Gemini API translation - <a href="https://github.com/BerriAI/litellm/pull/15029" target="_blank" rel="noopener noreferrer">PR #15029</a></li>
</ul>
</li>
<li>
<p><strong>Passthrough Gemini Routes</strong></p>
<ul>
<li>Add Gemini generateContent passthrough cost tracking - <a href="https://github.com/BerriAI/litellm/pull/15014" target="_blank" rel="noopener noreferrer">PR #15014</a></li>
<li>Add streamGenerateContent cost tracking in passthrough - <a href="https://github.com/BerriAI/litellm/pull/15199" target="_blank" rel="noopener noreferrer">PR #15199</a></li>
</ul>
</li>
<li>
<p><strong>Passthrough Vertex AI Routes</strong></p>
<ul>
<li>Add cost tracking for Vertex AI Passthrough <code>/predict</code> endpoint - <a href="https://github.com/BerriAI/litellm/pull/15019" target="_blank" rel="noopener noreferrer">PR #15019</a></li>
<li>Add cost tracking for Vertex AI Live API WebSocket Passthrough - <a href="https://github.com/BerriAI/litellm/pull/14956" target="_blank" rel="noopener noreferrer">PR #14956</a></li>
</ul>
</li>
<li>
<p><strong>General</strong></p>
<ul>
<li>Preserve Whitespace Characters in Model Response Streams - <a href="https://github.com/BerriAI/litellm/pull/15160" target="_blank" rel="noopener noreferrer">PR #15160</a></li>
<li>Add provider name to payload specification - <a href="https://github.com/BerriAI/litellm/pull/15130" target="_blank" rel="noopener noreferrer">PR #15130</a></li>
<li>Ensure query params are forwarded from origin url to downstream request - <a href="https://github.com/BerriAI/litellm/pull/15087" target="_blank" rel="noopener noreferrer">PR #15087</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong>Virtual Keys</strong></p>
<ul>
<li>Ensure LLM_API_KEYs can access pass through routes - <a href="https://github.com/BerriAI/litellm/pull/15115" target="_blank" rel="noopener noreferrer">PR #15115</a></li>
<li>Support &#x27;guaranteed_throughput&#x27; when setting limits on keys belonging to a team - <a href="https://github.com/BerriAI/litellm/pull/15120" target="_blank" rel="noopener noreferrer">PR #15120</a></li>
</ul>
</li>
<li>
<p><strong>Models + Endpoints</strong></p>
<ul>
<li>Ensure OCI secret fields not shared on /models and /v1/models endpoints - <a href="https://github.com/BerriAI/litellm/pull/15085" target="_blank" rel="noopener noreferrer">PR #15085</a></li>
<li>Add snowflake on UI - <a href="https://github.com/BerriAI/litellm/pull/15083" target="_blank" rel="noopener noreferrer">PR #15083</a></li>
<li>Make UI theme settings publicly accessible for custom branding - <a href="https://github.com/BerriAI/litellm/pull/15074" target="_blank" rel="noopener noreferrer">PR #15074</a></li>
</ul>
</li>
<li>
<p><strong>Admin Settings</strong></p>
<ul>
<li>Ensure OTEL settings are saved in DB after set on UI - <a href="https://github.com/BerriAI/litellm/pull/15118" target="_blank" rel="noopener noreferrer">PR #15118</a></li>
<li>Top api key tags - <a href="https://github.com/BerriAI/litellm/pull/15151" target="_blank" rel="noopener noreferrer">PR #15151</a>, <a href="https://github.com/BerriAI/litellm/pull/15156" target="_blank" rel="noopener noreferrer">PR #15156</a></li>
</ul>
</li>
<li>
<p><strong>MCP</strong></p>
<ul>
<li>show health status of MCP servers - <a href="https://github.com/BerriAI/litellm/pull/15185" target="_blank" rel="noopener noreferrer">PR #15185</a></li>
<li>allow setting extra headers on the UI - <a href="https://github.com/BerriAI/litellm/pull/15185" target="_blank" rel="noopener noreferrer">PR #15185</a></li>
<li>allow editing allowed tools on the UI - <a href="https://github.com/BerriAI/litellm/pull/15185" target="_blank" rel="noopener noreferrer">PR #15185</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes-1">Bug Fixes<a href="#bug-fixes-1" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h3>
<ul>
<li>
<p><strong>Virtual Keys</strong></p>
<ul>
<li>(security) prevent user key from updating other user keys - <a href="https://github.com/BerriAI/litellm/pull/15201" target="_blank" rel="noopener noreferrer">PR #15201</a></li>
<li>(security) don&#x27;t return all keys with blank key alias on /v2/key/info - <a href="https://github.com/BerriAI/litellm/pull/15201" target="_blank" rel="noopener noreferrer">PR #15201</a></li>
<li>Fix Session Token Cookie Infinite Logout Loop - <a href="https://github.com/BerriAI/litellm/pull/15146" target="_blank" rel="noopener noreferrer">PR #15146</a></li>
</ul>
</li>
<li>
<p><strong>Models + Endpoints</strong></p>
<ul>
<li>Make UI theme settings publicly accessible for custom branding - <a href="https://github.com/BerriAI/litellm/pull/15074" target="_blank" rel="noopener noreferrer">PR #15074</a></li>
</ul>
</li>
<li>
<p><strong>Teams</strong></p>
<ul>
<li>fix failed copy to clipboard for http ui - <a href="https://github.com/BerriAI/litellm/pull/15195" target="_blank" rel="noopener noreferrer">PR #15195</a></li>
</ul>
</li>
<li>
<p><strong>Logs</strong></p>
<ul>
<li>fix logs page render logs on filter lookup - <a href="https://github.com/BerriAI/litellm/pull/15195" target="_blank" rel="noopener noreferrer">PR #15195</a></li>
<li>fix lookup list of end users (migrate to more efficient /customers/list lookup) - <a href="https://github.com/BerriAI/litellm/pull/15195" target="_blank" rel="noopener noreferrer">PR #15195</a></li>
</ul>
</li>
<li>
<p><strong>Test key</strong></p>
<ul>
<li>update selected model on key change - <a href="https://github.com/BerriAI/litellm/pull/15197" target="_blank" rel="noopener noreferrer">PR #15197</a></li>
</ul>
</li>
<li>
<p><strong>Dashboard</strong></p>
<ul>
<li>Fix LiteLLM model name fallback in dashboard overview - <a href="https://github.com/BerriAI/litellm/pull/14998" target="_blank" rel="noopener noreferrer">PR #14998</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail--prompt-management-integrations">Logging / Guardrail / Prompt Management Integrations<a href="#logging--guardrail--prompt-management-integrations" class="hash-link" aria-label="Logging / Guardrail / Prompt Management Integrations" title="Logging / Guardrail / Prompt Management Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/observability/otel">OpenTelemetry</a></strong>
<ul>
<li>Use generation_name for span naming in logging method - <a href="https://github.com/BerriAI/litellm/pull/14799" target="_blank" rel="noopener noreferrer">PR #14799</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#langfuse">Langfuse</a></strong>
<ul>
<li>Handle non-serializable objects in Langfuse logging - <a href="https://github.com/BerriAI/litellm/pull/15148" target="_blank" rel="noopener noreferrer">PR #15148</a></li>
<li>Set usage_details.total in langfuse integration - <a href="https://github.com/BerriAI/litellm/pull/15015" target="_blank" rel="noopener noreferrer">PR #15015</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/prometheus">Prometheus</a></strong>
<ul>
<li>support custom metadata labels on key/team - <a href="https://github.com/BerriAI/litellm/pull/15094" target="_blank" rel="noopener noreferrer">PR #15094</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="guardrails">Guardrails<a href="#guardrails" class="hash-link" aria-label="Guardrails" title="Guardrails"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/guardrails">Javelin</a></strong>
<ul>
<li>Add Javelin standalone guardrails integration for LiteLLM Proxy - <a href="https://github.com/BerriAI/litellm/pull/14983" target="_blank" rel="noopener noreferrer">PR #14983</a></li>
<li>Add logging for important status fields in guardrails - <a href="https://github.com/BerriAI/litellm/pull/15090" target="_blank" rel="noopener noreferrer">PR #15090</a></li>
<li>Don&#x27;t run post_call guardrail if no text returned from Bedrock - <a href="https://github.com/BerriAI/litellm/pull/15106" target="_blank" rel="noopener noreferrer">PR #15106</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="prompt-management">Prompt Management<a href="#prompt-management" class="hash-link" aria-label="Prompt Management" title="Prompt Management"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/prompt_management">GitLab</a></strong>
<ul>
<li>GitLab based Prompt manager - <a href="https://github.com/BerriAI/litellm/pull/14988" target="_blank" rel="noopener noreferrer">PR #14988</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-budgets-and-rate-limiting">Spend Tracking, Budgets and Rate Limiting<a href="#spend-tracking-budgets-and-rate-limiting" class="hash-link" aria-label="Spend Tracking, Budgets and Rate Limiting" title="Spend Tracking, Budgets and Rate Limiting"></a></h2>
<ul>
<li><strong>Cost Tracking</strong>
<ul>
<li>Proxy: end user cost tracking in the responses API - <a href="https://github.com/BerriAI/litellm/pull/15124" target="_blank" rel="noopener noreferrer">PR #15124</a></li>
</ul>
</li>
<li><strong>Parallel Request Limiter v3</strong>
<ul>
<li>Use well known redis cluster hashing algorithm - <a href="https://github.com/BerriAI/litellm/pull/15052" target="_blank" rel="noopener noreferrer">PR #15052</a></li>
<li>Fixes to dynamic rate limiter v3 - add saturation detection - <a href="https://github.com/BerriAI/litellm/pull/15119" target="_blank" rel="noopener noreferrer">PR #15119</a></li>
<li>Dynamic Rate Limiter v3 - fixes for detecting saturation + fixes for post saturation behavior - <a href="https://github.com/BerriAI/litellm/pull/15192" target="_blank" rel="noopener noreferrer">PR #15192</a></li>
</ul>
</li>
<li><strong>Teams</strong>
<ul>
<li>Add model specific tpm/rpm limits to teams on LiteLLM - <a href="https://github.com/BerriAI/litellm/pull/15044" target="_blank" rel="noopener noreferrer">PR #15044</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway">MCP Gateway<a href="#mcp-gateway" class="hash-link" aria-label="MCP Gateway" title="MCP Gateway"></a></h2>
<ul>
<li><strong>Server Configuration</strong>
<ul>
<li>Specify forwardable headers, specify allowed/disallowed tools for MCP servers - <a href="https://github.com/BerriAI/litellm/pull/15002" target="_blank" rel="noopener noreferrer">PR #15002</a></li>
<li>Enforce server permissions on call tools - <a href="https://github.com/BerriAI/litellm/pull/15044" target="_blank" rel="noopener noreferrer">PR #15044</a></li>
<li>MCP Gateway Fine-grained Tools Addition - <a href="https://github.com/BerriAI/litellm/pull/15153" target="_blank" rel="noopener noreferrer">PR #15153</a></li>
</ul>
</li>
<li><strong>Bug Fixes</strong>
<ul>
<li>Remove servername prefix mcp tools tests - <a href="https://github.com/BerriAI/litellm/pull/14986" target="_blank" rel="noopener noreferrer">PR #14986</a></li>
<li>Resolve regression with duplicate Mcp-Protocol-Version header - <a href="https://github.com/BerriAI/litellm/pull/15050" target="_blank" rel="noopener noreferrer">PR #15050</a></li>
<li>Fix test_mcp_server.py - <a href="https://github.com/BerriAI/litellm/pull/15183" target="_blank" rel="noopener noreferrer">PR #15183</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<ul>
<li><strong>Router Optimizations</strong>
<ul>
<li><strong>+62.5% P99 Latency Improvement</strong> - Remove router inefficiencies (from O(M*N) to O(1)) - <a href="https://github.com/BerriAI/litellm/pull/15046" target="_blank" rel="noopener noreferrer">PR #15046</a></li>
<li>Remove hasattr checks in Router - <a href="https://github.com/BerriAI/litellm/pull/15082" target="_blank" rel="noopener noreferrer">PR #15082</a></li>
<li>Remove Double Lookups - <a href="https://github.com/BerriAI/litellm/pull/15084" target="_blank" rel="noopener noreferrer">PR #15084</a></li>
<li>Optimize _filter_cooldown_deployments from O(nm + kn) to O(n) - <a href="https://github.com/BerriAI/litellm/pull/15091" target="_blank" rel="noopener noreferrer">PR #15091</a></li>
<li>Optimize unhealthy deployment filtering in retry path (O(n*m)  O(n+m)) - <a href="https://github.com/BerriAI/litellm/pull/15110" target="_blank" rel="noopener noreferrer">PR #15110</a></li>
</ul>
</li>
<li><strong>Cache Optimizations</strong>
<ul>
<li>Reduce complexity of InMemoryCache.evict_cache from O(n*log(n)) to O(log(n)) - <a href="https://github.com/BerriAI/litellm/pull/15000" target="_blank" rel="noopener noreferrer">PR #15000</a></li>
<li>Avoiding expensive operations when cache isn&#x27;t available - <a href="https://github.com/BerriAI/litellm/pull/15182" target="_blank" rel="noopener noreferrer">PR #15182</a></li>
</ul>
</li>
<li><strong>Worker Management</strong>
<ul>
<li>Add proxy CLI option to recycle workers after N requests - <a href="https://github.com/BerriAI/litellm/pull/15007" target="_blank" rel="noopener noreferrer">PR #15007</a></li>
</ul>
</li>
<li><strong>Metrics &amp; Monitoring</strong>
<ul>
<li>LiteLLM Overhead metric tracking - Add support for tracking litellm overhead on cache hits - <a href="https://github.com/BerriAI/litellm/pull/15045" target="_blank" rel="noopener noreferrer">PR #15045</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="documentation-updates">Documentation Updates<a href="#documentation-updates" class="hash-link" aria-label="Documentation Updates" title="Documentation Updates"></a></h2>
<ul>
<li><strong>Provider Documentation</strong>
<ul>
<li>Update litellm docs from latest release - <a href="https://github.com/BerriAI/litellm/pull/15004" target="_blank" rel="noopener noreferrer">PR #15004</a></li>
<li>Add missing api_key parameter - <a href="https://github.com/BerriAI/litellm/pull/15058" target="_blank" rel="noopener noreferrer">PR #15058</a></li>
</ul>
</li>
<li><strong>General Documentation</strong>
<ul>
<li>Use docker compose instead of docker-compose - <a href="https://github.com/BerriAI/litellm/pull/15024" target="_blank" rel="noopener noreferrer">PR #15024</a></li>
<li>Add railtracks to projects that are using litellm - <a href="https://github.com/BerriAI/litellm/pull/15144" target="_blank" rel="noopener noreferrer">PR #15144</a></li>
<li>Perf: Last week improvement - <a href="https://github.com/BerriAI/litellm/pull/15193" target="_blank" rel="noopener noreferrer">PR #15193</a></li>
<li>Sync models GitHub documentation with Loom video and cross-reference - <a href="https://github.com/BerriAI/litellm/pull/15191" target="_blank" rel="noopener noreferrer">PR #15191</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="security-fixes">Security Fixes<a href="#security-fixes" class="hash-link" aria-label="Security Fixes" title="Security Fixes"></a></h2>
<ul>
<li><strong>JWT Token Security</strong> - Don&#x27;t log JWT SSO token on .info() log - <a href="https://github.com/BerriAI/litellm/pull/15145" target="_blank" rel="noopener noreferrer">PR #15145</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@herve-ves made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14998" target="_blank" rel="noopener noreferrer">PR #14998</a></li>
<li>@wenxi-onyx made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15008" target="_blank" rel="noopener noreferrer">PR #15008</a></li>
<li>@jpetrucciani made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15005" target="_blank" rel="noopener noreferrer">PR #15005</a></li>
<li>@abhijitjavelin made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14983" target="_blank" rel="noopener noreferrer">PR #14983</a></li>
<li>@ZeroClover made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15039" target="_blank" rel="noopener noreferrer">PR #15039</a></li>
<li>@cedarm made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15043" target="_blank" rel="noopener noreferrer">PR #15043</a></li>
<li>@Isydmr made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15025" target="_blank" rel="noopener noreferrer">PR #15025</a></li>
<li>@serializer made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15013" target="_blank" rel="noopener noreferrer">PR #15013</a></li>
<li>@eddierichter-amd made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14840" target="_blank" rel="noopener noreferrer">PR #14840</a></li>
<li>@malags made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15000" target="_blank" rel="noopener noreferrer">PR #15000</a></li>
<li>@henryhwang made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15029" target="_blank" rel="noopener noreferrer">PR #15029</a></li>
<li>@plafleur made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15111" target="_blank" rel="noopener noreferrer">PR #15111</a></li>
<li>@tyler-liner made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14799" target="_blank" rel="noopener noreferrer">PR #14799</a></li>
<li>@Amir-R25 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15144" target="_blank" rel="noopener noreferrer">PR #15144</a></li>
<li>@georg-wolflein made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15124" target="_blank" rel="noopener noreferrer">PR #15124</a></li>
<li>@niharm made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15140" target="_blank" rel="noopener noreferrer">PR #15140</a></li>
<li>@anthony-liner made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15015" target="_blank" rel="noopener noreferrer">PR #15015</a></li>
<li>@rishiganesh2002 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15153" target="_blank" rel="noopener noreferrer">PR #15153</a></li>
<li>@danielaskdd made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15160" target="_blank" rel="noopener noreferrer">PR #15160</a></li>
<li>@JVenberg made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15146" target="_blank" rel="noopener noreferrer">PR #15146</a></li>
<li>@speglich made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/15072" target="_blank" rel="noopener noreferrer">PR #15072</a></li>
<li>@daily-kim made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14764" target="_blank" rel="noopener noreferrer">PR #14764</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog"><strong><a href="https://github.com/BerriAI/litellm/compare/v1.77.5.rc.4...v1.77.7.rc.1" target="_blank" rel="noopener noreferrer">Full Changelog</a></strong><a href="#full-changelog" class="hash-link" aria-label="full-changelog" title="full-changelog"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-77-5">v1.77.5-stable - MCP OAuth 2.0 Support</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-09-29T10:00:00.000Z">2025929</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaff"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaff</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.77.5-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.77.5</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>MCP OAuth 2.0 Support</strong> - Enhanced authentication for Model Context Protocol integrations</li>
<li><strong>Scheduled Key Rotations</strong> - Automated key rotation capabilities for enhanced security</li>
<li><strong>New Gemini 2.5 Flash &amp; Flash-lite Models</strong> - Latest September 2025 preview models with improved pricing and features</li>
<li><strong>Performance Improvements</strong> - 54% RPS improvement</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="performance-improvements---54-rps-improvement">Performance Improvements - 54% RPS Improvement<a href="#performance-improvements---54-rps-improvement" class="hash-link" aria-label="Performance Improvements - 54% RPS Improvement" title="Performance Improvements - 54% RPS Improvement"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAtklEQVR4nB3GwWrCQBAG4H18oWgreFQEESriEwiKevVcWnuoBmMS426ISdZkJuP8peW7fCYM7xzHKbvMsi89owX7xnPe5P//U1PNxtoClyiCSy0Sl2CWLTB0E4zdFCM7wc/jBAhguCV9ylMr9tqLB7rOtupKp7nP9aP41E70ql/VQU3DBCjwHsyxTFcAAWFwRnxLUFYFgscZL99vMEQkTCzdfV+uRSrCItSSMLPUVIu2KpvjTn4BwG23kvGvqE0AAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="332"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/perf_77_5.b2137ff.640.png" srcset="/assets/ideal-img/perf_77_5.b2137ff.640.png 640w,/assets/ideal-img/perf_77_5.26432f4.1920.png 1920w" width="640" height="332"></noscript></div>
<br>
<p>This release brings a 54% RPS improvement (1,040  1,602 RPS, aggregated) per instance.</p>
<p>The improvement comes from fixing O(n) inefficiencies in the LiteLLM Router, primarily caused by repeated use of <code>in</code> statements inside loops over large arrays.</p>
<p>Tests were run with a database-only setup (no cache hits).</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="test-setup">Test Setup<a href="#test-setup" class="hash-link" aria-label="Test Setup" title="Test Setup"></a></h4>
<p>All benchmarks were executed using Locust with 1,000 concurrent users and a ramp-up of 500. The environment was configured to stress the routing layer and eliminate caching as a variable.</p>
<p><strong>System Specs</strong></p>
<ul>
<li><strong>CPU:</strong> 8 vCPUs</li>
<li><strong>Memory:</strong> 32 GB RAM</li>
</ul>
<p><strong>Configuration (config.yaml)</strong></p>
<p>View the complete configuration: <a href="https://gist.github.com/AlexsanderHamir/53f7d554a5d2afcf2c4edb5b6be68ff4" target="_blank" rel="noopener noreferrer">gist.github.com/AlexsanderHamir/config.yaml</a></p>
<p><strong>Load Script (no_cache_hits.py)</strong></p>
<p>View the complete load testing script: <a href="https://gist.github.com/AlexsanderHamir/42c33d7a4dc7a57f56a78b560dee3a42" target="_blank" rel="noopener noreferrer">gist.github.com/AlexsanderHamir/no_cache_hits.py</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-model-support">New Model Support<a href="#new-model-support" class="hash-link" aria-label="New Model Support" title="New Model Support"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Features</th></tr></thead><tbody><tr><td>Gemini</td><td><code>gemini-2.5-flash-preview-09-2025</code></td><td>1M</td><td>$0.30</td><td>$2.50</td><td>Chat, reasoning, vision, audio</td></tr><tr><td>Gemini</td><td><code>gemini-2.5-flash-lite-preview-09-2025</code></td><td>1M</td><td>$0.10</td><td>$0.40</td><td>Chat, reasoning, vision, audio</td></tr><tr><td>Gemini</td><td><code>gemini-flash-latest</code></td><td>1M</td><td>$0.30</td><td>$2.50</td><td>Chat, reasoning, vision, audio</td></tr><tr><td>Gemini</td><td><code>gemini-flash-lite-latest</code></td><td>1M</td><td>$0.10</td><td>$0.40</td><td>Chat, reasoning, vision, audio</td></tr><tr><td>DeepSeek</td><td><code>deepseek-chat</code></td><td>131K</td><td>$0.60</td><td>$1.70</td><td>Chat, function calling, caching</td></tr><tr><td>DeepSeek</td><td><code>deepseek-reasoner</code></td><td>131K</td><td>$0.60</td><td>$1.70</td><td>Chat, reasoning</td></tr><tr><td>Bedrock</td><td><code>deepseek.v3-v1:0</code></td><td>164K</td><td>$0.58</td><td>$1.68</td><td>Chat, reasoning, function calling</td></tr><tr><td>Azure</td><td><code>azure/gpt-5-codex</code></td><td>272K</td><td>$1.25</td><td>$10.00</td><td>Responses API, reasoning, vision</td></tr><tr><td>OpenAI</td><td><code>gpt-5-codex</code></td><td>272K</td><td>$1.25</td><td>$10.00</td><td>Responses API, reasoning, vision</td></tr><tr><td>SambaNova</td><td><code>sambanova/DeepSeek-V3.1</code></td><td>33K</td><td>$3.00</td><td>$4.50</td><td>Chat, reasoning, function calling</td></tr><tr><td>SambaNova</td><td><code>sambanova/gpt-oss-120b</code></td><td>131K</td><td>$3.00</td><td>$4.50</td><td>Chat, reasoning, function calling</td></tr><tr><td>Bedrock</td><td><code>qwen.qwen3-coder-480b-a35b-v1:0</code></td><td>262K</td><td>$0.22</td><td>$1.80</td><td>Chat, reasoning, function calling</td></tr><tr><td>Bedrock</td><td><code>qwen.qwen3-235b-a22b-2507-v1:0</code></td><td>262K</td><td>$0.22</td><td>$0.88</td><td>Chat, reasoning, function calling</td></tr><tr><td>Bedrock</td><td><code>qwen.qwen3-coder-30b-a3b-v1:0</code></td><td>262K</td><td>$0.15</td><td>$0.60</td><td>Chat, reasoning, function calling</td></tr><tr><td>Bedrock</td><td><code>qwen.qwen3-32b-v1:0</code></td><td>131K</td><td>$0.15</td><td>$0.60</td><td>Chat, reasoning, function calling</td></tr><tr><td>Vertex AI</td><td><code>vertex_ai/qwen/qwen3-next-80b-a3b-instruct-maas</code></td><td>262K</td><td>$0.15</td><td>$1.20</td><td>Chat, function calling</td></tr><tr><td>Vertex AI</td><td><code>vertex_ai/qwen/qwen3-next-80b-a3b-thinking-maas</code></td><td>262K</td><td>$0.15</td><td>$1.20</td><td>Chat, function calling</td></tr><tr><td>Vertex AI</td><td><code>vertex_ai/deepseek-ai/deepseek-v3.1-maas</code></td><td>164K</td><td>$1.35</td><td>$5.40</td><td>Chat, reasoning, function calling</td></tr><tr><td>OpenRouter</td><td><code>openrouter/x-ai/grok-4-fast:free</code></td><td>2M</td><td>$0.00</td><td>$0.00</td><td>Chat, reasoning, function calling</td></tr><tr><td>XAI</td><td><code>xai/grok-4-fast-reasoning</code></td><td>2M</td><td>$0.20</td><td>$0.50</td><td>Chat, reasoning, function calling</td></tr><tr><td>XAI</td><td><code>xai/grok-4-fast-non-reasoning</code></td><td>2M</td><td>$0.20</td><td>$0.50</td><td>Chat, function calling</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/gemini">Gemini</a></strong>
<ul>
<li>Added Gemini 2.5 Flash and Flash-lite preview models (September 2025 release) with improved pricing - <a href="https://github.com/BerriAI/litellm/pull/14948" target="_blank" rel="noopener noreferrer">PR #14948</a></li>
<li>Added new Anthropic web fetch tool support - <a href="https://github.com/BerriAI/litellm/pull/14951" target="_blank" rel="noopener noreferrer">PR #14951</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/xai">XAI</a></strong>
<ul>
<li>Add xai/grok-4-fast models - <a href="https://github.com/BerriAI/litellm/pull/14833" target="_blank" rel="noopener noreferrer">PR #14833</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Updated Claude Sonnet 4 configs to reflect million-token context window pricing - <a href="https://github.com/BerriAI/litellm/pull/14639" target="_blank" rel="noopener noreferrer">PR #14639</a></li>
<li>Added supported text field to anthropic citation response - <a href="https://github.com/BerriAI/litellm/pull/14164" target="_blank" rel="noopener noreferrer">PR #14164</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>
<ul>
<li>Added support for Qwen models family &amp; Deepseek 3.1 to Amazon Bedrock - <a href="https://github.com/BerriAI/litellm/pull/14845" target="_blank" rel="noopener noreferrer">PR #14845</a></li>
<li>Support requestMetadata in Bedrock Converse API - <a href="https://github.com/BerriAI/litellm/pull/14570" target="_blank" rel="noopener noreferrer">PR #14570</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex">Vertex AI</a></strong>
<ul>
<li>Added vertex_ai/qwen models and azure/gpt-5-codex - <a href="https://github.com/BerriAI/litellm/pull/14844" target="_blank" rel="noopener noreferrer">PR #14844</a></li>
<li>Update vertex ai qwen model pricing - <a href="https://github.com/BerriAI/litellm/pull/14828" target="_blank" rel="noopener noreferrer">PR #14828</a></li>
<li>Vertex AI Context Caching: use Vertex ai API v1 instead of v1beta1 and accept &#x27;cachedContent&#x27; param - <a href="https://github.com/BerriAI/litellm/pull/14831" target="_blank" rel="noopener noreferrer">PR #14831</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/sambanova">SambaNova</a></strong>
<ul>
<li>Add sambanova deepseek v3.1 and gpt-oss-120b - <a href="https://github.com/BerriAI/litellm/pull/14866" target="_blank" rel="noopener noreferrer">PR #14866</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openai">OpenAI</a></strong>
<ul>
<li>Fix inconsistent token configs for gpt-5 models - <a href="https://github.com/BerriAI/litellm/pull/14942" target="_blank" rel="noopener noreferrer">PR #14942</a></li>
<li>GPT-3.5-Turbo price updated - <a href="https://github.com/BerriAI/litellm/pull/14858" target="_blank" rel="noopener noreferrer">PR #14858</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openrouter">OpenRouter</a></strong>
<ul>
<li>Add gpt-5 and gpt-5-codex to OpenRouter cost map - <a href="https://github.com/BerriAI/litellm/pull/14879" target="_blank" rel="noopener noreferrer">PR #14879</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vllm">VLLM</a></strong>
<ul>
<li>Fix vllm passthrough - <a href="https://github.com/BerriAI/litellm/pull/14778" target="_blank" rel="noopener noreferrer">PR #14778</a></li>
</ul>
</li>
<li><strong><a href="/docs/image_generation">Flux</a></strong>
<ul>
<li>Support flux image edit - <a href="https://github.com/BerriAI/litellm/pull/14790" target="_blank" rel="noopener noreferrer">PR #14790</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes">Bug Fixes<a href="#bug-fixes" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h3>
<ul>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Fix: Support claude code auth via subscription (anthropic) - <a href="https://github.com/BerriAI/litellm/pull/14821" target="_blank" rel="noopener noreferrer">PR #14821</a></li>
<li>Fix Anthropic streaming IDs - <a href="https://github.com/BerriAI/litellm/pull/14965" target="_blank" rel="noopener noreferrer">PR #14965</a></li>
<li>Revert incorrect changes to sonnet-4 max output tokens - <a href="https://github.com/BerriAI/litellm/pull/14933" target="_blank" rel="noopener noreferrer">PR #14933</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openai">OpenAI</a></strong>
<ul>
<li>Fix a bug where openai image edit silently ignores multiple images - <a href="https://github.com/BerriAI/litellm/pull/14893" target="_blank" rel="noopener noreferrer">PR #14893</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vllm">VLLM</a></strong>
<ul>
<li>Fix: vLLM provider&#x27;s rerank endpoint from /v1/rerank to /rerank - <a href="https://github.com/BerriAI/litellm/pull/14938" target="_blank" rel="noopener noreferrer">PR #14938</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-provider-support">New Provider Support<a href="#new-provider-support" class="hash-link" aria-label="New Provider Support" title="New Provider Support"></a></h4>
<ul>
<li><strong><a href="/docs/providers/wandb">W&amp;B Inference</a></strong>
<ul>
<li>Add W&amp;B Inference to LiteLLM - <a href="https://github.com/BerriAI/litellm/pull/14416" target="_blank" rel="noopener noreferrer">PR #14416</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>General</strong>
<ul>
<li>Add SDK support for additional headers - <a href="https://github.com/BerriAI/litellm/pull/14761" target="_blank" rel="noopener noreferrer">PR #14761</a></li>
<li>Add shared_session parameter for aiohttp ClientSession reuse - <a href="https://github.com/BerriAI/litellm/pull/14721" target="_blank" rel="noopener noreferrer">PR #14721</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>General</strong>
<ul>
<li>Fix: Streaming tool call index assignment for multiple tool calls - <a href="https://github.com/BerriAI/litellm/pull/14587" target="_blank" rel="noopener noreferrer">PR #14587</a></li>
<li>Fix load credentials in token counter proxy - <a href="https://github.com/BerriAI/litellm/pull/14808" target="_blank" rel="noopener noreferrer">PR #14808</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Proxy CLI Auth</strong>
<ul>
<li>Allow re-using cli auth token - <a href="https://github.com/BerriAI/litellm/pull/14780" target="_blank" rel="noopener noreferrer">PR #14780</a></li>
<li>Create a python method to login using litellm proxy - <a href="https://github.com/BerriAI/litellm/pull/14782" target="_blank" rel="noopener noreferrer">PR #14782</a></li>
<li>Fixes for LiteLLM Proxy CLI to Auth to Gateway - <a href="https://github.com/BerriAI/litellm/pull/14836" target="_blank" rel="noopener noreferrer">PR #14836</a></li>
</ul>
</li>
</ul>
<p><strong>Virtual Keys</strong></p>
<ul>
<li>Initial support for scheduled key rotations - <a href="https://github.com/BerriAI/litellm/pull/14877" target="_blank" rel="noopener noreferrer">PR #14877</a></li>
<li>Allow scheduling key rotations when creating virtual keys - <a href="https://github.com/BerriAI/litellm/pull/14960" target="_blank" rel="noopener noreferrer">PR #14960</a></li>
</ul>
<p><strong>Models + Endpoints</strong></p>
<ul>
<li>Fix: added Oracle to provider&#x27;s list - <a href="https://github.com/BerriAI/litellm/pull/14835" target="_blank" rel="noopener noreferrer">PR #14835</a></li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>SSO</strong> - Fix: SSO &quot;Clear&quot; button writes empty values instead of removing SSO config - <a href="https://github.com/BerriAI/litellm/pull/14826" target="_blank" rel="noopener noreferrer">PR #14826</a></li>
<li><strong>Admin Settings</strong> - Remove useful links from admin settings - <a href="https://github.com/BerriAI/litellm/pull/14918" target="_blank" rel="noopener noreferrer">PR #14918</a></li>
<li><strong>Management Routes</strong> - Add /user/list to management routes - <a href="https://github.com/BerriAI/litellm/pull/14868" target="_blank" rel="noopener noreferrer">PR #14868</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail--prompt-management-integrations">Logging / Guardrail / Prompt Management Integrations<a href="#logging--guardrail--prompt-management-integrations" class="hash-link" aria-label="Logging / Guardrail / Prompt Management Integrations" title="Logging / Guardrail / Prompt Management Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/logging#datadog">DataDog</a></strong>
<ul>
<li>Logging - <code>datadog</code> callback Log message content w/o sending to datadog - <a href="https://github.com/BerriAI/litellm/pull/14909" target="_blank" rel="noopener noreferrer">PR #14909</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#langfuse">Langfuse</a></strong>
<ul>
<li>Adding langfuse usage details for cached tokens - <a href="https://github.com/BerriAI/litellm/pull/10955" target="_blank" rel="noopener noreferrer">PR #10955</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#opik">Opik</a></strong>
<ul>
<li>Improve opik integration code - <a href="https://github.com/BerriAI/litellm/pull/14888" target="_blank" rel="noopener noreferrer">PR #14888</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#sqs">SQS</a></strong>
<ul>
<li>Error logging support for SQS Logger - <a href="https://github.com/BerriAI/litellm/pull/14974" target="_blank" rel="noopener noreferrer">PR #14974</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="guardrails">Guardrails<a href="#guardrails" class="hash-link" aria-label="Guardrails" title="Guardrails"></a></h4>
<ul>
<li><strong>LakeraAI v2 Guardrail</strong> - Ensure exception is raised correctly - <a href="https://github.com/BerriAI/litellm/pull/14867" target="_blank" rel="noopener noreferrer">PR #14867</a></li>
<li><strong>Presidio Guardrail</strong> - Support custom entity types in Presidio guardrail with Union[PiiEntityType, str] - <a href="https://github.com/BerriAI/litellm/pull/14899" target="_blank" rel="noopener noreferrer">PR #14899</a></li>
<li><strong>Noma Guardrail</strong> - Add noma guardrail provider to ui - <a href="https://github.com/BerriAI/litellm/pull/14415" target="_blank" rel="noopener noreferrer">PR #14415</a></li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="prompt-management">Prompt Management<a href="#prompt-management" class="hash-link" aria-label="Prompt Management" title="Prompt Management"></a></h4>
<ul>
<li><strong>BitBucket Integration</strong> - Add BitBucket Integration for Prompt Management - <a href="https://github.com/BerriAI/litellm/pull/14882" target="_blank" rel="noopener noreferrer">PR #14882</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-budgets-and-rate-limiting">Spend Tracking, Budgets and Rate Limiting<a href="#spend-tracking-budgets-and-rate-limiting" class="hash-link" aria-label="Spend Tracking, Budgets and Rate Limiting" title="Spend Tracking, Budgets and Rate Limiting"></a></h2>
<ul>
<li><strong>Service Tier Pricing</strong> - Add service_tier based pricing support for openai (BOTH Service &amp; Priority Support) - <a href="https://github.com/BerriAI/litellm/pull/14796" target="_blank" rel="noopener noreferrer">PR #14796</a></li>
<li><strong>Cost Tracking</strong> - Show input, output, tool call cost breakdown in StandardLoggingPayload - <a href="https://github.com/BerriAI/litellm/pull/14921" target="_blank" rel="noopener noreferrer">PR #14921</a></li>
<li><strong>Parallel Request Limiter v3</strong>
<ul>
<li>Ensure Lua scripts can execute on redis cluster - <a href="https://github.com/BerriAI/litellm/pull/14968" target="_blank" rel="noopener noreferrer">PR #14968</a></li>
<li>Fix: get metadata info from both metadata and litellm_metadata fields - <a href="https://github.com/BerriAI/litellm/pull/14783" target="_blank" rel="noopener noreferrer">PR #14783</a></li>
</ul>
</li>
<li><strong>Priority Reservation</strong> - Fix: Priority Reservation: keys without priority metadata receive higher priority than keys with explicit priority configurations - <a href="https://github.com/BerriAI/litellm/pull/14832" target="_blank" rel="noopener noreferrer">PR #14832</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway">MCP Gateway<a href="#mcp-gateway" class="hash-link" aria-label="MCP Gateway" title="MCP Gateway"></a></h2>
<ul>
<li><strong>MCP Configuration</strong> - Enable custom fields in mcp_info configuration - <a href="https://github.com/BerriAI/litellm/pull/14794" target="_blank" rel="noopener noreferrer">PR #14794</a></li>
<li><strong>MCP Tools</strong> - Remove server_name prefix from list_tools - <a href="https://github.com/BerriAI/litellm/pull/14720" target="_blank" rel="noopener noreferrer">PR #14720</a></li>
<li><strong>OAuth Flow</strong> - Initial commit for v2 oauth flow - <a href="https://github.com/BerriAI/litellm/pull/14964" target="_blank" rel="noopener noreferrer">PR #14964</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<ul>
<li><strong>Memory Leak Fix</strong> - Fix InMemoryCache unbounded growth when TTLs are set - <a href="https://github.com/BerriAI/litellm/pull/14869" target="_blank" rel="noopener noreferrer">PR #14869</a></li>
<li><strong>Cache Performance</strong> - Fix: cache root cause - <a href="https://github.com/BerriAI/litellm/pull/14827" target="_blank" rel="noopener noreferrer">PR #14827</a></li>
<li><strong>Concurrency Fix</strong> - Fix concurrency/scaling when many Python threads do streaming using <em>sync</em> completions - <a href="https://github.com/BerriAI/litellm/pull/14816" target="_blank" rel="noopener noreferrer">PR #14816</a></li>
<li><strong>Performance Optimization</strong> - Fix: reduce get_deployment cost to O(1) - <a href="https://github.com/BerriAI/litellm/pull/14967" target="_blank" rel="noopener noreferrer">PR #14967</a></li>
<li><strong>Performance Optimization</strong> - Fix: remove slow string operation - <a href="https://github.com/BerriAI/litellm/pull/14955" target="_blank" rel="noopener noreferrer">PR #14955</a></li>
<li><strong>DB Connection Management</strong> - Fix: DB connection state retries - <a href="https://github.com/BerriAI/litellm/pull/14925" target="_blank" rel="noopener noreferrer">PR #14925</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="documentation-updates">Documentation Updates<a href="#documentation-updates" class="hash-link" aria-label="Documentation Updates" title="Documentation Updates"></a></h2>
<ul>
<li><strong>Provider Documentation</strong> - Fix docs for provider_specific_params.md - <a href="https://github.com/BerriAI/litellm/pull/14787" target="_blank" rel="noopener noreferrer">PR #14787</a></li>
<li><strong>Model References</strong> - Update model references from gemini-pro to gemini-2.5-pro - <a href="https://github.com/BerriAI/litellm/pull/14775" target="_blank" rel="noopener noreferrer">PR #14775</a></li>
<li><strong>Letta Guide</strong> - Add Letta Guide documentation - <a href="https://github.com/BerriAI/litellm/pull/14798" target="_blank" rel="noopener noreferrer">PR #14798</a></li>
<li><strong>README</strong> - Make the README document clearer - <a href="https://github.com/BerriAI/litellm/pull/14860" target="_blank" rel="noopener noreferrer">PR #14860</a></li>
<li><strong>Session Management</strong> - Update docs for session management availability - <a href="https://github.com/BerriAI/litellm/pull/14914" target="_blank" rel="noopener noreferrer">PR #14914</a></li>
<li><strong>Cost Documentation</strong> - Add documentation for additional cost-related keys in custom pricing - <a href="https://github.com/BerriAI/litellm/pull/14949" target="_blank" rel="noopener noreferrer">PR #14949</a></li>
<li><strong>Azure Passthrough</strong> - Add azure passthrough documentation - <a href="https://github.com/BerriAI/litellm/pull/14958" target="_blank" rel="noopener noreferrer">PR #14958</a></li>
<li><strong>General Documentation</strong> - Doc updates sept 2025 - <a href="https://github.com/BerriAI/litellm/pull/14769" target="_blank" rel="noopener noreferrer">PR #14769</a>
<ul>
<li>Clarified bridging between endpoints and mode in docs.</li>
<li>Added Vertex AI Gemini API configuration as an alternative in relevant guides.
Linked AWS authentication info in the Bedrock guardrails documentation.</li>
<li>Added Cancel Response API usage with code snippets</li>
<li>Clarified that SSO (Single Sign-On) is free for up to 5 users:</li>
<li>Alphabetized sidebar, leaving quick start / intros at top of categories</li>
<li>Documented max_connections under cache_params.</li>
<li>Clarified IAM AssumeRole Policy requirements.</li>
<li>Added transform utilities example to Getting Started (showing request transformation).</li>
<li>Added references to models.litellm.ai as the full models list in various docs.</li>
<li>Added a code snippet for async_post_call_success_hook.</li>
<li>Removed broken links to callbacks management guide. - Reformatted and linked cookbooks + other relevant docs</li>
</ul>
</li>
<li><strong>Documentation Corrections</strong> - Corrected docs updates sept 2025 - <a href="https://github.com/BerriAI/litellm/pull/14916" target="_blank" rel="noopener noreferrer">PR #14916</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@uzaxirr made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14761" target="_blank" rel="noopener noreferrer">PR #14761</a></li>
<li>@xprilion made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14416" target="_blank" rel="noopener noreferrer">PR #14416</a></li>
<li>@CH-GAGANRAJ made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14779" target="_blank" rel="noopener noreferrer">PR #14779</a></li>
<li>@otaviofbrito made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14778" target="_blank" rel="noopener noreferrer">PR #14778</a></li>
<li>@danielmklein made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14639" target="_blank" rel="noopener noreferrer">PR #14639</a></li>
<li>@Jetemple made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14826" target="_blank" rel="noopener noreferrer">PR #14826</a></li>
<li>@akshoop made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14818" target="_blank" rel="noopener noreferrer">PR #14818</a></li>
<li>@hazyone made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14821" target="_blank" rel="noopener noreferrer">PR #14821</a></li>
<li>@leventov made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14816" target="_blank" rel="noopener noreferrer">PR #14816</a></li>
<li>@fabriciojoc made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10955" target="_blank" rel="noopener noreferrer">PR #10955</a></li>
<li>@onlylonly made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14845" target="_blank" rel="noopener noreferrer">PR #14845</a></li>
<li>@Copilot made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14869" target="_blank" rel="noopener noreferrer">PR #14869</a></li>
<li>@arsh72 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14899" target="_blank" rel="noopener noreferrer">PR #14899</a></li>
<li>@berri-teddy made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14914" target="_blank" rel="noopener noreferrer">PR #14914</a></li>
<li>@vpbill made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14415" target="_blank" rel="noopener noreferrer">PR #14415</a></li>
<li>@kgritesh made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14893" target="_blank" rel="noopener noreferrer">PR #14893</a></li>
<li>@oytunkutrup1 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14858" target="_blank" rel="noopener noreferrer">PR #14858</a></li>
<li>@nherment made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14933" target="_blank" rel="noopener noreferrer">PR #14933</a></li>
<li>@deepanshululla made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14974" target="_blank" rel="noopener noreferrer">PR #14974</a></li>
<li>@TeddyAmkie made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14758" target="_blank" rel="noopener noreferrer">PR #14758</a></li>
<li>@SmartManoj made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14775" target="_blank" rel="noopener noreferrer">PR #14775</a></li>
<li>@uc4w6c made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14720" target="_blank" rel="noopener noreferrer">PR #14720</a></li>
<li>@luizrennocosta made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14783" target="_blank" rel="noopener noreferrer">PR #14783</a></li>
<li>@AlexsanderHamir made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14827" target="_blank" rel="noopener noreferrer">PR #14827</a></li>
<li>@dharamendrak made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14721" target="_blank" rel="noopener noreferrer">PR #14721</a></li>
<li>@TomeHirata made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14164" target="_blank" rel="noopener noreferrer">PR #14164</a></li>
<li>@mrFranklin made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14860" target="_blank" rel="noopener noreferrer">PR #14860</a></li>
<li>@luisfucros made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14866" target="_blank" rel="noopener noreferrer">PR #14866</a></li>
<li>@huangyafei made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14879" target="_blank" rel="noopener noreferrer">PR #14879</a></li>
<li>@thiswillbeyourgithub made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14949" target="_blank" rel="noopener noreferrer">PR #14949</a></li>
<li>@Maximgitman made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14965" target="_blank" rel="noopener noreferrer">PR #14965</a></li>
<li>@subnet-dev made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14938" target="_blank" rel="noopener noreferrer">PR #14938</a></li>
<li>@22mSqRi made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14972" target="_blank" rel="noopener noreferrer">PR #14972</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog"><strong><a href="https://github.com/BerriAI/litellm/compare/v1.77.3.rc.1...v1.77.5.rc.1" target="_blank" rel="noopener noreferrer">Full Changelog</a></strong><a href="#full-changelog" class="hash-link" aria-label="full-changelog" title="full-changelog"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-77-3">v1.77.3-stable - Priority Based Rate Limiting</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-09-21T10:00:00.000Z">2025921</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaff"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaff</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.77.3-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.77.3</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>+550 RPS Performance Improvements</strong> - Optimizations in request handling and object initialization.</li>
<li><strong>Priority Quota Reservation</strong> - Proxy admins can now reserve TPM/RPM capacity for specific keys.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="priority-quota-reservation">Priority Quota Reservation<a href="#priority-quota-reservation" class="hash-link" aria-label="Priority Quota Reservation" title="Priority Quota Reservation"></a></h2>
<p>This release adds support for priority quota reservation. This allows Proxy Admins to reserve specific percentages of model capacity for different use cases.</p>
<p>This is great for use cases where you want to ensure your realtime use cases must always get priority responses and background development jobs can take longer.</p>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAi0lEQVR4nEWOSw7CMAwFezROyIZLcBC2iK64AIgIqVFpksaOUxjUj9TFLJ4878mNc44Z33mKFmqt/KoxVcNspxERQgj0wwfXv4k5ch8qXS6oCjHLKtrW1FKolmm953BuOd5e8DVEyyaaMeaRlBJjDPiUOV0fXJ49U1VS1l1U1YX5jTlLCphkynJbF/9+Wr8wSsh6/wAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="314"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/quota.8eb7614.640.png" srcset="/assets/ideal-img/quota.8eb7614.640.png 640w,/assets/ideal-img/quota.afd7194.1920.png 1920w" width="640" height="314"></noscript></div>
<br>
<p>This release adds support for priority quota reservation. This allows <strong>Proxy Admins</strong> to reserve TPM/RPM capacity for keys based on metadata priority levels, ensuring critical production workloads get guaranteed access regardless of development traffic volume.</p>
<p>Get started <a href="/docs/proxy/dynamic_rate_limit#priority-quota-reservation">here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="550-rps-performance-improvements">+550 RPS Performance Improvements<a href="#550-rps-performance-improvements" class="hash-link" aria-label="+550 RPS Performance Improvements" title="+550 RPS Performance Improvements"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAt0lEQVR4nB3GwUrDQBQF0Pl9kaKCy4pURErBHygo0rXLItJNCUkTkkzSNplJMi933hU8q2Oy9Ioir1DbCr73oBD92OMyXv4fQ8QwDTC27pjnBW1pWV9rrtt3LpsVn5tXPtkXHoeEBGlkDhoR1YnXRfGon+1O277RszvrvvvRm/xOf91BzSQTqeRbsuG2+iCFPKUnZmVG5x0Tn/L2cE8TQoAEweL7AWVXYRYgSICIYJxG6BzxddzhD8Jxt5JFcXmTAAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="331"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/perf_imp.1e940ae.640.png" srcset="/assets/ideal-img/perf_imp.1e940ae.640.png 640w,/assets/ideal-img/perf_imp.dc7bf16.1920.png 1920w" width="640" height="331"></noscript></div>
<br>
<p>This release delivers significant RPS improvements through targeted optimizations.</p>
<p>We&#x27;ve achieved a +500 RPS boost by fixing cache type inconsistencies that were causing frequent cache misses, plus an additional +50 RPS by removing unnecessary coroutine checks from the hot path.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-model-support">New Model Support<a href="#new-model-support" class="hash-link" aria-label="New Model Support" title="New Model Support"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Features</th></tr></thead><tbody><tr><td>SambaNova</td><td><code>sambanova/deepseek-v3.1</code></td><td>128K</td><td>$0.90</td><td>$0.90</td><td>Chat completions</td></tr><tr><td>SambaNova</td><td><code>sambanova/gpt-oss-120b</code></td><td>128K</td><td>$0.72</td><td>$0.72</td><td>Chat completions</td></tr><tr><td>OVHCloud</td><td>Various models</td><td>Varies</td><td>Contact provider</td><td>Contact provider</td><td>Chat completions</td></tr><tr><td>CompactifAI</td><td>Various models</td><td>Varies</td><td>Contact provider</td><td>Contact provider</td><td>Chat completions</td></tr><tr><td>TwelveLabs</td><td><code>twelvelabs/marengo-embed-2.7</code></td><td>32K</td><td>$0.12</td><td>$0.00</td><td>Embeddings</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/ovhcloud">OVHCloud AI Endpoints</a></strong>
<ul>
<li>New provider support with comprehensive model catalog - <a href="https://github.com/BerriAI/litellm/pull/14494" target="_blank" rel="noopener noreferrer">PR #14494</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/compactifai">CompactifAI</a></strong>
<ul>
<li>New provider integration - <a href="https://github.com/BerriAI/litellm/pull/14532" target="_blank" rel="noopener noreferrer">PR #14532</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/sambanova">SambaNova</a></strong>
<ul>
<li>Added DeepSeek v3.1 and GPT-OSS-120B models - <a href="https://github.com/BerriAI/litellm/pull/14500" target="_blank" rel="noopener noreferrer">PR #14500</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>
<ul>
<li>Cross-region inference profile cost calculation - <a href="https://github.com/BerriAI/litellm/pull/14566" target="_blank" rel="noopener noreferrer">PR #14566</a></li>
<li>AWS external ID parameter support for authentication - <a href="https://github.com/BerriAI/litellm/pull/14582" target="_blank" rel="noopener noreferrer">PR #14582</a></li>
<li>CountTokens API implementation - <a href="https://github.com/BerriAI/litellm/pull/14557" target="_blank" rel="noopener noreferrer">PR #14557</a></li>
<li>Titan V2 encoding_format parameter support - <a href="https://github.com/BerriAI/litellm/pull/14687" target="_blank" rel="noopener noreferrer">PR #14687</a></li>
<li>Nova Canvas image generation inference profiles - <a href="https://github.com/BerriAI/litellm/pull/14578" target="_blank" rel="noopener noreferrer">PR #14578</a></li>
<li>Bedrock Batches API - batch processing support with file upload and request transformation - <a href="https://github.com/BerriAI/litellm/pull/14618" target="_blank" rel="noopener noreferrer">PR #14618</a></li>
<li>Bedrock Twelve Labs embedding provider support - <a href="https://github.com/BerriAI/litellm/pull/14697" target="_blank" rel="noopener noreferrer">PR #14697</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex">Vertex AI</a></strong>
<ul>
<li>Gemini labels field provider-aware filtering - <a href="https://github.com/BerriAI/litellm/pull/14563" target="_blank" rel="noopener noreferrer">PR #14563</a></li>
<li>Gemini Batch API support - <a href="https://github.com/BerriAI/litellm/pull/14733" target="_blank" rel="noopener noreferrer">PR #14733</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/volcengine">Volcengine</a></strong>
<ul>
<li>Fixed thinking parameters when disabled - <a href="https://github.com/BerriAI/litellm/pull/14569" target="_blank" rel="noopener noreferrer">PR #14569</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/cohere">Cohere</a></strong>
<ul>
<li>Handle Generate API deprecation, default to chat endpoints - <a href="https://github.com/BerriAI/litellm/pull/14676" target="_blank" rel="noopener noreferrer">PR #14676</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/twelvelabs">TwelveLabs</a></strong>
<ul>
<li>Added Marengo Embed 2.7 embedding support - <a href="https://github.com/BerriAI/litellm/pull/14674" target="_blank" rel="noopener noreferrer">PR #14674</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes">Bug Fixes<a href="#bug-fixes" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h3>
<ul>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>
<ul>
<li>Empty arguments handling in tool call invocation - <a href="https://github.com/BerriAI/litellm/pull/14583" target="_blank" rel="noopener noreferrer">PR #14583</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex">Vertex AI</a></strong>
<ul>
<li>Avoid deepcopy crash with non-pickleables in Gemini/Vertex - <a href="https://github.com/BerriAI/litellm/pull/14418" target="_blank" rel="noopener noreferrer">PR #14418</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/xai">XAI</a></strong>
<ul>
<li>Fix unsupported stop parameter for grok-code models - <a href="https://github.com/BerriAI/litellm/pull/14565" target="_blank" rel="noopener noreferrer">PR #14565</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/gemini">Gemini</a></strong>
<ul>
<li>Updated error message for Gemini API - <a href="https://github.com/BerriAI/litellm/pull/14589" target="_blank" rel="noopener noreferrer">PR #14589</a></li>
<li>Fixed 2.5 Flash Image Preview model routing - <a href="https://github.com/BerriAI/litellm/pull/14715" target="_blank" rel="noopener noreferrer">PR #14715</a></li>
<li>API key passing for token counting endpoints - <a href="https://github.com/BerriAI/litellm/pull/14744" target="_blank" rel="noopener noreferrer">PR #14744</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-provider-support">New Provider Support<a href="#new-provider-support" class="hash-link" aria-label="New Provider Support" title="New Provider Support"></a></h4>
<ul>
<li><strong><a href="/docs/providers/ovhcloud">OVHCloud AI Endpoints</a></strong>
<ul>
<li>Complete provider integration with model catalog and authentication - <a href="https://github.com/BerriAI/litellm/pull/14494" target="_blank" rel="noopener noreferrer">PR #14494</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/compactifai">CompactifAI</a></strong>
<ul>
<li>New provider support with documentation - <a href="https://github.com/BerriAI/litellm/pull/14532" target="_blank" rel="noopener noreferrer">PR #14532</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/response_api">/responses</a></strong>
<ul>
<li>Added cancel endpoint support for non-admin users - <a href="https://github.com/BerriAI/litellm/pull/14594" target="_blank" rel="noopener noreferrer">PR #14594</a></li>
<li>Improved response session handling and cold storage configuration with s3 - <a href="https://github.com/BerriAI/litellm/pull/14534" target="_blank" rel="noopener noreferrer">PR #14534</a></li>
<li>Added OpenAI &amp; Azure /responses/cancel endpoint support - <a href="https://github.com/BerriAI/litellm/pull/14561" target="_blank" rel="noopener noreferrer">PR #14561</a></li>
</ul>
</li>
<li><strong>General</strong>
<ul>
<li>Enhanced rate limit error messages with details - <a href="https://github.com/BerriAI/litellm/pull/14736" target="_blank" rel="noopener noreferrer">PR #14736</a></li>
<li>Middle-truncation for spend log payloads - <a href="https://github.com/BerriAI/litellm/pull/14637" target="_blank" rel="noopener noreferrer">PR #14637</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/completion/input">/chat/completions</a></strong>
<ul>
<li>Fixed completion chat ID handling - <a href="https://github.com/BerriAI/litellm/pull/14548" target="_blank" rel="noopener noreferrer">PR #14548</a></li>
<li>Prevent AttributeError for _get_tags_from_request_kwargs - <a href="https://github.com/BerriAI/litellm/pull/14735" target="_blank" rel="noopener noreferrer">PR #14735</a></li>
</ul>
</li>
<li><strong><a href="/docs/response_api">/responses</a></strong>
<ul>
<li>Fixed cost calculation - <a href="https://github.com/BerriAI/litellm/pull/14675" target="_blank" rel="noopener noreferrer">PR #14675</a></li>
</ul>
</li>
<li><strong>General</strong>
<ul>
<li>Rate limiter AttributeError fix - <a href="https://github.com/BerriAI/litellm/pull/14609" target="_blank" rel="noopener noreferrer">PR #14609</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-budgets-and-rate-limiting">Spend Tracking, Budgets and Rate Limiting<a href="#spend-tracking-budgets-and-rate-limiting" class="hash-link" aria-label="Spend Tracking, Budgets and Rate Limiting" title="Spend Tracking, Budgets and Rate Limiting"></a></h2>
<ul>
<li><strong>Responses API Cost Calculation</strong> fix - <a href="https://github.com/BerriAI/litellm/pull/14675" target="_blank" rel="noopener noreferrer">PR #14675</a></li>
<li><strong>Anthropic Cache Token Pricing</strong> - Separate 1-hour vs 5-minute cache creation costs - <a href="https://github.com/BerriAI/litellm/pull/14620" target="_blank" rel="noopener noreferrer">PR #14620</a>, <a href="https://github.com/BerriAI/litellm/pull/14652" target="_blank" rel="noopener noreferrer">PR #14652</a></li>
<li><strong>Indochina Time Timezone</strong> support for budget resets - <a href="https://github.com/BerriAI/litellm/pull/14666" target="_blank" rel="noopener noreferrer">PR #14666</a></li>
<li><strong>Soft Budget Alert Cache Issues</strong> - Resolved soft budget alert cache issues - <a href="https://github.com/BerriAI/litellm/pull/14491" target="_blank" rel="noopener noreferrer">PR #14491</a></li>
<li><strong>Dynamic Rate Limiter v3</strong> - Priority routing improvements - <a href="https://github.com/BerriAI/litellm/pull/14734" target="_blank" rel="noopener noreferrer">PR #14734</a></li>
<li><strong>Enhanced Rate Limit Errors</strong> - More detailed error messages - <a href="https://github.com/BerriAI/litellm/pull/14736" target="_blank" rel="noopener noreferrer">PR #14736</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Team Member Service Account Keys</strong> - Allow team members to view keys they create - <a href="https://github.com/BerriAI/litellm/pull/14619" target="_blank" rel="noopener noreferrer">PR #14619</a></li>
<li><strong>Default Budget for JWT Teams</strong> - Auto-assign budgets to generated teams - <a href="https://github.com/BerriAI/litellm/pull/14514" target="_blank" rel="noopener noreferrer">PR #14514</a></li>
<li><strong>SSO Access Control Groups</strong> - Enhanced token info endpoint integration - <a href="https://github.com/BerriAI/litellm/pull/14738" target="_blank" rel="noopener noreferrer">PR #14738</a></li>
<li><strong>Health Test Connect Protection</strong> - Restrict access based on model creation permissions - <a href="https://github.com/BerriAI/litellm/pull/14650" target="_blank" rel="noopener noreferrer">PR #14650</a></li>
<li><strong>Amazon Bedrock Guardrail Info View</strong> - Enhanced logging visualization - <a href="https://github.com/BerriAI/litellm/pull/14696" target="_blank" rel="noopener noreferrer">PR #14696</a></li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes-1">Bug Fixes<a href="#bug-fixes-1" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h4>
<ul>
<li><strong>SCIM v2</strong> - Fix group PUSH and PUT operations for non-existent members - <a href="https://github.com/BerriAI/litellm/pull/14581" target="_blank" rel="noopener noreferrer">PR #14581</a></li>
<li><strong>Guardrail View/Edit/Delete</strong> behavior fixes - <a href="https://github.com/BerriAI/litellm/pull/14622" target="_blank" rel="noopener noreferrer">PR #14622</a></li>
<li><strong>In-Memory Guardrail</strong> update failures - <a href="https://github.com/BerriAI/litellm/pull/14653" target="_blank" rel="noopener noreferrer">PR #14653</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/logging#datadog">DataDog</a></strong>
<ul>
<li>Enhanced spend tracking metrics - <a href="https://github.com/BerriAI/litellm/pull/14555" target="_blank" rel="noopener noreferrer">PR #14555</a></li>
<li>Stream support with is_streamed_request parameter - <a href="https://github.com/BerriAI/litellm/pull/14673" target="_blank" rel="noopener noreferrer">PR #14673</a></li>
<li>Fixed tool calls metadata passing - <a href="https://github.com/BerriAI/litellm/pull/14531" target="_blank" rel="noopener noreferrer">PR #14531</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#langfuse">Langfuse</a></strong>
<ul>
<li>Added logging support for Responses API - <a href="https://github.com/BerriAI/litellm/pull/14597" target="_blank" rel="noopener noreferrer">PR #14597</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#langsmith">Langsmith</a></strong>
<ul>
<li>Langsmith Sampling Rate - Key/Team-level tracing configuration - <a href="https://github.com/BerriAI/litellm/pull/14740" target="_blank" rel="noopener noreferrer">PR #14740</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#prometheus">Prometheus</a></strong>
<ul>
<li>Multi-worker support improvements - <a href="https://github.com/BerriAI/litellm/pull/14530" target="_blank" rel="noopener noreferrer">PR #14530</a></li>
<li>User email labels in monitoring - <a href="https://github.com/BerriAI/litellm/pull/14520" target="_blank" rel="noopener noreferrer">PR #14520</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#opik">Opik</a></strong>
<ul>
<li>Fixed timezone issue - <a href="https://github.com/BerriAI/litellm/pull/14708" target="_blank" rel="noopener noreferrer">PR #14708</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes-2">Bug Fixes<a href="#bug-fixes-2" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h3>
<ul>
<li><strong><a href="/docs/proxy/logging#s3-buckets">S3</a></strong>
<ul>
<li>Fixed 404 error when using s3_endpoint_url - <a href="https://github.com/BerriAI/litellm/pull/14559" target="_blank" rel="noopener noreferrer">PR #14559</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="guardrails">Guardrails<a href="#guardrails" class="hash-link" aria-label="Guardrails" title="Guardrails"></a></h4>
<ul>
<li><strong>Tool Permission Guardrail</strong> - Fine-grained tool access control - <a href="https://github.com/BerriAI/litellm/pull/14519" target="_blank" rel="noopener noreferrer">PR #14519</a></li>
<li><strong>Bedrock Guardrails</strong> - Selective guarding support with runtime endpoint configuration - <a href="https://github.com/BerriAI/litellm/pull/14575" target="_blank" rel="noopener noreferrer">PR #14575</a>, <a href="https://github.com/BerriAI/litellm/pull/14650" target="_blank" rel="noopener noreferrer">PR #14650</a></li>
<li><strong>Default Last Message</strong> in guardrails - <a href="https://github.com/BerriAI/litellm/pull/14640" target="_blank" rel="noopener noreferrer">PR #14640</a></li>
<li><strong>AWS exceptions handling despite 200 response</strong> - <a href="https://github.com/BerriAI/litellm/pull/14658" target="_blank" rel="noopener noreferrer">PR #14658</a></li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-integration">New Integration<a href="#new-integration" class="hash-link" aria-label="New Integration" title="New Integration"></a></h4>
<ul>
<li><strong><a href="/docs/observability/posthog">PostHog</a></strong> - Complete observability integration for LiteLLM usage tracking and analytics - <a href="https://github.com/BerriAI/litellm/pull/14610" target="_blank" rel="noopener noreferrer">PR #14610</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway">MCP Gateway<a href="#mcp-gateway" class="hash-link" aria-label="MCP Gateway" title="MCP Gateway"></a></h2>
<ul>
<li><strong>MCP Server Alias Parsing</strong> - Multi-part URL path support - <a href="https://github.com/BerriAI/litellm/pull/14558" target="_blank" rel="noopener noreferrer">PR #14558</a></li>
<li><strong>MCP Filter Recomputation</strong> - After server deletion - <a href="https://github.com/BerriAI/litellm/pull/14542" target="_blank" rel="noopener noreferrer">PR #14542</a></li>
<li><strong>MCP Gateway Tools List</strong> improvements - <a href="https://github.com/BerriAI/litellm/pull/14695" target="_blank" rel="noopener noreferrer">PR #14695</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<ul>
<li><strong>+500 RPS Performance Boost</strong> when sending the <code>user</code> field - <a href="https://github.com/BerriAI/litellm/pull/14616" target="_blank" rel="noopener noreferrer">PR #14616</a></li>
<li><strong>+50 RPS</strong> by removing iscoroutine from hot path - <a href="https://github.com/BerriAI/litellm/pull/14649" target="_blank" rel="noopener noreferrer">PR #14649</a></li>
<li><strong>7% reduction</strong> in <strong>init</strong> overhead - <a href="https://github.com/BerriAI/litellm/pull/14689" target="_blank" rel="noopener noreferrer">PR #14689</a></li>
<li><strong>Generic Object Pool</strong> implementation for better resource management - <a href="https://github.com/BerriAI/litellm/pull/14702" target="_blank" rel="noopener noreferrer">PR #14702</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<ul>
<li><strong>Middle-Truncation</strong> for spend log payloads - <a href="https://github.com/BerriAI/litellm/pull/14637" target="_blank" rel="noopener noreferrer">PR #14637</a></li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="security">Security<a href="#security" class="hash-link" aria-label="Security" title="Security"></a></h4>
<ul>
<li><strong>Security Update</strong> - Bump aiohttp==3.12.14, fix CVE-2025-53643 - <a href="https://github.com/BerriAI/litellm/pull/14638" target="_blank" rel="noopener noreferrer">PR #14638</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@luisfucros made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14500" target="_blank" rel="noopener noreferrer">PR #14500</a></li>
<li>@hanakannzashi made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14548" target="_blank" rel="noopener noreferrer">PR #14548</a></li>
<li>@eliasto made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14494" target="_blank" rel="noopener noreferrer">PR #14494</a></li>
<li>@Rasmusafj made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14491" target="_blank" rel="noopener noreferrer">PR #14491</a></li>
<li>@LingXuanYin made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14569" target="_blank" rel="noopener noreferrer">PR #14569</a></li>
<li>@ronaldpereira made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14613" target="_blank" rel="noopener noreferrer">PR #14613</a></li>
<li>@hula-la made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14534" target="_blank" rel="noopener noreferrer">PR #14534</a></li>
<li>@carlos-marchal-ph made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14610" target="_blank" rel="noopener noreferrer">PR #14610</a></li>
<li>@akraines made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14637" target="_blank" rel="noopener noreferrer">PR #14637</a></li>
<li>@mrFranklin made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14708" target="_blank" rel="noopener noreferrer">PR #14708</a></li>
<li>@tcx4c70 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14675" target="_blank" rel="noopener noreferrer">PR #14675</a></li>
<li>@michaeltansg made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14666" target="_blank" rel="noopener noreferrer">PR #14666</a></li>
<li>@tosi29 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14725" target="_blank" rel="noopener noreferrer">PR #14725</a></li>
<li>@gmdfalk made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14735" target="_blank" rel="noopener noreferrer">PR #14735</a></li>
<li>@FelipeRodriguesGare made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14733" target="_blank" rel="noopener noreferrer">PR #14733</a></li>
<li>@mritunjaysharma394 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14678" target="_blank" rel="noopener noreferrer">PR #14678</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog"><strong><a href="https://github.com/BerriAI/litellm/compare/v1.77.2.rc.1...v1.77.3.rc.1" target="_blank" rel="noopener noreferrer">Full Changelog</a></strong><a href="#full-changelog" class="hash-link" aria-label="full-changelog" title="full-changelog"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-77-2">v1.77.2-stable - Bedrock Batches API</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-09-13T10:00:00.000Z">2025913</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:main-v1.77.2-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.77.2.post1</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>Bedrock Batches API</strong> - Support for creating Batch Inference Jobs on Bedrock using LiteLLM&#x27;s unified batch API (OpenAI compatible)</li>
<li><strong>Qwen API Tiered Pricing</strong> - Cost tracking support for Dashscope (Qwen) models with multiple pricing tiers</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-model-support">New Model Support<a href="#new-model-support" class="hash-link" aria-label="New Model Support" title="New Model Support"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Pricing ($/1M tokens)</th><th>Features</th></tr></thead><tbody><tr><td>DeepInfra</td><td><code>deepinfra/deepseek-ai/DeepSeek-R1</code></td><td>164K</td><td><strong>Input:</strong> $0.70<br><strong>Output:</strong> $2.40</td><td>Chat completions, tool calling</td></tr><tr><td>Heroku</td><td><code>heroku/claude-4-sonnet</code></td><td>8K</td><td>Contact provider for pricing</td><td>Function calling, tool choice</td></tr><tr><td>Heroku</td><td><code>heroku/claude-3-7-sonnet</code></td><td>8K</td><td>Contact provider for pricing</td><td>Function calling, tool choice</td></tr><tr><td>Heroku</td><td><code>heroku/claude-3-5-sonnet-latest</code></td><td>8K</td><td>Contact provider for pricing</td><td>Function calling, tool choice</td></tr><tr><td>Heroku</td><td><code>heroku/claude-3-5-haiku</code></td><td>4K</td><td>Contact provider for pricing</td><td>Function calling, tool choice</td></tr><tr><td>Dashscope</td><td><code>dashscope/qwen-plus-latest</code></td><td>1M</td><td><strong>Tiered Pricing:</strong><br> 0-256K tokens: $0.40 / $1.20<br> 256K-1M tokens: $1.20 / $3.60</td><td>Function calling, reasoning</td></tr><tr><td>Dashscope</td><td><code>dashscope/qwen3-max-preview</code></td><td>262K</td><td><strong>Tiered Pricing:</strong><br> 0-32K tokens: $1.20 / $6.00<br> 32K-128K tokens: $2.40 / $12.00<br> 128K-252K tokens: $3.00 / $15.00</td><td>Function calling, reasoning</td></tr><tr><td>Dashscope</td><td><code>dashscope/qwen-flash</code></td><td>1M</td><td><strong>Tiered Pricing:</strong><br> 0-256K tokens: $0.05 / $0.40<br> 256K-1M tokens: $0.25 / $2.00</td><td>Function calling, reasoning</td></tr><tr><td>Dashscope</td><td><code>dashscope/qwen3-coder-plus</code></td><td>1M</td><td><strong>Tiered Pricing:</strong><br> 0-32K tokens: $1.00 / $5.00<br> 32K-128K tokens: $1.80 / $9.00<br> 128K-256K tokens: $3.00 / $15.00<br> 256K-1M tokens: $6.00 / $60.00</td><td>Function calling, reasoning, caching</td></tr><tr><td>Dashscope</td><td><code>dashscope/qwen3-coder-flash</code></td><td>1M</td><td><strong>Tiered Pricing:</strong><br> 0-32K tokens: $0.30 / $1.50<br> 32K-128K tokens: $0.50 / $2.50<br> 128K-256K tokens: $0.80 / $4.00<br> 256K-1M tokens: $1.60 / $9.60</td><td>Function calling, reasoning, caching</td></tr></tbody></table>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/bedrock_batches">Bedrock</a></strong>
<ul>
<li>Bedrock Batches API - batch processing support with file upload and request transformation - <a href="https://github.com/BerriAI/litellm/pull/14518" target="_blank" rel="noopener noreferrer">PR #14518</a>, <a href="https://github.com/BerriAI/litellm/pull/14522" target="_blank" rel="noopener noreferrer">PR #14522</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vllm">VLLM</a></strong>
<ul>
<li>Added transcription endpoint support - <a href="https://github.com/BerriAI/litellm/pull/14523" target="_blank" rel="noopener noreferrer">PR #14523</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/ollama">Ollama</a></strong>
<ul>
<li><code>ollama_chat/</code> - images, thinking, and content as list handling - <a href="https://github.com/BerriAI/litellm/pull/14523" target="_blank" rel="noopener noreferrer">PR #14523</a></li>
</ul>
</li>
<li><strong>General</strong>
<ul>
<li>New debug flag for detailed request/response logging <a href="https://github.com/BerriAI/litellm/pull/14482" target="_blank" rel="noopener noreferrer">PR #14482</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes">Bug Fixes<a href="#bug-fixes" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h4>
<ul>
<li><strong><a href="/docs/providers/azure">Azure OpenAI</a></strong>
<ul>
<li>Fixed extra_body injection causing payload rejection in image generation - <a href="https://github.com/BerriAI/litellm/pull/14475" target="_blank" rel="noopener noreferrer">PR #14475</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/lm-studio">LM Studio</a></strong>
<ul>
<li>Resolved illegal Bearer header value issue - <a href="https://github.com/BerriAI/litellm/pull/14512" target="_blank" rel="noopener noreferrer">PR #14512</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes-1">Bug Fixes<a href="#bug-fixes-1" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h4>
<ul>
<li><strong><a href="/docs/anthropic_unified">/messages</a></strong>
<ul>
<li>Don&#x27;t send content block after message w/ finish reason + usage block - <a href="https://github.com/BerriAI/litellm/pull/14477" target="_blank" rel="noopener noreferrer">PR #14477</a></li>
</ul>
</li>
<li><strong><a href="/docs/generateContent">/generateContent</a></strong>
<ul>
<li>Gemini CLI Integration - Fixed token count errors - <a href="https://github.com/BerriAI/litellm/pull/14451" target="_blank" rel="noopener noreferrer">PR #14451</a>, <a href="https://github.com/BerriAI/litellm/pull/14417" target="_blank" rel="noopener noreferrer">PR #14417</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-budgets-and-rate-limiting">Spend Tracking, Budgets and Rate Limiting<a href="#spend-tracking-budgets-and-rate-limiting" class="hash-link" aria-label="Spend Tracking, Budgets and Rate Limiting" title="Spend Tracking, Budgets and Rate Limiting"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/dashscope">Qwen API Tiered Pricing</a></strong> - Added comprehensive tiered cost tracking for Dashscope/Qwen models - <a href="https://github.com/BerriAI/litellm/pull/14471" target="_blank" rel="noopener noreferrer">PR #14471</a>, <a href="https://github.com/BerriAI/litellm/pull/14479" target="_blank" rel="noopener noreferrer">PR #14479</a></li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes-2">Bug Fixes<a href="#bug-fixes-2" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h4>
<ul>
<li><strong>Provider Budgets</strong> - Fixed provider budget calculations - <a href="https://github.com/BerriAI/litellm/pull/14459" target="_blank" rel="noopener noreferrer">PR #14459</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>User Headers Mapping</strong> - New X-LiteLLM Users mapping feature for enhanced user tracking - <a href="https://github.com/BerriAI/litellm/pull/14485" target="_blank" rel="noopener noreferrer">PR #14485</a></li>
<li><strong>Key Unblocking</strong> - Support for hashed tokens in <code>/key/unblock</code> endpoint - <a href="https://github.com/BerriAI/litellm/pull/14477" target="_blank" rel="noopener noreferrer">PR #14477</a></li>
<li><strong>Model Group Header Forwarding</strong> - Enhanced wildcard model support with documentation - <a href="https://github.com/BerriAI/litellm/pull/14528" target="_blank" rel="noopener noreferrer">PR #14528</a></li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes-3">Bug Fixes<a href="#bug-fixes-3" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h4>
<ul>
<li><strong>Log Tab Key Alias</strong> - Fixed filtering inaccuracies for failed logs - <a href="https://github.com/BerriAI/litellm/pull/14469" target="_blank" rel="noopener noreferrer">PR #14469</a>, <a href="https://github.com/BerriAI/litellm/pull/14529" target="_blank" rel="noopener noreferrer">PR #14529</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Noma Integration</strong> - Added non-blocking monitor mode with anonymize input support - <a href="https://github.com/BerriAI/litellm/pull/14401" target="_blank" rel="noopener noreferrer">PR #14401</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="performance">Performance<a href="#performance" class="hash-link" aria-label="Performance" title="Performance"></a></h4>
<ul>
<li>Removed dynamic creation of static values - <a href="https://github.com/BerriAI/litellm/pull/14538" target="_blank" rel="noopener noreferrer">PR #14538</a></li>
<li>Using <code>_PROXY_MaxParallelRequestsHandler_v3</code> by default for optimal throughput - <a href="https://github.com/BerriAI/litellm/pull/14450" target="_blank" rel="noopener noreferrer">PR #14450</a></li>
<li>Improved execution context propagation into logging tasks - <a href="https://github.com/BerriAI/litellm/pull/14455" target="_blank" rel="noopener noreferrer">PR #14455</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@Sameerlite made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14460" target="_blank" rel="noopener noreferrer">PR #14460</a></li>
<li>@holzman made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14459" target="_blank" rel="noopener noreferrer">PR #14459</a></li>
<li>@sashank5644 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14469" target="_blank" rel="noopener noreferrer">PR #14469</a></li>
<li>@TomAlon made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14401" target="_blank" rel="noopener noreferrer">PR #14401</a></li>
<li>@AlexsanderHamir made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14538" target="_blank" rel="noopener noreferrer">PR #14538</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog"><strong><a href="https://github.com/BerriAI/litellm/compare/v1.77.1.dev.2...v1.77.2.dev" target="_blank" rel="noopener noreferrer">Full Changelog</a></strong><a href="#full-changelog" class="hash-link" aria-label="full-changelog" title="full-changelog"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-76-3">v1.76.3-stable - Performance, Video Generation &amp; CloudZero Integration</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-09-06T10:00:00.000Z">202596</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span></div><div class="admonitionContent_BuS1"><p>This release has a known issue where startup is leading to Out of Memory errors when deploying on Kubernetes. We recommend waiting before upgrading to this version.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.76.3</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.76.3</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>Major Performance Improvements</strong> +400 RPS when using correct amount of workers + CPU cores combination</li>
<li><strong>Video Generation Support</strong> - Added Google AI Studio  and Vertex AI Veo Video Generation through LiteLLM Pass through routes</li>
<li><strong>CloudZero Integration</strong> - New cost tracking integration for exporting LiteLLM Usage and Spend data to CloudZero.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="major-changes">Major Changes<a href="#major-changes" class="hash-link" aria-label="Major Changes" title="Major Changes"></a></h2>
<ul>
<li>
<p><strong>Performance Optimization</strong>: LiteLLM Proxy now achieves +400 RPS when using correct amount of CPU cores - <a href="https://github.com/BerriAI/litellm/pull/14153" target="_blank" rel="noopener noreferrer">PR #14153</a>, <a href="https://github.com/BerriAI/litellm/pull/14242" target="_blank" rel="noopener noreferrer">PR #14242</a></p>
<p>By default, LiteLLM will now use <code>num_workers = os.cpu_count()</code> to achieve optimal performance.</p>
<p><strong>Override Options:</strong></p>
<p>Set environment variable:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">DEFAULT_NUM_WORKERS_LITELLM_PROXY=1</span><br></span></code></pre></div></div>
<p>Or start LiteLLM Proxy with:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">litellm --num_workers 1</span><br></span></code></pre></div></div>
</li>
<li>
<p><strong>Security Fix</strong>: Fixed memory_usage_in_mem_cache cache endpoint vulnerability - <a href="https://github.com/BerriAI/litellm/pull/14229" target="_blank" rel="noopener noreferrer">PR #14229</a></p>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance-improvements">Performance Improvements<a href="#performance-improvements" class="hash-link" aria-label="Performance Improvements" title="Performance Improvements"></a></h2>
<p>This release includes significant performance optimizations. On our internal benchmarks we saw 1 instance get +400 RPS when using correct amount of  workers + CPU cores combination.</p>
<ul>
<li><strong>+400 RPS Performance Boost</strong> - LiteLLM Proxy now uses correct amount of CPU cores for optimal performance - <a href="https://github.com/BerriAI/litellm/pull/14153" target="_blank" rel="noopener noreferrer">PR #14153</a></li>
<li><strong>Default CPU Workers</strong> - Changed DEFAULT_NUM_WORKERS_LITELLM_PROXY default to number of CPUs - <a href="https://github.com/BerriAI/litellm/pull/14242" target="_blank" rel="noopener noreferrer">PR #14242</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-model-support">New Model Support<a href="#new-model-support" class="hash-link" aria-label="New Model Support" title="New Model Support"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Features</th></tr></thead><tbody><tr><td>OpenRouter</td><td><code>openrouter/openai/gpt-4.1</code></td><td>1M</td><td>$2.00</td><td>$8.00</td><td>Chat completions with vision</td></tr><tr><td>OpenRouter</td><td><code>openrouter/openai/gpt-4.1-mini</code></td><td>1M</td><td>$0.40</td><td>$1.60</td><td>Efficient chat completions</td></tr><tr><td>OpenRouter</td><td><code>openrouter/openai/gpt-4.1-nano</code></td><td>1M</td><td>$0.10</td><td>$0.40</td><td>Ultra-efficient chat</td></tr><tr><td>Vertex AI</td><td><code>vertex_ai/openai/gpt-oss-20b-maas</code></td><td>131K</td><td>$0.075</td><td>$0.30</td><td>Reasoning support</td></tr><tr><td>Vertex AI</td><td><code>vertex_ai/openai/gpt-oss-120b-maas</code></td><td>131K</td><td>$0.15</td><td>$0.60</td><td>Advanced reasoning</td></tr><tr><td>Gemini</td><td><code>gemini/veo-3.0-generate-preview</code></td><td>1K</td><td>-</td><td>$0.75/sec</td><td>Video generation</td></tr><tr><td>Gemini</td><td><code>gemini/veo-3.0-fast-generate-preview</code></td><td>1K</td><td>-</td><td>$0.40/sec</td><td>Fast video generation</td></tr><tr><td>Gemini</td><td><code>gemini/veo-2.0-generate-001</code></td><td>1K</td><td>-</td><td>$0.35/sec</td><td>Video generation</td></tr><tr><td>Volcengine</td><td><code>doubao-embedding-large</code></td><td>4K</td><td>Free</td><td>Free</td><td>2048-dim embeddings</td></tr><tr><td>Together AI</td><td><code>together_ai/deepseek-ai/DeepSeek-V3.1</code></td><td>128K</td><td>$0.60</td><td>$1.70</td><td>Reasoning support</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/gemini">Google Gemini</a></strong>
<ul>
<li>Added &#x27;thoughtSignature&#x27; support via &#x27;thinking_blocks&#x27; - <a href="https://github.com/BerriAI/litellm/pull/14122" target="_blank" rel="noopener noreferrer">PR #14122</a></li>
<li>Added support for reasoning_effort=&#x27;minimal&#x27; for Gemini models - <a href="https://github.com/BerriAI/litellm/pull/14262" target="_blank" rel="noopener noreferrer">PR #14262</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openrouter">OpenRouter</a></strong>
<ul>
<li>Added GPT-4.1 model family - <a href="https://github.com/BerriAI/litellm/pull/14101" target="_blank" rel="noopener noreferrer">PR #14101</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/groq">Groq</a></strong>
<ul>
<li>Added support for reasoning_effort parameter - <a href="https://github.com/BerriAI/litellm/pull/14207" target="_blank" rel="noopener noreferrer">PR #14207</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/xai">X.AI</a></strong>
<ul>
<li>Fixed XAI cost calculation - <a href="https://github.com/BerriAI/litellm/pull/14127" target="_blank" rel="noopener noreferrer">PR #14127</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex">Vertex AI</a></strong>
<ul>
<li>Added support for GPT-OSS models on Vertex AI - <a href="https://github.com/BerriAI/litellm/pull/14184" target="_blank" rel="noopener noreferrer">PR #14184</a></li>
<li>Added additionalProperties to Vertex AI Schema definition - <a href="https://github.com/BerriAI/litellm/pull/14252" target="_blank" rel="noopener noreferrer">PR #14252</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vllm">VLLM</a></strong>
<ul>
<li>Handle output parsing responses API output - <a href="https://github.com/BerriAI/litellm/pull/14121" target="_blank" rel="noopener noreferrer">PR #14121</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/ollama">Ollama</a></strong>
<ul>
<li>Added unified &#x27;thinking&#x27; param support via <code>reasoning_content</code> - <a href="https://github.com/BerriAI/litellm/pull/14121" target="_blank" rel="noopener noreferrer">PR #14121</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Added supported text field to anthropic citation response - <a href="https://github.com/BerriAI/litellm/pull/14126" target="_blank" rel="noopener noreferrer">PR #14126</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/oci">OCI Provider</a></strong>
<ul>
<li>Handle assistant messages with both content and tool_calls - <a href="https://github.com/BerriAI/litellm/pull/14171" target="_blank" rel="noopener noreferrer">PR #14171</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>
<ul>
<li>Fixed structure output - <a href="https://github.com/BerriAI/litellm/pull/14130" target="_blank" rel="noopener noreferrer">PR #14130</a></li>
<li>Added initial support for Bedrock Batches API - <a href="https://github.com/BerriAI/litellm/pull/14190" target="_blank" rel="noopener noreferrer">PR #14190</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/databricks">Databricks</a></strong>
<ul>
<li>Added support for anthropic citation API in Databricks - <a href="https://github.com/BerriAI/litellm/pull/14077" target="_blank" rel="noopener noreferrer">PR #14077</a></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes">Bug Fixes<a href="#bug-fixes" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h3>
<ul>
<li><strong><a href="/docs/providers/gemini">Google Gemini (Google AI Studio + Vertex AI)</a></strong>
<ul>
<li>Fixed Gemini 2.5 Pro schema validation with OpenAI-style type arrays in tools - <a href="https://github.com/BerriAI/litellm/pull/14154" target="_blank" rel="noopener noreferrer">PR #14154</a></li>
<li>Fixed Gemini Tool Calling empty enum property - <a href="https://github.com/BerriAI/litellm/pull/14155" target="_blank" rel="noopener noreferrer">PR #14155</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-provider-support">New Provider Support<a href="#new-provider-support" class="hash-link" aria-label="New Provider Support" title="New Provider Support"></a></h4>
<ul>
<li><strong><a href="/docs/providers/volcengine">Volcengine</a></strong>
<ul>
<li>Added Volcengine embedding module with handler and transformation logic - <a href="https://github.com/BerriAI/litellm/pull/14028" target="_blank" rel="noopener noreferrer">PR #14028</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/image_generation">Images API</a></strong>
<ul>
<li>Added pass through image generation and image editing on OpenAI - <a href="https://github.com/BerriAI/litellm/pull/14292" target="_blank" rel="noopener noreferrer">PR #14292</a></li>
<li>Support extra_body parameter for image generation - <a href="https://github.com/BerriAI/litellm/pull/14211" target="_blank" rel="noopener noreferrer">PR #14211</a></li>
</ul>
</li>
<li><strong><a href="/docs/response_api">Responses API</a></strong>
<ul>
<li>Fixed response API for reasoning item in input for litellm proxy - <a href="https://github.com/BerriAI/litellm/pull/14200" target="_blank" rel="noopener noreferrer">PR #14200</a></li>
<li>Added structured output for SDK - <a href="https://github.com/BerriAI/litellm/pull/14206" target="_blank" rel="noopener noreferrer">PR #14206</a></li>
</ul>
</li>
<li><strong><a href="/docs/pass_through/bedrock">Bedrock Passthrough</a></strong>
<ul>
<li>Support AWS_BEDROCK_RUNTIME_ENDPOINT on bedrock passthrough - <a href="https://github.com/BerriAI/litellm/pull/14156" target="_blank" rel="noopener noreferrer">PR #14156</a></li>
</ul>
</li>
<li><strong><a href="/docs/pass_through/google_ai_studio">Google AI Studio Passthrough</a></strong>
<ul>
<li>Allow using Veo Video Generation through LiteLLM Pass through routes - <a href="https://github.com/BerriAI/litellm/pull/14228" target="_blank" rel="noopener noreferrer">PR #14228</a></li>
</ul>
</li>
<li><strong>General</strong>
<ul>
<li>Added support for safety_identifier parameter in chat.completions.create - <a href="https://github.com/BerriAI/litellm/pull/14174" target="_blank" rel="noopener noreferrer">PR #14174</a></li>
<li>Fixed misclassified 500 error on invalid image_url in /chat/completions request - <a href="https://github.com/BerriAI/litellm/pull/14149" target="_blank" rel="noopener noreferrer">PR #14149</a></li>
<li>Fixed token count error for Gemini CLI - <a href="https://github.com/BerriAI/litellm/pull/14133" target="_blank" rel="noopener noreferrer">PR #14133</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>General</strong>
<ul>
<li>Remove &quot;/&quot; or &quot;:&quot; from model name when being used as h11 header name - <a href="https://github.com/BerriAI/litellm/pull/14191" target="_blank" rel="noopener noreferrer">PR #14191</a></li>
<li>Bug fix for openai.gpt-oss when using reasoning_effort parameter - <a href="https://github.com/BerriAI/litellm/pull/14300" target="_blank" rel="noopener noreferrer">PR #14300</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-budgets-and-rate-limiting">Spend Tracking, Budgets and Rate Limiting<a href="#spend-tracking-budgets-and-rate-limiting" class="hash-link" aria-label="Spend Tracking, Budgets and Rate Limiting" title="Spend Tracking, Budgets and Rate Limiting"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h3>
<ul>
<li>Added header support for spend_logs_metadata - <a href="https://github.com/BerriAI/litellm/pull/14186" target="_blank" rel="noopener noreferrer">PR #14186</a></li>
<li>Litellm passthrough cost tracking for chat completion - <a href="https://github.com/BerriAI/litellm/pull/14256" target="_blank" rel="noopener noreferrer">PR #14256</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes-1">Bug Fixes<a href="#bug-fixes-1" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h3>
<ul>
<li>Fixed TPM Rate Limit Bug - <a href="https://github.com/BerriAI/litellm/pull/14237" target="_blank" rel="noopener noreferrer">PR #14237</a></li>
<li>Fixed Key Budget not resets at expectable times - <a href="https://github.com/BerriAI/litellm/pull/14241" target="_blank" rel="noopener noreferrer">PR #14241</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>UI Improvements</strong>
<ul>
<li>Logs page screen size fixed - <a href="https://github.com/BerriAI/litellm/pull/14135" target="_blank" rel="noopener noreferrer">PR #14135</a></li>
<li>Create Organization Tooltip added on Success - <a href="https://github.com/BerriAI/litellm/pull/14132" target="_blank" rel="noopener noreferrer">PR #14132</a></li>
<li>Back to Keys should say Back to Logs - <a href="https://github.com/BerriAI/litellm/pull/14134" target="_blank" rel="noopener noreferrer">PR #14134</a></li>
<li>Add client side pagination on All Models table - <a href="https://github.com/BerriAI/litellm/pull/14136" target="_blank" rel="noopener noreferrer">PR #14136</a></li>
<li>Model Filters UI improvement - <a href="https://github.com/BerriAI/litellm/pull/14131" target="_blank" rel="noopener noreferrer">PR #14131</a></li>
<li>Remove table filter on user info page - <a href="https://github.com/BerriAI/litellm/pull/14169" target="_blank" rel="noopener noreferrer">PR #14169</a></li>
<li>Team name badge added on the User Details - <a href="https://github.com/BerriAI/litellm/pull/14003" target="_blank" rel="noopener noreferrer">PR #14003</a></li>
<li>Fix: Log page parameter passing error - <a href="https://github.com/BerriAI/litellm/pull/14193" target="_blank" rel="noopener noreferrer">PR #14193</a></li>
</ul>
</li>
<li><strong>Authentication &amp; Authorization</strong>
<ul>
<li>Support for ES256/ES384/ES512 and EdDSA JWT verification - <a href="https://github.com/BerriAI/litellm/pull/14118" target="_blank" rel="noopener noreferrer">PR #14118</a></li>
<li>Ensure <code>team_id</code> is a required field for generating service account keys - <a href="https://github.com/BerriAI/litellm/pull/14270" target="_blank" rel="noopener noreferrer">PR #14270</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>General</strong>
<ul>
<li>Validate store model in db setting - <a href="https://github.com/BerriAI/litellm/pull/14269" target="_blank" rel="noopener noreferrer">PR #14269</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-4">Features<a href="#features-4" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/logging#datadog">Datadog</a></strong>
<ul>
<li>Ensure <code>apm_id</code> is set on DD LLM Observability traces - <a href="https://github.com/BerriAI/litellm/pull/14272" target="_blank" rel="noopener noreferrer">PR #14272</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#braintrust">Braintrust</a></strong>
<ul>
<li>Fix logging when OTEL is enabled - <a href="https://github.com/BerriAI/litellm/pull/14122" target="_blank" rel="noopener noreferrer">PR #14122</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#otel">OTEL</a></strong>
<ul>
<li>Optional Metrics and Logs following semantic conventions - <a href="https://github.com/BerriAI/litellm/pull/14179" target="_blank" rel="noopener noreferrer">PR #14179</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/alerting">Slack Alerting</a></strong>
<ul>
<li>Added alert type to alert message to slack for easier handling - <a href="https://github.com/BerriAI/litellm/pull/14176" target="_blank" rel="noopener noreferrer">PR #14176</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="guardrails">Guardrails<a href="#guardrails" class="hash-link" aria-label="Guardrails" title="Guardrails"></a></h4>
<ul>
<li>Added guardrail to the Anthropic API endpoint - <a href="https://github.com/BerriAI/litellm/pull/14107" target="_blank" rel="noopener noreferrer">PR #14107</a></li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-integration">New Integration<a href="#new-integration" class="hash-link" aria-label="New Integration" title="New Integration"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/cost_tracking">CloudZero</a></strong>
<ul>
<li>LiteLLM x CloudZero Integration for Cost Tracking - <a href="https://github.com/BerriAI/litellm/pull/14296" target="_blank" rel="noopener noreferrer">PR #14296</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-5">Features<a href="#features-5" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Performance</strong>
<ul>
<li>LiteLLM Proxy: +400 RPS when using correct amount of CPU cores - <a href="https://github.com/BerriAI/litellm/pull/14153" target="_blank" rel="noopener noreferrer">PR #14153</a></li>
<li>Allow using <code>x-litellm-stream-timeout</code> header for stream timeout in requests - <a href="https://github.com/BerriAI/litellm/pull/14147" target="_blank" rel="noopener noreferrer">PR #14147</a></li>
<li>Change DEFAULT_NUM_WORKERS_LITELLM_PROXY default to number CPUs - <a href="https://github.com/BerriAI/litellm/pull/14242" target="_blank" rel="noopener noreferrer">PR #14242</a></li>
</ul>
</li>
<li><strong>Monitoring</strong>
<ul>
<li>Added Prometheus missing metrics - <a href="https://github.com/BerriAI/litellm/pull/14139" target="_blank" rel="noopener noreferrer">PR #14139</a></li>
</ul>
</li>
<li><strong>Timeout</strong>
<ul>
<li><strong>Stream Timeout Control</strong> - Allow using <code>x-litellm-stream-timeout</code> header for stream timeout in requests - <a href="https://github.com/BerriAI/litellm/pull/14147" target="_blank" rel="noopener noreferrer">PR #14147</a></li>
</ul>
</li>
<li><strong>Routing</strong>
<ul>
<li>Fixed x-litellm-tags not routing with Responses API - <a href="https://github.com/BerriAI/litellm/pull/14289" target="_blank" rel="noopener noreferrer">PR #14289</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-2">Bugs<a href="#bugs-2" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Security</strong>
<ul>
<li>Fixed memory_usage_in_mem_cache cache endpoint vulnerability - <a href="https://github.com/BerriAI/litellm/pull/14229" target="_blank" rel="noopener noreferrer">PR #14229</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-6">Features<a href="#features-6" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>SCIM Support</strong>
<ul>
<li>Added better SCIM debugging - <a href="https://github.com/BerriAI/litellm/pull/14221" target="_blank" rel="noopener noreferrer">PR #14221</a></li>
<li>Bug fixes for handling SCIM Group Memberships - <a href="https://github.com/BerriAI/litellm/pull/14226" target="_blank" rel="noopener noreferrer">PR #14226</a></li>
</ul>
</li>
<li><strong>Kubernetes</strong>
<ul>
<li>Added optional PodDisruptionBudget for litellm proxy - <a href="https://github.com/BerriAI/litellm/pull/14093" target="_blank" rel="noopener noreferrer">PR #14093</a></li>
</ul>
</li>
<li><strong>Error Handling</strong>
<ul>
<li>Add model to azure error message - <a href="https://github.com/BerriAI/litellm/pull/14294" target="_blank" rel="noopener noreferrer">PR #14294</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@iabhi4 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14093" target="_blank" rel="noopener noreferrer">PR #14093</a></li>
<li>@zainhas made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14087" target="_blank" rel="noopener noreferrer">PR #14087</a></li>
<li>@LifeDJIK made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14146" target="_blank" rel="noopener noreferrer">PR #14146</a></li>
<li>@retanoj made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14133" target="_blank" rel="noopener noreferrer">PR #14133</a></li>
<li>@zhxlp made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14193" target="_blank" rel="noopener noreferrer">PR #14193</a></li>
<li>@kayoch1n made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14191" target="_blank" rel="noopener noreferrer">PR #14191</a></li>
<li>@kutsushitaneko made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14171" target="_blank" rel="noopener noreferrer">PR #14171</a></li>
<li>@mjmendo made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14176" target="_blank" rel="noopener noreferrer">PR #14176</a></li>
<li>@HarshavardhanK made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14213" target="_blank" rel="noopener noreferrer">PR #14213</a></li>
<li>@eycjur made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14207" target="_blank" rel="noopener noreferrer">PR #14207</a></li>
<li>@22mSqRi made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14241" target="_blank" rel="noopener noreferrer">PR #14241</a></li>
<li>@onlylhf made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14028" target="_blank" rel="noopener noreferrer">PR #14028</a></li>
<li>@btpemercier made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11319" target="_blank" rel="noopener noreferrer">PR #11319</a></li>
<li>@tremlin made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14287" target="_blank" rel="noopener noreferrer">PR #14287</a></li>
<li>@TobiMayr made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14262" target="_blank" rel="noopener noreferrer">PR #14262</a></li>
<li>@Eitan1112 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14252" target="_blank" rel="noopener noreferrer">PR #14252</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog"><strong><a href="https://github.com/BerriAI/litellm/compare/v1.76.1-nightly...v1.76.3-nightly" target="_blank" rel="noopener noreferrer">Full Changelog</a></strong><a href="#full-changelog" class="hash-link" aria-label="full-changelog" title="full-changelog"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-76-1">v1.76.1-stable - Gemini 2.5 Flash Image</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-08-30T10:00:00.000Z">2025830</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.76.1</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.76.1</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>Major Performance Improvements</strong> - 6.5x faster LiteLLM Python SDK completion with fastuuid integration.</li>
<li><strong>New Model Support</strong> - Gemini 2.5 Flash Image Preview, Grok Code Fast, and GPT Realtime models</li>
<li><strong>Enhanced Provider Support</strong> - DeepSeek-v3.1 pricing on Fireworks AI, Vercel AI Gateway, and improved Anthropic/GitHub Copilot integration</li>
<li><strong>MCP Improvements</strong> - Better connection testing and SSE MCP tools bug fixes</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="major-changes">Major Changes<a href="#major-changes" class="hash-link" aria-label="Major Changes" title="Major Changes"></a></h2>
<ul>
<li>Added support for using Gemini 2.5 Flash Image Preview with /chat/completions. <strong> Warning</strong> If you were using <code>gemini-2.0-flash-exp-image-generation</code> please follow this migration guide.
<a href="/docs/extras/gemini_img_migration">Gemini Image Generation Migration Guide</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance-improvements">Performance Improvements<a href="#performance-improvements" class="hash-link" aria-label="Performance Improvements" title="Performance Improvements"></a></h2>
<p>This release includes significant performance optimizations:</p>
<ul>
<li><strong>6.5x faster LiteLLM Python SDK Completion</strong> - Major performance boost for completion operations - <a href="https://github.com/BerriAI/litellm/pull/13990" target="_blank" rel="noopener noreferrer">PR #13990</a></li>
<li><strong>fastuuid Integration</strong> - 2.1x faster UUID generation with +80 RPS improvement for /chat/completions and other LLM endpoints - <a href="https://github.com/BerriAI/litellm/pull/13992" target="_blank" rel="noopener noreferrer">PR #13992</a>, <a href="https://github.com/BerriAI/litellm/pull/14016" target="_blank" rel="noopener noreferrer">PR #14016</a></li>
<li><strong>Optimized Request Logging</strong> - Don&#x27;t print request params by default for +50 RPS improvement - <a href="https://github.com/BerriAI/litellm/pull/14015" target="_blank" rel="noopener noreferrer">PR #14015</a></li>
<li><strong>Cache Performance</strong> - 21% speedup in InMemoryCache.evict_cache and 45% speedup in <code>_is_debugging_on</code> function - <a href="https://github.com/BerriAI/litellm/pull/14012" target="_blank" rel="noopener noreferrer">PR #14012</a>, <a href="https://github.com/BerriAI/litellm/pull/13988" target="_blank" rel="noopener noreferrer">PR #13988</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-model-support">New Model Support<a href="#new-model-support" class="hash-link" aria-label="New Model Support" title="New Model Support"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Features</th></tr></thead><tbody><tr><td>Google</td><td><code>gemini-2.5-flash-image-preview</code></td><td>1M</td><td>$0.30</td><td>$2.50</td><td>Chat completions + image generation ($0.039/image)</td></tr><tr><td>X.AI</td><td><code>xai/grok-code-fast</code></td><td>256K</td><td>$0.20</td><td>$1.50</td><td>Code generation</td></tr><tr><td>OpenAI</td><td><code>gpt-realtime</code></td><td>32K</td><td>$4.00</td><td>$16.00</td><td>Real-time conversation + audio</td></tr><tr><td>Vercel AI Gateway</td><td><code>vercel_ai_gateway/openai/o3</code></td><td>200K</td><td>$2.00</td><td>$8.00</td><td>Advanced reasoning</td></tr><tr><td>Vercel AI Gateway</td><td><code>vercel_ai_gateway/openai/o3-mini</code></td><td>200K</td><td>$1.10</td><td>$4.40</td><td>Efficient reasoning</td></tr><tr><td>Vercel AI Gateway</td><td><code>vercel_ai_gateway/openai/o4-mini</code></td><td>200K</td><td>$1.10</td><td>$4.40</td><td>Latest mini model</td></tr><tr><td>DeepInfra</td><td><code>deepinfra/zai-org/GLM-4.5</code></td><td>131K</td><td>$0.55</td><td>$2.00</td><td>Chat completions</td></tr><tr><td>Perplexity</td><td><code>perplexity/codellama-34b-instruct</code></td><td>16K</td><td>$0.35</td><td>$1.40</td><td>Code generation</td></tr><tr><td>Fireworks AI</td><td><code>fireworks_ai/accounts/fireworks/models/deepseek-v3p1</code></td><td>128K</td><td>$0.56</td><td>$1.68</td><td>Chat completions</td></tr></tbody></table>
<p><strong>Additional Models Added:</strong> Various other Vercel AI Gateway models were added too. See <a href="https://models.litellm.ai" target="_blank" rel="noopener noreferrer">models.litellm.ai</a> for the full list.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/gemini">Google Gemini</a></strong>
<ul>
<li>Added support for <code>gemini-2.5-flash-image-preview</code> with image return capability - <a href="https://github.com/BerriAI/litellm/pull/13979" target="_blank" rel="noopener noreferrer">PR #13979</a>, <a href="https://github.com/BerriAI/litellm/pull/13983" target="_blank" rel="noopener noreferrer">PR #13983</a></li>
<li>Support for requests with only system prompt - <a href="https://github.com/BerriAI/litellm/pull/14010" target="_blank" rel="noopener noreferrer">PR #14010</a></li>
<li>Fixed invalid model name error for Gemini Imagen models - <a href="https://github.com/BerriAI/litellm/pull/13991" target="_blank" rel="noopener noreferrer">PR #13991</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/xai">X.AI</a></strong>
<ul>
<li>Added <code>xai/grok-code-fast</code> model family support - <a href="https://github.com/BerriAI/litellm/pull/14054" target="_blank" rel="noopener noreferrer">PR #14054</a></li>
<li>Fixed frequency_penalty parameter for grok-4 models - <a href="https://github.com/BerriAI/litellm/pull/14078" target="_blank" rel="noopener noreferrer">PR #14078</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openai">OpenAI</a></strong>
<ul>
<li>Added support for gpt-realtime models - <a href="https://github.com/BerriAI/litellm/pull/14082" target="_blank" rel="noopener noreferrer">PR #14082</a></li>
<li>Support for reasoning and reasoning_effort parameters by default - <a href="https://github.com/BerriAI/litellm/pull/12865" target="_blank" rel="noopener noreferrer">PR #12865</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/fireworks_ai">Fireworks AI</a></strong>
<ul>
<li>Added DeepSeek-v3.1 pricing - <a href="https://github.com/BerriAI/litellm/pull/13958" target="_blank" rel="noopener noreferrer">PR #13958</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/deepinfra">DeepInfra</a></strong>
<ul>
<li>Fixed reasoning_effort setting for DeepSeek-V3.1 - <a href="https://github.com/BerriAI/litellm/pull/14053" target="_blank" rel="noopener noreferrer">PR #14053</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/github_copilot">GitHub Copilot</a></strong>
<ul>
<li>Added support for thinking and reasoning_effort parameters - <a href="https://github.com/BerriAI/litellm/pull/13691" target="_blank" rel="noopener noreferrer">PR #13691</a></li>
<li>Added image headers support - <a href="https://github.com/BerriAI/litellm/pull/13955" target="_blank" rel="noopener noreferrer">PR #13955</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Support for custom Anthropic-compatible API endpoints - <a href="https://github.com/BerriAI/litellm/pull/13945" target="_blank" rel="noopener noreferrer">PR #13945</a></li>
<li>Fixed /messages fallback from Anthropic API to Bedrock API - <a href="https://github.com/BerriAI/litellm/pull/13946" target="_blank" rel="noopener noreferrer">PR #13946</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/nebius">Nebius</a></strong>
<ul>
<li>Expanded provider models and normalized model IDs - <a href="https://github.com/BerriAI/litellm/pull/13965" target="_blank" rel="noopener noreferrer">PR #13965</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex">Vertex AI</a></strong>
<ul>
<li>Fixed Vertex Mistral streaming issues - <a href="https://github.com/BerriAI/litellm/pull/13952" target="_blank" rel="noopener noreferrer">PR #13952</a></li>
<li>Fixed anyOf corner cases for Gemini tool calls - <a href="https://github.com/BerriAI/litellm/pull/12797" target="_blank" rel="noopener noreferrer">PR #12797</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>
<ul>
<li>Fixed structure output issues - <a href="https://github.com/BerriAI/litellm/pull/14005" target="_blank" rel="noopener noreferrer">PR #14005</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openrouter">OpenRouter</a></strong>
<ul>
<li>Added GPT-5 family models pricing - <a href="https://github.com/BerriAI/litellm/pull/13536" target="_blank" rel="noopener noreferrer">PR #13536</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-provider-support">New Provider Support<a href="#new-provider-support" class="hash-link" aria-label="New Provider Support" title="New Provider Support"></a></h4>
<ul>
<li><strong><a href="/docs/providers/vercel_ai_gateway">Vercel AI Gateway</a></strong>
<ul>
<li>New provider support added - <a href="https://github.com/BerriAI/litellm/pull/13144" target="_blank" rel="noopener noreferrer">PR #13144</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/datarobot">DataRobot</a></strong>
<ul>
<li>Added provider documentation - <a href="https://github.com/BerriAI/litellm/pull/14038" target="_blank" rel="noopener noreferrer">PR #14038</a>, <a href="https://github.com/BerriAI/litellm/pull/14074" target="_blank" rel="noopener noreferrer">PR #14074</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/image_generation">Images API</a></strong>
<ul>
<li>Support for multiple images in OpenAI images/edits endpoint - <a href="https://github.com/BerriAI/litellm/pull/13916" target="_blank" rel="noopener noreferrer">PR #13916</a></li>
<li>Allow using dynamic <code>api_key</code> for image generation requests - <a href="https://github.com/BerriAI/litellm/pull/14007" target="_blank" rel="noopener noreferrer">PR #14007</a></li>
</ul>
</li>
<li><strong><a href="/docs/response_api">Responses API</a></strong>
<ul>
<li>Fixed <code>/responses</code> endpoint ignoring extra_headers in GitHub Copilot - <a href="https://github.com/BerriAI/litellm/pull/13775" target="_blank" rel="noopener noreferrer">PR #13775</a></li>
<li>Added support for new web_search tool - <a href="https://github.com/BerriAI/litellm/pull/14083" target="_blank" rel="noopener noreferrer">PR #14083</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure/azure">Azure Passthrough</a></strong>
<ul>
<li>Fixed Azure Passthrough request with streaming - <a href="https://github.com/BerriAI/litellm/pull/13831" target="_blank" rel="noopener noreferrer">PR #13831</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>General</strong>
<ul>
<li>Fixed handling of None metadata in batch requests - <a href="https://github.com/BerriAI/litellm/pull/13996" target="_blank" rel="noopener noreferrer">PR #13996</a></li>
<li>Fixed token_counter with special token input - <a href="https://github.com/BerriAI/litellm/pull/13374" target="_blank" rel="noopener noreferrer">PR #13374</a></li>
<li>Removed incorrect web search support for azure/gpt-4.1 family - <a href="https://github.com/BerriAI/litellm/pull/13566" target="_blank" rel="noopener noreferrer">PR #13566</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway"><a href="/docs/mcp">MCP Gateway</a><a href="#mcp-gateway" class="hash-link" aria-label="mcp-gateway" title="mcp-gateway"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>SSE MCP Tools</strong>
<ul>
<li>Bug fix for adding SSE MCP tools - improved connection testing when adding MCPs - <a href="https://github.com/BerriAI/litellm/pull/14048" target="_blank" rel="noopener noreferrer">PR #14048</a></li>
</ul>
</li>
</ul>
<p><a href="/docs/mcp">Read More</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Team Management</strong>
<ul>
<li>Allow setting Team Member RPM/TPM limits when creating a team - <a href="https://github.com/BerriAI/litellm/pull/13943" target="_blank" rel="noopener noreferrer">PR #13943</a></li>
</ul>
</li>
<li><strong>UI Improvements</strong>
<ul>
<li>Fixed Next.js Security Vulnerabilities in UI Dashboard - <a href="https://github.com/BerriAI/litellm/pull/14084" target="_blank" rel="noopener noreferrer">PR #14084</a></li>
<li>Fixed collapsible navbar design - <a href="https://github.com/BerriAI/litellm/pull/14075" target="_blank" rel="noopener noreferrer">PR #14075</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Authentication</strong>
<ul>
<li>Fixed Virtual keys with llm_api type causing Internal Server Error for /anthropic/* and other LLM passthrough routes - <a href="https://github.com/BerriAI/litellm/pull/14046" target="_blank" rel="noopener noreferrer">PR #14046</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-4">Features<a href="#features-4" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/logging#langfuse">Langfuse OTEL</a></strong>
<ul>
<li>Allow using LANGFUSE_OTEL_HOST for configuring host - <a href="https://github.com/BerriAI/litellm/pull/14013" target="_blank" rel="noopener noreferrer">PR #14013</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#braintrust">Braintrust</a></strong>
<ul>
<li>Added span name metadata feature - <a href="https://github.com/BerriAI/litellm/pull/13573" target="_blank" rel="noopener noreferrer">PR #13573</a></li>
<li>Fixed tests to reference moved attributes in <code>braintrust_logging</code> module - <a href="https://github.com/BerriAI/litellm/pull/13978" target="_blank" rel="noopener noreferrer">PR #13978</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#openmeter">OpenMeter</a></strong>
<ul>
<li>Set user from token user_id for OpenMeter integration - <a href="https://github.com/BerriAI/litellm/pull/13152" target="_blank" rel="noopener noreferrer">PR #13152</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-guardrail-support">New Guardrail Support<a href="#new-guardrail-support" class="hash-link" aria-label="New Guardrail Support" title="New Guardrail Support"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/guardrails">Noma Security</a></strong>
<ul>
<li>Added Noma Security guardrail support - <a href="https://github.com/BerriAI/litellm/pull/13572" target="_blank" rel="noopener noreferrer">PR #13572</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails">Pangea</a></strong>
<ul>
<li>Updated Pangea Guardrail to support new AIDR endpoint - <a href="https://github.com/BerriAI/litellm/pull/13160" target="_blank" rel="noopener noreferrer">PR #13160</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-5">Features<a href="#features-5" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Caching</strong>
<ul>
<li>Verify if cache entry has expired prior to serving it to client - <a href="https://github.com/BerriAI/litellm/pull/13933" target="_blank" rel="noopener noreferrer">PR #13933</a></li>
<li>Fixed error saving latency as timedelta on Redis - <a href="https://github.com/BerriAI/litellm/pull/14040" target="_blank" rel="noopener noreferrer">PR #14040</a></li>
</ul>
</li>
<li><strong>Router</strong>
<ul>
<li>Refactored router to choose weights by &#x27;weight&#x27;, &#x27;rpm&#x27;, &#x27;tpm&#x27; in one loop for simple_shuffle - <a href="https://github.com/BerriAI/litellm/pull/13562" target="_blank" rel="noopener noreferrer">PR #13562</a></li>
</ul>
</li>
<li><strong>Logging</strong>
<ul>
<li>Fixed LoggingWorker graceful shutdown to prevent CancelledError warnings - <a href="https://github.com/BerriAI/litellm/pull/14050" target="_blank" rel="noopener noreferrer">PR #14050</a></li>
<li>Enhanced logging for containers to log on files both with usual format and json format - <a href="https://github.com/BerriAI/litellm/pull/13394" target="_blank" rel="noopener noreferrer">PR #13394</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-2">Bugs<a href="#bugs-2" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Dependencies</strong>
<ul>
<li>Bumped <code>orjson</code> version to &quot;3.11.2&quot; - <a href="https://github.com/BerriAI/litellm/pull/13969" target="_blank" rel="noopener noreferrer">PR #13969</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-6">Features<a href="#features-6" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>AWS</strong>
<ul>
<li>Add support for AWS assume_role with a session token - <a href="https://github.com/BerriAI/litellm/pull/13919" target="_blank" rel="noopener noreferrer">PR #13919</a></li>
</ul>
</li>
<li><strong>OCI Provider</strong>
<ul>
<li>Added oci_key_file as an optional_parameter - <a href="https://github.com/BerriAI/litellm/pull/14036" target="_blank" rel="noopener noreferrer">PR #14036</a></li>
</ul>
</li>
<li><strong>Configuration</strong>
<ul>
<li>Allow configuration to set threshold before request entry in spend log gets truncated - <a href="https://github.com/BerriAI/litellm/pull/14042" target="_blank" rel="noopener noreferrer">PR #14042</a></li>
<li>Enhanced proxy_config configuration: add support for existing configmap in Helm charts - <a href="https://github.com/BerriAI/litellm/pull/14041" target="_blank" rel="noopener noreferrer">PR #14041</a></li>
</ul>
</li>
<li><strong>Docker</strong>
<ul>
<li>Added back supervisor to non-root image - <a href="https://github.com/BerriAI/litellm/pull/13922" target="_blank" rel="noopener noreferrer">PR #13922</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@ArthurRenault made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13922" target="_blank" rel="noopener noreferrer">PR #13922</a></li>
<li>@stevenmanton made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13919" target="_blank" rel="noopener noreferrer">PR #13919</a></li>
<li>@uc4w6c made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13914" target="_blank" rel="noopener noreferrer">PR #13914</a></li>
<li>@nielsbosma made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13573" target="_blank" rel="noopener noreferrer">PR #13573</a></li>
<li>@Yuki-Imajuku made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13567" target="_blank" rel="noopener noreferrer">PR #13567</a></li>
<li>@codeflash-ai[bot] made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13988" target="_blank" rel="noopener noreferrer">PR #13988</a></li>
<li>@ColeFrench made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13978" target="_blank" rel="noopener noreferrer">PR #13978</a></li>
<li>@dttran-glo made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13969" target="_blank" rel="noopener noreferrer">PR #13969</a></li>
<li>@manascb1344 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13965" target="_blank" rel="noopener noreferrer">PR #13965</a></li>
<li>@DorZion made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13572" target="_blank" rel="noopener noreferrer">PR #13572</a></li>
<li>@edwardsamuel made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13536" target="_blank" rel="noopener noreferrer">PR #13536</a></li>
<li>@blahgeek made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13374" target="_blank" rel="noopener noreferrer">PR #13374</a></li>
<li>@Deviad made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13394" target="_blank" rel="noopener noreferrer">PR #13394</a></li>
<li>@XSAM made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13775" target="_blank" rel="noopener noreferrer">PR #13775</a></li>
<li>@KRRT7 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14012" target="_blank" rel="noopener noreferrer">PR #14012</a></li>
<li>@ikaadil made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13991" target="_blank" rel="noopener noreferrer">PR #13991</a></li>
<li>@timelfrink made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13691" target="_blank" rel="noopener noreferrer">PR #13691</a></li>
<li>@qidu made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13562" target="_blank" rel="noopener noreferrer">PR #13562</a></li>
<li>@nagyv made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13243" target="_blank" rel="noopener noreferrer">PR #13243</a></li>
<li>@xywei made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12885" target="_blank" rel="noopener noreferrer">PR #12885</a></li>
<li>@ericgtkb made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12797" target="_blank" rel="noopener noreferrer">PR #12797</a></li>
<li>@NoWall57 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13945" target="_blank" rel="noopener noreferrer">PR #13945</a></li>
<li>@lmwang9527 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14050" target="_blank" rel="noopener noreferrer">PR #14050</a></li>
<li>@WilsonSunBritten made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14042" target="_blank" rel="noopener noreferrer">PR #14042</a></li>
<li>@Const-antine made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14041" target="_blank" rel="noopener noreferrer">PR #14041</a></li>
<li>@dmvieira made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14040" target="_blank" rel="noopener noreferrer">PR #14040</a></li>
<li>@gotsysdba made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14036" target="_blank" rel="noopener noreferrer">PR #14036</a></li>
<li>@moshemorad made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/14005" target="_blank" rel="noopener noreferrer">PR #14005</a></li>
<li>@joshualipman123 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13144" target="_blank" rel="noopener noreferrer">PR #13144</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog"><strong><a href="https://github.com/BerriAI/litellm/compare/v1.76.0-nightly...v1.76.1" target="_blank" rel="noopener noreferrer">Full Changelog</a></strong><a href="#full-changelog" class="hash-link" aria-label="full-changelog" title="full-changelog"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-76-0">v1.76.0-stable - RPS Improvements</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-08-23T10:00:00.000Z">2025823</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span></div><div class="admonitionContent_BuS1"><p>LiteLLM is hiring a <strong>Founding Backend Engineer</strong>, in San Francisco.</p><p><a href="https://www.ycombinator.com/companies/litellm/jobs/6uvoBp3-founding-backend-engineer" target="_blank" rel="noopener noreferrer">Apply here</a> if you&#x27;re interested!</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span></div><div class="admonitionContent_BuS1"><p>This release is not live yet.</p></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/providers/openai">OpenAI</a></strong>
<ul>
<li>Gpt-5 chat: clarify does not support function calling <a href="https://github.com/BerriAI/litellm/pull/13612" target="_blank" rel="noopener noreferrer">PR #13612</a>, s/o @<a href="https://github.com/superpoussin22" target="_blank" rel="noopener noreferrer">superpoussin22</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex">VertexAI</a></strong>
<ul>
<li>fix vertexai batch file format by@<a href="https://github.com/thiagosalvatore" target="_blank" rel="noopener noreferrer">thiagosalvatore</a>in<a href="https://github.com/BerriAI/litellm/pull/13576" target="_blank" rel="noopener noreferrer">PR #13576</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/litellm_proxy">LiteLLM Proxy</a></strong>
<ul>
<li>Add support for calling image_edits + image_generations via SDK to Proxy - <a href="https://github.com/BerriAI/litellm/pull/13735" target="_blank" rel="noopener noreferrer">PR #13735</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openrouter">OpenRouter</a></strong>
<ul>
<li>Fix max_output_tokens value for anthropic Claude 4 - <a href="https://github.com/BerriAI/litellm/pull/13526" target="_blank" rel="noopener noreferrer">PR #13526</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/gemini">Gemini</a></strong>
<ul>
<li>Fix prompt caching cost calculation - <a href="https://github.com/BerriAI/litellm/pull/13742" target="_blank" rel="noopener noreferrer">PR #13742</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure">Azure</a></strong>
<ul>
<li>Support <code>../openai/v1/respones</code> api base - <a href="https://github.com/BerriAI/litellm/pull/13526" target="_blank" rel="noopener noreferrer">PR #13526</a></li>
<li>Fix azure/gpt-5-chat max_input_tokens - <a href="https://github.com/BerriAI/litellm/pull/13660" target="_blank" rel="noopener noreferrer">PR #13660</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/groq">Groq</a></strong>
<ul>
<li>streaming ASCII encoding issue - <a href="https://github.com/BerriAI/litellm/pull/13675" target="_blank" rel="noopener noreferrer">PR #13675</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/baseten">Baseten</a></strong>
<ul>
<li>Refactored integration to use new openai-compatible endpoints - <a href="https://github.com/BerriAI/litellm/pull/13783" target="_blank" rel="noopener noreferrer">PR #13783</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>
<ul>
<li>fix application inference profile for pass-through endpoints for bedrock - <a href="https://github.com/BerriAI/litellm/pull/13881" target="_blank" rel="noopener noreferrer">PR #13881</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/datarobot">DataRobot</a></strong>
<ul>
<li>Updated URL handling for DataRobot provider URL - <a href="https://github.com/BerriAI/litellm/pull/13880" target="_blank" rel="noopener noreferrer">PR #13880</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/together">Together AI</a></strong>
<ul>
<li>Added Qwen3, Deepseek R1 0528 Throughput, GLM 4.5 and GPT-OSS models cost tracking - <a href="https://github.com/BerriAI/litellm/pull/13637" target="_blank" rel="noopener noreferrer">PR #13637</a>, s/o @<a href="https://github.com/Tasmay-Tibrewal" target="_blank" rel="noopener noreferrer">Tasmay-Tibrewal</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/fireworks_ai">Fireworks AI</a></strong>
<ul>
<li>add fireworks_ai/accounts/fireworks/models/deepseek-v3-0324 - <a href="https://github.com/BerriAI/litellm/pull/13821" target="_blank" rel="noopener noreferrer">PR #13821</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex">VertexAI</a></strong>
<ul>
<li>Add VertexAI qwen API Service - <a href="https://github.com/BerriAI/litellm/pull/13828" target="_blank" rel="noopener noreferrer">PR #13828</a></li>
<li>Add new VertexAI image modelsvertex_ai/imagen-4.0-generate-001,vertex_ai/imagen-4.0-ultra-generate-001,vertex_ai/imagen-4.0-fast-generate-001 - <a href="https://github.com/BerriAI/litellm/pull/13874" target="_blank" rel="noopener noreferrer">PR #13874</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Add long context support w/ cost tracking - <a href="https://github.com/BerriAI/litellm/pull/13759" target="_blank" rel="noopener noreferrer">PR #13759</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/deepinfra">DeepInfra</a></strong>
<ul>
<li>Add rerank endpoint support for deepinfra - <a href="https://github.com/BerriAI/litellm/pull/13820" target="_blank" rel="noopener noreferrer">PR #13820</a></li>
<li>Add new models for cost tracking - <a href="https://github.com/BerriAI/litellm/pull/13883" target="_blank" rel="noopener noreferrer">PR #13883</a>, s/o @<a href="https://github.com/Toy-97" target="_blank" rel="noopener noreferrer">Toy-97</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>
<ul>
<li>Add tool prompt caching on async calls - <a href="https://github.com/BerriAI/litellm/pull/13803" target="_blank" rel="noopener noreferrer">PR #13803</a>, s/o @<a href="https://github.com/UlookEE" target="_blank" rel="noopener noreferrer">UlookEE</a></li>
<li>role chaining and session name with webauthentication for aws bedrock - <a href="https://github.com/BerriAI/litellm/pull/13753" target="_blank" rel="noopener noreferrer">PR #13753</a>, s/o @<a href="https://github.com/RichardoC" target="_blank" rel="noopener noreferrer">RichardoC</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/ollama">Ollama</a></strong>
<ul>
<li>Handle Ollama null response when using tool calling with non-tool trained models - <a href="https://github.com/BerriAI/litellm/pull/13902" target="_blank" rel="noopener noreferrer">PR #13902</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openrouter">OpenRouter</a></strong>
<ul>
<li>Add deepseek/deepseek-chat-v3.1 support - <a href="https://github.com/BerriAI/litellm/pull/13897" target="_blank" rel="noopener noreferrer">PR #13897</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/mistral">Mistral</a></strong>
<ul>
<li>Add support for calling mistral files via chat completions - <a href="https://github.com/BerriAI/litellm/pull/13866" target="_blank" rel="noopener noreferrer">PR #13866</a>, s/o @<a href="https://github.com/jinskjoy" target="_blank" rel="noopener noreferrer">jinskjoy</a></li>
<li>Handle empty assistant content - <a href="https://github.com/BerriAI/litellm/pull/13671" target="_blank" rel="noopener noreferrer">PR #13671</a></li>
<li>Support new thinking response block - <a href="https://github.com/BerriAI/litellm/pull/13671" target="_blank" rel="noopener noreferrer">PR #13671</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/databricks">Databricks</a></strong>
<ul>
<li>remove deprecated dbrx models (dbrx-instruct, llama 3.1) - <a href="https://github.com/BerriAI/litellm/pull/13843" target="_blank" rel="noopener noreferrer">PR #13843</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/ai_ml_api">AI/ML API</a></strong>
<ul>
<li>Image gen api support - <a href="https://github.com/BerriAI/litellm/pull/13893" target="_blank" rel="noopener noreferrer">PR #13893</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/response_api">Responses API</a></strong>
<ul>
<li>add default api version for openai responses api calls - <a href="https://github.com/BerriAI/litellm/pull/13526" target="_blank" rel="noopener noreferrer">PR #13526</a></li>
<li>supportallowed_openai_params - <a href="https://github.com/BerriAI/litellm/pull/13671" target="_blank" rel="noopener noreferrer">PR #13671</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway">MCP Gateway<a href="#mcp-gateway" class="hash-link" aria-label="MCP Gateway" title="MCP Gateway"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-2">Bugs<a href="#bugs-2" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li>fix StreamableHTTPSessionManager .run() error - <a href="https://github.com/BerriAI/litellm/pull/13666" target="_blank" rel="noopener noreferrer">PR #13666</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="vector-stores">Vector Stores<a href="#vector-stores" class="hash-link" aria-label="Vector Stores" title="Vector Stores"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-3">Bugs<a href="#bugs-3" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>
<ul>
<li>Using LiteLLM Managed Credentials for Query - <a href="https://github.com/BerriAI/litellm/pull/13787" target="_blank" rel="noopener noreferrer">PR #13787</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-4">Bugs<a href="#bugs-4" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/pass_through/intro">Passthrough</a></strong>
<ul>
<li>Fix query passthrough deletion - <a href="https://github.com/BerriAI/litellm/pull/13622" target="_blank" rel="noopener noreferrer">PR #13622</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Models</strong>
<ul>
<li>Add Search Functionality for Public Model Names in Model Dashboard - <a href="https://github.com/BerriAI/litellm/pull/13687" target="_blank" rel="noopener noreferrer">PR #13687</a></li>
<li>Auto-Add <code>azure/</code> to deployment Name in UI - <a href="https://github.com/BerriAI/litellm/pull/13685" target="_blank" rel="noopener noreferrer">PR #13685</a></li>
<li>Models page row UI restructure - <a href="https://github.com/BerriAI/litellm/pull/13771" target="_blank" rel="noopener noreferrer">PR #13771</a></li>
</ul>
</li>
<li><strong>Notifications</strong>
<ul>
<li>Add new notifications toast UI everywhere - <a href="https://github.com/BerriAI/litellm/pull/13813" target="_blank" rel="noopener noreferrer">PR #13813</a></li>
</ul>
</li>
<li><strong>Keys</strong>
<ul>
<li>Fix key edit settings after regenerating a key - <a href="https://github.com/BerriAI/litellm/pull/13815" target="_blank" rel="noopener noreferrer">PR #13815</a></li>
<li>Require team_id when creating service account keys - <a href="https://github.com/BerriAI/litellm/pull/13873" target="_blank" rel="noopener noreferrer">PR #13873</a></li>
<li>Filter - show all options on filter option click - <a href="https://github.com/BerriAI/litellm/pull/13858" target="_blank" rel="noopener noreferrer">PR #13858</a></li>
</ul>
</li>
<li><strong>Usage</strong>
<ul>
<li>Fix Cannot read properties of undefined exception on user agent activity tab - <a href="https://github.com/BerriAI/litellm/pull/13892" target="_blank" rel="noopener noreferrer">PR #13892</a></li>
</ul>
</li>
<li><strong>SSO</strong>
<ul>
<li>Free SSO usage for up to 5 users - <a href="https://github.com/BerriAI/litellm/pull/13843" target="_blank" rel="noopener noreferrer">PR #13843</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-5">Bugs<a href="#bugs-5" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/guardrails/bedrock">Bedrock Guardrails</a></strong>
<ul>
<li>Add bedrock api key support - <a href="https://github.com/BerriAI/litellm/pull/13835" target="_blank" rel="noopener noreferrer">PR #13835</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/integrations/datadog">Datadog LLM Observability</a></strong>
<ul>
<li>Add support for Failure Logging<a href="https://github.com/BerriAI/litellm/pull/13726" target="_blank" rel="noopener noreferrer">PR #13726</a></li>
<li>Add time to first token, litellm overhead, guardrail overhead latency metrics - <a href="https://github.com/BerriAI/litellm/pull/13734" target="_blank" rel="noopener noreferrer">PR #13734</a></li>
<li>Add support for tracing guardrail input/output - <a href="https://github.com/BerriAI/litellm/pull/13767" target="_blank" rel="noopener noreferrer">PR #13767</a></li>
</ul>
</li>
<li><strong><a href="/docs/integrations/langfuse">Langfuse OTEL</a></strong>
<ul>
<li>Allow using Key/Team Based Logging - <a href="https://github.com/BerriAI/litellm/pull/13791" target="_blank" rel="noopener noreferrer">PR #13791</a></li>
</ul>
</li>
<li><strong><a href="/docs/integrations/aim">AIM</a></strong>
<ul>
<li>Migrate to new firewall API - <a href="https://github.com/BerriAI/litellm/pull/13748" target="_blank" rel="noopener noreferrer">PR #13748</a></li>
</ul>
</li>
<li><strong><a href="/docs/observability/opentelemetry_integration">OTEL</a></strong>
<ul>
<li>Add OTEL tracing for actual LLM API call - <a href="https://github.com/BerriAI/litellm/pull/13836" target="_blank" rel="noopener noreferrer">PR #13836</a></li>
</ul>
</li>
<li><strong><a href="/docs/observability/mlflow_integration">MLFlow</a></strong>
<ul>
<li>Include predicted output in MLflow tracing - <a href="https://github.com/BerriAI/litellm/pull/13795" target="_blank" rel="noopener noreferrer">PR #13795</a>, s/o@TomeHirata</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-6">Bugs<a href="#bugs-6" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/routing#how-cooldowns-work">Cooldowns</a></strong>
<ul>
<li>don&#x27;t return raw Azure Exceptions to client (can contain prompt leakage) - <a href="https://github.com/BerriAI/litellm/pull/13529" target="_blank" rel="noopener noreferrer">PR #13529</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/auto_routing">Auto-router</a></strong>
<ul>
<li>Ensures the relevant dependencies for auto router existing on LiteLLM Docker - <a href="https://github.com/BerriAI/litellm/pull/13788" target="_blank" rel="noopener noreferrer">PR #13788</a></li>
</ul>
</li>
<li><strong>Model Alias</strong>
<ul>
<li>Fix calling key with access to model alias - <a href="https://github.com/BerriAI/litellm/pull/13830" target="_blank" rel="noopener noreferrer">PR #13830</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/caching">S3 Caching</a></strong>
<ul>
<li>Use namespace as prefix for s3 cache - <a href="https://github.com/BerriAI/litellm/pull/13704" target="_blank" rel="noopener noreferrer">PR #13704</a></li>
<li>Async S3 Caching support (4x RPS improvement) - <a href="https://github.com/BerriAI/litellm/pull/13852" target="_blank" rel="noopener noreferrer">PR #13852</a>, s/o @<a href="https://github.com/michal-otmianowski" target="_blank" rel="noopener noreferrer">michal-otmianowski</a></li>
</ul>
</li>
<li><strong>Model Group header forwarding</strong>
<ul>
<li>reuse same logic as global header forwarding - <a href="https://github.com/BerriAI/litellm/pull/13741" target="_blank" rel="noopener noreferrer">PR #13741</a></li>
<li>add support for hosted_vllm on UI - <a href="https://github.com/BerriAI/litellm/pull/13885" target="_blank" rel="noopener noreferrer">PR #13885</a></li>
</ul>
</li>
<li><strong>Performance</strong>
<ul>
<li>Improve LiteLLM Python SDK RPS by +200 RPS (braintrust import + aiohttp transport fixes) - <a href="https://github.com/BerriAI/litellm/pull/13839" target="_blank" rel="noopener noreferrer">PR #13839</a></li>
<li>Use O(1) Set lookups for model routing - <a href="https://github.com/BerriAI/litellm/pull/13879" target="_blank" rel="noopener noreferrer">PR #13879</a></li>
<li>Reduce Significant CPU overhead from litellm_logging.py - <a href="https://github.com/BerriAI/litellm/pull/13895" target="_blank" rel="noopener noreferrer">PR #13895</a></li>
<li>Improvements for Async Success Handler (Logging Callbacks) - Approx +130 RPS - <a href="https://github.com/BerriAI/litellm/pull/13905" target="_blank" rel="noopener noreferrer">PR #13905</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-7">Bugs<a href="#bugs-7" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>SDK</strong>
<ul>
<li>Fix litellm compatibility with newest release of openAI (&gt;v1.100.0) - <a href="https://github.com/BerriAI/litellm/pull/13728" target="_blank" rel="noopener noreferrer">PR #13728</a></li>
</ul>
</li>
<li><strong>Helm</strong>
<ul>
<li>Add possibility to configure resources for migrations-job - <a href="https://github.com/BerriAI/litellm/pull/13617" target="_blank" rel="noopener noreferrer">PR #13617</a></li>
<li>Ensure Helm chart auto generated master keys follow sk-xxxx format - <a href="https://github.com/BerriAI/litellm/pull/13871" target="_blank" rel="noopener noreferrer">PR #13871</a></li>
<li>Enhance database configuration: add support for optional endpointKey - <a href="https://github.com/BerriAI/litellm/pull/13763" target="_blank" rel="noopener noreferrer">PR #13763</a></li>
</ul>
</li>
<li><strong>Rate Limits</strong>
<ul>
<li>fixing descriptor/response size mismatch on parallel_request_limiter_v3 - <a href="https://github.com/BerriAI/litellm/pull/13863" target="_blank" rel="noopener noreferrer">PR #13863</a>, s/o @<a href="https://github.com/luizrennocosta" target="_blank" rel="noopener noreferrer">luizrennocosta</a></li>
</ul>
</li>
<li><strong>Non-root</strong>
<ul>
<li>fix permission access on prisma migrate in non-root image - <a href="https://github.com/BerriAI/litellm/pull/13848" target="_blank" rel="noopener noreferrer">PR #13848</a>, s/o @<a href="https://github.com/Ithanil" target="_blank" rel="noopener noreferrer">Ithanil</a></li>
</ul>
</li>
</ul></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-75-8">v1.75.8-stable - Team Member Rate Limits</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-08-16T10:00:00.000Z">2025816</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.75.8-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.75.8</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>Team Member Rate Limits</strong> - Individual rate limiting for team members with JWT authentication support.</li>
<li><strong>Performance Improvements</strong> - New experimental HTTP handler flag for 100+ RPS improvement on OpenAI calls.</li>
<li><strong>GPT-5 Model Family Support</strong> - Full support for OpenAI&#x27;s GPT-5 models with <code>reasoning_effort</code> parameter and Azure OpenAI integration.</li>
<li><strong>Azure AI Flux Image Generation</strong> - Support for Azure AI&#x27;s Flux image generation models.</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="team-member-rate-limits">Team Member Rate Limits<a href="#team-member-rate-limits" class="hash-link" aria-label="Team Member Rate Limits" title="Team Member Rate Limits"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAaElEQVR4nFWNQQoFIQxDvf8h3QiuR0RoazSfKH9gCo+2NE1SrZU5Z5ZSqLn3zoigu39IYwy21i5Po5kTAJdY6yXpe85JYNLson3vTZW6OEJZq9/bfoXAoruMcIV/Vzc/0R5x4iU8N4A/qJnDTgdDYzwAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/team_member_rate_limits.2820c45.640.png" srcset="/assets/ideal-img/team_member_rate_limits.2820c45.640.png 640w,/assets/ideal-img/team_member_rate_limits.ae9e3aa.1920.png 1920w" width="640" height="334"></noscript></div>
<p style="text-align:left;color:#666"></p><p>LiteLLM MCP Architecture: Use MCP tools with all LiteLLM supported models</p><p></p>
<p>This release adds support for setting rate limits on individual members (including machine users) within a team. Teams can now give each agent its own rate limitsso that heavy-traffic agents dont impact other agents or human users.</p>
<p>Agents can authenticate with LiteLLM using JWT and the same team role as human users, while still enforcing per-agent rate limits.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-model-support">New Model Support<a href="#new-model-support" class="hash-link" aria-label="New Model Support" title="New Model Support"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Features</th></tr></thead><tbody><tr><td>Azure AI</td><td><code>azure_ai/FLUX-1.1-pro</code></td><td>-</td><td>-</td><td>$40/image</td><td>Image generation</td></tr><tr><td>Azure AI</td><td><code>azure_ai/FLUX.1-Kontext-pro</code></td><td>-</td><td>-</td><td>$40/image</td><td>Image generation</td></tr><tr><td>Vertex AI</td><td><code>vertex_ai/deepseek-ai/deepseek-r1-0528-maas</code></td><td>65k</td><td>$1.35</td><td>$5.4</td><td>Chat completions + reasoning</td></tr><tr><td>OpenRouter</td><td><code>openrouter/deepseek/deepseek-chat-v3-0324</code></td><td>65k</td><td>$0.14</td><td>$0.28</td><td>Chat completions</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/openai">OpenAI</a></strong>
<ul>
<li>Added <code>reasoning_effort</code> parameter support for GPT-5 model family - <a href="https://github.com/BerriAI/litellm/pull/13475" target="_blank" rel="noopener noreferrer">PR #13475</a>, <a href="/docs/providers/openai#openai-chat-completion-models">Get Started</a></li>
<li>Support for <code>reasoning</code> parameter in Responses API - <a href="https://github.com/BerriAI/litellm/pull/13475" target="_blank" rel="noopener noreferrer">PR #13475</a>, <a href="/docs/response_api">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure/azure">Azure OpenAI</a></strong>
<ul>
<li>GPT-5 support with max_tokens and <code>reasoning</code> parameter - <a href="https://github.com/BerriAI/litellm/pull/13510" target="_blank" rel="noopener noreferrer">PR #13510</a>, <a href="/docs/providers/azure/azure#gpt-5-models">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">AWS Bedrock</a></strong>
<ul>
<li>Streaming support for bedrock gpt-oss model family - <a href="https://github.com/BerriAI/litellm/pull/13346" target="_blank" rel="noopener noreferrer">PR #13346</a>, <a href="/docs/providers/bedrock#openai-gpt-oss">Get Started</a></li>
<li><code>/messages</code> endpoint compatibility with <code>bedrock/converse/&lt;model&gt;</code> - <a href="https://github.com/BerriAI/litellm/pull/13627" target="_blank" rel="noopener noreferrer">PR #13627</a></li>
<li>Cache point support for assistant and tool messages - <a href="https://github.com/BerriAI/litellm/pull/13640" target="_blank" rel="noopener noreferrer">PR #13640</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure">Azure AI</a></strong>
<ul>
<li>New Azure AI Flux Image Generation provider - <a href="https://github.com/BerriAI/litellm/pull/13592" target="_blank" rel="noopener noreferrer">PR #13592</a>, <a href="/docs/providers/azure_ai_img">Get Started</a></li>
<li>Fixed Content-Type header for image generation - <a href="https://github.com/BerriAI/litellm/pull/13584" target="_blank" rel="noopener noreferrer">PR #13584</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/comet">CometAPI</a></strong>
<ul>
<li>New provider support with chat completions and streaming - <a href="https://github.com/BerriAI/litellm/pull/13458" target="_blank" rel="noopener noreferrer">PR #13458</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/sambanova">SambaNova</a></strong>
<ul>
<li>Added embedding model support - <a href="https://github.com/BerriAI/litellm/pull/13308" target="_blank" rel="noopener noreferrer">PR #13308</a>, <a href="/docs/providers/sambanova#sambanova---embeddings">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex">Vertex AI</a></strong>
<ul>
<li>Added <code>/countTokens</code> endpoint support for Gemini CLI integration - <a href="https://github.com/BerriAI/litellm/pull/13545" target="_blank" rel="noopener noreferrer">PR #13545</a></li>
<li>Token counter support for VertexAI models - <a href="https://github.com/BerriAI/litellm/pull/13558" target="_blank" rel="noopener noreferrer">PR #13558</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vllm">hosted_vllm</a></strong>
<ul>
<li>Added <code>reasoning_effort</code> parameter support - <a href="https://github.com/BerriAI/litellm/pull/13620" target="_blank" rel="noopener noreferrer">PR #13620</a>, <a href="/docs/providers/vllm#reasoning-effort">Get Started</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/providers/oci">OCI</a></strong>
<ul>
<li>Fixed streaming issues - <a href="https://github.com/BerriAI/litellm/pull/13437" target="_blank" rel="noopener noreferrer">PR #13437</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/ollama">Ollama</a></strong>
<ul>
<li>Fixed GPT-OSS streaming with &#x27;thinking&#x27; field - <a href="https://github.com/BerriAI/litellm/pull/13375" target="_blank" rel="noopener noreferrer">PR #13375</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/volcengine">VolcEngine</a></strong>
<ul>
<li>Fixed thinking disabled parameter handling - <a href="https://github.com/BerriAI/litellm/pull/13598" target="_blank" rel="noopener noreferrer">PR #13598</a></li>
</ul>
</li>
<li><strong><a href="/docs/completion/stream">Streaming</a></strong>
<ul>
<li>Consistent &#x27;finish_reason&#x27; chunk indexing - <a href="https://github.com/BerriAI/litellm/pull/13560" target="_blank" rel="noopener noreferrer">PR #13560</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/anthropic/messages">/messages</a></strong>
<ul>
<li>Tool use arguments properly returned for non-anthropic models - <a href="https://github.com/BerriAI/litellm/pull/13638" target="_blank" rel="noopener noreferrer">PR #13638</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/realtime">Real-time API</a></strong>
<ul>
<li>Fixed endpoint for no intent scenarios - <a href="https://github.com/BerriAI/litellm/pull/13476" target="_blank" rel="noopener noreferrer">PR #13476</a></li>
</ul>
</li>
<li><strong><a href="/docs/response_api">Responses API</a></strong>
<ul>
<li>Fixed <code>stream=True</code> + <code>background=True</code> with Responses API - <a href="https://github.com/BerriAI/litellm/pull/13654" target="_blank" rel="noopener noreferrer">PR #13654</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway"><a href="/docs/mcp">MCP Gateway</a><a href="#mcp-gateway" class="hash-link" aria-label="mcp-gateway" title="mcp-gateway"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Access Control &amp; Configuration</strong>
<ul>
<li>Enhanced MCPServerManager with access groups and description support - <a href="https://github.com/BerriAI/litellm/pull/13549" target="_blank" rel="noopener noreferrer">PR #13549</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-2">Bugs<a href="#bugs-2" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Authentication</strong>
<ul>
<li>Fixed MCP gateway key authentication - <a href="https://github.com/BerriAI/litellm/pull/13630" target="_blank" rel="noopener noreferrer">PR #13630</a></li>
</ul>
</li>
</ul>
<p><a href="/docs/mcp">Read More</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Team Management</strong>
<ul>
<li>Team Member Rate Limits implementation - <a href="https://github.com/BerriAI/litellm/pull/13601" target="_blank" rel="noopener noreferrer">PR #13601</a></li>
<li>JWT authentication support for team member rate limits - <a href="https://github.com/BerriAI/litellm/pull/13601" target="_blank" rel="noopener noreferrer">PR #13601</a></li>
<li>Show team member TPM/RPM limits in UI - <a href="https://github.com/BerriAI/litellm/pull/13662" target="_blank" rel="noopener noreferrer">PR #13662</a></li>
<li>Allow editing team member RPM/TPM limits - <a href="https://github.com/BerriAI/litellm/pull/13669" target="_blank" rel="noopener noreferrer">PR #13669</a></li>
<li>Allow unsetting TPM and RPM in Teams Settings - <a href="https://github.com/BerriAI/litellm/pull/13430" target="_blank" rel="noopener noreferrer">PR #13430</a></li>
<li>Team Member Permissions Page access column changes - <a href="https://github.com/BerriAI/litellm/pull/13145" target="_blank" rel="noopener noreferrer">PR #13145</a></li>
</ul>
</li>
<li><strong>Key Management</strong>
<ul>
<li>Display errors from backend on the UI Keys page - <a href="https://github.com/BerriAI/litellm/pull/13435" target="_blank" rel="noopener noreferrer">PR #13435</a></li>
<li>Added confirmation modal before deleting keys - <a href="https://github.com/BerriAI/litellm/pull/13655" target="_blank" rel="noopener noreferrer">PR #13655</a></li>
<li>Support for <code>user</code> parameter in LiteLLM SDK to Proxy communication - <a href="https://github.com/BerriAI/litellm/pull/13555" target="_blank" rel="noopener noreferrer">PR #13555</a></li>
</ul>
</li>
<li><strong>UI Improvements</strong>
<ul>
<li>Fixed internal users table overflow - <a href="https://github.com/BerriAI/litellm/pull/12736" target="_blank" rel="noopener noreferrer">PR #12736</a></li>
<li>Enhanced chart readability with short-form notation for large numbers - <a href="https://github.com/BerriAI/litellm/pull/12370" target="_blank" rel="noopener noreferrer">PR #12370</a></li>
<li>Fixed image overflow in LiteLLM model display - <a href="https://github.com/BerriAI/litellm/pull/13639" target="_blank" rel="noopener noreferrer">PR #13639</a></li>
<li>Removed ambiguous network response errors - <a href="https://github.com/BerriAI/litellm/pull/13582" target="_blank" rel="noopener noreferrer">PR #13582</a></li>
</ul>
</li>
<li><strong>Credentials</strong>
<ul>
<li>Added CredentialDeleteModal component and integration with CredentialsPanel - <a href="https://github.com/BerriAI/litellm/pull/13550" target="_blank" rel="noopener noreferrer">PR #13550</a></li>
</ul>
</li>
<li><strong>Admin &amp; Permissions</strong>
<ul>
<li>Allow routes for admin viewer - <a href="https://github.com/BerriAI/litellm/pull/13588" target="_blank" rel="noopener noreferrer">PR #13588</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-3">Bugs<a href="#bugs-3" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>SCIM Integration</strong>
<ul>
<li>Fixed SCIM Team Memberships metadata handling - <a href="https://github.com/BerriAI/litellm/pull/13553" target="_blank" rel="noopener noreferrer">PR #13553</a></li>
</ul>
</li>
<li><strong>Authentication</strong>
<ul>
<li>Fixed incorrect key info endpoint - <a href="https://github.com/BerriAI/litellm/pull/13633" target="_blank" rel="noopener noreferrer">PR #13633</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-4">Features<a href="#features-4" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/logging#langfuse">Langfuse OTEL</a></strong>
<ul>
<li>Added key/team logging for Langfuse OTEL Logger - <a href="https://github.com/BerriAI/litellm/pull/13512" target="_blank" rel="noopener noreferrer">PR #13512</a></li>
<li>Fixed LangfuseOtelSpanAttributes constants to match expected values - <a href="https://github.com/BerriAI/litellm/pull/13659" target="_blank" rel="noopener noreferrer">PR #13659</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#mlflow">MLflow</a></strong>
<ul>
<li>Updated MLflow logger usage span attributes - <a href="https://github.com/BerriAI/litellm/pull/13561" target="_blank" rel="noopener noreferrer">PR #13561</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-4">Bugs<a href="#bugs-4" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Security</strong>
<ul>
<li>Hide sensitive data in <code>/model/info</code> - azure entra client_secret - <a href="https://github.com/BerriAI/litellm/pull/13577" target="_blank" rel="noopener noreferrer">PR #13577</a></li>
<li>Fixed trivy/secrets false positives - <a href="https://github.com/BerriAI/litellm/pull/13631" target="_blank" rel="noopener noreferrer">PR #13631</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-5">Features<a href="#features-5" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>HTTP Performance</strong>
<ul>
<li>New &#x27;EXPERIMENTAL_OPENAI_BASE_LLM_HTTP_HANDLER&#x27; flag for +100 RPS improvement on OpenAI calls - <a href="https://github.com/BerriAI/litellm/pull/13625" target="_blank" rel="noopener noreferrer">PR #13625</a></li>
</ul>
</li>
<li><strong>Database Monitoring</strong>
<ul>
<li>Added DB metrics to Prometheus - <a href="https://github.com/BerriAI/litellm/pull/13626" target="_blank" rel="noopener noreferrer">PR #13626</a></li>
</ul>
</li>
<li><strong>Error Handling</strong>
<ul>
<li>Added safe divide by 0 protection to prevent crashes - <a href="https://github.com/BerriAI/litellm/pull/13624" target="_blank" rel="noopener noreferrer">PR #13624</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-5">Bugs<a href="#bugs-5" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Dependencies</strong>
<ul>
<li>Updated boto3 to 1.36.0 and aioboto3 to 13.4.0 - <a href="https://github.com/BerriAI/litellm/pull/13665" target="_blank" rel="noopener noreferrer">PR #13665</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-6">Features<a href="#features-6" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Database</strong>
<ul>
<li>Removed redundant <code>use_prisma_migrate</code> flag - now default - <a href="https://github.com/BerriAI/litellm/pull/13555" target="_blank" rel="noopener noreferrer">PR #13555</a></li>
</ul>
</li>
<li><strong>LLM Translation</strong>
<ul>
<li>Added model ID check - <a href="https://github.com/BerriAI/litellm/pull/13507" target="_blank" rel="noopener noreferrer">PR #13507</a></li>
<li>Refactored Anthropic configurations and added support for <code>anthropic_beta</code> headers - <a href="https://github.com/BerriAI/litellm/pull/13590" target="_blank" rel="noopener noreferrer">PR #13590</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@TensorNull made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13458" target="_blank" rel="noopener noreferrer">PR #13458</a></li>
<li>@MajorD00m made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13577" target="_blank" rel="noopener noreferrer">PR #13577</a></li>
<li>@VerunicaM made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13584" target="_blank" rel="noopener noreferrer">PR #13584</a></li>
<li>@huangyafei made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13607" target="_blank" rel="noopener noreferrer">PR #13607</a></li>
<li>@TomeHirata made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13561" target="_blank" rel="noopener noreferrer">PR #13561</a></li>
<li>@willfinnigan made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13659" target="_blank" rel="noopener noreferrer">PR #13659</a></li>
<li>@dcbark01 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13633" target="_blank" rel="noopener noreferrer">PR #13633</a></li>
<li>@javacruft made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13631" target="_blank" rel="noopener noreferrer">PR #13631</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog"><strong><a href="https://github.com/BerriAI/litellm/compare/v1.75.5-stable.rc-draft...v1.75.8-nightly" target="_blank" rel="noopener noreferrer">Full Changelog</a></strong><a href="#full-changelog" class="hash-link" aria-label="full-changelog" title="full-changelog"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-75-5">v1.75.5-stable - Redis latency improvements</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-08-10T10:00:00.000Z">2025810</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.75.5-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.75.5.post2</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>Redis - Latency Improvements</strong> - Reduces P99 latency by 50% with Redis enabled.</li>
<li><strong>Responses API Session Management</strong> - Support for managing responses API sessions with images.</li>
<li><strong>Oracle Cloud Infrastructure</strong> - New LLM provider for calling models on Oracle Cloud Infrastructure.</li>
<li><strong>Digital Ocean&#x27;s Gradient AI</strong> - New LLM provider for calling models on Digital Ocean&#x27;s Gradient AI platform.</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="risk-of-upgrade">Risk of Upgrade<a href="#risk-of-upgrade" class="hash-link" aria-label="Risk of Upgrade" title="Risk of Upgrade"></a></h3>
<p>If you build the proxy from the pip package, you should hold off on upgrading. This version makes <code>prisma migrate deploy</code> our default for managing the DB. This is safer, as it doesn&#x27;t reset the DB, but it requires a manual <code>prisma generate</code> step.</p>
<p>Users of our Docker image, are <strong>not</strong> affected by this change.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="redis-latency-improvements">Redis Latency Improvements<a href="#redis-latency-improvements" class="hash-link" aria-label="Redis Latency Improvements" title="Redis Latency Improvements"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAGCAYAAAD68A/GAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAZklEQVR4nI2MSQoDMQwE9f8H5g8+eAHZ2ozpIEOGHEdQ9AItaq2hlAIRwd4bEXH59wmNMVBrxVrrKd0dZnb1lymXc07k5977Q46Z+faqCjKzw8zH3Y+qXi8iN6dmzp7w8igiPm/4As9v6EE2ip9vAAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="363"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/faster_caching_calls.8ab8368.640.png" srcset="/assets/ideal-img/faster_caching_calls.8ab8368.640.png 640w,/assets/ideal-img/faster_caching_calls.4c1bced.1920.png 1920w" width="640" height="363"></noscript></div>
<br>
<p>This release adds in-memory caching for Redis requests, enabling faster response times in high-traffic. Now, LiteLLM instances will check their in-memory cache for a cache hit, before checking Redis. This reduces caching-related latency from 100ms for LLM API calls to sub-1ms, on cache hits.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="responses-api-session-management-w-images">Responses API Session Management w/ Images<a href="#responses-api-session-management-w-images" class="hash-link" aria-label="Responses API Session Management w/ Images" title="Responses API Session Management w/ Images"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAFAAoDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAYI/8QAIxAAAgEDAQkAAAAAAAAAAAAAAQIEAAMFBggREhMVITFU0v/EABQBAQAAAAAAAAAAAAAAAAAAAAD/xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIRAxEAPwCy2opMvEaKw5xc2XEdshws9q8ysw5TdiQRvFZcbVOoQxHXst59y59UpQf/2Q==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/responses_api_session_mgt_images.d628659.640.jpg" srcset="/assets/ideal-img/responses_api_session_mgt_images.d628659.640.jpg 640w,/assets/ideal-img/responses_api_session_mgt_images.fd3948f.1920.jpg 1920w" width="640" height="334"></noscript></div>
<br>
<p>LiteLLM now supports session management for Responses API requests with images. This is great for use-cases like chatbots, that are using the Responses API to track the state of a conversation. LiteLLM session management works across <strong>ALL</strong> LLM API&#x27;s (including Anthropic, Bedrock, OpenAI, etc). LiteLLM session management works by storing the request and response content in an s3 bucket, you can specify.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-model-support">New Model Support<a href="#new-model-support" class="hash-link" aria-label="New Model Support" title="New Model Support"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th></tr></thead><tbody><tr><td>Bedrock</td><td><code>bedrock/us.anthropic.claude-opus-4-1-20250805-v1:0</code></td><td>200k</td><td>$15</td><td>$75</td></tr><tr><td>Bedrock</td><td><code>bedrock/openai.gpt-oss-20b-1:0</code></td><td>200k</td><td>0.07</td><td>0.3</td></tr><tr><td>Bedrock</td><td><code>bedrock/openai.gpt-oss-120b-1:0</code></td><td>200k</td><td>0.15</td><td>0.6</td></tr><tr><td>Fireworks AI</td><td><code>fireworks_ai/accounts/fireworks/models/glm-4p5</code></td><td>128k</td><td>0.55</td><td>2.19</td></tr><tr><td>Fireworks AI</td><td><code>fireworks_ai/accounts/fireworks/models/glm-4p5-air</code></td><td>128k</td><td>0.22</td><td>0.88</td></tr><tr><td>Fireworks AI</td><td><code>fireworks_ai/accounts/fireworks/models/gpt-oss-120b</code></td><td>131072</td><td>0.15</td><td>0.6</td></tr><tr><td>Fireworks AI</td><td><code>fireworks_ai/accounts/fireworks/models/gpt-oss-20b</code></td><td>131072</td><td>0.05</td><td>0.2</td></tr><tr><td>Groq</td><td><code>groq/openai/gpt-oss-20b</code></td><td>131072</td><td>0.1</td><td>0.5</td></tr><tr><td>Groq</td><td><code>groq/openai/gpt-oss-120b</code></td><td>131072</td><td>0.15</td><td>0.75</td></tr><tr><td>OpenAI</td><td><code>openai/gpt-5</code></td><td>400k</td><td>1.25</td><td>10</td></tr><tr><td>OpenAI</td><td><code>openai/gpt-5-2025-08-07</code></td><td>400k</td><td>1.25</td><td>10</td></tr><tr><td>OpenAI</td><td><code>openai/gpt-5-mini</code></td><td>400k</td><td>0.25</td><td>2</td></tr><tr><td>OpenAI</td><td><code>openai/gpt-5-mini-2025-08-07</code></td><td>400k</td><td>0.25</td><td>2</td></tr><tr><td>OpenAI</td><td><code>openai/gpt-5-nano</code></td><td>400k</td><td>0.05</td><td>0.4</td></tr><tr><td>OpenAI</td><td><code>openai/gpt-5-nano-2025-08-07</code></td><td>400k</td><td>0.05</td><td>0.4</td></tr><tr><td>OpenAI</td><td><code>openai/gpt-5-chat</code></td><td>400k</td><td>1.25</td><td>10</td></tr><tr><td>OpenAI</td><td><code>openai/gpt-5-chat-latest</code></td><td>400k</td><td>1.25</td><td>10</td></tr><tr><td>Azure</td><td><code>azure/gpt-5</code></td><td>400k</td><td>1.25</td><td>10</td></tr><tr><td>Azure</td><td><code>azure/gpt-5-2025-08-07</code></td><td>400k</td><td>1.25</td><td>10</td></tr><tr><td>Azure</td><td><code>azure/gpt-5-mini</code></td><td>400k</td><td>0.25</td><td>2</td></tr><tr><td>Azure</td><td><code>azure/gpt-5-mini-2025-08-07</code></td><td>400k</td><td>0.25</td><td>2</td></tr><tr><td>Azure</td><td><code>azure/gpt-5-nano-2025-08-07</code></td><td>400k</td><td>0.05</td><td>0.4</td></tr><tr><td>Azure</td><td><code>azure/gpt-5-nano</code></td><td>400k</td><td>0.05</td><td>0.4</td></tr><tr><td>Azure</td><td><code>azure/gpt-5-chat</code></td><td>400k</td><td>1.25</td><td>10</td></tr><tr><td>Azure</td><td><code>azure/gpt-5-chat-latest</code></td><td>400k</td><td>1.25</td><td>10</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/oci">OCI</a></strong>
<ul>
<li>New LLM provider - <a href="https://github.com/BerriAI/litellm/pull/13206" target="_blank" rel="noopener noreferrer">PR #13206</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/jina_ai">JinaAI</a></strong>
<ul>
<li>support multimodal embedding models - <a href="https://github.com/BerriAI/litellm/pull/13181" target="_blank" rel="noopener noreferrer">PR #13181</a></li>
</ul>
</li>
<li><strong>GPT-5 (<a href="/docs/providers/openai">OpenAI</a>/<a href="/docs/providers/azure">Azure</a>)</strong>
<ul>
<li>Support drop_params for temperature - <a href="https://github.com/BerriAI/litellm/pull/13390" target="_blank" rel="noopener noreferrer">PR #13390</a></li>
<li>Map max_tokens to max_completion_tokens - <a href="https://github.com/BerriAI/litellm/pull/13390" target="_blank" rel="noopener noreferrer">PR #13390</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Add claude-opus-4-1 on model cost map - <a href="https://github.com/BerriAI/litellm/pull/13384" target="_blank" rel="noopener noreferrer">PR #13384</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openrouter">OpenRouter</a></strong>
<ul>
<li>Add gpt-oss to model cost map - <a href="https://github.com/BerriAI/litellm/pull/13442" target="_blank" rel="noopener noreferrer">PR #13442</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/cerebras">Cerebras</a></strong>
<ul>
<li>Add gpt-oss to model cost map - <a href="https://github.com/BerriAI/litellm/pull/13442" target="_blank" rel="noopener noreferrer">PR #13442</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure">Azure</a></strong>
<ul>
<li>Support drop params for temperature on o-series models - <a href="https://github.com/BerriAI/litellm/pull/13353" target="_blank" rel="noopener noreferrer">PR #13353</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/gradient_ai">GradientAI</a></strong>
<ul>
<li>New LLM Provider - <a href="https://github.com/BerriAI/litellm/pull/12169" target="_blank" rel="noopener noreferrer">PR #12169</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/providers/openai">OpenAI</a></strong>
<ul>
<li>Add service_tier and safety_identifier as supported responses api params - <a href="https://github.com/BerriAI/litellm/pull/13258" target="_blank" rel="noopener noreferrer">PR #13258</a></li>
<li>Correct pricing for web search on 4o-mini - <a href="https://github.com/BerriAI/litellm/pull/13269" target="_blank" rel="noopener noreferrer">PR #13269</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/mistral">Mistral</a></strong>
<ul>
<li>Handle $id and $schema fields when calling mistral - <a href="https://github.com/BerriAI/litellm/pull/13389" target="_blank" rel="noopener noreferrer">PR #13389</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><code>/responses</code>
<ul>
<li>Responses API Session Handling w/ support for images - <a href="https://github.com/BerriAI/litellm/pull/13347" target="_blank" rel="noopener noreferrer">PR #13347</a></li>
<li>failed if input containing ResponseReasoningItem - <a href="https://github.com/BerriAI/litellm/pull/13465" target="_blank" rel="noopener noreferrer">PR #13465</a></li>
<li>Support custom tools - <a href="https://github.com/BerriAI/litellm/pull/13418" target="_blank" rel="noopener noreferrer">PR #13418</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><code>/chat/completions</code>
<ul>
<li>Fix completion_token_details usage object missing text tokens - <a href="https://github.com/BerriAI/litellm/pull/13234" target="_blank" rel="noopener noreferrer">PR #13234</a></li>
<li>(SDK) handle tool being a pydantic object - <a href="https://github.com/BerriAI/litellm/pull/13274" target="_blank" rel="noopener noreferrer">PR #13274</a></li>
<li>include cost in streaming usage object - <a href="https://github.com/BerriAI/litellm/pull/13418" target="_blank" rel="noopener noreferrer">PR #13418</a></li>
<li>Exclude none fields on/chat/completion - allows usage with n8n - <a href="https://github.com/BerriAI/litellm/pull/13320" target="_blank" rel="noopener noreferrer">PR #13320</a></li>
</ul>
</li>
<li><code>/responses</code>
<ul>
<li>Transform function call in response for non-openai models (gemini/anthropic) - <a href="https://github.com/BerriAI/litellm/pull/13260" target="_blank" rel="noopener noreferrer">PR #13260</a></li>
<li>Fix unsupported operand error with model groups - <a href="https://github.com/BerriAI/litellm/pull/13293" target="_blank" rel="noopener noreferrer">PR #13293</a></li>
<li>Responses api session management for streaming responses - <a href="https://github.com/BerriAI/litellm/pull/13396" target="_blank" rel="noopener noreferrer">PR #13396</a></li>
</ul>
</li>
<li><code>/v1/messages</code>
<ul>
<li>Added litellm claude code count tokens - <a href="https://github.com/BerriAI/litellm/pull/13261" target="_blank" rel="noopener noreferrer">PR #13261</a></li>
</ul>
</li>
<li><code>/vector_stores</code>
<ul>
<li>Fix create/search vector store errors - <a href="https://github.com/BerriAI/litellm/pull/13285" target="_blank" rel="noopener noreferrer">PR #13285</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway"><a href="/docs/mcp">MCP Gateway</a><a href="#mcp-gateway" class="hash-link" aria-label="mcp-gateway" title="mcp-gateway"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>Add route check for internal users - <a href="https://github.com/BerriAI/litellm/pull/13350" target="_blank" rel="noopener noreferrer">PR #13350</a></li>
<li>MCP Guardrails - docs - <a href="https://github.com/BerriAI/litellm/pull/13392" target="_blank" rel="noopener noreferrer">PR #13392</a></li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-2">Bugs<a href="#bugs-2" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li>Fix auth on UI for bearer token servers - <a href="https://github.com/BerriAI/litellm/pull/13312" target="_blank" rel="noopener noreferrer">PR #13312</a></li>
<li>allow access group on mcp tool retrieval - <a href="https://github.com/BerriAI/litellm/pull/13425" target="_blank" rel="noopener noreferrer">PR #13425</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Teams</strong>
<ul>
<li>Add team deletion check for teams with keys - <a href="https://github.com/BerriAI/litellm/pull/12953" target="_blank" rel="noopener noreferrer">PR #12953</a></li>
</ul>
</li>
<li><strong>Models</strong>
<ul>
<li>Add ability to set model alias per key/team - <a href="https://github.com/BerriAI/litellm/pull/13276" target="_blank" rel="noopener noreferrer">PR #13276</a></li>
<li>New button to reload model pricing from model cost map - <a href="https://github.com/BerriAI/litellm/pull/13464" target="_blank" rel="noopener noreferrer">PR #13464</a>, <a href="https://github.com/BerriAI/litellm/pull/13470" target="_blank" rel="noopener noreferrer">PR #13470</a></li>
</ul>
</li>
<li><strong>Keys</strong>
<ul>
<li>Make team field required when creating service account keys - <a href="https://github.com/BerriAI/litellm/pull/13302" target="_blank" rel="noopener noreferrer">PR #13302</a></li>
<li>Gray out key-based logging settings for non-enterprise users - prevents confusion on if logging all up is supported - <a href="https://github.com/BerriAI/litellm/pull/13431" target="_blank" rel="noopener noreferrer">PR #13431</a></li>
</ul>
</li>
<li><strong>Navbar</strong>
<ul>
<li>Add logo customization for LiteLLM admin UI - <a href="https://github.com/BerriAI/litellm/pull/12958" target="_blank" rel="noopener noreferrer">PR #12958</a></li>
</ul>
</li>
<li><strong>Logs</strong>
<ul>
<li>Add token breakdowns on logs + session page - <a href="https://github.com/BerriAI/litellm/pull/13357" target="_blank" rel="noopener noreferrer">PR #13357</a></li>
</ul>
</li>
<li><strong>Usage</strong>
<ul>
<li>Ensure Usage Page loads after the DB has large entries - <a href="https://github.com/BerriAI/litellm/pull/13400" target="_blank" rel="noopener noreferrer">PR #13400</a></li>
</ul>
</li>
<li><strong>Test Key Page</strong>
<ul>
<li>allow uploading images for /chat/completions and /responses - <a href="https://github.com/BerriAI/litellm/pull/13445" target="_blank" rel="noopener noreferrer">PR #13445</a></li>
</ul>
</li>
<li><strong>MCP</strong>
<ul>
<li>Add auth tokens to local storage auth - <a href="https://github.com/BerriAI/litellm/pull/13473" target="_blank" rel="noopener noreferrer">PR #13473</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-3">Bugs<a href="#bugs-3" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Custom Root Path</strong>
<ul>
<li>Fix login route when SSO is enabled - <a href="https://github.com/BerriAI/litellm/pull/13267" target="_blank" rel="noopener noreferrer">PR #13267</a></li>
</ul>
</li>
<li><strong>Customers/End-users</strong>
<ul>
<li>Allow calling/v1/modelswhen end user over budget - allows model listing to work on OpenWebUI when customer over budget - <a href="https://github.com/BerriAI/litellm/pull/13320" target="_blank" rel="noopener noreferrer">PR #13320</a></li>
</ul>
</li>
<li><strong>Teams</strong>
<ul>
<li>Remove user - team membership, when user removed from team - <a href="https://github.com/BerriAI/litellm/pull/13433" target="_blank" rel="noopener noreferrer">PR #13433</a></li>
</ul>
</li>
<li><strong>Errors</strong>
<ul>
<li>Bubble up network errors to user for Logging and Alerts page - <a href="https://github.com/BerriAI/litellm/pull/13427" target="_blank" rel="noopener noreferrer">PR #13427</a></li>
</ul>
</li>
<li><strong>Model Hub</strong>
<ul>
<li>Show pricing for azure models, when base model is set - <a href="https://github.com/BerriAI/litellm/pull/13418" target="_blank" rel="noopener noreferrer">PR #13418</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-4">Features<a href="#features-4" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Bedrock Guardrails</strong>
<ul>
<li>Redacted sensitive information in bedrock guardrails error message - <a href="https://github.com/BerriAI/litellm/pull/13356" target="_blank" rel="noopener noreferrer">PR #13356</a></li>
</ul>
</li>
<li><strong>Standard Logging Payload</strong>
<ul>
<li>Fix cant register atextexit bug - <a href="https://github.com/BerriAI/litellm/pull/13436" target="_blank" rel="noopener noreferrer">PR #13436</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-4">Bugs<a href="#bugs-4" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Braintrust</strong>
<ul>
<li>Allow setting of braintrust callback base url - <a href="https://github.com/BerriAI/litellm/pull/13368" target="_blank" rel="noopener noreferrer">PR #13368</a></li>
</ul>
</li>
<li><strong>OTEL</strong>
<ul>
<li>Track pre_call hook latency  - <a href="https://github.com/BerriAI/litellm/pull/13362" target="_blank" rel="noopener noreferrer">PR #13362</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-5">Features<a href="#features-5" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Team-BYOK models</strong>
<ul>
<li>Add wildcard model support - <a href="https://github.com/BerriAI/litellm/pull/13278" target="_blank" rel="noopener noreferrer">PR #13278</a></li>
</ul>
</li>
<li><strong>Caching</strong>
<ul>
<li>GCP IAM auth support for caching - <a href="https://github.com/BerriAI/litellm/pull/13275" target="_blank" rel="noopener noreferrer">PR #13275</a></li>
</ul>
</li>
<li><strong>Latency</strong>
<ul>
<li>reduce p99 latency w/ redis enabled by 50% - only updates model usage if tpm/rpm limits set - <a href="https://github.com/BerriAI/litellm/pull/13362" target="_blank" rel="noopener noreferrer">PR #13362</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-6">Features<a href="#features-6" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Models</strong>
<ul>
<li>Support /v1/models/{model_id} retrieval - <a href="https://github.com/BerriAI/litellm/pull/13268" target="_blank" rel="noopener noreferrer">PR #13268</a></li>
</ul>
</li>
<li><strong>Multi-instance</strong>
<ul>
<li>Ensure disable_llm_api_endpoints works - <a href="https://github.com/BerriAI/litellm/pull/13278" target="_blank" rel="noopener noreferrer">PR #13278</a></li>
</ul>
</li>
<li><strong>Logs</strong>
<ul>
<li>Add apscheduler log suppress - <a href="https://github.com/BerriAI/litellm/pull/13299" target="_blank" rel="noopener noreferrer">PR #13299</a></li>
</ul>
</li>
<li><strong>Helm</strong>
<ul>
<li>Add labels to migrations job template - <a href="https://github.com/BerriAI/litellm/pull/13343" target="_blank" rel="noopener noreferrer">PR #13343</a> s/o <a href="https://github.com/unique-jakub" target="_blank" rel="noopener noreferrer">@unique-jakub</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-5">Bugs<a href="#bugs-5" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Non-root image</strong>
<ul>
<li>Fix non-root image for migration - <a href="https://github.com/BerriAI/litellm/pull/13379" target="_blank" rel="noopener noreferrer">PR #13379</a></li>
</ul>
</li>
<li><strong>Get Routes</strong>
<ul>
<li>Load get routes when using fastapi-offline - <a href="https://github.com/BerriAI/litellm/pull/13466" target="_blank" rel="noopener noreferrer">PR #13466</a></li>
</ul>
</li>
<li><strong>Health checks</strong>
<ul>
<li>Generate unique trace IDs for Langfuse health checks - <a href="https://github.com/BerriAI/litellm/pull/13468" target="_blank" rel="noopener noreferrer">PR #13468</a></li>
</ul>
</li>
<li><strong>Swagger</strong>
<ul>
<li>Allow using Swagger for /chat/completions - <a href="https://github.com/BerriAI/litellm/pull/13469" target="_blank" rel="noopener noreferrer">PR #13469</a></li>
</ul>
</li>
<li><strong>Auth</strong>
<ul>
<li>Fix JWTs access not working with model access groups - <a href="https://github.com/BerriAI/litellm/pull/13474" target="_blank" rel="noopener noreferrer">PR #13474</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@bbartels made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13244" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/13244</a></li>
<li>@breno-aumo made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13206" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/13206</a></li>
<li>@pascalwhoop made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13122" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/13122</a></li>
<li>@ZPerling made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13045" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/13045</a></li>
<li>@zjx20 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13181" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/13181</a></li>
<li>@edwarddamato made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13368" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/13368</a></li>
<li>@msannan2 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12169" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12169</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog"><strong><a href="https://github.com/BerriAI/litellm/compare/v1.74.15-stable...v1.75.5-stable.rc-draft" target="_blank" rel="noopener noreferrer">Full Changelog</a></strong><a href="#full-changelog" class="hash-link" aria-label="full-changelog" title="full-changelog"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-74-15">v1.74.15-stable</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-08-02T10:00:00.000Z">202582</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.74.15-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.74.15.post2</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>User Agent Activity Tracking</strong> - Track how much usage each coding tool gets.</li>
<li><strong>Prompt Management</strong> - Use Git-Ops style prompt management with prompt templates.</li>
<li><strong>MCP Gateway: Guardrails</strong> - Support for using Guardrails with MCP servers.</li>
<li><strong>Google AI Studio Imagen4</strong> - Support for using Imagen4 models on Google AI Studio.</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="user-agent-activity-tracking">User Agent Activity Tracking<a href="#user-agent-activity-tracking" class="hash-link" aria-label="User Agent Activity Tracking" title="User Agent Activity Tracking"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAc0lEQVR4nDVNSw5DIQj0/nd0a6Kbir5WAXUaaCWZTID5hJQSYowopUBVwSwQEagurPUDMyMQEXLOMGYR9Oftj3MO7owxESzFXMaW9Bnjn6guvoZgR0sQYcw58aqEvTeq8Tmg1l3swgurrtS8obXuhtYf37/P0sL15LMXQAAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/agent_1.63645bb.640.png" srcset="/assets/ideal-img/agent_1.63645bb.640.png 640w,/assets/ideal-img/agent_1.13487bf.1920.png 1920w" width="640" height="334"></noscript></div>
<br>
<p>This release brings support for tracking usage and costs for AI-powered coding tools like Claude Code, Roo Code, Gemini CLI through LiteLLM. You can now track LLM cost, total tokens used, and DAU/WAU/MAU for each coding tool.</p>
<p>This is great to central AI Platform teams looking to track how they are helping developer productivity.</p>
<p><a href="https://docs.litellm.ai/docs/tutorials/cost_tracking_coding" target="_blank" rel="noopener noreferrer">Read More</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="prompt-management">Prompt Management<a href="#prompt-management" class="hash-link" aria-label="Prompt Management" title="Prompt Management"></a></h2>
<br>
<p><a href="/docs/proxy/prompt_management">Read More</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-model-support">New Model Support<a href="#new-model-support" class="hash-link" aria-label="New Model Support" title="New Model Support"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Cost per Image</th></tr></thead><tbody><tr><td>OpenRouter</td><td><code>openrouter/x-ai/grok-4</code></td><td>256k</td><td>$3</td><td>$15</td><td>N/A</td></tr><tr><td>Google AI Studio</td><td><code>gemini/imagen-4.0-generate-001</code></td><td>N/A</td><td>N/A</td><td>N/A</td><td>$0.04</td></tr><tr><td>Google AI Studio</td><td><code>gemini/imagen-4.0-ultra-generate-001</code></td><td>N/A</td><td>N/A</td><td>N/A</td><td>$0.06</td></tr><tr><td>Google AI Studio</td><td><code>gemini/imagen-4.0-fast-generate-001</code></td><td>N/A</td><td>N/A</td><td>N/A</td><td>$0.02</td></tr><tr><td>Google AI Studio</td><td><code>gemini/imagen-3.0-generate-002</code></td><td>N/A</td><td>N/A</td><td>N/A</td><td>$0.04</td></tr><tr><td>Google AI Studio</td><td><code>gemini/imagen-3.0-generate-001</code></td><td>N/A</td><td>N/A</td><td>N/A</td><td>$0.04</td></tr><tr><td>Google AI Studio</td><td><code>gemini/imagen-3.0-fast-generate-001</code></td><td>N/A</td><td>N/A</td><td>N/A</td><td>$0.02</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/gemini">Google AI Studio</a></strong>
<ul>
<li>Added Google AI Studio Imagen4 model family support - <a href="https://github.com/BerriAI/litellm/pull/13065" target="_blank" rel="noopener noreferrer">PR #13065</a>, <a href="/docs/providers/google_ai_studio/image_gen">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure/azure">Azure OpenAI</a></strong>
<ul>
<li>Azure <code>api_version=&quot;preview&quot;</code> support - <a href="https://github.com/BerriAI/litellm/pull/13072" target="_blank" rel="noopener noreferrer">PR #13072</a>, <a href="/docs/providers/azure/azure#setting-api-version">Get Started</a></li>
<li>Password protected certificate files support - <a href="https://github.com/BerriAI/litellm/pull/12995" target="_blank" rel="noopener noreferrer">PR #12995</a>, <a href="/docs/providers/azure/azure#authentication">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">AWS Bedrock</a></strong>
<ul>
<li>Cost tracking via Anthropic <code>/v1/messages</code> - <a href="https://github.com/BerriAI/litellm/pull/13072" target="_blank" rel="noopener noreferrer">PR #13072</a></li>
<li>Computer use support - <a href="https://github.com/BerriAI/litellm/pull/13150" target="_blank" rel="noopener noreferrer">PR #13150</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openrouter">OpenRouter</a></strong>
<ul>
<li>Added Grok4 model support - <a href="https://github.com/BerriAI/litellm/pull/13018" target="_blank" rel="noopener noreferrer">PR #13018</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Auto Cache Control Injection - Improved cache_control_injection_points with negative index support - <a href="https://github.com/BerriAI/litellm/pull/13187" target="_blank" rel="noopener noreferrer">PR #13187</a>, <a href="/docs/tutorials/prompt_caching">Get Started</a></li>
<li>Working mid-stream fallbacks with token usage tracking - <a href="https://github.com/BerriAI/litellm/pull/13149" target="_blank" rel="noopener noreferrer">PR #13149</a>, <a href="https://github.com/BerriAI/litellm/pull/13170" target="_blank" rel="noopener noreferrer">PR #13170</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/perplexity">Perplexity</a></strong>
<ul>
<li>Citation annotations support - <a href="https://github.com/BerriAI/litellm/pull/13225" target="_blank" rel="noopener noreferrer">PR #13225</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/providers/gemini">Gemini</a></strong>
<ul>
<li>Fix merge_reasoning_content_in_choices parameter issue - <a href="https://github.com/BerriAI/litellm/pull/13066" target="_blank" rel="noopener noreferrer">PR #13066</a>, <a href="/docs/tutorials/openweb_ui#render-thinking-content-on-open-webui">Get Started</a></li>
<li>Added support for using <code>GOOGLE_API_KEY</code> environment variable for Google AI Studio - <a href="https://github.com/BerriAI/litellm/pull/12507" target="_blank" rel="noopener noreferrer">PR #12507</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vllm">vLLM/OpenAI-like</a></strong>
<ul>
<li>Fix missing extra_headers support for embeddings - <a href="https://github.com/BerriAI/litellm/pull/13198" target="_blank" rel="noopener noreferrer">PR #13198</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/generateContent">/generateContent</a></strong>
<ul>
<li>Support for query_params in generateContent routes for API Key setting - <a href="https://github.com/BerriAI/litellm/pull/13100" target="_blank" rel="noopener noreferrer">PR #13100</a></li>
<li>Ensure &quot;x-goog-api-key&quot; is used for auth to google ai studio when using /generateContent on LiteLLM - <a href="https://github.com/BerriAI/litellm/pull/13098" target="_blank" rel="noopener noreferrer">PR #13098</a></li>
<li>Ensure tool calling works as expected on generateContent - <a href="https://github.com/BerriAI/litellm/pull/13189" target="_blank" rel="noopener noreferrer">PR #13189</a></li>
</ul>
</li>
<li><strong><a href="/docs/pass_through/vertex_ai">/vertex_ai (Passthrough)</a></strong>
<ul>
<li>Ensure multimodal embedding responses are logged properly - <a href="https://github.com/BerriAI/litellm/pull/13050" target="_blank" rel="noopener noreferrer">PR #13050</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway"><a href="/docs/mcp">MCP Gateway</a><a href="#mcp-gateway" class="hash-link" aria-label="mcp-gateway" title="mcp-gateway"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Health Check Improvements</strong>
<ul>
<li>Add health check endpoints for MCP servers - <a href="https://github.com/BerriAI/litellm/pull/13106" target="_blank" rel="noopener noreferrer">PR #13106</a></li>
</ul>
</li>
<li><strong>Guardrails Integration</strong>
<ul>
<li>Add pre and during call hooks initialization - <a href="https://github.com/BerriAI/litellm/pull/13067" target="_blank" rel="noopener noreferrer">PR #13067</a></li>
<li>Move pre and during hooks to ProxyLogging - <a href="https://github.com/BerriAI/litellm/pull/13109" target="_blank" rel="noopener noreferrer">PR #13109</a></li>
<li>MCP pre and during guardrails implementation - <a href="https://github.com/BerriAI/litellm/pull/13188" target="_blank" rel="noopener noreferrer">PR #13188</a></li>
</ul>
</li>
<li><strong>Protocol &amp; Header Support</strong>
<ul>
<li>Add protocol headers support - <a href="https://github.com/BerriAI/litellm/pull/13062" target="_blank" rel="noopener noreferrer">PR #13062</a></li>
</ul>
</li>
<li><strong>URL &amp; Namespacing</strong>
<ul>
<li>Improve MCP server URL validation for internal/Kubernetes URLs - <a href="https://github.com/BerriAI/litellm/pull/13099" target="_blank" rel="noopener noreferrer">PR #13099</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-2">Bugs<a href="#bugs-2" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>UI</strong>
<ul>
<li>Fix scrolling issue with MCP tools - <a href="https://github.com/BerriAI/litellm/pull/13015" target="_blank" rel="noopener noreferrer">PR #13015</a></li>
<li>Fix MCP client list failure - <a href="https://github.com/BerriAI/litellm/pull/13114" target="_blank" rel="noopener noreferrer">PR #13114</a></li>
</ul>
</li>
</ul>
<p><a href="/docs/mcp">Read More</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>
<p><strong>Usage Analytics</strong></p>
<ul>
<li>New tab for user agent activity tracking - <a href="https://github.com/BerriAI/litellm/pull/13146" target="_blank" rel="noopener noreferrer">PR #13146</a></li>
<li>Daily usage per user analytics - <a href="https://github.com/BerriAI/litellm/pull/13147" target="_blank" rel="noopener noreferrer">PR #13147</a></li>
<li>Default usage chart date range set to last 7 days - <a href="https://github.com/BerriAI/litellm/pull/12917" target="_blank" rel="noopener noreferrer">PR #12917</a></li>
<li>New advanced date range picker component - <a href="https://github.com/BerriAI/litellm/pull/13141" target="_blank" rel="noopener noreferrer">PR #13141</a>, <a href="https://github.com/BerriAI/litellm/pull/13221" target="_blank" rel="noopener noreferrer">PR #13221</a></li>
<li>Show loader on usage cost charts after date selection - <a href="https://github.com/BerriAI/litellm/pull/13113" target="_blank" rel="noopener noreferrer">PR #13113</a></li>
</ul>
</li>
<li>
<p><strong>Models</strong></p>
<ul>
<li>Added Voyage, Jinai, Deepinfra and VolcEngine providers on UI - <a href="https://github.com/BerriAI/litellm/pull/13131" target="_blank" rel="noopener noreferrer">PR #13131</a></li>
<li>Added Sagemaker on UI - <a href="https://github.com/BerriAI/litellm/pull/13117" target="_blank" rel="noopener noreferrer">PR #13117</a></li>
<li>Preserve model order in <code>/v1/models</code> and <code>/model_group/info</code> endpoints - <a href="https://github.com/BerriAI/litellm/pull/13178" target="_blank" rel="noopener noreferrer">PR #13178</a></li>
</ul>
</li>
<li>
<p><strong>Key Management</strong></p>
<ul>
<li>Properly parse JSON options for key generation in UI - <a href="https://github.com/BerriAI/litellm/pull/12989" target="_blank" rel="noopener noreferrer">PR #12989</a></li>
</ul>
</li>
<li>
<p><strong>Authentication</strong></p>
<ul>
<li><strong>JWT Fields</strong>
<ul>
<li>Add dot notation support for all JWT fields - <a href="https://github.com/BerriAI/litellm/pull/13013" target="_blank" rel="noopener noreferrer">PR #13013</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-3">Bugs<a href="#bugs-3" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Permissions</strong>
<ul>
<li>Fix object permission for organizations - <a href="https://github.com/BerriAI/litellm/pull/13142" target="_blank" rel="noopener noreferrer">PR #13142</a></li>
<li>Fix list team v2 security check - <a href="https://github.com/BerriAI/litellm/pull/13094" target="_blank" rel="noopener noreferrer">PR #13094</a></li>
</ul>
</li>
<li><strong>Models</strong>
<ul>
<li>Fix model reload on model update - <a href="https://github.com/BerriAI/litellm/pull/13216" target="_blank" rel="noopener noreferrer">PR #13216</a></li>
</ul>
</li>
<li><strong>Router Settings</strong>
<ul>
<li>Fix displaying models for fallbacks in UI - <a href="https://github.com/BerriAI/litellm/pull/13191" target="_blank" rel="noopener noreferrer">PR #13191</a></li>
<li>Fix wildcard model name handling with custom values - <a href="https://github.com/BerriAI/litellm/pull/13116" target="_blank" rel="noopener noreferrer">PR #13116</a></li>
<li>Fix fallback delete functionality - <a href="https://github.com/BerriAI/litellm/pull/12606" target="_blank" rel="noopener noreferrer">PR #12606</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/logging#mlflow">MLFlow</a></strong>
<ul>
<li>Allow adding tags for MLFlow logging requests - <a href="https://github.com/BerriAI/litellm/pull/13108" target="_blank" rel="noopener noreferrer">PR #13108</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#langfuse">Langfuse OTEL</a></strong>
<ul>
<li>Add comprehensive metadata support to Langfuse OpenTelemetry integration - <a href="https://github.com/BerriAI/litellm/pull/12956" target="_blank" rel="noopener noreferrer">PR #12956</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#datadog">Datadog LLM Observability</a></strong>
<ul>
<li>Allow redacting message/response content for specific logging integrations - <a href="https://github.com/BerriAI/litellm/pull/13158" target="_blank" rel="noopener noreferrer">PR #13158</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-4">Bugs<a href="#bugs-4" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>API Key Logging</strong>
<ul>
<li>Fix API Key being logged inappropriately - <a href="https://github.com/BerriAI/litellm/pull/12978" target="_blank" rel="noopener noreferrer">PR #12978</a></li>
</ul>
</li>
<li><strong>MCP Spend Tracking</strong>
<ul>
<li>Set default value for MCP namespace tool name in spend table - <a href="https://github.com/BerriAI/litellm/pull/12894" target="_blank" rel="noopener noreferrer">PR #12894</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-4">Features<a href="#features-4" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Background Health Checks</strong>
<ul>
<li>Allow disabling background health checks for specific deployments - <a href="https://github.com/BerriAI/litellm/pull/13186" target="_blank" rel="noopener noreferrer">PR #13186</a></li>
</ul>
</li>
<li><strong>Database Connection Management</strong>
<ul>
<li>Ensure stale Prisma clients disconnect DB connections properly - <a href="https://github.com/BerriAI/litellm/pull/13140" target="_blank" rel="noopener noreferrer">PR #13140</a></li>
</ul>
</li>
<li><strong>Jitter Improvements</strong>
<ul>
<li>Fix jitter calculation (should be added not multiplied) - <a href="https://github.com/BerriAI/litellm/pull/12901" target="_blank" rel="noopener noreferrer">PR #12901</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-5">Bugs<a href="#bugs-5" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Anthropic Streaming</strong>
<ul>
<li>Always use choice index=0 for Anthropic streaming responses - <a href="https://github.com/BerriAI/litellm/pull/12666" target="_blank" rel="noopener noreferrer">PR #12666</a></li>
</ul>
</li>
<li><strong>Custom Auth</strong>
<ul>
<li>Bubble up custom exceptions properly - <a href="https://github.com/BerriAI/litellm/pull/13093" target="_blank" rel="noopener noreferrer">PR #13093</a></li>
</ul>
</li>
<li><strong>OTEL with Managed Files</strong>
<ul>
<li>Fix using managed files with OTEL integration - <a href="https://github.com/BerriAI/litellm/pull/13171" target="_blank" rel="noopener noreferrer">PR #13171</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-5">Features<a href="#features-5" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Database Migration</strong>
<ul>
<li>Move to use_prisma_migrate by default - <a href="https://github.com/BerriAI/litellm/pull/13117" target="_blank" rel="noopener noreferrer">PR #13117</a></li>
<li>Resolve team-only models on auth checks - <a href="https://github.com/BerriAI/litellm/pull/13117" target="_blank" rel="noopener noreferrer">PR #13117</a></li>
</ul>
</li>
<li><strong>Infrastructure</strong>
<ul>
<li>Loosened MCP Python version restrictions - <a href="https://github.com/BerriAI/litellm/pull/13102" target="_blank" rel="noopener noreferrer">PR #13102</a></li>
<li>Migrate build_and_test to CI/CD Postgres DB - <a href="https://github.com/BerriAI/litellm/pull/13166" target="_blank" rel="noopener noreferrer">PR #13166</a></li>
</ul>
</li>
<li><strong>Helm Charts</strong>
<ul>
<li>Allow Helm hooks for migration jobs - <a href="https://github.com/BerriAI/litellm/pull/13174" target="_blank" rel="noopener noreferrer">PR #13174</a></li>
<li>Fix Helm migration job schema updates - <a href="https://github.com/BerriAI/litellm/pull/12809" target="_blank" rel="noopener noreferrer">PR #12809</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-6">Bugs<a href="#bugs-6" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Docker</strong>
<ul>
<li>Remove obsolete <code>version</code> attribute in docker-compose - <a href="https://github.com/BerriAI/litellm/pull/13172" target="_blank" rel="noopener noreferrer">PR #13172</a></li>
<li>Add openssl in runtime stage for non-root Dockerfile - <a href="https://github.com/BerriAI/litellm/pull/13168" target="_blank" rel="noopener noreferrer">PR #13168</a></li>
</ul>
</li>
<li><strong>Database Configuration</strong>
<ul>
<li>Fix DB config through environment variables - <a href="https://github.com/BerriAI/litellm/pull/13111" target="_blank" rel="noopener noreferrer">PR #13111</a></li>
</ul>
</li>
<li><strong>Logging</strong>
<ul>
<li>Suppress httpx logging - <a href="https://github.com/BerriAI/litellm/pull/13217" target="_blank" rel="noopener noreferrer">PR #13217</a></li>
</ul>
</li>
<li><strong>Token Counting</strong>
<ul>
<li>Ignore unsupported keys like prefix in token counter - <a href="https://github.com/BerriAI/litellm/pull/11954" target="_blank" rel="noopener noreferrer">PR #11954</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@5731la made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12989" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12989</a></li>
<li>@restato made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12980" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12980</a></li>
<li>@strickvl made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12956" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12956</a></li>
<li>@Ne0-1 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12995" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12995</a></li>
<li>@maxrabin made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13079" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/13079</a></li>
<li>@lvuna made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12894" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12894</a></li>
<li>@Maximgitman made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12666" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12666</a></li>
<li>@pathikrit made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12901" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12901</a></li>
<li>@huetterma made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12809" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12809</a></li>
<li>@betterthanbreakfast made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13029" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/13029</a></li>
<li>@phosae made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12606" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12606</a></li>
<li>@sahusiddharth made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12507" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12507</a></li>
<li>@Amit-kr26 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11954" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11954</a></li>
<li>@kowyo made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13172" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/13172</a></li>
<li>@AnandKhinvasara made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13187" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/13187</a></li>
<li>@unique-jakub made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13174" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/13174</a></li>
<li>@tyumentsev4 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13134" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/13134</a></li>
<li>@aayush-malviya-acquia made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12978" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12978</a></li>
<li>@kankute-sameer made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13225" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/13225</a></li>
<li>@AlexanderYastrebov made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/13178" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/13178</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog"><strong><a href="https://github.com/BerriAI/litellm/compare/v1.74.9-stable...v1.74.15.rc" target="_blank" rel="noopener noreferrer">Full Changelog</a></strong><a href="#full-changelog" class="hash-link" aria-label="full-changelog" title="full-changelog"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-74-9">v1.74.9-stable - Auto-Router</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-07-27T10:00:00.000Z">2025727</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.74.9-stable.patch.1</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.74.9.post2</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>Auto-Router</strong> - Automatically route requests to specific models based on request content.</li>
<li><strong>Model-level Guardrails</strong> - Only run guardrails when specific models are used.</li>
<li><strong>MCP Header Propagation</strong> - Propagate headers from client to backend MCP.</li>
<li><strong>New LLM Providers</strong> - Added Bedrock inpainting support and Recraft API image generation  / image edits support.</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="auto-router">Auto-Router<a href="#auto-router" class="hash-link" aria-label="Auto-Router" title="Auto-Router"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAGCAYAAAD68A/GAAAACXBIWXMAABCcAAAQnAEmzTo0AAAAoElEQVR4nE2PQWoDMQxFff9j9AjJKcqQXXeBQCFdZNEykWdsj2TphXEDyddGoP8/T0lEIufMS0FEjE1EmKYJd4/Ue8fMCPdhcKDUQqNxuV44Ho7jnlprrGVlLr/85Dsfn2duiwCOmmGq7GVJVSm1krMgpXL6vjHnla02zJy/JXCH9Ab3RAz2sOnGUpSvq2EewzjId74x/38MbZtyl0zv5g9SCOi7zhOo7gAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="388"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/auto_router.da3f982.640.png" srcset="/assets/ideal-img/auto_router.da3f982.640.png 640w,/assets/ideal-img/auto_router.4a32c5e.1920.png 1920w" width="640" height="388"></noscript></div>
<br>
<p>This release introduces auto-routing to models based on request content. This means <strong>Proxy Admins</strong> can define a set of keywords that always routes to specific models when <strong>users</strong> opt in to using the auto-router.</p>
<p>This is great for internal use cases where you don&#x27;t want <strong>users</strong> to think about which model to use - for example, use Claude models for coding vs GPT models for generating ad copy.</p>
<p><a href="/docs/proxy/auto_routing">Read More</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="model-level-guardrails">Model-level Guardrails<a href="#model-level-guardrails" class="hash-link" aria-label="Model-level Guardrails" title="Model-level Guardrails"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAFAAoDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAMI/8QAHxAAAQQABwAAAAAAAAAAAAAAAwABAhESEyEiUqHB/8QAFAEBAAAAAAAAAAAAAAAAAAAAAf/EABURAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIRAxEAPwDTo4SAOWIsy1rvrxmVMxuPaIkV/9k=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="293"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/model_level_guardrails.12ef9b4.640.jpg" srcset="/assets/ideal-img/model_level_guardrails.12ef9b4.640.jpg 640w,/assets/ideal-img/model_level_guardrails.43826a4.1920.jpg 1920w" width="640" height="293"></noscript></div>
<br>
<p>This release brings model-level guardrails support to your config.yaml + UI. This is great for cases when you have an on-prem and hosted model, and just want to run prevent sending PII to the hosted model.</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">model_list</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">model_name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> claude</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">sonnet</span><span class="token punctuation" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">4</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">litellm_params</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">model</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> anthropic/claude</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">sonnet</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">4</span><span class="token punctuation" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">20250514</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">api_key</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> os.environ/ANTHROPIC_API_KEY</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">api_base</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> https</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">//api.anthropic.com/v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">guardrails</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&quot;azure-text-moderation&quot;</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token comment" style="color:#999988;font-style:italic">#  KEY CHANGE</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">guardrails</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">guardrail_name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> azure</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">text</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">moderation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">litellm_params</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">guardrail</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> azure/text_moderations</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">mode</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;post_call&quot;</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">api_key</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> os.environ/AZURE_GUARDRAIL_API_KEY</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">api_base</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> os.environ/AZURE_GUARDRAIL_API_BASE </span><br></span></code></pre></div></div>
<p><a href="/docs/proxy/guardrails/quick_start#model-level-guardrails">Read More</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-header-propagation">MCP Header Propagation<a href="#mcp-header-propagation" class="hash-link" aria-label="MCP Header Propagation" title="MCP Header Propagation"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAHCAYAAAAxrNxjAAAACXBIWXMAACxLAAAsSwGlPZapAAAAlklEQVR4nG1PQQrDMAzL/x+zD3Rf6GWwcw67jFJolrmpXTvVcDo6GLURtnSwrDCOIxzTNIGIICKnCDMRcs5IKaEsBWoGVYWZHXAeaq3Ahh98bBtc9+nte2DmdilzwlAIl9sDz/zCvLwRLaJDB6mCUEoBi7RL86q43iPoy4dlQJ96sPBu7RZn1fSK3fr/8VW1BTqCmLYwH2XtEIp+S2oAAAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="416"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/mcp_header_propogation.5d2aa12.640.png" srcset="/assets/ideal-img/mcp_header_propogation.5d2aa12.640.png 640w,/assets/ideal-img/mcp_header_propogation.5d1d622.1920.png 1920w" width="640" height="416"></noscript></div>
<br>
<p>v1.74.9-stable allows you to propagate MCP server specific authentication headers via LiteLLM</p>
<ul>
<li>Allowing users to specify which <code>header_name</code> is to be propagated to which <code>mcp_server</code> via headers</li>
<li>Allows adding of different deployments of same MCP server type to use different authentication headers</li>
</ul>
<p><a href="https://docs.litellm.ai/docs/mcp#new-server-specific-auth-headers-recommended" target="_blank" rel="noopener noreferrer">Read More</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="pricing--context-window-updates">Pricing / Context Window Updates<a href="#pricing--context-window-updates" class="hash-link" aria-label="Pricing / Context Window Updates" title="Pricing / Context Window Updates"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th></tr></thead><tbody><tr><td>Fireworks AI</td><td><code>fireworks/models/kimi-k2-instruct</code></td><td>131k</td><td>$0.6</td><td>$2.5</td></tr><tr><td>OpenRouter</td><td><code>openrouter/qwen/qwen-vl-plus</code></td><td>8192</td><td>$0.21</td><td>$0.63</td></tr><tr><td>OpenRouter</td><td><code>openrouter/qwen/qwen3-coder</code></td><td>8192</td><td>$1</td><td>$5</td></tr><tr><td>OpenRouter</td><td><code>openrouter/bytedance/ui-tars-1.5-7b</code></td><td>128k</td><td>$0.10</td><td>$0.20</td></tr><tr><td>Groq</td><td><code>groq/qwen/qwen3-32b</code></td><td>131k</td><td>$0.29</td><td>$0.59</td></tr><tr><td>VertexAI</td><td><code>vertex_ai/meta/llama-3.1-8b-instruct-maas</code></td><td>128k</td><td>$0.00</td><td>$0.00</td></tr><tr><td>VertexAI</td><td><code>vertex_ai/meta/llama-3.1-405b-instruct-maas</code></td><td>128k</td><td>$5</td><td>$16</td></tr><tr><td>VertexAI</td><td><code>vertex_ai/meta/llama-3.2-90b-vision-instruct-maas</code></td><td>128k</td><td>$0.00</td><td>$0.00</td></tr><tr><td>Google AI Studio</td><td><code>gemini/gemini-2.0-flash-live-001</code></td><td>1,048,576</td><td>$0.35</td><td>$1.5</td></tr><tr><td>Google AI Studio</td><td><code>gemini/gemini-2.5-flash-lite</code></td><td>1,048,576</td><td>$0.1</td><td>$0.4</td></tr><tr><td>VertexAI</td><td><code>vertex_ai/gemini-2.0-flash-lite-001</code></td><td>1,048,576</td><td>$0.35</td><td>$1.5</td></tr><tr><td>OpenAI</td><td><code>gpt-4o-realtime-preview-2025-06-03</code></td><td>128k</td><td>$5</td><td>$20</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/lambda_ai">Lambda AI</a></strong>
<ul>
<li>New LLM API provider - <a href="https://github.com/BerriAI/litellm/pull/12817" target="_blank" rel="noopener noreferrer">PR #12817</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/github_copilot">Github Copilot</a></strong>
<ul>
<li>Dynamic endpoint support - <a href="https://github.com/BerriAI/litellm/pull/12827" target="_blank" rel="noopener noreferrer">PR #12827</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/morph">Morph</a></strong>
<ul>
<li>New LLM API provider - <a href="https://github.com/BerriAI/litellm/pull/12821" target="_blank" rel="noopener noreferrer">PR #12821</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/groq">Groq</a></strong>
<ul>
<li>Remove deprecated groq/qwen-qwq-32b - <a href="https://github.com/BerriAI/litellm/pull/12831" target="_blank" rel="noopener noreferrer">PR #12832</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/recraft">Recraft</a></strong>
<ul>
<li>New image generation API - <a href="https://github.com/BerriAI/litellm/pull/12832" target="_blank" rel="noopener noreferrer">PR #12832</a></li>
<li>New image edits api - <a href="https://github.com/BerriAI/litellm/pull/12874" target="_blank" rel="noopener noreferrer">PR #12874</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure/azure">Azure OpenAI</a></strong>
<ul>
<li>Support DefaultAzureCredential without hard-coded environment variables - <a href="https://github.com/BerriAI/litellm/pull/12841" target="_blank" rel="noopener noreferrer">PR #12841</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/hyperbolic">Hyperbolic</a></strong>
<ul>
<li>New LLM API provider - <a href="https://github.com/BerriAI/litellm/pull/12826" target="_blank" rel="noopener noreferrer">PR #12826</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openai">OpenAI</a></strong>
<ul>
<li><code>/realtime</code> API - pass through intent query param - <a href="https://github.com/BerriAI/litellm/pull/12838" target="_blank" rel="noopener noreferrer">PR #12838</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>
<ul>
<li>Add inpainting support for Amazon Nova Canvas - <a href="https://github.com/BerriAI/litellm/pull/12949" target="_blank" rel="noopener noreferrer">PR #12949</a> s/o @<a href="https://github.com/SantoshDhaladhuli" target="_blank" rel="noopener noreferrer">SantoshDhaladhuli</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Gemini (<a href="/docs/providers/gemini">Google AI Studio</a> + <a href="/docs/providers/vertex">VertexAI</a>)</strong>
<ul>
<li>Fix leaking file descriptor error on sync calls - <a href="https://github.com/BerriAI/litellm/pull/12824" target="_blank" rel="noopener noreferrer">PR #12824</a></li>
</ul>
</li>
<li><strong>IBM Watsonx</strong>
<ul>
<li>use correct parameter name for tool choice - <a href="https://github.com/BerriAI/litellm/pull/9980" target="_blank" rel="noopener noreferrer">PR #9980</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Only show reasoning_effort for supported models - <a href="https://github.com/BerriAI/litellm/pull/12847" target="_blank" rel="noopener noreferrer">PR #12847</a></li>
<li>Handle $id and $schema in tool call requests (Anthropic API stopped accepting them) - <a href="https://github.com/BerriAI/litellm/pull/12959" target="_blank" rel="noopener noreferrer">PR #12959</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openrouter">Openrouter</a></strong>
<ul>
<li>filter out cache_control flag for non-anthropic models (allows usage with claude code) <a href="https://github.com/BerriAI/litellm/pull/12850" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12850</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/gemini">Gemini</a></strong>
<ul>
<li>Shorten Gemini tool_call_id for Open AI compatibility - <a href="https://github.com/BerriAI/litellm/pull/12941" target="_blank" rel="noopener noreferrer">PR #12941</a> s/o @<a href="https://github.com/tonga54" target="_blank" rel="noopener noreferrer">tonga54</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/pass_through/">Passthrough endpoints</a></strong>
<ul>
<li>Make key/user/team cost tracking OSS - <a href="https://github.com/BerriAI/litellm/pull/12847" target="_blank" rel="noopener noreferrer">PR #12847</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/passthrough">/v1/models</a></strong>
<ul>
<li>Return fallback models as part of api response - <a href="https://github.com/BerriAI/litellm/pull/12811" target="_blank" rel="noopener noreferrer">PR #12811</a> s/o @<a href="https://github.com/murad-khafizov" target="_blank" rel="noopener noreferrer">murad-khafizov</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/passthrough">/vector_stores</a></strong>
<ul>
<li>Make permission management OSS - <a href="https://github.com/BerriAI/litellm/pull/12990" target="_blank" rel="noopener noreferrer">PR #12990</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ol>
<li><code>/batches</code>
<ol>
<li>Skip invalid batch during cost tracking check (prev. Would stop all checks) - <a href="https://github.com/BerriAI/litellm/pull/12782" target="_blank" rel="noopener noreferrer">PR #12782</a></li>
</ol>
</li>
<li><code>/chat/completions</code>
<ol>
<li>Fix async retryer on.acompletion() - <a href="https://github.com/BerriAI/litellm/pull/12886" target="_blank" rel="noopener noreferrer">PR #12886</a></li>
</ol>
</li>
</ol>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway"><a href="/docs/mcp">MCP Gateway</a><a href="#mcp-gateway" class="hash-link" aria-label="mcp-gateway" title="mcp-gateway"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/mcp#grouping-mcps-access-groups">Permission Management</a></strong>
<ul>
<li>Make permission management by key/team OSS - <a href="https://github.com/BerriAI/litellm/pull/12988" target="_blank" rel="noopener noreferrer">PR #12988</a></li>
</ul>
</li>
<li><strong><a href="/docs/mcp#mcp-aliases">MCP Alias</a></strong>
<ul>
<li>Support mcp server aliases (useful for calling long mcp server names on Cursor) - <a href="https://github.com/BerriAI/litellm/pull/12994" target="_blank" rel="noopener noreferrer">PR #12994</a></li>
</ul>
</li>
<li><strong>Header Propagation</strong>
<ul>
<li>Support propagating headers from client to backend MCP (useful for sending personal access tokens to backend MCP) - <a href="https://github.com/BerriAI/litellm/pull/13003" target="_blank" rel="noopener noreferrer">PR #13003</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Usage</strong>
<ul>
<li>Support viewing usage by model group - <a href="https://github.com/BerriAI/litellm/pull/12890" target="_blank" rel="noopener noreferrer">PR #12890</a></li>
</ul>
</li>
<li><strong>Virtual Keys</strong>
<ul>
<li>New <code>key_type</code> field on <code>/key/generate</code> - allows specifying if key can call LLM API vs. Management routes - <a href="https://github.com/BerriAI/litellm/pull/12909" target="_blank" rel="noopener noreferrer">PR #12909</a></li>
</ul>
</li>
<li><strong>Models</strong>
<ul>
<li>Add auto router on UI - <a href="https://github.com/BerriAI/litellm/pull/12960" target="_blank" rel="noopener noreferrer">PR #12960</a></li>
<li>Show global retry policy on UI - <a href="https://github.com/BerriAI/litellm/pull/12969" target="_blank" rel="noopener noreferrer">PR #12969</a></li>
<li>Add model-level guardrails on create + update - <a href="https://github.com/BerriAI/litellm/pull/13006" target="_blank" rel="noopener noreferrer">PR #13006</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-2">Bugs<a href="#bugs-2" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>SSO</strong>
<ul>
<li>Fix logout when SSO is enabled - <a href="https://github.com/BerriAI/litellm/pull/12703" target="_blank" rel="noopener noreferrer">PR #12703</a></li>
<li>Fix reset SSO when ui_access_mode is updated - <a href="https://github.com/BerriAI/litellm/pull/13011" target="_blank" rel="noopener noreferrer">PR #13011</a></li>
</ul>
</li>
<li><strong>Guardrails</strong>
<ul>
<li>Show correct guardrails when editing a team - <a href="https://github.com/BerriAI/litellm/pull/12823" target="_blank" rel="noopener noreferrer">PR #12823</a></li>
</ul>
</li>
<li><strong>Virtual Keys</strong>
<ul>
<li>Get updated token on regenerate key - <a href="https://github.com/BerriAI/litellm/pull/12788" target="_blank" rel="noopener noreferrer">PR #12788</a></li>
<li>Fix CVE with key injection - <a href="https://github.com/BerriAI/litellm/pull/12840" target="_blank" rel="noopener noreferrer">PR #12840</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-4">Features<a href="#features-4" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/guardrails/model_armor">Google Cloud Model Armor</a></strong>
<ul>
<li>Document new guardrail - <a href="https://github.com/BerriAI/litellm/pull/12492" target="_blank" rel="noopener noreferrer">PR #12492</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/pillar_security">Pillar Security</a></strong>
<ul>
<li>New LLM Guardrail - <a href="https://github.com/BerriAI/litellm/pull/12791" target="_blank" rel="noopener noreferrer">PR #12791</a></li>
</ul>
</li>
<li><strong>CloudZero</strong>
<ul>
<li>Allow exporting spend to cloudzero - <a href="https://github.com/BerriAI/litellm/pull/12908" target="_blank" rel="noopener noreferrer">PR #12908</a></li>
</ul>
</li>
<li><strong>Model-level Guardrails</strong>
<ul>
<li>Support model-level guardrails - <a href="https://github.com/BerriAI/litellm/pull/12968" target="_blank" rel="noopener noreferrer">PR #12968</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-3">Bugs<a href="#bugs-3" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/prometheus">Prometheus</a></strong>
<ul>
<li>Fix <code>[tag]=false</code> when tag is set for tag-based metrics - <a href="https://github.com/BerriAI/litellm/pull/12916" target="_blank" rel="noopener noreferrer">PR #12916</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/guardrails_ai">Guardrails AI</a></strong>
<ul>
<li>Use validatedOutput to allow usage of fix guards - <a href="https://github.com/BerriAI/litellm/pull/12891" target="_blank" rel="noopener noreferrer">PR #12891</a> s/o @<a href="https://github.com/DmitriyAlergant" target="_blank" rel="noopener noreferrer">DmitriyAlergant</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-5">Features<a href="#features-5" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/auto_routing">Auto-Router</a></strong>
<ul>
<li>New auto-router powered by <code>semantic-router</code> - <a href="https://github.com/BerriAI/litellm/pull/12955" target="_blank" rel="noopener noreferrer">PR #12955</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-4">Bugs<a href="#bugs-4" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>forward_clientside_headers</strong>
<ul>
<li>Filter out <code>content-length</code> from headers (caused backend requests to hang) - <a href="https://github.com/BerriAI/litellm/pull/12886/files" target="_blank" rel="noopener noreferrer">PR #12886</a></li>
</ul>
</li>
<li><strong>Message Redaction</strong>
<ul>
<li>Fix cannot pickle coroutine object error - <a href="https://github.com/BerriAI/litellm/pull/13005" target="_blank" rel="noopener noreferrer">PR #13005</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-6">Features<a href="#features-6" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Benchmarks</strong>
<ul>
<li>Updated litellm proxy benchmarks (p50, p90, p99 overhead) - <a href="https://github.com/BerriAI/litellm/pull/12842" target="_blank" rel="noopener noreferrer">PR #12842</a></li>
</ul>
</li>
<li><strong>Request Headers</strong>
<ul>
<li>Added new <code>x-litellm-num-retries</code> request header</li>
</ul>
</li>
<li><strong>Swagger</strong>
<ul>
<li>Support local swagger on custom root paths - <a href="https://github.com/BerriAI/litellm/pull/12911" target="_blank" rel="noopener noreferrer">PR #12911</a></li>
</ul>
</li>
<li><strong>Health</strong>
<ul>
<li>Track cost + add tags for health checks done by LiteLLM Proxy - <a href="https://github.com/BerriAI/litellm/pull/12880" target="_blank" rel="noopener noreferrer">PR #12880</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-5">Bugs<a href="#bugs-5" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Proxy Startup</strong>
<ul>
<li>Fixes issue on startup where team member budget is None would block startup - <a href="https://github.com/BerriAI/litellm/pull/12843" target="_blank" rel="noopener noreferrer">PR #12843</a></li>
</ul>
</li>
<li><strong>Docker</strong>
<ul>
<li>Move non-root docker to chain guard image (fewer vulnerabilities) - <a href="https://github.com/BerriAI/litellm/pull/12707" target="_blank" rel="noopener noreferrer">PR #12707</a></li>
<li>addazure-keyvault==4.2.0to Docker img - <a href="https://github.com/BerriAI/litellm/pull/12873" target="_blank" rel="noopener noreferrer">PR #12873</a></li>
</ul>
</li>
<li><strong>Separate Health App</strong>
<ul>
<li>Pass through cmd args via supervisord (enables user config to still work via docker) - <a href="https://github.com/BerriAI/litellm/pull/12871" target="_blank" rel="noopener noreferrer">PR #12871</a></li>
</ul>
</li>
<li><strong>Swagger</strong>
<ul>
<li>Bump DOMPurify version (fixes vulnerability) - <a href="https://github.com/BerriAI/litellm/pull/12911" target="_blank" rel="noopener noreferrer">PR #12911</a></li>
<li>Add back local swagger bundle (enables swagger to work in air gapped env.) - <a href="https://github.com/BerriAI/litellm/pull/12911" target="_blank" rel="noopener noreferrer">PR #12911</a></li>
</ul>
</li>
<li><strong>Request Headers</strong>
<ul>
<li>Make user_header_name field check case insensitive (fixes customer budget enforcement for OpenWebUi) - <a href="https://github.com/BerriAI/litellm/pull/12950" target="_blank" rel="noopener noreferrer">PR #12950</a></li>
</ul>
</li>
<li><strong>SpendLogs</strong>
<ul>
<li>Fix issues writing to DB whencustom_llm_provideris None - <a href="https://github.com/BerriAI/litellm/pull/13001" target="_blank" rel="noopener noreferrer">PR #13001</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@magicalne made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12804" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12804</a></li>
<li>@pavangudiwada made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12798" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12798</a></li>
<li>@mdiloreto made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12707" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12707</a></li>
<li>@murad-khafizov made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12811" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12811</a></li>
<li>@eagle-p made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12791" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12791</a></li>
<li>@apoorv-sharma made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12920" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12920</a></li>
<li>@SantoshDhaladhuli made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12949" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12949</a></li>
<li>@tonga54 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12941" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12941</a></li>
<li>@sings-to-bees-on-wednesdays made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12950" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12950</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog"><strong><a href="https://github.com/BerriAI/litellm/compare/v1.74.7-stable...v1.74.9.rc-draft" target="_blank" rel="noopener noreferrer">Full Changelog</a></strong><a href="#full-changelog" class="hash-link" aria-label="full-changelog" title="full-changelog"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-74-7">v1.74.7-stable</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-07-19T10:00:00.000Z">2025719</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.74.7-stable.patch.1</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.74.7.post2</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>Vector Stores</strong> - Support for Vertex RAG Engine, PG Vector, OpenAI &amp; Azure OpenAI Vector Stores.</li>
<li><strong>Bulk Editing Users</strong> - Bulk editing users on the UI.</li>
<li><strong>Health Check Improvements</strong> - Prevent unnecessary pod restarts during high traffic.</li>
<li><strong>New LLM Providers</strong> - Added Moonshot AI and Vercel v0 provider support.</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="vector-stores-api">Vector Stores API<a href="#vector-stores-api" class="hash-link" aria-label="Vector Stores API" title="Vector Stores API"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAfElEQVR4nE2O2woDMQgF8/8fmbdCw5a9sMlqomGKgUKFgz7MwUnndXEcB7VWVBURYYyBmeFma9swUimFnDPbtvG0RmuNmCrOfgu3OlUnaTXMmHMuSFQX+NlPXu9Ck44OJ/Xe+Y+7L7D30HiY7rg5KXx+TuEXcEx8iFIk7i+uUcLcO8fRzQAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/vector_stores.ee74d50.640.png" srcset="/assets/ideal-img/vector_stores.ee74d50.640.png 640w,/assets/ideal-img/vector_stores.8788cf5.1920.png 1920w" width="640" height="334"></noscript></div>
<p>This release introduces support for using VertexAI RAG Engine, PG Vector, Bedrock Knowledge Bases, and OpenAI Vector Stores with LiteLLM.</p>
<p>This is ideal for use cases requiring external knowledge sources with LLMs.</p>
<p>This brings the following benefits for LiteLLM users:</p>
<p><strong>Proxy Admin Benefits:</strong></p>
<ul>
<li>Fine-grained access control: determine which Keys and Teams can access specific Vector Stores</li>
<li>Complete usage tracking and monitoring across all vector store operations</li>
</ul>
<p><strong>Developer Benefits:</strong></p>
<ul>
<li>Simple, unified interface for querying vector stores and using them with LLM API requests</li>
<li>Consistent API experience across all supported vector store providers</li>
</ul>
<p><a href="/docs/completion/knowledgebase">Get started</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bulk-editing-users">Bulk Editing Users<a href="#bulk-editing-users" class="hash-link" aria-label="Bulk Editing Users" title="Bulk Editing Users"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAbklEQVR4nEWMSQoFIRBDvf8lXajQG3FqZyWfCPZfPEKlkghjDLTWeJ4HIQSklNBaw94ba61PhVIKUkpQGbTWopSCMcYp5JzRe4egQdiac+J933PfYIwRtdZ/kC3CB1cIl6//BS/OOXjvzwoXr/8DEJ3Bb2uXSWUAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/bulk_edit_graphic.d3d3cc3.640.png" srcset="/assets/ideal-img/bulk_edit_graphic.d3d3cc3.640.png 640w,/assets/ideal-img/bulk_edit_graphic.f14e791.1920.png 1920w" width="640" height="334"></noscript></div>
<p>v1.74.7-stable introduces Bulk Editing Users on the UI. This is useful for:</p>
<ul>
<li>granting all existing users to a default team (useful for controlling access / tracking spend by team)</li>
<li>controlling personal model access for existing users</li>
</ul>
<p><a href="https://docs.litellm.ai/docs/proxy/ui/bulk_edit_users" target="_blank" rel="noopener noreferrer">Read more</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="health-check-server">Health Check Server<a href="#health-check-server" class="hash-link" aria-label="Health Check Server" title="Health Check Server"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAGCAYAAAD68A/GAAAACXBIWXMAACxLAAAsSwGlPZapAAAAhklEQVR4nF2OTQ5DIQiE3/3v5BH0Au5cmthXLfifTAOvbrqYAJkPmKvWitYa9t5Ya2GMoZW4Y4yJOR9dIQR47xFjhCz13lFKQXw15Fx0VtA5B2MMrLVIKalBRMjUUD6kvYJinEvyXmJUZhQauO83auUHPBlEkk8WBF5ragTmHyjm0YH/ZwG/nHznSTLAaOcAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="384"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/separate_health_app_architecture.e273e89.640.png" srcset="/assets/ideal-img/separate_health_app_architecture.e273e89.640.png 640w,/assets/ideal-img/separate_health_app_architecture.0558eeb.1920.png 1920w" width="640" height="384"></noscript></div>
<p>This release brings reliability improvements that prevent unnecessary pod restarts during high traffic. Previously, when the main LiteLLM app was busy serving traffic, health endpoints would timeout even when pods were healthy.</p>
<p>Starting with this release, you can run health endpoints on an isolated process with a dedicated port. This ensures liveness and readiness probes remain responsive even when the main LiteLLM app is under heavy load.</p>
<p><a href="https://docs.litellm.ai/docs/proxy/prod#10-use-a-separate-health-check-app" target="_blank" rel="noopener noreferrer">Read More</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="pricing--context-window-updates">Pricing / Context Window Updates<a href="#pricing--context-window-updates" class="hash-link" aria-label="Pricing / Context Window Updates" title="Pricing / Context Window Updates"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th></tr></thead><tbody><tr><td>Azure AI</td><td><code>azure_ai/grok-3</code></td><td>131k</td><td>$3.30</td><td>$16.50</td></tr><tr><td>Azure AI</td><td><code>azure_ai/global/grok-3</code></td><td>131k</td><td>$3.00</td><td>$15.00</td></tr><tr><td>Azure AI</td><td><code>azure_ai/global/grok-3-mini</code></td><td>131k</td><td>$0.25</td><td>$1.27</td></tr><tr><td>Azure AI</td><td><code>azure_ai/grok-3-mini</code></td><td>131k</td><td>$0.275</td><td>$1.38</td></tr><tr><td>Azure AI</td><td><code>azure_ai/jais-30b-chat</code></td><td>8k</td><td>$3200</td><td>$9710</td></tr><tr><td>Groq</td><td><code>groq/moonshotai-kimi-k2-instruct</code></td><td>131k</td><td>$1.00</td><td>$3.00</td></tr><tr><td>AI21</td><td><code>jamba-large-1.7</code></td><td>256k</td><td>$2.00</td><td>$8.00</td></tr><tr><td>AI21</td><td><code>jamba-mini-1.7</code></td><td>256k</td><td>$0.20</td><td>$0.40</td></tr><tr><td>Together.ai</td><td><code>together_ai/moonshotai/Kimi-K2-Instruct</code></td><td>131k</td><td>$1.00</td><td>$3.00</td></tr><tr><td>v0</td><td><code>v0/v0-1.0-md</code></td><td>128k</td><td>$3.00</td><td>$15.00</td></tr><tr><td>v0</td><td><code>v0/v0-1.5-md</code></td><td>128k</td><td>$3.00</td><td>$15.00</td></tr><tr><td>v0</td><td><code>v0/v0-1.5-lg</code></td><td>512k</td><td>$15.00</td><td>$75.00</td></tr><tr><td>Moonshot</td><td><code>moonshot/moonshot-v1-8k</code></td><td>8k</td><td>$0.20</td><td>$2.00</td></tr><tr><td>Moonshot</td><td><code>moonshot/moonshot-v1-32k</code></td><td>32k</td><td>$1.00</td><td>$3.00</td></tr><tr><td>Moonshot</td><td><code>moonshot/moonshot-v1-128k</code></td><td>131k</td><td>$2.00</td><td>$5.00</td></tr><tr><td>Moonshot</td><td><code>moonshot/moonshot-v1-auto</code></td><td>131k</td><td>$2.00</td><td>$5.00</td></tr><tr><td>Moonshot</td><td><code>moonshot/kimi-k2-0711-preview</code></td><td>131k</td><td>$0.60</td><td>$2.50</td></tr><tr><td>Moonshot</td><td><code>moonshot/moonshot-v1-32k-0430</code></td><td>32k</td><td>$1.00</td><td>$3.00</td></tr><tr><td>Moonshot</td><td><code>moonshot/moonshot-v1-128k-0430</code></td><td>131k</td><td>$2.00</td><td>$5.00</td></tr><tr><td>Moonshot</td><td><code>moonshot/moonshot-v1-8k-0430</code></td><td>8k</td><td>$0.20</td><td>$2.00</td></tr><tr><td>Moonshot</td><td><code>moonshot/kimi-latest</code></td><td>131k</td><td>$2.00</td><td>$5.00</td></tr><tr><td>Moonshot</td><td><code>moonshot/kimi-latest-8k</code></td><td>8k</td><td>$0.20</td><td>$2.00</td></tr><tr><td>Moonshot</td><td><code>moonshot/kimi-latest-32k</code></td><td>32k</td><td>$1.00</td><td>$3.00</td></tr><tr><td>Moonshot</td><td><code>moonshot/kimi-latest-128k</code></td><td>131k</td><td>$2.00</td><td>$5.00</td></tr><tr><td>Moonshot</td><td><code>moonshot/kimi-thinking-preview</code></td><td>131k</td><td>$30.00</td><td>$30.00</td></tr><tr><td>Moonshot</td><td><code>moonshot/moonshot-v1-8k-vision-preview</code></td><td>8k</td><td>$0.20</td><td>$2.00</td></tr><tr><td>Moonshot</td><td><code>moonshot/moonshot-v1-32k-vision-preview</code></td><td>32k</td><td>$1.00</td><td>$3.00</td></tr><tr><td>Moonshot</td><td><code>moonshot/moonshot-v1-128k-vision-preview</code></td><td>131k</td><td>$2.00</td><td>$5.00</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/moonshot"> Moonshot API (Kimi)</a></strong>
<ul>
<li>New LLM API integration for accessing Kimi models - <a href="https://github.com/BerriAI/litellm/pull/12592" target="_blank" rel="noopener noreferrer">PR #12592</a>, <a href="/docs/providers/moonshot">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/v0"> v0 Provider</a></strong>
<ul>
<li>New provider integration for v0.dev - <a href="https://github.com/BerriAI/litellm/pull/12751" target="_blank" rel="noopener noreferrer">PR #12751</a>, <a href="/docs/providers/v0">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openai">OpenAI</a></strong>
<ul>
<li>Use OpenAI DeepResearch models with <code>litellm.completion</code> (<code>/chat/completions</code>) - <a href="https://github.com/BerriAI/litellm/pull/12627" target="_blank" rel="noopener noreferrer">PR #12627</a> <strong>DOC NEEDED</strong></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure_openai">Azure OpenAI</a></strong>
<ul>
<li>Use Azure OpenAI DeepResearch models with <code>litellm.completion</code> (<code>/chat/completions</code>) - <a href="https://github.com/BerriAI/litellm/pull/12627" target="_blank" rel="noopener noreferrer">PR #12627</a> <strong>DOC NEEDED</strong></li>
<li>Added <code>response_format</code> support for openai gpt-4.1 models - <a href="https://github.com/BerriAI/litellm/pull/12745" target="_blank" rel="noopener noreferrer">PR #12745</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Tool cache control support - <a href="https://github.com/BerriAI/litellm/pull/12668" target="_blank" rel="noopener noreferrer">PR #12668</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>
<ul>
<li>Claude 4 /invoke route support - <a href="https://github.com/BerriAI/litellm/pull/12599" target="_blank" rel="noopener noreferrer">PR #12599</a>, <a href="/docs/providers/bedrock">Get Started</a></li>
<li>Application inference profile tool choice support - <a href="https://github.com/BerriAI/litellm/pull/12599" target="_blank" rel="noopener noreferrer">PR #12599</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/gemini">Gemini</a></strong>
<ul>
<li>Custom TTL support for context caching - <a href="https://github.com/BerriAI/litellm/pull/12541" target="_blank" rel="noopener noreferrer">PR #12541</a></li>
<li>Fix implicit caching cost calculation for Gemini 2.x models - <a href="https://github.com/BerriAI/litellm/pull/12585" target="_blank" rel="noopener noreferrer">PR #12585</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex">VertexAI</a></strong>
<ul>
<li>Added Vertex AI RAG Engine support (use with OpenAI compatible <code>/vector_stores</code> API) - <a href="https://github.com/BerriAI/litellm/pull/12595" target="_blank" rel="noopener noreferrer">PR #12752</a>, <a href="/docs/completion/knowledgebase">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vllm">vLLM</a></strong>
<ul>
<li>Added support for using Rerank endpoints with vLLM - <a href="https://github.com/BerriAI/litellm/pull/12738" target="_blank" rel="noopener noreferrer">PR #12738</a>, <a href="/docs/providers/vllm#rerank">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/ai21">AI21</a></strong>
<ul>
<li>Added ai21/jamba-1.7 model family pricing - <a href="https://github.com/BerriAI/litellm/pull/12593" target="_blank" rel="noopener noreferrer">PR #12593</a>, <a href="/docs/providers/ai21">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/together_ai">Together.ai</a></strong>
<ul>
<li>[New Model] add together_ai/moonshotai/Kimi-K2-Instruct - <a href="https://github.com/BerriAI/litellm/pull/12645" target="_blank" rel="noopener noreferrer">PR #12645</a>, <a href="/docs/providers/together_ai">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/groq">Groq</a></strong>
<ul>
<li>Add groq/moonshotai-kimi-k2-instruct model configuration - <a href="https://github.com/BerriAI/litellm/pull/12648" target="_blank" rel="noopener noreferrer">PR #12648</a>, <a href="/docs/providers/groq">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/github_copilot">Github Copilot</a></strong>
<ul>
<li>Change System prompts to assistant prompts for GH Copilot - <a href="https://github.com/BerriAI/litellm/pull/12742" target="_blank" rel="noopener noreferrer">PR #12742</a>, <a href="/docs/providers/github_copilot">Get Started</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Fix streaming + response_format + tools bug - <a href="https://github.com/BerriAI/litellm/pull/12463" target="_blank" rel="noopener noreferrer">PR #12463</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/xai">XAI</a></strong>
<ul>
<li>grok-4 does not support the <code>stop</code> param - <a href="https://github.com/BerriAI/litellm/pull/12646" target="_blank" rel="noopener noreferrer">PR #12646</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">AWS</a></strong>
<ul>
<li>Role chaining with web authentication for AWS Bedrock - <a href="https://github.com/BerriAI/litellm/pull/12607" target="_blank" rel="noopener noreferrer">PR #12607</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex">VertexAI</a></strong>
<ul>
<li>Add project_id to cached credentials - <a href="https://github.com/BerriAI/litellm/pull/12661" target="_blank" rel="noopener noreferrer">PR #12661</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>
<ul>
<li>Fix bedrock nova micro and nova lite context window info in <a href="https://github.com/BerriAI/litellm/pull/12619" target="_blank" rel="noopener noreferrer">PR #12619</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/completion/input">/chat/completions</a></strong>
<ul>
<li>Include tool calls in output of trim_messages - <a href="https://github.com/BerriAI/litellm/pull/11517" target="_blank" rel="noopener noreferrer">PR #11517</a></li>
</ul>
</li>
<li><strong><a href="/docs/vector_stores/search">/v1/vector_stores</a></strong>
<ul>
<li>New OpenAI-compatible vector store endpoints - <a href="https://github.com/BerriAI/litellm/pull/12699" target="_blank" rel="noopener noreferrer">PR #12699</a>, <a href="/docs/vector_stores/search">Get Started</a></li>
<li>Vector store search endpoint - <a href="https://github.com/BerriAI/litellm/pull/12749" target="_blank" rel="noopener noreferrer">PR #12749</a>, <a href="/docs/vector_stores/search">Get Started</a></li>
<li>Support for using PG Vector as a vector store - <a href="https://github.com/BerriAI/litellm/pull/12667" target="_blank" rel="noopener noreferrer">PR #12667</a>, <a href="/docs/completion/knowledgebase">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/generateContent">/streamGenerateContent</a></strong>
<ul>
<li>Non-gemini model support - <a href="https://github.com/BerriAI/litellm/pull/12647" target="_blank" rel="noopener noreferrer">PR #12647</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/vector_stores/search">/vector_stores</a></strong>
<ul>
<li>Knowledge Base Call returning error when passing as <code>tools</code> - <a href="https://github.com/BerriAI/litellm/pull/12628" target="_blank" rel="noopener noreferrer">PR #12628</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway"><a href="/docs/mcp">MCP Gateway</a><a href="#mcp-gateway" class="hash-link" aria-label="mcp-gateway" title="mcp-gateway"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/mcp#grouping-mcps-access-groups">Access Groups</a></strong>
<ul>
<li>Allow MCP access groups to be added via litellm proxy config.yaml - <a href="https://github.com/BerriAI/litellm/pull/12654" target="_blank" rel="noopener noreferrer">PR #12654</a></li>
<li>List tools from access list for keys - <a href="https://github.com/BerriAI/litellm/pull/12657" target="_blank" rel="noopener noreferrer">PR #12657</a></li>
</ul>
</li>
<li><strong><a href="/docs/mcp#mcp-namespacing">Namespacing</a></strong>
<ul>
<li>URL-based namespacing for better segregation - <a href="https://github.com/BerriAI/litellm/pull/12658" target="_blank" rel="noopener noreferrer">PR #12658</a></li>
<li>Make MCP_TOOL_PREFIX_SEPARATOR configurable from env - <a href="https://github.com/BerriAI/litellm/pull/12603" target="_blank" rel="noopener noreferrer">PR #12603</a></li>
</ul>
</li>
<li><strong><a href="/docs/mcp#mcp-gateway-features">Gateway Features</a></strong>
<ul>
<li>Allow using MCPs with all LLM APIs (VertexAI, Gemini, Groq, etc.) when using /responses - <a href="https://github.com/BerriAI/litellm/pull/12546" target="_blank" rel="noopener noreferrer">PR #12546</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-2">Bugs<a href="#bugs-2" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li>Fix to update object permission on update/delete key/team - <a href="https://github.com/BerriAI/litellm/pull/12701" target="_blank" rel="noopener noreferrer">PR #12701</a></li>
<li>Include /mcp in list of available routes on proxy - <a href="https://github.com/BerriAI/litellm/pull/12612" target="_blank" rel="noopener noreferrer">PR #12612</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Keys</strong>
<ul>
<li>Regenerate Key State Management improvements - <a href="https://github.com/BerriAI/litellm/pull/12729" target="_blank" rel="noopener noreferrer">PR #12729</a></li>
</ul>
</li>
<li><strong>Models</strong>
<ul>
<li>Wildcard model filter support - <a href="https://github.com/BerriAI/litellm/pull/12597" target="_blank" rel="noopener noreferrer">PR #12597</a></li>
<li>Fixes for handling team only models on UI - <a href="https://github.com/BerriAI/litellm/pull/12632" target="_blank" rel="noopener noreferrer">PR #12632</a></li>
</ul>
</li>
<li><strong>Usage Page</strong>
<ul>
<li>Fix Y-axis labels overlap on Spend per Tag chart - <a href="https://github.com/BerriAI/litellm/pull/12754" target="_blank" rel="noopener noreferrer">PR #12754</a></li>
</ul>
</li>
<li><strong>Teams</strong>
<ul>
<li>Allow setting custom key duration + show key creation stats - <a href="https://github.com/BerriAI/litellm/pull/12722" target="_blank" rel="noopener noreferrer">PR #12722</a></li>
<li>Enable team admins to update member roles - <a href="https://github.com/BerriAI/litellm/pull/12629" target="_blank" rel="noopener noreferrer">PR #12629</a></li>
</ul>
</li>
<li><strong>Users</strong>
<ul>
<li>New <code>/user/bulk_update</code> endpoint - <a href="https://github.com/BerriAI/litellm/pull/12720" target="_blank" rel="noopener noreferrer">PR #12720</a></li>
</ul>
</li>
<li><strong>Logs Page</strong>
<ul>
<li>Add <code>end_user</code> filter on UI Logs Page - <a href="https://github.com/BerriAI/litellm/pull/12663" target="_blank" rel="noopener noreferrer">PR #12663</a></li>
</ul>
</li>
<li><strong>MCP Servers</strong>
<ul>
<li>Copy MCP Server name functionality - <a href="https://github.com/BerriAI/litellm/pull/12760" target="_blank" rel="noopener noreferrer">PR #12760</a></li>
</ul>
</li>
<li><strong>Vector Stores</strong>
<ul>
<li>UI support for clicking into Vector Stores - <a href="https://github.com/BerriAI/litellm/pull/12741" target="_blank" rel="noopener noreferrer">PR #12741</a></li>
<li>Allow adding Vertex RAG Engine, OpenAI, Azure through UI - <a href="https://github.com/BerriAI/litellm/pull/12752" target="_blank" rel="noopener noreferrer">PR #12752</a></li>
</ul>
</li>
<li><strong>General</strong>
<ul>
<li>Add Copy-on-Click for all IDs (Key, Team, Organization, MCP Server) - <a href="https://github.com/BerriAI/litellm/pull/12615" target="_blank" rel="noopener noreferrer">PR #12615</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/scim">SCIM</a></strong>
<ul>
<li>Add GET /ServiceProviderConfig endpoint - <a href="https://github.com/BerriAI/litellm/pull/12664" target="_blank" rel="noopener noreferrer">PR #12664</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-3">Bugs<a href="#bugs-3" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Teams</strong>
<ul>
<li>Ensure user id correctly added when creating new teams - <a href="https://github.com/BerriAI/litellm/pull/12719" target="_blank" rel="noopener noreferrer">PR #12719</a></li>
<li>Fixes for handling team-only models on UI - <a href="https://github.com/BerriAI/litellm/pull/12632" target="_blank" rel="noopener noreferrer">PR #12632</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-4">Features<a href="#features-4" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/guardrails/google_cloud_model_armor">Google Cloud Model Armor</a></strong>
<ul>
<li>New guardrails integration - <a href="https://github.com/BerriAI/litellm/pull/12492" target="_blank" rel="noopener noreferrer">PR #12492</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/bedrock">Bedrock Guardrails</a></strong>
<ul>
<li>Allow disabling exception on &#x27;BLOCKED&#x27; action - <a href="https://github.com/BerriAI/litellm/pull/12693" target="_blank" rel="noopener noreferrer">PR #12693</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/guardrails_ai">Guardrails AI</a></strong>
<ul>
<li>Support <code>llmOutput</code> based guardrails as pre-call hooks - <a href="https://github.com/BerriAI/litellm/pull/12674" target="_blank" rel="noopener noreferrer">PR #12674</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#datadog">DataDog LLM Observability</a></strong>
<ul>
<li>Add support for tracking the correct span type based on LLM Endpoint used - <a href="https://github.com/BerriAI/litellm/pull/12652" target="_blank" rel="noopener noreferrer">PR #12652</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging">Custom Logging</a></strong>
<ul>
<li>Allow reading custom logger python scripts from S3 or GCS Bucket - <a href="https://github.com/BerriAI/litellm/pull/12623" target="_blank" rel="noopener noreferrer">PR #12623</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-4">Bugs<a href="#bugs-4" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/logging">General Logging</a></strong>
<ul>
<li>StandardLoggingPayload on cache_hits should track custom llm provider - <a href="https://github.com/BerriAI/litellm/pull/12652" target="_blank" rel="noopener noreferrer">PR #12652</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#s3-buckets">S3 Buckets</a></strong>
<ul>
<li>S3 v2 log uploader crashes when using with guardrails - <a href="https://github.com/BerriAI/litellm/pull/12733" target="_blank" rel="noopener noreferrer">PR #12733</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-5">Features<a href="#features-5" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Health Checks</strong>
<ul>
<li>Separate health app for liveness probes - <a href="https://github.com/BerriAI/litellm/pull/12669" target="_blank" rel="noopener noreferrer">PR #12669</a></li>
<li>Health check app on separate port - <a href="https://github.com/BerriAI/litellm/pull/12718" target="_blank" rel="noopener noreferrer">PR #12718</a></li>
</ul>
</li>
<li><strong>Caching</strong>
<ul>
<li>Add Azure Blob cache support - <a href="https://github.com/BerriAI/litellm/pull/12587" target="_blank" rel="noopener noreferrer">PR #12587</a></li>
</ul>
</li>
<li><strong>Router</strong>
<ul>
<li>Handle ZeroDivisionError with zero completion tokens in lowest_latency strategy - <a href="https://github.com/BerriAI/litellm/pull/12734" target="_blank" rel="noopener noreferrer">PR #12734</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-5">Bugs<a href="#bugs-5" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Database</strong>
<ul>
<li>Use upsert for managed object table to avoid UniqueViolationError - <a href="https://github.com/BerriAI/litellm/pull/11795" target="_blank" rel="noopener noreferrer">PR #11795</a></li>
<li>Refactor to support use_prisma_migrate for helm hook - <a href="https://github.com/BerriAI/litellm/pull/12600" target="_blank" rel="noopener noreferrer">PR #12600</a></li>
</ul>
</li>
<li><strong>Cache</strong>
<ul>
<li>Fix: redis caching for embedding response models - <a href="https://github.com/BerriAI/litellm/pull/12750" target="_blank" rel="noopener noreferrer">PR #12750</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="helm-chart">Helm Chart<a href="#helm-chart" class="hash-link" aria-label="Helm Chart" title="Helm Chart"></a></h2>
<ul>
<li>DB Migration Hook: refactor to support use_prisma_migrate - for helm hook <a href="https://github.com/BerriAI/litellm/pull/12600" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add envVars and extraEnvVars support to Helm migrations job - <a href="https://github.com/BerriAI/litellm/pull/12591" target="_blank" rel="noopener noreferrer">PR #12591</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-6">Features<a href="#features-6" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Control Plane + Data Plane Architecture</strong>
<ul>
<li>Control Plane + Data Plane support - <a href="https://github.com/BerriAI/litellm/pull/12601" target="_blank" rel="noopener noreferrer">PR #12601</a></li>
</ul>
</li>
<li><strong>Proxy CLI</strong>
<ul>
<li>Add &quot;keys import&quot; command to CLI - <a href="https://github.com/BerriAI/litellm/pull/12620" target="_blank" rel="noopener noreferrer">PR #12620</a></li>
</ul>
</li>
<li><strong>Swagger Documentation</strong>
<ul>
<li>Add swagger docs for LiteLLM /chat/completions, /embeddings, /responses - <a href="https://github.com/BerriAI/litellm/pull/12618" target="_blank" rel="noopener noreferrer">PR #12618</a></li>
</ul>
</li>
<li><strong>Dependencies</strong>
<ul>
<li>Loosen rich version from ==13.7.1 to &gt;=13.7.1 - <a href="https://github.com/BerriAI/litellm/pull/12704" target="_blank" rel="noopener noreferrer">PR #12704</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-6">Bugs<a href="#bugs-6" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li>
<p>Verbose log is enabled by default fix - <a href="https://github.com/BerriAI/litellm/pull/12596" target="_blank" rel="noopener noreferrer">PR #12596</a></p>
</li>
<li>
<p>Add support for disabling callbacks in request body - <a href="https://github.com/BerriAI/litellm/pull/12762" target="_blank" rel="noopener noreferrer">PR #12762</a></p>
</li>
<li>
<p>Handle circular references in spend tracking metadata JSON serialization - <a href="https://github.com/BerriAI/litellm/pull/12643" target="_blank" rel="noopener noreferrer">PR #12643</a></p>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@AntonioKL made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12591" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12591</a></li>
<li>@marcelodiaz558 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12541" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12541</a></li>
<li>@dmcaulay made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12463" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12463</a></li>
<li>@demoray made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12587" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12587</a></li>
<li>@staeiou made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12631" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12631</a></li>
<li>@stefanc-ai2 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12622" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12622</a></li>
<li>@RichardoC made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12607" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12607</a></li>
<li>@yeahyung made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11795" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11795</a></li>
<li>@mnguyen96 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12619" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12619</a></li>
<li>@rgambee made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11517" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11517</a></li>
<li>@jvanmelckebeke made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12725" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12725</a></li>
<li>@jlaurendi made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12704" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12704</a></li>
<li>@doublerr made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12661" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12661</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog"><strong><a href="https://github.com/BerriAI/litellm/compare/v1.74.3-stable...v1.74.7-stable" target="_blank" rel="noopener noreferrer">Full Changelog</a></strong><a href="#full-changelog" class="hash-link" aria-label="full-changelog" title="full-changelog"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-74-3-stable">v1.74.3-stable</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-07-12T10:00:00.000Z">2025712</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.74.3-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.74.3.post1</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>MCP: Model Access Groups</strong> - Add mcp servers to access groups, for easily managing access to users and teams.</li>
<li><strong>MCP: Tool Cost Tracking</strong> - Set prices for each MCP tool.</li>
<li><strong>Model Hub v2</strong> - New OSS Model Hub for telling developers what models are available on the proxy.</li>
<li><strong>Bytez</strong> - New LLM API Provider.</li>
<li><strong>Dashscope API</strong> - Call Alibaba&#x27;s qwen models via new Dashscope API Provider.</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway-model-access-groups">MCP Gateway: Model Access Groups<a href="#mcp-gateway-model-access-groups" class="hash-link" aria-label="MCP Gateway: Model Access Groups" title="MCP Gateway: Model Access Groups"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAqUlEQVR4nFXKPwuCUByF4UtQEATtUUTQ1n9dogRrcAuzoKaw79jUVFNGNOTHqMC6P+Eq9zqc0EHowMO7HMaJwDkHESGOY3w55TiFOeb7PjzPw/v1xPlyg2ZtoVkb9Odr9GYrdE0HHdMBIwoRBB+kOxxPKDV0VNtjVFo6ys0RivUhCrUBWJIkkEplx+v9gclih6ntwrDdvMZyDyalREoplVWICCKK/isi/ADfspbXcoavKQAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/mcp_access_groups.3423360.640.png" srcset="/assets/ideal-img/mcp_access_groups.3423360.640.png 640w,/assets/ideal-img/mcp_access_groups.32a15e4.1920.png 1920w" width="640" height="334"></noscript></div>
<br>
<p>v1.74.3-stable adds support for adding MCP servers to access groups, this makes it <strong>easier for Proxy Admins</strong> to manage access to MCP servers across users and teams.</p>
<p>For <strong>developers</strong>, this means you can now connect to multiple MCP servers by passing the access group name in the <code>x-mcp-servers</code> header.</p>
<p>Read more <a href="https://docs.litellm.ai/docs/mcp#grouping-mcps-access-groups" target="_blank" rel="noopener noreferrer">here</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway-tool-cost-tracking">MCP Gateway: Tool Cost Tracking<a href="#mcp-gateway-tool-cost-tracking" class="hash-link" aria-label="MCP Gateway: Tool Cost Tracking" title="MCP Gateway: Tool Cost Tracking"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAbUlEQVR4nGWOwQrEIAxE/f+/FEQvugtba2oSnSWlLbQNPJjDGyYupQTvPXLO4N7RD0TkhiulIISAGCOICEQbmAXPc2bPOTHG2JsmiuhbPKcMFcVvWVDXepUNy46ZcaJqYkX+fNFoQ22EtdH+yh/eGcMl/Nz5CwAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/mcp_tool_cost_tracking.b437620.640.png" srcset="/assets/ideal-img/mcp_tool_cost_tracking.b437620.640.png 640w,/assets/ideal-img/mcp_tool_cost_tracking.2845538.1920.png 1920w" width="640" height="334"></noscript></div>
<br>
<p>This release adds cost tracking for MCP tool calls. This is great for <strong>Proxy Admins</strong> giving MCP access to developers as you can now attribute MCP tool call costs to specific LiteLLM keys and teams.</p>
<p>You can set:</p>
<ul>
<li><strong>Uniform server cost</strong>: Set a uniform cost for all tools from a server</li>
<li><strong>Individual tool cost</strong>: Define individual costs for specific tools (e.g., search_tool costs $10, get_weather costs $5).</li>
<li><strong>Dynamic costs</strong>: For use cases where you want to set costs based on the MCP&#x27;s response, you can write a custom post mcp call hook to parse responses and set costs dynamically.</li>
</ul>
<p><a href="https://docs.litellm.ai/docs/mcp#mcp-cost-tracking" target="_blank" rel="noopener noreferrer">Get started</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="model-hub-v2">Model Hub v2<a href="#model-hub-v2" class="hash-link" aria-label="Model Hub v2" title="Model Hub v2"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAdklEQVR4nFWOSwrDMAwFff9TJhi8S02bRn/5lWgRqGCYzSCpjTGw7zvmnLiIIaLITETEH23bNvTe8ToOiCrenxOZC/estR4aEYGZCzWrkFggamUWgd8bzQwP7hW7B9wTxIzz/CIi0VS1orI7LglE1mWoWv18+wecusLH1wHu+gAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/model_hub_v2.c583a5a.640.png" srcset="/assets/ideal-img/model_hub_v2.c583a5a.640.png 640w,/assets/ideal-img/model_hub_v2.3fb2d79.1920.png 1920w" width="640" height="334"></noscript></div>
<br>
<p>v1.74.3-stable introduces a new OSS Model Hub for telling developers what models are available on the proxy.</p>
<p>This is great for <strong>Proxy Admins</strong> as you can now tell developers what models are available on the proxy.</p>
<p>This improves on the previous model hub by enabling:</p>
<ul>
<li>The ability to show <strong>Developers</strong> models, even if they don&#x27;t have a LiteLLM key.</li>
<li>The ability for <strong>Proxy Admins</strong> to select specific models to be public on the model hub.</li>
<li>Improved search and filtering capabilities:<!-- -->
<ul>
<li>search for models by partial name (e.g. <code>xai grok-4</code>)</li>
<li>filter by provider and feature (e.g. &#x27;vision&#x27; models)</li>
<li>sort by cost (e.g. cheapest vision model from OpenAI)</li>
</ul>
</li>
</ul>
<p><a href="/docs/proxy/model_hub">Get started</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="pricing--context-window-updates">Pricing / Context Window Updates<a href="#pricing--context-window-updates" class="hash-link" aria-label="Pricing / Context Window Updates" title="Pricing / Context Window Updates"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Type</th></tr></thead><tbody><tr><td>Xai</td><td><code>xai/grok-4</code></td><td>256k</td><td>$3.00</td><td>$15.00</td><td>New</td></tr><tr><td>Xai</td><td><code>xai/grok-4-0709</code></td><td>256k</td><td>$3.00</td><td>$15.00</td><td>New</td></tr><tr><td>Xai</td><td><code>xai/grok-4-latest</code></td><td>256k</td><td>$3.00</td><td>$15.00</td><td>New</td></tr><tr><td>Mistral</td><td><code>mistral/devstral-small-2507</code></td><td>128k</td><td>$0.1</td><td>$0.3</td><td>New</td></tr><tr><td>Mistral</td><td><code>mistral/devstral-medium-2507</code></td><td>128k</td><td>$0.4</td><td>$2</td><td>New</td></tr><tr><td>Azure OpenAI</td><td><code>azure/o3-deep-research</code></td><td>200k</td><td>$10</td><td>$40</td><td>New</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/xinference">Xinference</a></strong>
<ul>
<li>Image generation API support - <a href="https://github.com/BerriAI/litellm/pull/12439" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>
<ul>
<li>API Key Auth support for AWS Bedrock API - <a href="https://github.com/BerriAI/litellm/pull/12495" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/dashscope"> Dashscope</a></strong>
<ul>
<li>New integration from Alibaba (enables qwen usage) - <a href="https://github.com/BerriAI/litellm/pull/12361" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bytez"> Bytez</a></strong>
<ul>
<li>New /chat/completion integration - <a href="https://github.com/BerriAI/litellm/pull/12121" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/providers/github_copilot">Github Copilot</a></strong>
<ul>
<li>Fix API base url for Github Copilot - <a href="https://github.com/BerriAI/litellm/pull/12418" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>
<ul>
<li>Ensure supportedbedrock/converse/params =bedrock/params - <a href="https://github.com/BerriAI/litellm/pull/12466" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix cache token cost calculation - <a href="https://github.com/BerriAI/litellm/pull/12488" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/xai">XAI</a></strong>
<ul>
<li>ensure finish_reason includes tool calls when xai responses with tool calls - <a href="https://github.com/BerriAI/litellm/pull/12545" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/text_completion">/completions</a></strong>
<ul>
<li>Return reasoning_content on streaming - <a href="https://github.com/BerriAI/litellm/pull/12377" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/completion/input">/chat/completions</a></strong>
<ul>
<li>Add &#x27;thinking blocks&#x27; to stream chunk builder - <a href="https://github.com/BerriAI/litellm/pull/12395" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/anthropic_unified">/v1/messages</a></strong>
<ul>
<li>Fallbacks support - <a href="https://github.com/BerriAI/litellm/pull/12440" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>tool call handling for non-anthropic models (/v1/messages to /chat/completion bridge) - <a href="https://github.com/BerriAI/litellm/pull/12473" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway"><a href="/docs/mcp">MCP Gateway</a><a href="#mcp-gateway" class="hash-link" aria-label="mcp-gateway" title="mcp-gateway"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAbUlEQVR4nGWOwQrEIAxE/f+/FEQvugtba2oSnSWlLbQNPJjDGyYupQTvPXLO4N7RD0TkhiulIISAGCOICEQbmAXPc2bPOTHG2JsmiuhbPKcMFcVvWVDXepUNy46ZcaJqYkX+fNFoQ22EtdH+yh/eGcMl/Nz5CwAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/mcp_tool_cost_tracking.b437620.640.png" srcset="/assets/ideal-img/mcp_tool_cost_tracking.b437620.640.png 640w,/assets/ideal-img/mcp_tool_cost_tracking.2845538.1920.png 1920w" width="640" height="334"></noscript></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/mcp#-mcp-cost-tracking">Cost Tracking</a></strong>
<ul>
<li>Add Cost Tracking - <a href="https://github.com/BerriAI/litellm/pull/12385" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add usage tracking - <a href="https://github.com/BerriAI/litellm/pull/12397" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add custom cost configuration for each MCP tool - <a href="https://github.com/BerriAI/litellm/pull/12499" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add support for editing MCP cost per tool - <a href="https://github.com/BerriAI/litellm/pull/12501" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Allow using custom post call MCP hook for cost tracking - <a href="https://github.com/BerriAI/litellm/pull/12469" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/mcp#using-your-mcp-with-client-side-credentials">Auth</a></strong>
<ul>
<li>Allow customizing what client side auth header to use - <a href="https://github.com/BerriAI/litellm/pull/12460" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Raises error when MCP server header is malformed in the request - <a href="https://github.com/BerriAI/litellm/pull/12494" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/mcp#adding-your-mcp">MCP Server</a></strong>
<ul>
<li>Allow using stdio MCPs with LiteLLM (enables using Circle CI MCP w/ LiteLLM) - <a href="https://github.com/BerriAI/litellm/pull/12530" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/mcp#adding-a-stdio-mcp-server">Get Started</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>General</strong>
<ul>
<li>Fix task group is not initialized error - <a href="https://github.com/BerriAI/litellm/pull/12411" target="_blank" rel="noopener noreferrer">PR</a> s/o <a href="https://github.com/juancarlosm" target="_blank" rel="noopener noreferrer">@juancarlosm</a></li>
</ul>
</li>
<li><strong><a href="/docs/mcp#adding-your-mcp">MCP Server</a></strong>
<ul>
<li>Fix mcp tool separator to work with Claude code - <a href="https://github.com/BerriAI/litellm/pull/12430" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/mcp#adding-your-mcp">Get Started</a></li>
<li>Add validation to mcp server name to not allow &quot;-&quot; (enables namespaces to work) - <a href="https://github.com/BerriAI/litellm/pull/12515" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAdklEQVR4nFWOSwrDMAwFff9TJhi8S02bRn/5lWgRqGCYzSCpjTGw7zvmnLiIIaLITETEH23bNvTe8ToOiCrenxOZC/estR4aEYGZCzWrkFggamUWgd8bzQwP7hW7B9wTxIzz/CIi0VS1orI7LglE1mWoWv18+wecusLH1wHu+gAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/model_hub_v2.c583a5a.640.png" srcset="/assets/ideal-img/model_hub_v2.c583a5a.640.png 640w,/assets/ideal-img/model_hub_v2.3fb2d79.1920.png 1920w" width="640" height="334"></noscript></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Model Hub</strong>
<ul>
<li>new model hub table view - <a href="https://github.com/BerriAI/litellm/pull/12468" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>new/public/model_hubendpoint - <a href="https://github.com/BerriAI/litellm/pull/12468" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Make Model Hub OSS - <a href="https://github.com/BerriAI/litellm/pull/12553" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>New make public modal flow for showing proxy models on public model hub - <a href="https://github.com/BerriAI/litellm/pull/12555" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>MCP</strong>
<ul>
<li>support for internal users to use and manage MCP servers - <a href="https://github.com/BerriAI/litellm/pull/12458" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Adds UI support to add MCP access groups (similar to namespaces) - <a href="https://github.com/BerriAI/litellm/pull/12470" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>MCP Tool Testing Playground - <a href="https://github.com/BerriAI/litellm/pull/12520" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Show cost config on root of MCP settings - <a href="https://github.com/BerriAI/litellm/pull/12526" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Test Key</strong>
<ul>
<li>Stick sessions - <a href="https://github.com/BerriAI/litellm/pull/12365" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>MCP Access Groups - allow mcp access groups - <a href="https://github.com/BerriAI/litellm/pull/12529" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Usage</strong>
<ul>
<li>Truncate long labels and improve tooltip in Top API Keys chart - <a href="https://github.com/BerriAI/litellm/pull/12371" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Improve Chart Readability for Tag Usage - <a href="https://github.com/BerriAI/litellm/pull/12378" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Teams</strong>
<ul>
<li>Prevent navigation reset after team member operations - <a href="https://github.com/BerriAI/litellm/pull/12424" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Team Members - reset budget, if duration set - <a href="https://github.com/BerriAI/litellm/pull/12534" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Use central team member budget when max_budget_in_team set on UI - <a href="https://github.com/BerriAI/litellm/pull/12533" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>SSO</strong>
<ul>
<li>Allow users to run a custom sso login handler - <a href="https://github.com/BerriAI/litellm/pull/12465" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Navbar</strong>
<ul>
<li>improve user dropdown UI with premium badge and cleaner layout - <a href="https://github.com/BerriAI/litellm/pull/12502" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>General</strong>
<ul>
<li>Consistent layout for Create and Back buttons on all the pages - <a href="https://github.com/BerriAI/litellm/pull/12542" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Align Show Password with Checkbox - <a href="https://github.com/BerriAI/litellm/pull/12538" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Prevent writing default user setting updates to yaml (causes error in non-root env) - <a href="https://github.com/BerriAI/litellm/pull/12533" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-2">Bugs<a href="#bugs-2" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Model Hub</strong>
<ul>
<li>fix duplicates in/model_group/info- <a href="https://github.com/BerriAI/litellm/pull/12468" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>MCP</strong>
<ul>
<li>Fix UI not syncing MCP access groups properly with object permissions - <a href="https://github.com/BerriAI/litellm/pull/12523" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-4">Features<a href="#features-4" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/observability/langfuse_integration">Langfuse</a></strong>
<ul>
<li>Version bump - <a href="https://github.com/BerriAI/litellm/pull/12376" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>LANGFUSE_TRACING_ENVIRONMENT support - <a href="https://github.com/BerriAI/litellm/pull/12376" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/bedrock">Bedrock Guardrails</a></strong>
<ul>
<li>Raise Bedrock output text on &#x27;BLOCKED&#x27; actions from guardrail - <a href="https://github.com/BerriAI/litellm/pull/12435" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/observability/opentelemetry_integration">OTEL</a></strong>
<ul>
<li><code>OTEL_RESOURCE_ATTRIBUTES</code> support - <a href="https://github.com/BerriAI/litellm/pull/12468" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/guardrails_ai">Guardrails AI</a></strong>
<ul>
<li>pre-call + logging only guardrail (pii detection/competitor names) support - <a href="https://github.com/BerriAI/litellm/pull/12506" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/quick_start">Guardrails</a></strong>
<ul>
<li>[Enterprise] Support tag based mode for guardrails - <a href="https://github.com/BerriAI/litellm/pull/12508" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/proxy/guardrails/quick_start#-tag-based-guardrail-modes">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/openai_moderation">OpenAI Moderations API</a></strong>
<ul>
<li>New guardrail integration - <a href="https://github.com/BerriAI/litellm/pull/12519" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/prometheus">Prometheus</a></strong>
<ul>
<li>support tag based metrics (enables prometheus metrics for measuring roo-code/cline/claude code engagement) - <a href="https://github.com/BerriAI/litellm/pull/12534" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/proxy/prometheus#custom-tags">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/observability/datadog">Datadog LLM Observability</a></strong>
<ul>
<li>Added <code>total_cost</code> field to track costs in DataDog LLM observability metrics - <a href="https://github.com/BerriAI/litellm/pull/12467" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-3">Bugs<a href="#bugs-3" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/prometheus">Prometheus</a></strong>
<ul>
<li>Remove experimental <code>_by_tag</code> metrics (fixes cardinality issue) - <a href="https://github.com/BerriAI/litellm/pull/12395" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/alerting">Slack Alerting</a></strong>
<ul>
<li>Fix slack alerting for outage and region outage alerts - <a href="https://github.com/BerriAI/litellm/pull/12464" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/proxy/alerting#region-outage-alerting--enterprise-feature">Get Started</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-4">Bugs<a href="#bugs-4" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/response_api#calling-non-responses-api-endpoints-responses-to-chatcompletions-bridge">Responses API Bridge</a></strong>
<ul>
<li>add image support for Responses API when falling back on Chat Completions - <a href="https://github.com/BerriAI/litellm/pull/12204" target="_blank" rel="noopener noreferrer">PR</a> s/o <a href="https://github.com/ryan-castner" target="_blank" rel="noopener noreferrer">@ryan-castner</a></li>
</ul>
</li>
<li><strong>aiohttp</strong>
<ul>
<li>Properly close aiohttp client sessions to prevent resource leaks - <a href="https://github.com/BerriAI/litellm/pull/12251" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Router</strong>
<ul>
<li>don&#x27;t add invalid deployment to router pattern match - <a href="https://github.com/BerriAI/litellm/pull/12459" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-5">Bugs<a href="#bugs-5" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>S3</strong>
<ul>
<li>s3 config.yaml file - ensure yaml safe load is used - <a href="https://github.com/BerriAI/litellm/pull/12373" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Audit Logs</strong>
<ul>
<li>Add audit logs for model updates - <a href="https://github.com/BerriAI/litellm/pull/12396" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Startup</strong>
<ul>
<li>Multiple API Keys Created on Startup when max_budget is enabled - <a href="https://github.com/BerriAI/litellm/pull/12436" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Auth</strong>
<ul>
<li>Resolve model group alias on Auth (if user has access to underlying model, allow alias request to work) - <a href="https://github.com/BerriAI/litellm/pull/12440" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>config.yaml</strong>
<ul>
<li>fix parsing environment_variables from config.yaml - <a href="https://github.com/BerriAI/litellm/pull/12482" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Security</strong>
<ul>
<li>Log hashed jwt w/ prefix instead of actual value - <a href="https://github.com/BerriAI/litellm/pull/12524" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-5">Features<a href="#features-5" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>MCP</strong>
<ul>
<li>Bump mcp version on docker img - <a href="https://github.com/BerriAI/litellm/pull/12362" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Request Headers</strong>
<ul>
<li>Forward anthropic-beta header when forward_client_headers_to_llm_api is true - <a href="https://github.com/BerriAI/litellm/pull/12462" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@kanaka made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12418" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12418</a></li>
<li>@juancarlosm made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12411" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12411</a></li>
<li>@DmitriyAlergant made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12356" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12356</a></li>
<li>@Rayshard made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12487" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12487</a></li>
<li>@minghao51 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12361" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12361</a></li>
<li>@jdietzsch91 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12488" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12488</a></li>
<li>@iwinux made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12473" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12473</a></li>
<li>@andresC98 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12413" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12413</a></li>
<li>@EmaSuriano made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12509" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12509</a></li>
<li>@strawgate made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12528" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12528</a></li>
<li>@inf3rnus made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12121" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12121</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="git-diff"><strong><a href="https://github.com/BerriAI/litellm/compare/v1.74.0-stable...v1.74.3-stable" target="_blank" rel="noopener noreferrer">Git Diff</a></strong><a href="#git-diff" class="hash-link" aria-label="git-diff" title="git-diff"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-74-0-stable">v1.74.0-stable</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-07-05T10:00:00.000Z">202575</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.74.0-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.74.0.post2</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>MCP Gateway Namespace Servers</strong> - Clients connecting to LiteLLM can now specify which MCP servers to use.</li>
<li><strong>Key/Team Based Logging on UI</strong> - Proxy Admins can configure team or key-based logging settings directly in the UI.</li>
<li><strong>Azure Content Safety Guardrails</strong> - Added support for prompt injection and text moderation with Azure Content Safety Guardrails.</li>
<li><strong>VertexAI Deepseek Models</strong> - Support for calling VertexAI Deepseek models with LiteLLM&#x27;s/chat/completions or /responses API.</li>
<li><strong>Github Copilot API</strong> - You can now use Github Copilot as an LLM API provider.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-gateway-namespaced-mcp-servers">MCP Gateway: Namespaced MCP Servers<a href="#mcp-gateway-namespaced-mcp-servers" class="hash-link" aria-label="MCP Gateway: Namespaced MCP Servers" title="MCP Gateway: Namespaced MCP Servers"></a></h3>
<p>This release brings support for namespacing MCP Servers on LiteLLM MCP Gateway.  This means you can specify the <code>x-mcp-servers</code> header to specify which servers to list tools from.</p>
<p>This is useful when you want to point MCP clients to specific MCP Servers on LiteLLM.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="usage">Usage<a href="#usage" class="hash-link" aria-label="Usage" title="Usage"></a></h4>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">OpenAI API</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">LiteLLM Proxy</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Cursor IDE</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">cURL Example with Server Segregation</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv codeBlockLinesWithNumbering_o6Pm" style="counter-reset:line-count 0"><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">curl --location &#x27;https://api.openai.com/v1/responses&#x27; \</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">--header &#x27;Content-Type: application/json&#x27; \</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">--header &quot;Authorization: Bearer $OPENAI_API_KEY&quot; \</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">--data &#x27;{</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    &quot;model&quot;: &quot;gpt-4o&quot;,</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    &quot;tools&quot;: [</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">        {</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">            &quot;type&quot;: &quot;mcp&quot;,</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">            &quot;server_label&quot;: &quot;litellm&quot;,</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">            &quot;server_url&quot;: &quot;&lt;your-litellm-proxy-base-url&gt;/mcp&quot;,</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">            &quot;require_approval&quot;: &quot;never&quot;,</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">            &quot;headers&quot;: {</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">                &quot;x-litellm-api-key&quot;: &quot;Bearer YOUR_LITELLM_API_KEY&quot;,</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">                &quot;x-mcp-servers&quot;: &quot;Zapier_Gmail&quot;</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">            }</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">        }</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    ],</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    &quot;input&quot;: &quot;Run available tools&quot;,</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    &quot;tool_choice&quot;: &quot;required&quot;</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">}&#x27;</span></span><br></span></code></pre></div></div><p>In this example, the request will only have access to tools from the &quot;Zapier_Gmail&quot; MCP server.</p></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">cURL Example with Server Segregation</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv codeBlockLinesWithNumbering_o6Pm" style="counter-reset:line-count 0"><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">curl --location &#x27;&lt;your-litellm-proxy-base-url&gt;/v1/responses&#x27; \</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">--header &#x27;Content-Type: application/json&#x27; \</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">--header &quot;Authorization: Bearer $LITELLM_API_KEY&quot; \</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">--data &#x27;{</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    &quot;model&quot;: &quot;gpt-4o&quot;,</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    &quot;tools&quot;: [</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">        {</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">            &quot;type&quot;: &quot;mcp&quot;,</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">            &quot;server_label&quot;: &quot;litellm&quot;,</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">            &quot;server_url&quot;: &quot;&lt;your-litellm-proxy-base-url&gt;/mcp&quot;,</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">            &quot;require_approval&quot;: &quot;never&quot;,</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">            &quot;headers&quot;: {</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">                &quot;x-litellm-api-key&quot;: &quot;Bearer YOUR_LITELLM_API_KEY&quot;,</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">                &quot;x-mcp-servers&quot;: &quot;Zapier_Gmail,Server2&quot;</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">            }</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">        }</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    ],</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    &quot;input&quot;: &quot;Run available tools&quot;,</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    &quot;tool_choice&quot;: &quot;required&quot;</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">}&#x27;</span></span><br></span></code></pre></div></div><p>This configuration restricts the request to only use tools from the specified MCP servers.</p></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">Cursor MCP Configuration with Server Segregation</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv codeBlockLinesWithNumbering_o6Pm" style="counter-reset:line-count 0"><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;mcpServers&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    </span><span class="token property" style="color:#36acaa">&quot;LiteLLM&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">      </span><span class="token property" style="color:#36acaa">&quot;url&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;&lt;your-litellm-proxy-base-url&gt;/mcp&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">      </span><span class="token property" style="color:#36acaa">&quot;headers&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;x-litellm-api-key&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Bearer $LITELLM_API_KEY&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;x-mcp-servers&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Zapier_Gmail,Server2&quot;</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">      </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span></span><br></span></code></pre></div></div><p>This configuration in Cursor IDE settings will limit tool access to only the specified MCP server.</p></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="team--key-based-logging-on-ui">Team / Key Based Logging on UI<a href="#team--key-based-logging-on-ui" class="hash-link" aria-label="Team / Key Based Logging on UI" title="Team / Key Based Logging on UI"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAeklEQVR4nDWOYQrFIAyDvf+JPIoH8NeDxypTZ1trhh0WAoF+TROICq7rQu8dzIwxBlTVvYi43wopJcQYkXP2RWsNe+acMFtY61OotYKIPFFEUcoNVXH4QA6eaH8jCioNQwxqwGAFy3cUdpejDf/+BKoD96N4WGBewfACeGXCYHXE0+UAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/team_key_logging.eb3a91a.640.png" srcset="/assets/ideal-img/team_key_logging.eb3a91a.640.png 640w,/assets/ideal-img/team_key_logging.2e32834.1920.png 1920w" width="640" height="334"></noscript></div>
<br>
<p>This release brings support for Proxy Admins to configure Team/Key Based Logging Settings on the UI. This allows routing LLM request/response logs to different Langfuse/Arize projects based on the team or key.</p>
<p>For developers using LiteLLM, their logs are automatically routed to their specific Arize/Langfuse projects. On this release, we support the following integrations for key/team based logging:</p>
<ul>
<li><code>langfuse</code></li>
<li><code>arize</code></li>
<li><code>langsmith</code></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="azure-content-safety-guardrails">Azure Content Safety Guardrails<a href="#azure-content-safety-guardrails" class="hash-link" aria-label="Azure Content Safety Guardrails" title="Azure Content Safety Guardrails"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAFAAoDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAMI/8QAHRAAAQQCAwAAAAAAAAAAAAAAAAECAxESMhQhM//EABUBAQEAAAAAAAAAAAAAAAAAAAAB/8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEQMRAD8A01jx2ekkluTd1l0k61QAo//Z&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/azure_content_safety_guardrails.5b882f7.640.jpg" srcset="/assets/ideal-img/azure_content_safety_guardrails.5b882f7.640.jpg 640w,/assets/ideal-img/azure_content_safety_guardrails.741f464.1920.jpg 1920w" width="640" height="334"></noscript></div>
<br>
<p>LiteLLM now supports <strong>Azure Content Safety Guardrails</strong> for Prompt Injection and Text Moderation. This is <strong>great for internal chat-ui</strong> use cases, as you can now create guardrails with detection for Azures Harm Categories, specify custom severity thresholds and run them across 100+ LLMs for just that use-case (or across all your calls).</p>
<p><a href="/docs/proxy/guardrails/azure_content_guardrail">Get Started</a></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="python-sdk-23-second-faster-import-times">Python SDK: 2.3 Second Faster Import Times<a href="#python-sdk-23-second-faster-import-times" class="hash-link" aria-label="Python SDK: 2.3 Second Faster Import Times" title="Python SDK: 2.3 Second Faster Import Times"></a></h3>
<p>This release brings significant performance improvements to the Python SDK with 2.3 seconds faster import times. We&#x27;ve refactored the initialization process to reduce startup overhead, making LiteLLM more efficient for applications that need quick initialization. This is a major improvement for applications that need to initialize LiteLLM quickly.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="pricing--context-window-updates">Pricing / Context Window Updates<a href="#pricing--context-window-updates" class="hash-link" aria-label="Pricing / Context Window Updates" title="Pricing / Context Window Updates"></a></h4>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Type</th></tr></thead><tbody><tr><td>Watsonx</td><td><code>watsonx/mistralai/mistral-large</code></td><td>131k</td><td>$3.00</td><td>$10.00</td><td>New</td></tr><tr><td>Azure AI</td><td><code>azure_ai/cohere-rerank-v3.5</code></td><td>4k</td><td>$2.00/1k queries</td><td>-</td><td>New (Rerank)</td></tr></tbody></table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/github_copilot"> GitHub Copilot</a></strong> - Use GitHub Copilot API with LiteLLM - <a href="https://github.com/BerriAI/litellm/pull/12325" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/providers/github_copilot">Get Started</a></li>
<li><strong><a href="/docs/providers/vertex"> VertexAI DeepSeek</a></strong> - Add support for VertexAI DeepSeek models - <a href="https://github.com/BerriAI/litellm/pull/12312" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/providers/vertex_partner#vertexai-deepseek">Get Started</a></li>
<li><strong><a href="/docs/providers/azure_ai">Azure AI</a></strong>
<ul>
<li>Add azure_ai cohere rerank v3.5 - <a href="https://github.com/BerriAI/litellm/pull/12283" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/providers/azure_ai#rerank-endpoint">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex">Vertex AI</a></strong>
<ul>
<li>Add size parameter support for image generation - <a href="https://github.com/BerriAI/litellm/pull/12292" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/providers/vertex_image">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/custom_llm_server">Custom LLM</a></strong>
<ul>
<li>Pass through extra_ properties on &quot;custom&quot; llm provider - <a href="https://github.com/BerriAI/litellm/pull/12185" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/providers/mistral">Mistral</a></strong>
<ul>
<li>Fix transform_response handling for empty string content - <a href="https://github.com/BerriAI/litellm/pull/12202" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Turn Mistral to use llm_http_handler - <a href="https://github.com/BerriAI/litellm/pull/12245" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/gemini">Gemini</a></strong>
<ul>
<li>Fix tool call sequence - <a href="https://github.com/BerriAI/litellm/pull/11999" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix custom api_base path preservation - <a href="https://github.com/BerriAI/litellm/pull/12215" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Fix user_id validation logic - <a href="https://github.com/BerriAI/litellm/pull/11432" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>
<ul>
<li>Support optional args for bedrock - <a href="https://github.com/BerriAI/litellm/pull/12287" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/ollama">Ollama</a></strong>
<ul>
<li>Fix default parameters for ollama-chat - <a href="https://github.com/BerriAI/litellm/pull/12201" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vllm">VLLM</a></strong>
<ul>
<li>Add &#x27;audio_url&#x27; message type support - <a href="https://github.com/BerriAI/litellm/pull/12270" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/batches">/batches</a></strong>
<ul>
<li>Support batch retrieve with target model Query Param - <a href="https://github.com/BerriAI/litellm/pull/12228" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Anthropic completion bridge improvements - <a href="https://github.com/BerriAI/litellm/pull/12228" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/response_api">/responses</a></strong>
<ul>
<li>Azure responses api bridge improvements - <a href="https://github.com/BerriAI/litellm/pull/12224" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix responses api error handling - <a href="https://github.com/BerriAI/litellm/pull/12225" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/mcp">/mcp (MCP Gateway)</a></strong>
<ul>
<li>Add MCP url masking on frontend - <a href="https://github.com/BerriAI/litellm/pull/12247" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add MCP servers header to scope - <a href="https://github.com/BerriAI/litellm/pull/12266" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Litellm mcp tool prefix - <a href="https://github.com/BerriAI/litellm/pull/12289" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Segregate MCP tools on connections using headers - <a href="https://github.com/BerriAI/litellm/pull/12296" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added changes to mcp url wrapping - <a href="https://github.com/BerriAI/litellm/pull/12207" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/anthropic_unified">/v1/messages</a></strong>
<ul>
<li>Remove hardcoded model name on streaming - <a href="https://github.com/BerriAI/litellm/pull/12131" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support lowest latency routing - <a href="https://github.com/BerriAI/litellm/pull/12180" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Non-anthropic models token usage returned - <a href="https://github.com/BerriAI/litellm/pull/12184" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/anthropic_unified">/chat/completions</a></strong>
<ul>
<li>Support Cursor IDE tool_choice format <code>{&quot;type&quot;: &quot;auto&quot;}</code> - <a href="https://github.com/BerriAI/litellm/pull/12168" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/generate_content">/generateContent</a></strong>
<ul>
<li>Allow passing litellm_params - <a href="https://github.com/BerriAI/litellm/pull/12177" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Only pass supported params when using OpenAI models - <a href="https://github.com/BerriAI/litellm/pull/12297" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix using gemini-cli with Vertex Anthropic Models - <a href="https://github.com/BerriAI/litellm/pull/12246" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Streaming</strong>
<ul>
<li>Fix Error code: 307 for LlamaAPI Streaming Chat - <a href="https://github.com/BerriAI/litellm/pull/11946" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Store finish reason even if is_finished - <a href="https://github.com/BerriAI/litellm/pull/12250" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking--budget-improvements">Spend Tracking / Budget Improvements<a href="#spend-tracking--budget-improvements" class="hash-link" aria-label="Spend Tracking / Budget Improvements" title="Spend Tracking / Budget Improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-2">Bugs<a href="#bugs-2" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li>Fix allow strings in calculate cost - <a href="https://github.com/BerriAI/litellm/pull/12200" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>VertexAI Anthropic streaming cost tracking with prompt caching fixes - <a href="https://github.com/BerriAI/litellm/pull/12188" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-3">Bugs<a href="#bugs-3" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Team Management</strong>
<ul>
<li>Prevent team model reset on model add - <a href="https://github.com/BerriAI/litellm/pull/12144" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Return team-only models on /v2/model/info - <a href="https://github.com/BerriAI/litellm/pull/12144" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Render team member budget correctly - <a href="https://github.com/BerriAI/litellm/pull/12144" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>UI Rendering</strong>
<ul>
<li>Fix rendering ui on non-root images - <a href="https://github.com/BerriAI/litellm/pull/12226" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Correctly display &#x27;Internal Viewer&#x27; user role - <a href="https://github.com/BerriAI/litellm/pull/12284" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Configuration</strong>
<ul>
<li>Handle empty config.yaml - <a href="https://github.com/BerriAI/litellm/pull/12189" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix gemini /models - replace models/ as expected - <a href="https://github.com/BerriAI/litellm/pull/12189" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Team Management</strong>
<ul>
<li>Allow adding team specific logging callbacks - <a href="https://github.com/BerriAI/litellm/pull/12261" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add Arize Team Based Logging - <a href="https://github.com/BerriAI/litellm/pull/12264" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Allow Viewing/Editing Team Based Callbacks - <a href="https://github.com/BerriAI/litellm/pull/12265" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>UI Improvements</strong>
<ul>
<li>Comma separated spend and budget display - <a href="https://github.com/BerriAI/litellm/pull/12317" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add logos to callback list - <a href="https://github.com/BerriAI/litellm/pull/12244" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>CLI</strong>
<ul>
<li>Add litellm-proxy cli login for starting to use litellm proxy - <a href="https://github.com/BerriAI/litellm/pull/12216" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Email Templates</strong>
<ul>
<li>Customizable Email template - Subject and Signature - <a href="https://github.com/BerriAI/litellm/pull/12218" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li>Guardrails<!-- -->
<ul>
<li>All guardrails are now supported on the UI - <a href="https://github.com/BerriAI/litellm/pull/12349" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/guardrails/azure_content_safety">Azure Content Safety</a></strong>
<ul>
<li>Add Azure Content Safety Guardrails to LiteLLM proxy - <a href="https://github.com/BerriAI/litellm/pull/12268" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add azure content safety guardrails to the UI - <a href="https://github.com/BerriAI/litellm/pull/12309" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/observability/deepeval_integration">DeepEval</a></strong>
<ul>
<li>Fix DeepEval logging format for failure events - <a href="https://github.com/BerriAI/litellm/pull/12303" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#arize">Arize</a></strong>
<ul>
<li>Add Arize Team Based Logging - <a href="https://github.com/BerriAI/litellm/pull/12264" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#langfuse">Langfuse</a></strong>
<ul>
<li>Langfuse prompt_version support - <a href="https://github.com/BerriAI/litellm/pull/12301" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/observability/sentry">Sentry Integration</a></strong>
<ul>
<li>Add sentry scrubbing - <a href="https://github.com/BerriAI/litellm/pull/12210" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#aws-sqs">AWS SQS Logging</a></strong>
<ul>
<li>New AWS SQS Logging Integration - <a href="https://github.com/BerriAI/litellm/pull/12176" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#s3-buckets">S3 Logger</a></strong>
<ul>
<li>Add failure logging support - <a href="https://github.com/BerriAI/litellm/pull/12299" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/prometheus">Prometheus Metrics</a></strong>
<ul>
<li>Add better error validation for prometheus metrics and labels - <a href="https://github.com/BerriAI/litellm/pull/12182" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-4">Bugs<a href="#bugs-4" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Security</strong>
<ul>
<li>Ensure only LLM API route fails get logged on Langfuse - <a href="https://github.com/BerriAI/litellm/pull/12308" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>OpenMeter</strong>
<ul>
<li>Integration error handling fix - <a href="https://github.com/BerriAI/litellm/pull/12147" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Message Redaction</strong>
<ul>
<li>Ensure message redaction works for responses API logging - <a href="https://github.com/BerriAI/litellm/pull/12291" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Bedrock Guardrails</strong>
<ul>
<li>Fix bedrock guardrails post_call for streaming responses - <a href="https://github.com/BerriAI/litellm/pull/12252" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-4">Features<a href="#features-4" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Python SDK</strong>
<ul>
<li>2 second faster import times - <a href="https://github.com/BerriAI/litellm/pull/12135" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Reduce python sdk import time by .3s - <a href="https://github.com/BerriAI/litellm/pull/12140" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Error Handling</strong>
<ul>
<li>Add error handling for MCP tools not found or invalid server - <a href="https://github.com/BerriAI/litellm/pull/12223" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>SSL/TLS</strong>
<ul>
<li>Fix SSL certificate error - <a href="https://github.com/BerriAI/litellm/pull/12327" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix custom ca bundle support in aiohttp transport - <a href="https://github.com/BerriAI/litellm/pull/12281" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<ul>
<li><strong>Startup</strong>
<ul>
<li>Add new banner on startup - <a href="https://github.com/BerriAI/litellm/pull/12328" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Dependencies</strong>
<ul>
<li>Update pydantic version - <a href="https://github.com/BerriAI/litellm/pull/12213" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@wildcard made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12157" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12157</a></li>
<li>@colesmcintosh made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12168" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12168</a></li>
<li>@seyeong-han made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11946" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11946</a></li>
<li>@dinggh made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12162" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12162</a></li>
<li>@raz-alon made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11432" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11432</a></li>
<li>@tofarr made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12200" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12200</a></li>
<li>@szafranek made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12179" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12179</a></li>
<li>@SamBoyd made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12147" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12147</a></li>
<li>@lizzij made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12219" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12219</a></li>
<li>@cipri-tom made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12201" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12201</a></li>
<li>@zsimjee made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12185" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12185</a></li>
<li>@jroberts2600 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12175" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12175</a></li>
<li>@njbrake made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12202" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12202</a></li>
<li>@NANDINI-star made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12244" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12244</a></li>
<li>@utsumi-fj made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12230" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12230</a></li>
<li>@dcieslak19973 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12283" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12283</a></li>
<li>@hanouticelina made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12286" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12286</a></li>
<li>@lowjiansheng made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11999" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11999</a></li>
<li>@JoostvDoorn made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12281" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12281</a></li>
<li>@takashiishida made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12239" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12239</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="git-diff"><strong><a href="https://github.com/BerriAI/litellm/compare/v1.73.6-stable...v1.74.0-stable" target="_blank" rel="noopener noreferrer">Git Diff</a></strong><a href="#git-diff" class="hash-link" aria-label="git-diff" title="git-diff"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-73-6-stable">v1.73.6-stable</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-06-28T10:00:00.000Z">2025628</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.73.6-stable.patch.1</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.73.6.post1</span><br></span></code></pre></div></div></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="claude-on-gemini-cli">Claude on gemini-cli<a href="#claude-on-gemini-cli" class="hash-link" aria-label="Claude on gemini-cli" title="Claude on gemini-cli"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAyklEQVR4nBXOu2rCUACA4fMsdtCoaM3xhuKltE7NUpVWcJBujtIUFFro5Ogz5RUCLZpDRaygg0IgXiAh4F/yw7f/4ng84XkeQRCw2+5QykGpOQtnzo+z4NtR/C5XCNu2sSyLzd+a8deMbPUZozOh1x4zeBzy2uqRyVYRruuyPxy4nE+MJlNSpS4t442HxoD2XZ+X2hOaVkSEYUgkyjQ/SGgFbvUK8aTkJpEjpkmSqRIievN9H7hivn+SzlQolu/R80102UDmm+RknX++5oT6TmzUggAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="325"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/gemini_cli.31b2727.640.png" srcset="/assets/ideal-img/gemini_cli.31b2727.640.png 640w,/assets/ideal-img/gemini_cli.4d67727.1920.png 1920w" width="640" height="325"></noscript></div>
<br>
<p>This release brings support for using gemini-cli with LiteLLM.</p>
<p>You can use claude-sonnet-4, gemini-2.5-flash (Vertex AI &amp; Google AI Studio), gpt-4.1 and any LiteLLM supported model on gemini-cli.</p>
<p>When you use gemini-cli with LiteLLM you get the following benefits:</p>
<p><strong>Developer Benefits:</strong></p>
<ul>
<li>Universal Model Access: Use any LiteLLM supported model (Anthropic, OpenAI, Vertex AI, Bedrock, etc.) through the gemini-cli interface.</li>
<li>Higher Rate Limits &amp; Reliability: Load balance across multiple models and providers to avoid hitting individual provider limits, with fallbacks to ensure you get responses even if one provider fails.</li>
</ul>
<p><strong>Proxy Admin Benefits:</strong></p>
<ul>
<li>Centralized Management: Control access to all models through a single LiteLLM proxy instance without giving your developers API Keys to each provider.</li>
<li>Budget Controls: Set spending limits and track costs across all gemini-cli usage.</li>
</ul>
<p><a href="/docs/tutorials/litellm_gemini_cli">Get Started</a></p>
<br>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="batch-api-cost-tracking">Batch API Cost Tracking<a href="#batch-api-cost-tracking" class="hash-link" aria-label="Batch API Cost Tracking" title="Batch API Cost Tracking"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAFAAoDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAII/8QAHhAAAgEDBQAAAAAAAAAAAAAAAAECAxETEiJBUpH/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEQMRAD8A05BYdmupPm8pXZeRdX6AB//Z&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/batch_api_cost_tracking.addc090.640.jpg" srcset="/assets/ideal-img/batch_api_cost_tracking.addc090.640.jpg 640w,/assets/ideal-img/batch_api_cost_tracking.a7f7cd9.1920.jpg 1920w" width="640" height="334"></noscript></div>
<br>
<p>v1.73.6 brings cost tracking for <a href="/docs/proxy/managed_batches">LiteLLM Managed Batch API</a> calls to LiteLLM. Previously, this was not being done for Batch API calls using LiteLLM Managed Files. Now, LiteLLM will store the status of each batch call in the DB and poll incomplete batch jobs in the background, emitting a spend log for cost tracking once the batch is complete.</p>
<p>There is no new flag / change needed on your end. Over the next few weeks we hope to extend this to cover batch cost tracking for the Anthropic passthrough as well.</p>
<p><a href="/docs/proxy/managed_batches">Get Started</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="pricing--context-window-updates">Pricing / Context Window Updates<a href="#pricing--context-window-updates" class="hash-link" aria-label="Pricing / Context Window Updates" title="Pricing / Context Window Updates"></a></h3>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Type</th></tr></thead><tbody><tr><td>Azure OpenAI</td><td><code>azure/o3-pro</code></td><td>200k</td><td>$20.00</td><td>$80.00</td><td>New</td></tr><tr><td>OpenRouter</td><td><code>openrouter/mistralai/mistral-small-3.2-24b-instruct</code></td><td>32k</td><td>$0.1</td><td>$0.3</td><td>New</td></tr><tr><td>OpenAI</td><td><code>o3-deep-research</code></td><td>200k</td><td>$10.00</td><td>$40.00</td><td>New</td></tr><tr><td>OpenAI</td><td><code>o3-deep-research-2025-06-26</code></td><td>200k</td><td>$10.00</td><td>$40.00</td><td>New</td></tr><tr><td>OpenAI</td><td><code>o4-mini-deep-research</code></td><td>200k</td><td>$2.00</td><td>$8.00</td><td>New</td></tr><tr><td>OpenAI</td><td><code>o4-mini-deep-research-2025-06-26</code></td><td>200k</td><td>$2.00</td><td>$8.00</td><td>New</td></tr><tr><td>Deepseek</td><td><code>deepseek-r1</code></td><td>65k</td><td>$0.55</td><td>$2.19</td><td>New</td></tr><tr><td>Deepseek</td><td><code>deepseek-v3</code></td><td>65k</td><td>$0.27</td><td>$0.07</td><td>New</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="updated-models">Updated Models<a href="#updated-models" class="hash-link" aria-label="Updated Models" title="Updated Models"></a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/providers/sambanova">Sambanova</a></strong>
<ul>
<li>Handle float timestamps - <a href="https://github.com/BerriAI/litellm/pull/11971" target="_blank" rel="noopener noreferrer">PR</a> s/o <a href="https://github.com/neubig" target="_blank" rel="noopener noreferrer">@neubig</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure">Azure</a></strong>
<ul>
<li>support Azure Authentication method (azure ad token, api keys) on Responses API - <a href="https://github.com/BerriAI/litellm/pull/11941" target="_blank" rel="noopener noreferrer">PR</a> s/o <a href="https://github.com/hsuyuming" target="_blank" rel="noopener noreferrer">@hsuyuming</a></li>
<li>Map image_url str as nested dict - <a href="https://github.com/BerriAI/litellm/pull/12075" target="_blank" rel="noopener noreferrer">PR</a> s/o <a href="https://github.com/davis-featherstone" target="_blank" rel="noopener noreferrer">@davis-featherstone</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/watsonx">Watsonx</a></strong>
<ul>
<li>Set model field to None when model is part of a custom deployment - fixes error raised by WatsonX in those cases - <a href="https://github.com/BerriAI/litellm/pull/11854" target="_blank" rel="noopener noreferrer">PR</a> s/o <a href="https://github.com/cbjuan" target="_blank" rel="noopener noreferrer">@cbjuan</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/perplexity">Perplexity</a></strong>
<ul>
<li>Support web_search_options - <a href="https://github.com/BerriAI/litellm/pull/11983" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support citation token and search queries cost calculation - <a href="https://github.com/BerriAI/litellm/pull/11938" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Null value in usage block handling - <a href="https://github.com/BerriAI/litellm/pull/12068" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Gemini (<a href="/docs/providers/gemini">Google AI Studio</a> + <a href="/docs/providers/vertex">VertexAI</a>)</strong>
<ul>
<li>Only use accepted format values (enum and datetime) - else gemini raises errors - <a href="https://github.com/BerriAI/litellm/pull/11989" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Cache tools if passed alongside cached content (else gemini raises an error) - <a href="https://github.com/BerriAI/litellm/pull/11989" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Json schema translation improvement: Fix unpack_def handling of nested $ref inside anyof items - <a href="https://github.com/BerriAI/litellm/pull/11964" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/mistral">Mistral</a></strong>
<ul>
<li>Fix thinking prompt to match hugging face recommendation - <a href="https://github.com/BerriAI/litellm/pull/12007" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add <code>supports_response_schema: true</code> for all mistral models except codestral-mamba - <a href="https://github.com/BerriAI/litellm/pull/12024" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/ollama">Ollama</a></strong>
<ul>
<li>Fix unnecessary await on embedding calls - <a href="https://github.com/BerriAI/litellm/pull/12024" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/azure">Azure OpenAI</a></strong>
<ul>
<li>Check if o-series model supports reasoning effort (enables drop_params to work for o1 models)</li>
<li>Assistant + tool use cost tracking - <a href="https://github.com/BerriAI/litellm/pull/12045" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/nvidia_nim">Nvidia Nim</a></strong>
<ul>
<li>Add response_format param support - <a href="https://github.com/BerriAI/litellm/pull/12003" target="_blank" rel="noopener noreferrer">PR</a> @shagunb-acn</li>
</ul>
</li>
<li><strong><a href="/docs/providers/elevenlabs">ElevenLabs</a></strong>
<ul>
<li>New STT provider - <a href="https://github.com/BerriAI/litellm/pull/12119" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><a href="/docs/mcp"><strong>/mcp</strong></a>
<ul>
<li>Send appropriate auth string value to <code>/tool/call</code> endpoint with <code>x-mcp-auth</code> - <a href="https://github.com/BerriAI/litellm/pull/11968" target="_blank" rel="noopener noreferrer">PR</a> s/o <a href="https://github.com/wagnerjt" target="_blank" rel="noopener noreferrer">@wagnerjt</a></li>
</ul>
</li>
<li><a href="/docs/anthropic_unified"><strong>/v1/messages</strong></a>
<ul>
<li><a href="/docs/providers/custom_llm_server#anthropic-v1messages">Custom LLM</a> support - <a href="https://github.com/BerriAI/litellm/pull/12016" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><a href="/docs/completion/input"><strong>/chat/completions</strong></a>
<ul>
<li>Azure Responses API via chat completion support - <a href="https://github.com/BerriAI/litellm/pull/12016" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><a href="/docs/response_api"><strong>/responses</strong></a>
<ul>
<li>Add reasoning content support for non-openai providers - <a href="https://github.com/BerriAI/litellm/pull/12055" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>[NEW] /generateContent</strong>
<ul>
<li>New endpoints for gemini cli support - <a href="https://github.com/BerriAI/litellm/pull/12040" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support calling Google AI Studio / VertexAI Gemini models in their native format - <a href="https://github.com/BerriAI/litellm/pull/12046" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add logging + cost tracking for stream + non-stream vertex/google ai studio routes - <a href="https://github.com/BerriAI/litellm/pull/12058" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add Bridge from generateContent to /chat/completions - <a href="https://github.com/BerriAI/litellm/pull/12081" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><a href="/docs/batches"><strong>/batches</strong></a>
<ul>
<li>Filter deployments to only those where managed file was written to - <a href="https://github.com/BerriAI/litellm/pull/12048" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Save all model / file id mappings in db (previously it was just the first one) - enables true loadbalancing - <a href="https://github.com/BerriAI/litellm/pull/12048" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support List Batches with target model name specified - <a href="https://github.com/BerriAI/litellm/pull/12049" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking--budget-improvements">Spend Tracking / Budget Improvements<a href="#spend-tracking--budget-improvements" class="hash-link" aria-label="Spend Tracking / Budget Improvements" title="Spend Tracking / Budget Improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><a href="/docs/pass_through"><strong>Passthrough</strong></a>
<ul>
<li><a href="/docs/pass_through/bedrock">Bedrock</a> - cost tracking (<code>/invoke</code> + <code>/converse</code> routes) on streaming + non-streaming - <a href="https://github.com/BerriAI/litellm/pull/12123" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><a href="/docs/pass_through/vertex_ai">VertexAI</a> - anthropic cost calculation support - <a href="https://github.com/BerriAI/litellm/pull/11992" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><a href="/docs/batches"><strong>Batches</strong></a>
<ul>
<li>Background job for cost tracking LiteLLM Managed batches - <a href="https://github.com/BerriAI/litellm/pull/12125" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>General UI</strong>
<ul>
<li>Fix today selector date mutation in dashboard components - <a href="https://github.com/BerriAI/litellm/pull/12042" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Usage</strong>
<ul>
<li>Aggregate usage data across all pages of paginated endpoint - <a href="https://github.com/BerriAI/litellm/pull/12033" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Teams</strong>
<ul>
<li>De-duplicate models in team settings dropdown - <a href="https://github.com/BerriAI/litellm/pull/12074" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Models</strong>
<ul>
<li>Preserve public model name when selecting test connect with azure model (previously would reset) - <a href="https://github.com/BerriAI/litellm/pull/11713" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Invitation Links</strong>
<ul>
<li>Ensure Invite links email contain the correct invite id when using tf provider - <a href="https://github.com/BerriAI/litellm/pull/12130" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Models</strong>
<ul>
<li>Add last success column to health check table - <a href="https://github.com/BerriAI/litellm/pull/11903" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>MCP</strong>
<ul>
<li>New UI component to support auth types: api key, bearer token, basic auth - <a href="https://github.com/BerriAI/litellm/pull/11968" target="_blank" rel="noopener noreferrer">PR</a> s/o <a href="https://github.com/wagnerjt" target="_blank" rel="noopener noreferrer">@wagnerjt</a></li>
<li>Ensure internal users can access /mcp and /mcp/ routes - <a href="https://github.com/BerriAI/litellm/pull/12106" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>SCIM</strong>
<ul>
<li>Ensure default_internal_user_params are applied for new users - <a href="https://github.com/BerriAI/litellm/pull/12015" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Team</strong>
<ul>
<li>Support default key expiry for team member keys - <a href="https://github.com/BerriAI/litellm/pull/12023" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Expand team member add check to cover user email - <a href="https://github.com/BerriAI/litellm/pull/12082" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>UI</strong>
<ul>
<li>Restrict UI access by SSO group - <a href="https://github.com/BerriAI/litellm/pull/12023" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Keys</strong>
<ul>
<li>Add newnew_keyparam for regenerating key - <a href="https://github.com/BerriAI/litellm/pull/12087" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Test Keys</strong>
<ul>
<li>New get code button for getting runnable python code snippet based on ui configuration - <a href="https://github.com/BerriAI/litellm/pull/11629" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-2">Bugs<a href="#bugs-2" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Braintrust</strong>
<ul>
<li>Adds model to metadata to enable braintrust cost estimation - <a href="https://github.com/BerriAI/litellm/pull/12022" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-4">Features<a href="#features-4" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Callbacks</strong>
<ul>
<li>(Enterprise) - disable logging callbacks in request headers - <a href="https://github.com/BerriAI/litellm/pull/11985" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add List Callbacks API Endpoint - <a href="https://github.com/BerriAI/litellm/pull/11987" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Bedrock Guardrail</strong>
<ul>
<li>Don&#x27;t raise exception on intervene action - <a href="https://github.com/BerriAI/litellm/pull/11875" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Ensure PII Masking is applied on response streaming or non streaming content when using post call- <a href="https://github.com/BerriAI/litellm/pull/12086" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>[NEW] Palo Alto Networks Prisma AIRS Guardrail</strong>
<ul>
<li><a href="https://github.com/BerriAI/litellm/pull/12116" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>ElasticSearch</strong>
<ul>
<li>New Elasticsearch Logging Tutorial - <a href="https://github.com/BerriAI/litellm/pull/11761" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Message Redaction</strong>
<ul>
<li>Preserve usage / model information  for Embedding redaction - <a href="https://github.com/BerriAI/litellm/pull/12088" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-3">Bugs<a href="#bugs-3" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>Team-only models</strong>
<ul>
<li>Filter team-only models from routing logic for non-team calls</li>
</ul>
</li>
<li><strong>Context Window Exceeded error</strong>
<ul>
<li>Catch anthropic exceptions - <a href="https://github.com/BerriAI/litellm/pull/12113" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-5">Features<a href="#features-5" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Router</strong>
<ul>
<li>allow using dynamic cooldown time for a specific deployment - <a href="https://github.com/BerriAI/litellm/pull/12037" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>handle cooldown_time = 0 for deployments - <a href="https://github.com/BerriAI/litellm/pull/12108" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Redis</strong>
<ul>
<li>Add better debugging to see what variables are set - <a href="https://github.com/BerriAI/litellm/pull/12073" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-4">Bugs<a href="#bugs-4" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>aiohttp</strong>
<ul>
<li>Check HTTP_PROXY vars in networking requests</li>
<li>Allow using HTTP_ Proxy settings with trust_env</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-6">Features<a href="#features-6" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Docs</strong>
<ul>
<li>Add recommended spec - <a href="https://github.com/BerriAI/litellm/pull/11980" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Swagger</strong>
<ul>
<li>Introduce new environment variable NO_REDOC to opt-out Redoc - <a href="https://github.com/BerriAI/litellm/pull/12092" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@mukesh-dream11 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11969" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11969</a></li>
<li>@cbjuan made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11854" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11854</a></li>
<li>@ryan-castner made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12055" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12055</a></li>
<li>@davis-featherstone made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12075" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12075</a></li>
<li>@Gum-Joe made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12068" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12068</a></li>
<li>@jroberts2600 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12116" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12116</a></li>
<li>@ohmeow made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12022" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12022</a></li>
<li>@amarrella made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11942" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11942</a></li>
<li>@zhangyoufu made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12092" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12092</a></li>
<li>@bougou made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12088" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12088</a></li>
<li>@codeugar made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11972" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11972</a></li>
<li>@glgh made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/12133" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/12133</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="git-diff"><strong><a href="https://github.com/BerriAI/litellm/compare/v1.73.0-stable...v1.73.6.rc-draft" target="_blank" rel="noopener noreferrer">Git Diff</a></strong><a href="#git-diff" class="hash-link" aria-label="git-diff" title="git-diff"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-73-0-stable">v1.73.0-stable - Set default team for new users</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-06-21T10:00:00.000Z">2025621</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span></div><div class="admonitionContent_BuS1"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="known-issues">Known Issues<a href="#known-issues" class="hash-link" aria-label="Known Issues" title="Known Issues"></a></h2><p>The <code>non-root</code> docker image has a known issue around the UI not loading. If you use the <code>non-root</code> docker image we recommend waiting before upgrading to this version. We will post a patch fix for this.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:v1.73.0-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.73.0.post1</span><br></span></code></pre></div></div></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="tldr">TLDR<a href="#tldr" class="hash-link" aria-label="TLDR" title="TLDR"></a></h2>
<ul>
<li><strong>Why Upgrade</strong>
<ul>
<li>User Management: Set default team for new users - enables giving all users $10 API keys for exploration.</li>
<li>Passthrough Endpoints v2: Enhanced support for subroutes and custom cost tracking for passthrough endpoints.</li>
<li>Health Check Dashboard: New frontend UI for monitoring model health and status.</li>
</ul>
</li>
<li><strong>Who Should Read</strong>
<ul>
<li>Teams using <strong>Passthrough Endpoints</strong></li>
<li>Teams using <strong>User Management</strong> on LiteLLM</li>
<li>Teams using <strong>Health Check Dashboard</strong> for models</li>
<li>Teams using <strong>Claude Code</strong> with LiteLLM</li>
</ul>
</li>
<li><strong>Risk of Upgrade</strong>
<ul>
<li><strong>Low</strong>
<ul>
<li>No major breaking changes to existing functionality.</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><strong>Major Changes</strong>
<ul>
<li><code>User Agent</code> will be auto-tracked as a tag in LiteLLM UI Logs Page. This means for all LLM requests you will see a <code>User Agent</code> tag in the logs page.</li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="set-default-team-for-new-users">Set Default Team for New Users<a href="#set-default-team-for-new-users" class="hash-link" aria-label="Set Default Team for New Users" title="Set Default Team for New Users"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAFAAoDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAMI/8QAGxAAAgMAAwAAAAAAAAAAAAAAAAECESEDQmH/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEQMRAD8A1DxQUFjk97OyteIAD//Z&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/default_teams_product_ss.fcca7c4.640.jpg" srcset="/assets/ideal-img/default_teams_product_ss.fcca7c4.640.jpg 640w,/assets/ideal-img/default_teams_product_ss.388f27e.1920.jpg 1920w" width="640" height="334"></noscript></div>
<br>
<p>v1.73.0 introduces the ability to assign new users to Default Teams. This makes it much easier to enable experimentation with LLMs within your company, while also <strong>ensuring spend for exploration is tracked correctly.</strong></p>
<p>What this means for <strong>Proxy Admins</strong>:</p>
<ul>
<li>Set a max budget per team member: This sets a max amount an individual can spend within a team.</li>
<li>Set a default team for new users: When a new user signs in via SSO / invitation link, they will be automatically added to this team.</li>
</ul>
<p>What this means for <strong>Developers</strong>:</p>
<ul>
<li>View models across teams: You can now go to <code>Models + Endpoints</code> and view the models you have access to, across all teams you&#x27;re a member of.</li>
<li>Safe create key modal: If you have no model access outside of a team (default behaviour), you are now nudged to select a team on the Create Key modal. This resolves a common confusion point for new users onboarding to the proxy.</li>
</ul>
<p><a href="https://docs.litellm.ai/docs/tutorials/default_team_self_serve" target="_blank" rel="noopener noreferrer">Get Started</a></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="passthrough-endpoints-v2">Passthrough Endpoints v2<a href="#passthrough-endpoints-v2" class="hash-link" aria-label="Passthrough Endpoints v2" title="Passthrough Endpoints v2"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAcUlEQVR4nFWOQQrEIAxFc//reQMXQouzsaZN1PiHWGSYwCOL//MI5fzBcZzIOeO6KlQbVBWttUXvfUExRoQQkFJCrRXM9yqOMWBm2EPMjFIKRGSFzyPLtGfOF9p6325yo4hizvkHebh5//KjDjMv/IxfkkfDFF9Kco0AAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/v2_pt.287ba0a.640.png" srcset="/assets/ideal-img/v2_pt.287ba0a.640.png 640w,/assets/ideal-img/v2_pt.59ee958.1920.png 1920w" width="640" height="334"></noscript></div>
<br>
<p>This release brings support for adding billing and full URL forwarding for passthrough endpoints.</p>
<p>Previously, you could only map simple endpoints, but now you can add just <code>/bria</code> and all subroutes automatically get forwarded - for example, <code>/bria/v1/text-to-image/base/model</code> and <code>/bria/v1/enhance_image</code> will both be forwarded to the target URL with the same path structure.</p>
<p>This means you as Proxy Admin can onboard third-party endpoints like Bria API and Mistral OCR, set a cost per request, and give your developers access to the complete API functionality.</p>
<p><a href="/docs/proxy/pass_through">Learn more about Passthrough Endpoints</a></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="v2-health-checks">v2 Health Checks<a href="#v2-health-checks" class="hash-link" aria-label="v2 Health Checks" title="v2 Health Checks"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAGCAYAAAD68A/GAAAACXBIWXMAACxLAAAsSwGlPZapAAAAk0lEQVR4nGWPwY7DIAwF+f+/zCE5tMlKWzDGNmQqqNRLLb2LPZonp8fzyXEcXNeFiBARuPtP0nmebNvGvu/knInev0ebMaeZk6ZBqlKk4h54M8KD0QfhA22DZoOkzZA2KEUW+Mr/SMmENrgHIhXVRpoV476XtXvnCqHEhFhTq35AM1sP5FyQIvxpJqvg5ms/m6b1Dc6b6VXU6JajAAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="364"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/v2_health.8bf7c55.640.png" srcset="/assets/ideal-img/v2_health.8bf7c55.640.png 640w,/assets/ideal-img/v2_health.1f07b7f.1920.png 1920w" width="640" height="364"></noscript></div>
<br>
<p>This release brings support for Proxy Admins to select which specific models to health check and see the health status as soon as its individual check completes, along with last check times.</p>
<p>This allows Proxy Admins to immediately identify which specific models are in a bad state and view the full error stack trace for faster troubleshooting.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new--updated-models">New / Updated Models<a href="#new--updated-models" class="hash-link" aria-label="New / Updated Models" title="New / Updated Models"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="pricing--context-window-updates">Pricing / Context Window Updates<a href="#pricing--context-window-updates" class="hash-link" aria-label="Pricing / Context Window Updates" title="Pricing / Context Window Updates"></a></h3>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Type</th></tr></thead><tbody><tr><td>Google VertexAI</td><td><code>vertex_ai/imagen-4</code></td><td>N/A</td><td>Image Generation</td><td>Image Generation</td><td>New</td></tr><tr><td>Google VertexAI</td><td><code>vertex_ai/imagen-4-preview</code></td><td>N/A</td><td>Image Generation</td><td>Image Generation</td><td>New</td></tr><tr><td>Gemini</td><td><code>gemini-2.5-pro</code></td><td>2M</td><td>$1.25</td><td>$5.00</td><td>New</td></tr><tr><td>Gemini</td><td><code>gemini-2.5-flash-lite</code></td><td>1M</td><td>$0.075</td><td>$0.30</td><td>New</td></tr><tr><td>OpenRouter</td><td>Various models</td><td>Updated</td><td>Updated</td><td>Updated</td><td>Updated</td></tr><tr><td>Azure</td><td><code>azure/o3</code></td><td>200k</td><td>$2.00</td><td>$8.00</td><td>Updated</td></tr><tr><td>Azure</td><td><code>azure/o3-pro</code></td><td>200k</td><td>$2.00</td><td>$8.00</td><td>Updated</td></tr><tr><td>Azure OpenAI</td><td>Azure Codex Models</td><td>Various</td><td>Various</td><td>Various</td><td>New</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="updated-models">Updated Models<a href="#updated-models" class="hash-link" aria-label="Updated Models" title="Updated Models"></a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/azure">Azure</a></strong>
<ul>
<li>Support for new /v1 preview Azure OpenAI API - <a href="https://github.com/BerriAI/litellm/pull/11934" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/providers/azure/azure_responses#azure-codex-models">Get Started</a></li>
<li>Add Azure Codex Models support - <a href="https://github.com/BerriAI/litellm/pull/11934" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/providers/azure/azure_responses#azure-codex-models">Get Started</a></li>
<li>Make Azure AD scope configurable - <a href="https://github.com/BerriAI/litellm/pull/11621" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Handle more GPT custom naming patterns - <a href="https://github.com/BerriAI/litellm/pull/11914" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Update o3 pricing to match OpenAI pricing - <a href="https://github.com/BerriAI/litellm/pull/11937" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex">VertexAI</a></strong>
<ul>
<li>Add Vertex Imagen-4 models - <a href="https://github.com/BerriAI/litellm/pull/11767" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/providers/vertex_image">Get Started</a></li>
<li>Anthropic streaming passthrough cost tracking - <a href="https://github.com/BerriAI/litellm/pull/11734" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/gemini">Gemini</a></strong>
<ul>
<li>Working Gemini TTS support via <code>/v1/speech</code> endpoint - <a href="https://github.com/BerriAI/litellm/pull/11832" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix gemini 2.5 flash config - <a href="https://github.com/BerriAI/litellm/pull/11830" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add missing <code>flash-2.5-flash-lite</code> model and fix pricing - <a href="https://github.com/BerriAI/litellm/pull/11901" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Mark all gemini-2.5 models as supporting PDF input - <a href="https://github.com/BerriAI/litellm/pull/11907" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add <code>gemini-2.5-pro</code> with reasoning support - <a href="https://github.com/BerriAI/litellm/pull/11927" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">AWS Bedrock</a></strong>
<ul>
<li>AWS credentials no longer mandatory - <a href="https://github.com/BerriAI/litellm/pull/11765" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add AWS Bedrock profiles for APAC region - <a href="https://github.com/BerriAI/litellm/pull/11883" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix AWS Bedrock Claude tool call index - <a href="https://github.com/BerriAI/litellm/pull/11842" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Handle base64 file data with <code>qs:..</code> prefix - <a href="https://github.com/BerriAI/litellm/pull/11908" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add Mistral Small to BEDROCK_CONVERSE_MODELS - <a href="https://github.com/BerriAI/litellm/pull/11760" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/mistral">Mistral</a></strong>
<ul>
<li>Enhance Mistral API with parallel tool calls support - <a href="https://github.com/BerriAI/litellm/pull/11770" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/meta_llama">Meta Llama API</a></strong>
<ul>
<li>Enable tool calling for meta_llama models - <a href="https://github.com/BerriAI/litellm/pull/11895" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/volcengine">Volcengine</a></strong>
<ul>
<li>Add thinking parameter support - <a href="https://github.com/BerriAI/litellm/pull/11914" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/providers/vertex">VertexAI</a></strong>
<ul>
<li>Handle missing tokenCount in promptTokensDetails - <a href="https://github.com/BerriAI/litellm/pull/11896" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix vertex AI claude thinking params - <a href="https://github.com/BerriAI/litellm/pull/11796" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/gemini">Gemini</a></strong>
<ul>
<li>Fix web search error with responses API - <a href="https://github.com/BerriAI/litellm/pull/11894" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/completion/web_search#responses-litellmresponses">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/custom_llm_server">Custom LLM</a></strong>
<ul>
<li>Set anthropic custom LLM provider property - <a href="https://github.com/BerriAI/litellm/pull/11907" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Bump anthropic package version - <a href="https://github.com/BerriAI/litellm/pull/11851" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/ollama">Ollama</a></strong>
<ul>
<li>Update ollama_embeddings to work on sync API - <a href="https://github.com/BerriAI/litellm/pull/11746" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix response_format not working - <a href="https://github.com/BerriAI/litellm/pull/11880" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/response_api">Responses API</a></strong>
<ul>
<li>Day-0 support for OpenAI re-usable prompts Responses API - <a href="https://github.com/BerriAI/litellm/pull/11782" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/providers/openai/responses_api#reusable-prompts">Get Started</a></li>
<li>Support passing image URLs in Completion-to-Responses bridge - <a href="https://github.com/BerriAI/litellm/pull/11833" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/mcp">MCP Gateway</a></strong>
<ul>
<li>Add Allowed MCPs to Creating/Editing Organizations - <a href="https://github.com/BerriAI/litellm/pull/11893" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/mcp#-mcp-permission-management">Get Started</a></li>
<li>Allow connecting to MCP with authentication headers - <a href="https://github.com/BerriAI/litellm/pull/11891" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/mcp#using-your-mcp-with-client-side-credentials">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/speech">Speech API</a></strong>
<ul>
<li>Working Gemini TTS support via OpenAI&#x27;s <code>/v1/speech</code> endpoint - <a href="https://github.com/BerriAI/litellm/pull/11832" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/pass_through">Passthrough Endpoints</a></strong>
<ul>
<li>Add support for subroutes for passthrough endpoints - <a href="https://github.com/BerriAI/litellm/pull/11827" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support for setting custom cost per passthrough request - <a href="https://github.com/BerriAI/litellm/pull/11870" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Ensure &quot;Request&quot; is tracked for passthrough requests on LiteLLM Proxy - <a href="https://github.com/BerriAI/litellm/pull/11873" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add V2 Passthrough endpoints on UI - <a href="https://github.com/BerriAI/litellm/pull/11905" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Move passthrough endpoints under Models + Endpoints in UI - <a href="https://github.com/BerriAI/litellm/pull/11871" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>QA improvements for adding passthrough endpoints - <a href="https://github.com/BerriAI/litellm/pull/11909" target="_blank" rel="noopener noreferrer">PR</a>, <a href="https://github.com/BerriAI/litellm/pull/11939" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/completion/model_alias">Models API</a></strong>
<ul>
<li>Allow <code>/models</code> to return correct models for custom wildcard prefixes - <a href="https://github.com/BerriAI/litellm/pull/11784" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/anthropic_unified">Messages API</a></strong>
<ul>
<li>Fix <code>/v1/messages</code> endpoint always using us-central1 with vertex_ai-anthropic models - <a href="https://github.com/BerriAI/litellm/pull/11831" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix model_group tracking for <code>/v1/messages</code> and <code>/moderations</code> - <a href="https://github.com/BerriAI/litellm/pull/11933" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix cost tracking and logging via <code>/v1/messages</code> API when using Claude Code - <a href="https://github.com/BerriAI/litellm/pull/11928" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/mcp">MCP Gateway</a></strong>
<ul>
<li>Fix using MCPs defined on config.yaml - <a href="https://github.com/BerriAI/litellm/pull/11824" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/completion/input">Chat Completion API</a></strong>
<ul>
<li>Allow dict for tool_choice argument in acompletion - <a href="https://github.com/BerriAI/litellm/pull/11860" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/pass_through/langfuse">Passthrough Endpoints</a></strong>
<ul>
<li>Don&#x27;t log request to Langfuse passthrough on Langfuse - <a href="https://github.com/BerriAI/litellm/pull/11768" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking">Spend Tracking<a href="#spend-tracking" class="hash-link" aria-label="Spend Tracking" title="Spend Tracking"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/cost_tracking">User Agent Tracking</a></strong>
<ul>
<li>Automatically track spend by user agent (allows cost tracking for Claude Code) - <a href="https://github.com/BerriAI/litellm/pull/11781" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add user agent tags in spend logs payload - <a href="https://github.com/BerriAI/litellm/pull/11872" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/cost_tracking">Tag Management</a></strong>
<ul>
<li>Support adding public model names in tag management - <a href="https://github.com/BerriAI/litellm/pull/11908" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Test Key Page</strong>
<ul>
<li>Allow testing <code>/v1/messages</code> on the Test Key Page - <a href="https://github.com/BerriAI/litellm/pull/11930" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/sso">SSO</a></strong>
<ul>
<li>Allow passing additional headers - <a href="https://github.com/BerriAI/litellm/pull/11781" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/jwt_auth">JWT Auth</a></strong>
<ul>
<li>Correctly return user email - <a href="https://github.com/BerriAI/litellm/pull/11783" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/model_management">Model Management</a></strong>
<ul>
<li>Allow editing model access group for existing model - <a href="https://github.com/BerriAI/litellm/pull/11783" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/team_management">Team Management</a></strong>
<ul>
<li>Allow setting default team for new users - <a href="https://github.com/BerriAI/litellm/pull/11874" target="_blank" rel="noopener noreferrer">PR</a>, <a href="https://github.com/BerriAI/litellm/pull/11877" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix default team settings - <a href="https://github.com/BerriAI/litellm/pull/11887" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/scim">SCIM</a></strong>
<ul>
<li>Add error handling for existing user on SCIM - <a href="https://github.com/BerriAI/litellm/pull/11862" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add SCIM PATCH and PUT operations for users - <a href="https://github.com/BerriAI/litellm/pull/11863" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Health Check Dashboard</strong>
<ul>
<li>Implement health check backend API and storage functionality - <a href="https://github.com/BerriAI/litellm/pull/11852" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add LiteLLM_HealthCheckTable to database schema - <a href="https://github.com/BerriAI/litellm/pull/11677" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Implement health check frontend UI components and dashboard integration - <a href="https://github.com/BerriAI/litellm/pull/11679" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add success modal for health check responses - <a href="https://github.com/BerriAI/litellm/pull/11899" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix clickable model ID in health check table - <a href="https://github.com/BerriAI/litellm/pull/11898" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix health check UI table design - <a href="https://github.com/BerriAI/litellm/pull/11897" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrails-integrations">Logging / Guardrails Integrations<a href="#logging--guardrails-integrations" class="hash-link" aria-label="Logging / Guardrails Integrations" title="Logging / Guardrails Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-2">Bugs<a href="#bugs-2" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/observability/prometheus">Prometheus</a></strong>
<ul>
<li>Fix bug for using prometheus metrics config - <a href="https://github.com/BerriAI/litellm/pull/11779" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="security--reliability">Security &amp; Reliability<a href="#security--reliability" class="hash-link" aria-label="Security &amp; Reliability" title="Security &amp; Reliability"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="security-fixes">Security Fixes<a href="#security-fixes" class="hash-link" aria-label="Security Fixes" title="Security Fixes"></a></h4>
<ul>
<li><strong><a href="/docs">Documentation Security</a></strong>
<ul>
<li>Security fixes for docs - <a href="https://github.com/BerriAI/litellm/pull/11776" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add Trivy Security Scan for UI + Docs folder - remove all vulnerabilities - <a href="https://github.com/BerriAI/litellm/pull/11778" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="reliability-improvements">Reliability Improvements<a href="#reliability-improvements" class="hash-link" aria-label="Reliability Improvements" title="Reliability Improvements"></a></h4>
<ul>
<li><strong><a href="/docs">Dependencies</a></strong>
<ul>
<li>Fix aiohttp version requirement - <a href="https://github.com/BerriAI/litellm/pull/11777" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Bump next from 14.2.26 to 14.2.30 in UI dashboard - <a href="https://github.com/BerriAI/litellm/pull/11720" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs">Networking</a></strong>
<ul>
<li>Allow using CA Bundles - <a href="https://github.com/BerriAI/litellm/pull/11906" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add workload identity federation between GCP and AWS - <a href="https://github.com/BerriAI/litellm/pull/10210" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-4">Features<a href="#features-4" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/deploy">Deployment</a></strong>
<ul>
<li>Add deployment annotations for Kubernetes - <a href="https://github.com/BerriAI/litellm/pull/11849" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add ciphers in command and pass to hypercorn for proxy - <a href="https://github.com/BerriAI/litellm/pull/11916" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/deploy">Custom Root Path</a></strong>
<ul>
<li>Fix loading UI on custom root path - <a href="https://github.com/BerriAI/litellm/pull/11912" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/reliability">SDK Improvements</a></strong>
<ul>
<li>LiteLLM SDK / Proxy improvement (don&#x27;t transform message client-side) - <a href="https://github.com/BerriAI/litellm/pull/11908" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-3">Bugs<a href="#bugs-3" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/observability">Observability</a></strong>
<ul>
<li>Fix boto3 tracer wrapping for observability - <a href="https://github.com/BerriAI/litellm/pull/11869" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@kjoth made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11621" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>@shagunb-acn made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11760" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>@MadsRC made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11765" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>@Abiji-2020 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11746" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>@salzubi401 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11803" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>@orolega made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11826" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>@X4tar made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11796" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>@karen-veigas made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11858" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>@Shankyg made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11859" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>@pascallim made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10210" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>@lgruen-vcgs made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11883" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>@rinormaloku made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11851" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>@InvisibleMan1306 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11849" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>@ervwalter made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11937" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>@ThakeeNathees made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11880" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>@jnhyperion made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11842" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>@Jannchie made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11860" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="demo-instance">Demo Instance<a href="#demo-instance" class="hash-link" aria-label="Demo Instance" title="Demo Instance"></a></h2>
<p>Here&#x27;s a Demo Instance to test changes:</p>
<ul>
<li>Instance: <a href="https://demo.litellm.ai/" target="_blank" rel="noopener noreferrer">https://demo.litellm.ai/</a></li>
<li>Login Credentials:<!-- -->
<ul>
<li>Username: admin</li>
<li>Password: sk-1234</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="git-diff"><a href="https://github.com/BerriAI/litellm/compare/v1.72.6-stable...v1.73.0.rc" target="_blank" rel="noopener noreferrer">Git Diff</a><a href="#git-diff" class="hash-link" aria-label="git-diff" title="git-diff"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-72-6-stable">v1.72.6-stable - MCP Gateway Permission Management</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-06-14T10:00:00.000Z">2025614</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:main-v1.72.6-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.72.6.post2</span><br></span></code></pre></div></div></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="tldr">TLDR<a href="#tldr" class="hash-link" aria-label="TLDR" title="TLDR"></a></h2>
<ul>
<li><strong>Why Upgrade</strong>
<ul>
<li>Codex-mini on Claude Code: You can now use <code>codex-mini</code> (OpenAIs code assistant model) via Claude Code.</li>
<li>MCP Permissions Management: Manage permissions for MCP Servers by Keys, Teams, Organizations (entities) on LiteLLM.</li>
<li>UI: Turn on/off auto refresh on logs view.</li>
<li>Rate Limiting: Support for output token-only rate limiting.</li>
</ul>
</li>
<li><strong>Who Should Read</strong>
<ul>
<li>Teams using <code>/v1/messages</code> API (Claude Code)</li>
<li>Teams using <strong>MCP</strong></li>
<li>Teams giving access to self-hosted models and setting rate limits</li>
</ul>
</li>
<li><strong>Risk of Upgrade</strong>
<ul>
<li><strong>Low</strong>
<ul>
<li>No major changes to existing functionality or package updates.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="mcp-permissions-management">MCP Permissions Management<a href="#mcp-permissions-management" class="hash-link" aria-label="MCP Permissions Management" title="MCP Permissions Management"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAdElEQVR4nE2M2wrCMBBE8/+/aYMFFRKb626zR1ItdeAwD3NxIQRijKSUyDkjogeqJ3Lg1nVlWRa898xRLo33VtFdkV95jIG7lsq+K60prQg2xlGYmBmu946IcHppldvjTugbxiU3w3+6Cs8aeNXIsO/bfPwA1ZXDCjomFB4AAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/mcp_permissions.e51ff92.640.png" srcset="/assets/ideal-img/mcp_permissions.e51ff92.640.png 640w,/assets/ideal-img/mcp_permissions.3e4a75f.1920.png 1920w" width="640" height="334"></noscript></div>
<p>This release brings support for managing permissions for MCP Servers by Keys, Teams, Organizations (entities) on LiteLLM. When a MCP client attempts to list tools, LiteLLM will only return the tools the entity has permissions to access.</p>
<p>This is great for use cases that require access to restricted data (e.g Jira MCP) that you don&#x27;t want everyone to use.</p>
<p>For Proxy Admins, this enables centralized management of all MCP Servers with access control. For developers, this means you&#x27;ll only see the MCP tools assigned to you.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="codex-mini-on-claude-code">Codex-mini on Claude Code<a href="#codex-mini-on-claude-code" class="hash-link" aria-label="Codex-mini on Claude Code" title="Codex-mini on Claude Code"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAFAAoDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAYI/8QAIhAAAAUDBAMAAAAAAAAAAAAAAAECAwUEBhEIExZBVpTR/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAP/xAAXEQEAAwAAAAAAAAAAAAAAAAAAAQIh/9oADAMBAAIRAxEAPwC71LSUhA2dFOREjXUjq680qcZfUhRltqPBmXWehm/nN1+TTXuufQAXpGD/2Q==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/codex_on_claude_code.53cb6d5.640.jpg" srcset="/assets/ideal-img/codex_on_claude_code.53cb6d5.640.jpg 640w,/assets/ideal-img/codex_on_claude_code.b473784.1920.jpg 1920w" width="640" height="334"></noscript></div>
<p>This release brings support for calling <code>codex-mini</code> (OpenAIs code assistant model) via Claude Code.</p>
<p>This is done by LiteLLM enabling any Responses API model (including <code>o3-pro</code>) to be called via <code>/chat/completions</code> and <code>/v1/messages</code> endpoints. This includes:</p>
<ul>
<li>Streaming calls</li>
<li>Non-streaming calls</li>
<li>Cost Tracking on success + failure for Responses API models</li>
</ul>
<p>Here&#x27;s how to use it <a href="/docs/tutorials/claude_responses_api">today</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new--updated-models">New / Updated Models<a href="#new--updated-models" class="hash-link" aria-label="New / Updated Models" title="New / Updated Models"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="pricing--context-window-updates">Pricing / Context Window Updates<a href="#pricing--context-window-updates" class="hash-link" aria-label="Pricing / Context Window Updates" title="Pricing / Context Window Updates"></a></h3>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th><th>Type</th></tr></thead><tbody><tr><td>VertexAI</td><td><code>vertex_ai/claude-opus-4</code></td><td>200K</td><td>$15.00</td><td>$75.00</td><td>New</td></tr><tr><td>OpenAI</td><td><code>gpt-4o-audio-preview-2025-06-03</code></td><td>128k</td><td>$2.5 (text), $40 (audio)</td><td>$10 (text), $80 (audio)</td><td>New</td></tr><tr><td>OpenAI</td><td><code>o3-pro</code></td><td>200k</td><td>20</td><td>80</td><td>New</td></tr><tr><td>OpenAI</td><td><code>o3-pro-2025-06-10</code></td><td>200k</td><td>20</td><td>80</td><td>New</td></tr><tr><td>OpenAI</td><td><code>o3</code></td><td>200k</td><td>2</td><td>8</td><td>Updated</td></tr><tr><td>OpenAI</td><td><code>o3-2025-04-16</code></td><td>200k</td><td>2</td><td>8</td><td>Updated</td></tr><tr><td>Azure</td><td><code>azure/gpt-4o-mini-transcribe</code></td><td>16k</td><td>1.25 (text), 3 (audio)</td><td>5 (text)</td><td>New</td></tr><tr><td>Mistral</td><td><code>mistral/magistral-medium-latest</code></td><td>40k</td><td>2</td><td>5</td><td>New</td></tr><tr><td>Mistral</td><td><code>mistral/magistral-small-latest</code></td><td>40k</td><td>0.5</td><td>1.5</td><td>New</td></tr></tbody></table>
<ul>
<li>Deepgram: <code>nova-3</code> cost per second pricing is <a href="https://github.com/BerriAI/litellm/pull/11634" target="_blank" rel="noopener noreferrer">now supported</a>.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="updated-models">Updated Models<a href="#updated-models" class="hash-link" aria-label="Updated Models" title="Updated Models"></a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs">Bugs<a href="#bugs" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/providers/watsonx">Watsonx</a></strong>
<ul>
<li>Ignore space id on Watsonx deployments (throws json errors) - <a href="https://github.com/BerriAI/litellm/pull/11527" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/ollama">Ollama</a></strong>
<ul>
<li>Set tool call id for streaming calls - <a href="https://github.com/BerriAI/litellm/pull/11528" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Gemini (<a href="/docs/providers/vertex">VertexAI</a> + <a href="/docs/providers/gemini">Google AI Studio</a>)</strong>
<ul>
<li>Fix tool call indexes - <a href="https://github.com/BerriAI/litellm/pull/11558" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Handle empty string for arguments in function calls - <a href="https://github.com/BerriAI/litellm/pull/11601" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add audio/ogg mime type support when inferring from file urls - <a href="https://github.com/BerriAI/litellm/pull/11635" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/custom_llm_server">Custom LLM</a></strong>
<ul>
<li>Fix passing api_base, api_key, litellm_params_dict to custom_llm embedding methods - <a href="https://github.com/BerriAI/litellm/pull/11450" target="_blank" rel="noopener noreferrer">PR</a> s/o <a href="https://github.com/ElefHead" target="_blank" rel="noopener noreferrer">ElefHead</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/huggingface">Huggingface</a></strong>
<ul>
<li>Add /chat/completions to endpoint url when missing - <a href="https://github.com/BerriAI/litellm/pull/11630" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/deepgram">Deepgram</a></strong>
<ul>
<li>Support async httpx calls - <a href="https://github.com/BerriAI/litellm/pull/11641" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Append prefix (if set) to assistant content start - <a href="https://github.com/BerriAI/litellm/pull/11719" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features">Features<a href="#features" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/providers/vertex">VertexAI</a></strong>
<ul>
<li>Support vertex credentials set via env var on passthrough - <a href="https://github.com/BerriAI/litellm/pull/11527" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support for choosing global region when model is only available there - <a href="https://github.com/BerriAI/litellm/pull/11566" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Anthropic passthrough cost calculation + token tracking - <a href="https://github.com/BerriAI/litellm/pull/11611" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support global vertex region on passthrough - <a href="https://github.com/BerriAI/litellm/pull/11661" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>none tool choice param support - <a href="https://github.com/BerriAI/litellm/pull/11695" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/providers/anthropic#disable-tool-calling">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/perplexity">Perplexity</a></strong>
<ul>
<li>Add reasoning_effort support - <a href="https://github.com/BerriAI/litellm/pull/11562" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/providers/perplexity#reasoning-effort">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/mistral">Mistral</a></strong>
<ul>
<li>Add mistral reasoning support - <a href="https://github.com/BerriAI/litellm/pull/11642" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/providers/mistral#reasoning">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openai_compatible">SGLang</a></strong>
<ul>
<li>Map context window exceeded error for proper handling - <a href="https://github.com/BerriAI/litellm/pull/11575/" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/deepgram">Deepgram</a></strong>
<ul>
<li>Provider specific params support - <a href="https://github.com/BerriAI/litellm/pull/11638" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure">Azure</a></strong>
<ul>
<li>Return content safety filter results - <a href="https://github.com/BerriAI/litellm/pull/11655" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-1">Bugs<a href="#bugs-1" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/completion/input">Chat Completion</a></strong>
<ul>
<li>Streaming - Ensure consistent created across chunks - <a href="https://github.com/BerriAI/litellm/pull/11528" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-1">Features<a href="#features-1" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>MCP</strong>
<ul>
<li>Add controls for MCP Permission Management - <a href="https://github.com/BerriAI/litellm/pull/11598" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/mcp#-mcp-permission-management">Docs</a></li>
<li>Add permission management for MCP List + Call Tool operations - <a href="https://github.com/BerriAI/litellm/pull/11682" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/mcp#-mcp-permission-management">Docs</a></li>
<li>Streamable HTTP server support - <a href="https://github.com/BerriAI/litellm/pull/11628" target="_blank" rel="noopener noreferrer">PR</a>, <a href="https://github.com/BerriAI/litellm/pull/11645" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/mcp#using-your-mcp">Docs</a></li>
<li>Use Experimental dedicated Rest endpoints for list, calling MCP tools - <a href="https://github.com/BerriAI/litellm/pull/11684" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/response_api">Responses API</a></strong>
<ul>
<li>NEW API Endpoint - List input items - <a href="https://github.com/BerriAI/litellm/pull/11602" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Background mode for OpenAI + Azure OpenAI - <a href="https://github.com/BerriAI/litellm/pull/11640" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Langfuse/other Logging support on responses api requests - <a href="https://github.com/BerriAI/litellm/pull/11685" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/completion/input">Chat Completions</a></strong>
<ul>
<li>Bridge for Responses API - allows calling codex-mini via <code>/chat/completions</code> and <code>/v1/messages</code> - <a href="https://github.com/BerriAI/litellm/pull/11632" target="_blank" rel="noopener noreferrer">PR</a>, <a href="https://github.com/BerriAI/litellm/pull/11685" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking">Spend Tracking<a href="#spend-tracking" class="hash-link" aria-label="Spend Tracking" title="Spend Tracking"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-2">Bugs<a href="#bugs-2" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/customers">End Users</a></strong>
<ul>
<li>Update enduser spend and budget reset date based on budget duration - <a href="https://github.com/BerriAI/litellm/pull/8460" target="_blank" rel="noopener noreferrer">PR</a> (s/o <a href="https://github.com/laurien16" target="_blank" rel="noopener noreferrer">laurien16</a>)</li>
</ul>
</li>
<li><strong><a href="/docs/proxy/custom_pricing">Custom Pricing</a></strong>
<ul>
<li>Convert scientific notation str to int - <a href="https://github.com/BerriAI/litellm/pull/11655" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-3">Bugs<a href="#bugs-3" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/users">Users</a></strong>
<ul>
<li><code>/user/info</code> - fix passing user with <code>+</code> in user id</li>
<li>Add admin-initiated password reset flow - <a href="https://github.com/BerriAI/litellm/pull/11618" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixes default user settings UI rendering error - <a href="https://github.com/BerriAI/litellm/pull/11674" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/users">Budgets</a></strong>
<ul>
<li>Correct success message when new user budget is created - <a href="https://github.com/BerriAI/litellm/pull/11608" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-2">Features<a href="#features-2" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>Leftnav</strong>
<ul>
<li>Show remaining Enterprise users on UI</li>
</ul>
</li>
<li><strong>MCP</strong>
<ul>
<li>New server add form - <a href="https://github.com/BerriAI/litellm/pull/11604" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Allow editing mcp servers - <a href="https://github.com/BerriAI/litellm/pull/11693" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Models</strong>
<ul>
<li>Add deepgram models on UI</li>
<li>Model Access Group support on UI - <a href="https://github.com/BerriAI/litellm/pull/11719" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Keys</strong>
<ul>
<li>Trim long user ids - <a href="https://github.com/BerriAI/litellm/pull/11488" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Logs</strong>
<ul>
<li>Add live tail feature to logs view, allows user to disable auto refresh in high traffic - <a href="https://github.com/BerriAI/litellm/pull/11712" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Audit Logs - preview screenshot - <a href="https://github.com/BerriAI/litellm/pull/11715" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrails-integrations">Logging / Guardrails Integrations<a href="#logging--guardrails-integrations" class="hash-link" aria-label="Logging / Guardrails Integrations" title="Logging / Guardrails Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-4">Bugs<a href="#bugs-4" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/observability/arize_integration">Arize</a></strong>
<ul>
<li>Change space_key header to space_id - <a href="https://github.com/BerriAI/litellm/pull/11595" target="_blank" rel="noopener noreferrer">PR</a> (s/o <a href="https://github.com/vanities" target="_blank" rel="noopener noreferrer">vanities</a>)</li>
</ul>
</li>
<li><strong><a href="/docs/proxy/prometheus">Prometheus</a></strong>
<ul>
<li>Fix total requests increment - <a href="https://github.com/BerriAI/litellm/pull/11718" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-3">Features<a href="#features-3" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/guardrails/lasso_security">Lasso Guardrails</a></strong>
<ul>
<li>[NEW] Lasso Guardrails support - <a href="https://github.com/BerriAI/litellm/pull/11565" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/users">Users</a></strong>
<ul>
<li>New <code>organizations</code> param on <code>/user/new</code> - allows adding users to orgs on creation - <a href="https://github.com/BerriAI/litellm/pull/11572/files" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Prevent double logging when using bridge logic</strong> - <a href="https://github.com/BerriAI/litellm/pull/11687" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--reliability-improvements">Performance / Reliability Improvements<a href="#performance--reliability-improvements" class="hash-link" aria-label="Performance / Reliability Improvements" title="Performance / Reliability Improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-5">Bugs<a href="#bugs-5" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/tag_routing">Tag based routing</a></strong>
<ul>
<li>Do not consider default models when request specifies a tag - <a href="https://github.com/BerriAI/litellm/pull/11454" target="_blank" rel="noopener noreferrer">PR</a> (s/o <a href="https://github.com/thiagosalvatore" target="_blank" rel="noopener noreferrer">thiagosalvatore</a>)</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-4">Features<a href="#features-4" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong><a href="/docs/caching/all_caches">Caching</a></strong>
<ul>
<li>New optional litellm[caching] pip install for adding disk cache dependencies - <a href="https://github.com/BerriAI/litellm/pull/11600" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="bugs-6">Bugs<a href="#bugs-6" class="hash-link" aria-label="Bugs" title="Bugs"></a></h4>
<ul>
<li><strong>aiohttp</strong>
<ul>
<li>fixes for transfer encoding error on aiohttp transport - <a href="https://github.com/BerriAI/litellm/pull/11561" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="features-5">Features<a href="#features-5" class="hash-link" aria-label="Features" title="Features"></a></h4>
<ul>
<li><strong>aiohttp</strong>
<ul>
<li>Enable System Proxy Support for aiohttp transport - <a href="https://github.com/BerriAI/litellm/pull/11616" target="_blank" rel="noopener noreferrer">PR</a> (s/o <a href="https://github.com/idootop" target="_blank" rel="noopener noreferrer">idootop</a>)</li>
</ul>
</li>
<li><strong>CLI</strong>
<ul>
<li>Make all commands show server URL - <a href="https://github.com/BerriAI/litellm/pull/10801" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Unicorn</strong>
<ul>
<li>Allow setting keep alive timeout - <a href="https://github.com/BerriAI/litellm/pull/11594" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Experimental Rate Limiting v2</strong> (enable via <code>EXPERIMENTAL_MULTI_INSTANCE_RATE_LIMITING=&quot;True&quot;</code>)<!-- -->
<ul>
<li>Support specifying rate limit by output_tokens only - <a href="https://github.com/BerriAI/litellm/pull/11646" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Decrement parallel requests on call failure - <a href="https://github.com/BerriAI/litellm/pull/11646" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>In-memory only rate limiting support - <a href="https://github.com/BerriAI/litellm/pull/11646" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Return remaining rate limits by key/user/team - <a href="https://github.com/BerriAI/litellm/pull/11646" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Helm</strong>
<ul>
<li>support extraContainers in migrations-job.yaml - <a href="https://github.com/BerriAI/litellm/pull/11649" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li>@laurien16 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/8460" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/8460</a></li>
<li>@fengbohello made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11547" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11547</a></li>
<li>@lapinek made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11570" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11570</a></li>
<li>@yanwork made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11586" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11586</a></li>
<li>@dhs-shine made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11575" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11575</a></li>
<li>@ElefHead made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11450" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11450</a></li>
<li>@idootop made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11616" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11616</a></li>
<li>@stevenaldinger made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11649" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11649</a></li>
<li>@thiagosalvatore made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11454" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11454</a></li>
<li>@vanities made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11595" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11595</a></li>
<li>@alvarosevilla95 made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11661" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/11661</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="demo-instance">Demo Instance<a href="#demo-instance" class="hash-link" aria-label="Demo Instance" title="Demo Instance"></a></h2>
<p>Here&#x27;s a Demo Instance to test changes:</p>
<ul>
<li>Instance: <a href="https://demo.litellm.ai/" target="_blank" rel="noopener noreferrer">https://demo.litellm.ai/</a></li>
<li>Login Credentials:<!-- -->
<ul>
<li>Username: admin</li>
<li>Password: sk-1234</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="git-diff"><a href="https://github.com/BerriAI/litellm/compare/v1.72.2-stable...1.72.6.rc" target="_blank" rel="noopener noreferrer">Git Diff</a><a href="#git-diff" class="hash-link" aria-label="git-diff" title="git-diff"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-72-2-stable">v1.72.2-stable</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-06-07T10:00:00.000Z">202567</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1298587542745358340/DZv3Oj-h_400x400.jpg" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:main-v1.72.2-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.72.2.post1</span><br></span></code></pre></div></div></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="tldr">TLDR<a href="#tldr" class="hash-link" aria-label="TLDR" title="TLDR"></a></h2>
<ul>
<li><strong>Why Upgrade</strong>
<ul>
<li>Performance Improvements for /v1/messages: For this endpoint LiteLLM Proxy overhead is now down to 50ms at 250 RPS.</li>
<li>Accurate Rate Limiting: Multi-instance rate limiting now tracks rate limits across keys, models, teams, and users with 0 spillover.</li>
<li>Audit Logs on UI: Track when Keys, Teams, and Models were deleted by viewing Audit Logs on the LiteLLM UI.</li>
<li>/v1/messages all models support: You can now use all LiteLLM models (<code>gpt-4.1</code>, <code>o1-pro</code>, <code>gemini-2.5-pro</code>) with /v1/messages API.</li>
<li><a href="/docs/providers/anthropic#mcp-tool-calling">Anthropic MCP</a>: Use remote MCP Servers with Anthropic Models.</li>
</ul>
</li>
<li><strong>Who Should Read</strong>
<ul>
<li>Teams using <code>/v1/messages</code> API (Claude Code)</li>
<li>Proxy Admins using LiteLLM Virtual Keys and setting rate limits</li>
</ul>
</li>
<li><strong>Risk of Upgrade</strong>
<ul>
<li><strong>Medium</strong>
<ul>
<li>Upgraded <code>ddtrace==3.8.0</code>, if you use DataDog tracing this is a medium level risk. We recommend monitoring logs for any issues.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="v1messages-performance-improvements"><code>/v1/messages</code> Performance Improvements<a href="#v1messages-performance-improvements" class="hash-link" aria-label="v1messages-performance-improvements" title="v1messages-performance-improvements"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAmElEQVR4nDWMW47DIBRD2f98zIaq2cDsIX1GaQKUvCg3F5pTFan+sY9k29xuo/S9lWWZJaUkqiofffO2bZWNszNdd2ewA+u6Mk0TIQTGcawuIpRSMDkrqkrJBe98LcYYic9Yh9bayiaJQIHD9Y/mcWLPLy7nC821ITwDkgTnHObzljXz8/9Lt9zRpLRty7E70c8D+2vHe88bdlK9/EvlVPQAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="332"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/v1_messages_perf.03ec89d.640.png" srcset="/assets/ideal-img/v1_messages_perf.03ec89d.640.png 640w,/assets/ideal-img/v1_messages_perf.f405b8c.1920.png 1920w" width="640" height="332"></noscript></div>
<p>This release brings significant performance improvements to the /v1/messages API on LiteLLM.</p>
<p>For this endpoint LiteLLM Proxy overhead latency is now down to 50ms, and each instance can handle 250 RPS. We validated these improvements through load testing with payloads containing over 1,000 streaming chunks.</p>
<p>This is great for real time use cases with large requests (eg. multi turn conversations, Claude Code, etc.).</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="multi-instance-rate-limiting-improvements">Multi-Instance Rate Limiting Improvements<a href="#multi-instance-rate-limiting-improvements" class="hash-link" aria-label="Multi-Instance Rate Limiting Improvements" title="Multi-Instance Rate Limiting Improvements"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAFAAoDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAYI/8QAIhAAAQMEAAcAAAAAAAAAAAAAAQIDBAAFBhEIExYhVpXS/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/EABQRAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhEDEQA/ALvignzrRg1rXaJ8yA6u5BKnIz621Ecpw6JB3rYHasx9X5P5NfvYu/VKUH//2Q==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="335"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/multi_instance_rate_limits_v3.1d13d55.640.jpg" srcset="/assets/ideal-img/multi_instance_rate_limits_v3.1d13d55.640.jpg 640w,/assets/ideal-img/multi_instance_rate_limits_v3.8210428.1800.jpg 1800w" width="640" height="335"></noscript></div>
<p>LiteLLM now accurately tracks rate limits across keys, models, teams, and users with 0 spillover.</p>
<p>This is a significant improvement over the previous version, which faced issues with leakage and spillover in high traffic, multi-instance setups.</p>
<p><strong>Key Changes:</strong></p>
<ul>
<li>Redis is now part of the rate limit check, instead of being a background sync. This ensures accuracy and reduces read/write operations during low activity.</li>
<li>LiteLLM now uses Lua scripts to ensure all checks are atomic.</li>
<li>In-memory caching uses Redis values. This prevents drift, and reduces Redis queries once objects are over their limit.</li>
</ul>
<p>These changes are currently behind the feature flag - <code>EXPERIMENTAL_ENABLE_MULTI_INSTANCE_RATE_LIMITING=True</code>. We plan to GA this in our next release - subject to feedback.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="audit-logs-on-ui">Audit Logs on UI<a href="#audit-logs-on-ui" class="hash-link" aria-label="Audit Logs on UI" title="Audit Logs on UI"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAb0lEQVR4nGWOywpDIRBD/f+/dONCN8LoOA81RW8LLQ0cAgNJJogIzAxzzutm+vZfQkoJMUYQEdwdIg6zeYPfhForSilgZogoiDrWwp/CafmkeAzknNE7Q1Wx1sbeD+Hsn+Pzi4Naxxhyaa3fBeaBFyKswzMholNzAAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="321"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/ui_audit_log.b6390b3.640.png" srcset="/assets/ideal-img/ui_audit_log.b6390b3.640.png 640w,/assets/ideal-img/ui_audit_log.92de7cb.1920.png 1920w" width="640" height="321"></noscript></div>
<p>This release introduces support for viewing audit logs in the UI. As a Proxy Admin, you can now check if and when a key was deleted, along with who performed the action.</p>
<p>LiteLLM tracks changes to the following entities and actions:</p>
<ul>
<li><strong>Entities:</strong> Keys, Teams, Users, Models</li>
<li><strong>Actions:</strong> Create, Update, Delete, Regenerate</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<p><strong>Newly Added Models</strong></p>
<table><thead><tr><th>Provider</th><th>Model</th><th>Context Window</th><th>Input ($/1M tokens)</th><th>Output ($/1M tokens)</th></tr></thead><tbody><tr><td>Anthropic</td><td><code>claude-4-opus-20250514</code></td><td>200K</td><td>$15.00</td><td>$75.00</td></tr><tr><td>Anthropic</td><td><code>claude-4-sonnet-20250514</code></td><td>200K</td><td>$3.00</td><td>$15.00</td></tr><tr><td>VertexAI, Google AI Studio</td><td><code>gemini-2.5-pro-preview-06-05</code></td><td>1M</td><td>$1.25</td><td>$10.00</td></tr><tr><td>OpenAI</td><td><code>codex-mini-latest</code></td><td>200K</td><td>$1.50</td><td>$6.00</td></tr><tr><td>Cerebras</td><td><code>qwen-3-32b</code></td><td>128K</td><td>$0.40</td><td>$0.80</td></tr><tr><td>SambaNova</td><td><code>DeepSeek-R1</code></td><td>32K</td><td>$5.00</td><td>$7.00</td></tr><tr><td>SambaNova</td><td><code>DeepSeek-R1-Distill-Llama-70B</code></td><td>131K</td><td>$0.70</td><td>$1.40</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="model-updates">Model Updates<a href="#model-updates" class="hash-link" aria-label="Model Updates" title="Model Updates"></a></h3>
<ul>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Cost tracking added for new Claude models - <a href="https://github.com/BerriAI/litellm/pull/11339" target="_blank" rel="noopener noreferrer">PR</a>
<ul>
<li><code>claude-4-opus-20250514</code></li>
<li><code>claude-4-sonnet-20250514</code></li>
</ul>
</li>
<li>Support for MCP tool calling with Anthropic models - <a href="https://github.com/BerriAI/litellm/pull/11474" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/gemini">Google AI Studio</a></strong>
<ul>
<li>Google Gemini 2.5 Pro Preview 06-05 support - <a href="https://github.com/BerriAI/litellm/pull/11447" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Gemini streaming thinking content parsing with <code>reasoning_content</code> - <a href="https://github.com/BerriAI/litellm/pull/11298" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support for no reasoning option for Gemini models - <a href="https://github.com/BerriAI/litellm/pull/11393" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>URL context support for Gemini models - <a href="https://github.com/BerriAI/litellm/pull/11351" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Gemini embeddings-001 model prices and context window - <a href="https://github.com/BerriAI/litellm/pull/11332" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openai">OpenAI</a></strong>
<ul>
<li>Cost tracking for <code>codex-mini-latest</code> - <a href="https://github.com/BerriAI/litellm/pull/11492" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex">Vertex AI</a></strong>
<ul>
<li>Cache token tracking on streaming calls - <a href="https://github.com/BerriAI/litellm/pull/11387" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Return response_id matching upstream response ID for stream and non-stream - <a href="https://github.com/BerriAI/litellm/pull/11456" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/cerebras">Cerebras</a></strong>
<ul>
<li>Cerebras/qwen-3-32b model pricing and context window - <a href="https://github.com/BerriAI/litellm/pull/11373" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/huggingface">HuggingFace</a></strong>
<ul>
<li>Fixed embeddings using non-default <code>input_type</code> - <a href="https://github.com/BerriAI/litellm/pull/11452" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/datarobot">DataRobot</a></strong>
<ul>
<li>New provider integration for enterprise AI workflows - <a href="https://github.com/BerriAI/litellm/pull/10385" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/together_ai">DeepSeek</a></strong>
<ul>
<li>DeepSeek R1 family model configuration via Together AI - <a href="https://github.com/BerriAI/litellm/pull/11394" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>DeepSeek R1 pricing and context window configuration - <a href="https://github.com/BerriAI/litellm/pull/11339" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<ul>
<li><strong><a href="/docs/image_generation">Images API</a></strong>
<ul>
<li>Azure endpoint support for image endpoints - <a href="https://github.com/BerriAI/litellm/pull/11482" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/completion/chat">Anthropic Messages API</a></strong>
<ul>
<li>Support for ALL LiteLLM Providers (OpenAI, Azure, Bedrock, Vertex, DeepSeek, etc.) on /v1/messages API Spec - <a href="https://github.com/BerriAI/litellm/pull/11502" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Performance improvements for /v1/messages route - <a href="https://github.com/BerriAI/litellm/pull/11421" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Return streaming usage statistics when using LiteLLM with Bedrock models - <a href="https://github.com/BerriAI/litellm/pull/11469" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/embedding/supported_embedding">Embeddings API</a></strong>
<ul>
<li>Provider-specific optional params handling for embedding calls - <a href="https://github.com/BerriAI/litellm/pull/11346" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Proper Sagemaker request attribute usage for embeddings - <a href="https://github.com/BerriAI/litellm/pull/11362" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/rerank/supported_rerank">Rerank API</a></strong>
<ul>
<li>New HuggingFace rerank provider support - <a href="https://github.com/BerriAI/litellm/pull/11438" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/providers/huggingface_rerank">Guide</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking">Spend Tracking<a href="#spend-tracking" class="hash-link" aria-label="Spend Tracking" title="Spend Tracking"></a></h2>
<ul>
<li>Added token tracking for anthropic batch calls via /anthropic passthrough route- <a href="https://github.com/BerriAI/litellm/pull/11388" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<ul>
<li><strong>SSO/Authentication</strong>
<ul>
<li>SSO configuration endpoints and UI integration with persistent settings - <a href="https://github.com/BerriAI/litellm/pull/11417" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Update proxy admin ID role in DB + Handle SSO redirects with custom root path - <a href="https://github.com/BerriAI/litellm/pull/11384" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support returning virtual key in custom auth - <a href="https://github.com/BerriAI/litellm/pull/11346" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>User ID validation to ensure it is not an email or phone number - <a href="https://github.com/BerriAI/litellm/pull/10102" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Teams</strong>
<ul>
<li>Fixed Create/Update team member API 500 error - <a href="https://github.com/BerriAI/litellm/pull/10479" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Enterprise feature gating for RegenerateKeyModal in KeyInfoView - <a href="https://github.com/BerriAI/litellm/pull/11400" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>SCIM</strong>
<ul>
<li>Fixed SCIM running patch operation case sensitivity - <a href="https://github.com/BerriAI/litellm/pull/11335" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>General</strong>
<ul>
<li>Converted action buttons to sticky footer action buttons - <a href="https://github.com/BerriAI/litellm/pull/11293" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Custom Server Root Path - support for serving UI on a custom root path - <a href="/docs/proxy/custom_root_ui">Guide</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrails-integrations">Logging / Guardrails Integrations<a href="#logging--guardrails-integrations" class="hash-link" aria-label="Logging / Guardrails Integrations" title="Logging / Guardrails Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="logging">Logging<a href="#logging" class="hash-link" aria-label="Logging" title="Logging"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/logging#s3">S3</a></strong>
<ul>
<li>Async + Batched S3 Logging for improved performance - <a href="https://github.com/BerriAI/litellm/pull/11340" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/observability/datadog_integration">DataDog</a></strong>
<ul>
<li>Add instrumentation for streaming chunks - <a href="https://github.com/BerriAI/litellm/pull/11338" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add DD profiler to monitor Python profile of LiteLLM CPU% - <a href="https://github.com/BerriAI/litellm/pull/11375" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Bump DD trace version - <a href="https://github.com/BerriAI/litellm/pull/11426" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/prometheus">Prometheus</a></strong>
<ul>
<li>Pass custom metadata labels in litellm_total_token metrics - <a href="https://github.com/BerriAI/litellm/pull/11414" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#google-cloud-storage">GCS</a></strong>
<ul>
<li>Update GCSBucketBase to handle GSM project ID if passed - <a href="https://github.com/BerriAI/litellm/pull/11409" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="guardrails">Guardrails<a href="#guardrails" class="hash-link" aria-label="Guardrails" title="Guardrails"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/guardrails/presidio">Presidio</a></strong>
<ul>
<li>Add presidio_language yaml configuration support for guardrails - <a href="https://github.com/BerriAI/litellm/pull/11331" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--reliability-improvements">Performance / Reliability Improvements<a href="#performance--reliability-improvements" class="hash-link" aria-label="Performance / Reliability Improvements" title="Performance / Reliability Improvements"></a></h2>
<ul>
<li><strong>Performance Optimizations</strong>
<ul>
<li>Don&#x27;t run auth on /health/liveliness endpoints - <a href="https://github.com/BerriAI/litellm/pull/11378" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Don&#x27;t create 1 task for every hanging request alert - <a href="https://github.com/BerriAI/litellm/pull/11385" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add debugging endpoint to track active /asyncio-tasks - <a href="https://github.com/BerriAI/litellm/pull/11382" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Make batch size for maximum retention in spend logs controllable - <a href="https://github.com/BerriAI/litellm/pull/11459" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Expose flag to disable token counter - <a href="https://github.com/BerriAI/litellm/pull/11344" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support pipeline redis lpop for older redis versions - <a href="https://github.com/BerriAI/litellm/pull/11425" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes">Bug Fixes<a href="#bug-fixes" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h2>
<ul>
<li><strong>LLM API Fixes</strong>
<ul>
<li><strong>Anthropic</strong>: Fix regression when passing file url&#x27;s to the &#x27;file_id&#x27; parameter - <a href="https://github.com/BerriAI/litellm/pull/11387" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong>Vertex AI</strong>: Fix Vertex AI any_of issues for Description and Default. - <a href="https://github.com/BerriAI/litellm/issues/11383" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix transcription model name mapping - <a href="https://github.com/BerriAI/litellm/pull/11333" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong>Image Generation</strong>: Fix None values in usage field for gpt-image-1 model responses - <a href="https://github.com/BerriAI/litellm/pull/11448" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong>Responses API</strong>: Fix _transform_responses_api_content_to_chat_completion_content doesn&#x27;t support file content type - <a href="https://github.com/BerriAI/litellm/pull/11494" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong>Fireworks AI</strong>: Fix rate limit exception mapping - detect &quot;rate limit&quot; text in error messages - <a href="https://github.com/BerriAI/litellm/pull/11455" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Spend Tracking/Budgets</strong>
<ul>
<li>Respect user_header_name property for budget selection and user identification - <a href="https://github.com/BerriAI/litellm/pull/11419" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>MCP Server</strong>
<ul>
<li>Remove duplicate server_id MCP config servers - <a href="https://github.com/BerriAI/litellm/pull/11327" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Function Calling</strong>
<ul>
<li>supports_function_calling works with llm_proxy models - <a href="https://github.com/BerriAI/litellm/pull/11381" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Knowledge Base</strong>
<ul>
<li>Fixed Knowledge Base Call returning error - <a href="https://github.com/BerriAI/litellm/pull/11467" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li><a href="https://github.com/mjnitz02" target="_blank" rel="noopener noreferrer">@mjnitz02</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10385" target="_blank" rel="noopener noreferrer">#10385</a></li>
<li><a href="https://github.com/hagan" target="_blank" rel="noopener noreferrer">@hagan</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10479" target="_blank" rel="noopener noreferrer">#10479</a></li>
<li><a href="https://github.com/wwells" target="_blank" rel="noopener noreferrer">@wwells</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11409" target="_blank" rel="noopener noreferrer">#11409</a></li>
<li><a href="https://github.com/likweitan" target="_blank" rel="noopener noreferrer">@likweitan</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11400" target="_blank" rel="noopener noreferrer">#11400</a></li>
<li><a href="https://github.com/raz-alon" target="_blank" rel="noopener noreferrer">@raz-alon</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10102" target="_blank" rel="noopener noreferrer">#10102</a></li>
<li><a href="https://github.com/jtsai-quid" target="_blank" rel="noopener noreferrer">@jtsai-quid</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11394" target="_blank" rel="noopener noreferrer">#11394</a></li>
<li><a href="https://github.com/tmbo" target="_blank" rel="noopener noreferrer">@tmbo</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11362" target="_blank" rel="noopener noreferrer">#11362</a></li>
<li><a href="https://github.com/wangsha" target="_blank" rel="noopener noreferrer">@wangsha</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11351" target="_blank" rel="noopener noreferrer">#11351</a></li>
<li><a href="https://github.com/seankwalker" target="_blank" rel="noopener noreferrer">@seankwalker</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11452" target="_blank" rel="noopener noreferrer">#11452</a></li>
<li><a href="https://github.com/pazevedo-hyland" target="_blank" rel="noopener noreferrer">@pazevedo-hyland</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11381" target="_blank" rel="noopener noreferrer">#11381</a></li>
<li><a href="https://github.com/cainiaoit" target="_blank" rel="noopener noreferrer">@cainiaoit</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11438" target="_blank" rel="noopener noreferrer">#11438</a></li>
<li><a href="https://github.com/vuanhtu52" target="_blank" rel="noopener noreferrer">@vuanhtu52</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11508" target="_blank" rel="noopener noreferrer">#11508</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="demo-instance">Demo Instance<a href="#demo-instance" class="hash-link" aria-label="Demo Instance" title="Demo Instance"></a></h2>
<p>Here&#x27;s a Demo Instance to test changes:</p>
<ul>
<li>Instance: <a href="https://demo.litellm.ai/" target="_blank" rel="noopener noreferrer">https://demo.litellm.ai/</a></li>
<li>Login Credentials:<!-- -->
<ul>
<li>Username: admin</li>
<li>Password: sk-1234</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="git-diff"><a href="https://github.com/BerriAI/litellm/releases/tag/v1.72.2-stable" target="_blank" rel="noopener noreferrer">Git Diff</a><a href="#git-diff" class="hash-link" aria-label="git-diff" title="git-diff"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1-72-0-stable">v1.72.0-stable</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-05-31T10:00:00.000Z">2025531</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:main-v1.72.0-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.72.0</span><br></span></code></pre></div></div></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<p>LiteLLM v1.72.0-stable.rc is live now. Here are the key highlights of this release:</p>
<ul>
<li><strong>Vector Store Permissions</strong>: Control Vector Store access at the Key, Team, and Organization level.</li>
<li><strong>Rate Limiting Sliding Window support</strong>: Improved accuracy for Key/Team/User rate limits with request tracking across minutes.</li>
<li><strong>Aiohttp Transport used by default</strong>: Aiohttp transport is now the default transport for LiteLLM networking requests. This gives users 2x higher RPS per instance with a 40ms median latency overhead.</li>
<li><strong>Bedrock Agents</strong>: Call Bedrock Agents with <code>/chat/completions</code>, <code>/response</code> endpoints.</li>
<li><strong>Anthropic File API</strong>: Upload and analyze CSV files with Claude-4 on Anthropic via LiteLLM.</li>
<li><strong>Prometheus</strong>: End users (<code>end_user</code>) will no longer be tracked by default on Prometheus. Tracking end_users on prometheus is now opt-in. This is done to prevent the response from <code>/metrics</code> from  becoming too large. <a href="/docs/proxy/prometheus#tracking-end_user-on-prometheus">Read More</a></li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="vector-store-permissions">Vector Store Permissions<a href="#vector-store-permissions" class="hash-link" aria-label="Vector Store Permissions" title="Vector Store Permissions"></a></h2>
<p>This release brings support for managing permissions for vector stores by Keys, Teams, Organizations (entities) on LiteLLM. When a request attempts to query a vector store, LiteLLM will block it if the requesting entity lacks the proper permissions.</p>
<p>This is great for use cases that require access to restricted data that you don&#x27;t want everyone to use.</p>
<p>Over the next week we plan on adding permission management for MCP Servers.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="aiohttp-transport-used-by-default">Aiohttp Transport used by default<a href="#aiohttp-transport-used-by-default" class="hash-link" aria-label="Aiohttp Transport used by default" title="Aiohttp Transport used by default"></a></h2>
<p>Aiohttp transport is now the default transport for LiteLLM networking requests. This gives users 2x higher RPS per instance with a 40ms median latency overhead. This has been live on LiteLLM Cloud for a week + gone through alpha users testing for a week.</p>
<p>If you encounter any issues, you can disable using the aiohttp transport in the following ways:</p>
<p><strong>On LiteLLM Proxy</strong></p>
<p>Set the <code>DISABLE_AIOHTTP_TRANSPORT=True</code> in the environment variables.</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">Environment Variable</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv codeBlockLinesWithNumbering_o6Pm" style="counter-reset:line-count 0"><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">export DISABLE_AIOHTTP_TRANSPORT=&quot;True&quot;</span></span><br></span></code></pre></div></div>
<p><strong>On LiteLLM Python SDK</strong></p>
<p>Set the <code>disable_aiohttp_transport=True</code> to disable aiohttp transport.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">Python SDK</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv codeBlockLinesWithNumbering_o6Pm" style="counter-reset:line-count 0"><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> litellm</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain" style="display:inline-block"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">litellm</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">disable_aiohttp_transport </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">True</span><span class="token plain"> </span><span class="token comment" style="color:#999988;font-style:italic"># default is False, enable this to disable aiohttp transport</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">result </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> litellm</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">completion</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    model</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;openai/gpt-4o&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    messages</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">{</span><span class="token string" style="color:#e3116c">&quot;role&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;user&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;content&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Hello, world!&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain"></span><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">result</span><span class="token punctuation" style="color:#393A34">)</span></span><br></span></code></pre></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<ul>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>
<ul>
<li>Video support for Bedrock Converse - <a href="https://github.com/BerriAI/litellm/pull/11166" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>InvokeAgents support as /chat/completions route - <a href="https://github.com/BerriAI/litellm/pull/11239" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/providers/bedrock_agents">Get Started</a></li>
<li>AI21 Jamba models compatibility fixes - <a href="https://github.com/BerriAI/litellm/pull/11233" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed duplicate maxTokens parameter for Claude with thinking - <a href="https://github.com/BerriAI/litellm/pull/11181" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="https://docs.litellm.ai/docs/providers/gemini" target="_blank" rel="noopener noreferrer">Gemini (Google AI Studio + Vertex AI)</a></strong>
<ul>
<li>Parallel tool calling support with <code>parallel_tool_calls</code> parameter - <a href="https://github.com/BerriAI/litellm/pull/11125" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>All Gemini models now support parallel function calling - <a href="https://github.com/BerriAI/litellm/pull/11225" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex">VertexAI</a></strong>
<ul>
<li>codeExecution tool support and anyOf handling - <a href="https://github.com/BerriAI/litellm/pull/11195" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Vertex AI Anthropic support on /v1/messages - <a href="https://github.com/BerriAI/litellm/pull/11246" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Thinking, global regions, and parallel tool calling improvements - <a href="https://github.com/BerriAI/litellm/pull/11194" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Web Search Support <a href="https://github.com/BerriAI/litellm/commit/06484f6e5a7a2f4e45c490266782ed28b51b7db6" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Thinking blocks on streaming support - <a href="https://github.com/BerriAI/litellm/pull/11194" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Files API with form-data support on passthrough - <a href="https://github.com/BerriAI/litellm/pull/11256" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>File ID support on /chat/completion - <a href="https://github.com/BerriAI/litellm/pull/11256" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/xai">xAI</a></strong>
<ul>
<li>Web Search Support <a href="https://github.com/BerriAI/litellm/commit/06484f6e5a7a2f4e45c490266782ed28b51b7db6" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/gemini">Google AI Studio</a></strong>
<ul>
<li>Web Search Support <a href="https://github.com/BerriAI/litellm/commit/06484f6e5a7a2f4e45c490266782ed28b51b7db6" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/mistral">Mistral</a></strong>
<ul>
<li>Updated mistral-medium prices and context sizes - <a href="https://github.com/BerriAI/litellm/pull/10729" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/ollama">Ollama</a></strong>
<ul>
<li>Tool calls parsing on streaming - <a href="https://github.com/BerriAI/litellm/pull/11171" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/cohere">Cohere</a></strong>
<ul>
<li>Swapped Cohere and Cohere Chat provider positioning - <a href="https://github.com/BerriAI/litellm/pull/11173" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/nebius">Nebius AI Studio</a></strong>
<ul>
<li>New provider integration - <a href="https://github.com/BerriAI/litellm/pull/11143" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<ul>
<li><strong><a href="/docs/image_generation">Image Edits API</a></strong>
<ul>
<li>Azure support for /v1/images/edits - <a href="https://github.com/BerriAI/litellm/pull/11160" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Cost tracking for image edits endpoint (OpenAI, Azure) - <a href="https://github.com/BerriAI/litellm/pull/11186" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/completion/chat">Completions API</a></strong>
<ul>
<li>Codestral latency overhead tracking on /v1/completions - <a href="https://github.com/BerriAI/litellm/pull/10879" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/audio/speech">Audio Transcriptions API</a></strong>
<ul>
<li>GPT-4o mini audio preview pricing without date - <a href="https://github.com/BerriAI/litellm/pull/11207" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Non-default params support for audio transcription - <a href="https://github.com/BerriAI/litellm/pull/11212" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/response_api">Responses API</a></strong>
<ul>
<li>Session management fixes for using Non-OpenAI models - <a href="https://github.com/BerriAI/litellm/pull/11254" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<ul>
<li><strong>Vector Stores</strong>
<ul>
<li>Permission management for LiteLLM Keys, Teams, and Organizations - <a href="https://github.com/BerriAI/litellm/pull/11213" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>UI display of vector store permissions - <a href="https://github.com/BerriAI/litellm/pull/11277" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Vector store access controls enforcement - <a href="https://github.com/BerriAI/litellm/pull/11281" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Object permissions fixes and QA improvements - <a href="https://github.com/BerriAI/litellm/pull/11291" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Teams</strong>
<ul>
<li>&quot;All proxy models&quot; display when no models selected - <a href="https://github.com/BerriAI/litellm/pull/11187" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Removed redundant teamInfo call, using existing teamsList - <a href="https://github.com/BerriAI/litellm/pull/11051" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Improved model tags display on Keys, Teams and Org pages - <a href="https://github.com/BerriAI/litellm/pull/11022" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>SSO/SCIM</strong>
<ul>
<li>Bug fixes for showing SCIM token on UI - <a href="https://github.com/BerriAI/litellm/pull/11220" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>General UI</strong>
<ul>
<li>Fix &quot;UI Session Expired. Logging out&quot; - <a href="https://github.com/BerriAI/litellm/pull/11279" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support for forwarding /sso/key/generate to server root path URL - <a href="https://github.com/BerriAI/litellm/pull/11165" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrails-integrations">Logging / Guardrails Integrations<a href="#logging--guardrails-integrations" class="hash-link" aria-label="Logging / Guardrails Integrations" title="Logging / Guardrails Integrations"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="logging">Logging<a href="#logging" class="hash-link" aria-label="Logging" title="Logging"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/prometheus">Prometheus</a></strong>
<ul>
<li>End users will no longer be tracked by default on Prometheus. Tracking end_users on prometheus is now opt-in. <a href="https://github.com/BerriAI/litellm/pull/11192" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#langfuse">Langfuse</a></strong>
<ul>
<li>Performance improvements: Fixed &quot;Max langfuse clients reached&quot; issue - <a href="https://github.com/BerriAI/litellm/pull/11285" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/observability/helicone_integration">Helicone</a></strong>
<ul>
<li>Base URL support - <a href="https://github.com/BerriAI/litellm/pull/11211" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#sentry">Sentry</a></strong>
<ul>
<li>Added sentry sample rate configuration - <a href="https://github.com/BerriAI/litellm/pull/10283" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="guardrails">Guardrails<a href="#guardrails" class="hash-link" aria-label="Guardrails" title="Guardrails"></a></h4>
<ul>
<li><strong><a href="/docs/proxy/guardrails/bedrock">Bedrock Guardrails</a></strong>
<ul>
<li>Streaming support for bedrock post guard - <a href="https://github.com/BerriAI/litellm/pull/11247" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Auth parameter persistence fixes - <a href="https://github.com/BerriAI/litellm/pull/11270" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/pangea">Pangea Guardrails</a></strong>
<ul>
<li>Added Pangea provider to Guardrails hook - <a href="https://github.com/BerriAI/litellm/pull/10775" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--reliability-improvements">Performance / Reliability Improvements<a href="#performance--reliability-improvements" class="hash-link" aria-label="Performance / Reliability Improvements" title="Performance / Reliability Improvements"></a></h2>
<ul>
<li><strong>aiohttp Transport</strong>
<ul>
<li>Handling for aiohttp.ClientPayloadError - <a href="https://github.com/BerriAI/litellm/pull/11162" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>SSL verification settings support - <a href="https://github.com/BerriAI/litellm/pull/11162" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Rollback to httpx==0.27.0 for stability - <a href="https://github.com/BerriAI/litellm/pull/11146" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Request Limiting</strong>
<ul>
<li>Sliding window logic for parallel request limiter v2 - <a href="https://github.com/BerriAI/litellm/pull/11283" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes">Bug Fixes<a href="#bug-fixes" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h2>
<ul>
<li><strong>LLM API Fixes</strong>
<ul>
<li>Added missing request_kwargs to get_available_deployment call - <a href="https://github.com/BerriAI/litellm/pull/11202" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed calling Azure O-series models - <a href="https://github.com/BerriAI/litellm/pull/11212" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support for dropping non-OpenAI params via additional_drop_params - <a href="https://github.com/BerriAI/litellm/pull/11246" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed frequency_penalty to repeat_penalty parameter mapping - <a href="https://github.com/BerriAI/litellm/pull/11284" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix for embedding cache hits on string input - <a href="https://github.com/BerriAI/litellm/pull/11211" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>General</strong>
<ul>
<li>OIDC provider improvements and audience bug fix - <a href="https://github.com/BerriAI/litellm/pull/10054" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Removed AzureCredentialType restriction on AZURE_CREDENTIAL - <a href="https://github.com/BerriAI/litellm/pull/11272" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Prevention of sensitive key leakage to Langfuse - <a href="https://github.com/BerriAI/litellm/pull/11165" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed healthcheck test using curl when curl not in image - <a href="https://github.com/BerriAI/litellm/pull/9737" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li><a href="https://github.com/agajdosi" target="_blank" rel="noopener noreferrer">@agajdosi</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/9737" target="_blank" rel="noopener noreferrer">#9737</a></li>
<li><a href="https://github.com/ketangangal" target="_blank" rel="noopener noreferrer">@ketangangal</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11161" target="_blank" rel="noopener noreferrer">#11161</a></li>
<li><a href="https://github.com/Aktsvigun" target="_blank" rel="noopener noreferrer">@Aktsvigun</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11143" target="_blank" rel="noopener noreferrer">#11143</a></li>
<li><a href="https://github.com/ryanmeans" target="_blank" rel="noopener noreferrer">@ryanmeans</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10775" target="_blank" rel="noopener noreferrer">#10775</a></li>
<li><a href="https://github.com/nikoizs" target="_blank" rel="noopener noreferrer">@nikoizs</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10054" target="_blank" rel="noopener noreferrer">#10054</a></li>
<li><a href="https://github.com/Nitro963" target="_blank" rel="noopener noreferrer">@Nitro963</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11202" target="_blank" rel="noopener noreferrer">#11202</a></li>
<li><a href="https://github.com/Jacobh2" target="_blank" rel="noopener noreferrer">@Jacobh2</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11207" target="_blank" rel="noopener noreferrer">#11207</a></li>
<li><a href="https://github.com/regismesquita" target="_blank" rel="noopener noreferrer">@regismesquita</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10729" target="_blank" rel="noopener noreferrer">#10729</a></li>
<li><a href="https://github.com/Vinnie-Singleton-NN" target="_blank" rel="noopener noreferrer">@Vinnie-Singleton-NN</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10283" target="_blank" rel="noopener noreferrer">#10283</a></li>
<li><a href="https://github.com/trashhalo" target="_blank" rel="noopener noreferrer">@trashhalo</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11219" target="_blank" rel="noopener noreferrer">#11219</a></li>
<li><a href="https://github.com/VigneshwarRajasekaran" target="_blank" rel="noopener noreferrer">@VigneshwarRajasekaran</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11223" target="_blank" rel="noopener noreferrer">#11223</a></li>
<li><a href="https://github.com/AnilAren" target="_blank" rel="noopener noreferrer">@AnilAren</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11233" target="_blank" rel="noopener noreferrer">#11233</a></li>
<li><a href="https://github.com/fadil4u" target="_blank" rel="noopener noreferrer">@fadil4u</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11242" target="_blank" rel="noopener noreferrer">#11242</a></li>
<li><a href="https://github.com/whitfin" target="_blank" rel="noopener noreferrer">@whitfin</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11279" target="_blank" rel="noopener noreferrer">#11279</a></li>
<li><a href="https://github.com/hcoona" target="_blank" rel="noopener noreferrer">@hcoona</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11272" target="_blank" rel="noopener noreferrer">#11272</a></li>
<li><a href="https://github.com/keyute" target="_blank" rel="noopener noreferrer">@keyute</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11173" target="_blank" rel="noopener noreferrer">#11173</a></li>
<li><a href="https://github.com/emmanuel-ferdman" target="_blank" rel="noopener noreferrer">@emmanuel-ferdman</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/11230" target="_blank" rel="noopener noreferrer">#11230</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="demo-instance">Demo Instance<a href="#demo-instance" class="hash-link" aria-label="Demo Instance" title="Demo Instance"></a></h2>
<p>Here&#x27;s a Demo Instance to test changes:</p>
<ul>
<li>Instance: <a href="https://demo.litellm.ai/" target="_blank" rel="noopener noreferrer">https://demo.litellm.ai/</a></li>
<li>Login Credentials:<!-- -->
<ul>
<li>Username: admin</li>
<li>Password: sk-1234</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="git-diff"><a href="https://github.com/BerriAI/litellm/releases" target="_blank" rel="noopener noreferrer">Git Diff</a><a href="#git-diff" class="hash-link" aria-label="git-diff" title="git-diff"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.71.1-stable">v1.71.1-stable - 2x Higher Requests Per Second (RPS)</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-05-24T10:00:00.000Z">2025524</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:main-v1.71.1-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.71.1</span><br></span></code></pre></div></div></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<p>LiteLLM v1.71.1-stable is live now. Here are the key highlights of this release:</p>
<ul>
<li><strong>Performance improvements</strong>: LiteLLM can now scale to 200 RPS per instance with a 74ms median response time.</li>
<li><strong>File Permissions</strong>:  Control file access across OpenAI, Azure, VertexAI.</li>
<li><strong>MCP x OpenAI</strong>: Use MCP servers with OpenAI Responses API.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance-improvements">Performance Improvements<a href="#performance-improvements" class="hash-link" aria-label="Performance Improvements" title="Performance Improvements"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAgElEQVR4nEWMyQrEMAxD8/8/WHroJSVrSUmzNiGgwYaZOQgb6UlCa4PzPGGthTGGr1IKMUbMOX8SIQRorRny3uO6LjjnkHPGWusP9t7xPA/DtDLGwPu+XKLl+75BjCilMLRtGwe1VqSUcBwH9n2HlBLECGpTSK3WGq+R6Cfvm30AvM+91wb8Rr4AAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/perf_imp.58abd30.640.png" srcset="/assets/ideal-img/perf_imp.58abd30.640.png 640w,/assets/ideal-img/perf_imp.9a9352a.1920.png 1920w" width="640" height="334"></noscript></div>
<br>
<p>This release brings aiohttp support for all LLM api providers. This means that LiteLLM can now scale to 200 RPS per instance with a 40ms median latency overhead.</p>
<p>This change doubles the RPS LiteLLM can scale to at this latency overhead.</p>
<p>You can opt into this by enabling the flag below. (We expect to make this the default in 1 week.)</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="flag-to-enable">Flag to enable<a href="#flag-to-enable" class="hash-link" aria-label="Flag to enable" title="Flag to enable"></a></h3>
<p><strong>On LiteLLM Proxy</strong></p>
<p>Set the <code>USE_AIOHTTP_TRANSPORT=True</code> in the environment variables.</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">Environment Variable</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv codeBlockLinesWithNumbering_o6Pm" style="counter-reset:line-count 0"><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">export USE_AIOHTTP_TRANSPORT=&quot;True&quot;</span></span><br></span></code></pre></div></div>
<p><strong>On LiteLLM Python SDK</strong></p>
<p>Set the <code>use_aiohttp_transport=True</code> to enable aiohttp transport.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">Python SDK</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv codeBlockLinesWithNumbering_o6Pm" style="counter-reset:line-count 0"><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> litellm</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain" style="display:inline-block"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">litellm</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">use_aiohttp_transport </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">True</span><span class="token plain"> </span><span class="token comment" style="color:#999988;font-style:italic"># default is False, enable this to use aiohttp transport</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">result </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> litellm</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">completion</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    model</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;openai/gpt-4o&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    messages</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">{</span><span class="token string" style="color:#e3116c">&quot;role&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;user&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;content&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Hello, world!&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain"></span><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">result</span><span class="token punctuation" style="color:#393A34">)</span></span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="file-permissions">File Permissions<a href="#file-permissions" class="hash-link" aria-label="File Permissions" title="File Permissions"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAGCAYAAAD68A/GAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAl0lEQVR4nGWOTYoCQRSD68yextO4mrXgFZx2UYu2pp36y3vVfFIwOIKLEAIfSUKMkduysG0b7o5JmNlLY98xk4ecMyklprsbPpzhhslovfNId1qRh9lSa8XNyF0cvq6clge4yLXys67k3+JBEq01TJ0m43j+5hInaORSiGuitubh/c8U+9+0GVInl0qXPsGuCfznfQwk+RMFeee/zq4zyQAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="363"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/files_api_graphic.d591e4b.640.png" srcset="/assets/ideal-img/files_api_graphic.d591e4b.640.png 640w,/assets/ideal-img/files_api_graphic.ba48eca.1920.png 1920w" width="640" height="363"></noscript></div>
<br>
<p>This release brings support for <a href="/docs/proxy/litellm_managed_files#file-permissions">File Permissions</a> and <a href="/docs/proxy/managed_finetuning">Finetuning APIs</a> to <a href="/docs/proxy/litellm_managed_files">LiteLLM Managed Files</a>. This is great for:</p>
<ul>
<li><strong>Proxy Admins</strong>: as users can only view/edit/delete files theyve created - even when using shared OpenAI/Azure/Vertex deployments.</li>
<li><strong>Developers</strong>: get a standard interface to use Files across Chat/Finetuning/Batch APIs.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<ul>
<li><strong>Gemini <a href="https://docs.litellm.ai/docs/providers/vertex" target="_blank" rel="noopener noreferrer">VertexAI</a>, <a href="https://docs.litellm.ai/docs/providers/gemini" target="_blank" rel="noopener noreferrer">Google AI Studio</a></strong>
<ul>
<li>New gemini models - <a href="https://github.com/BerriAI/litellm/pull/10991" target="_blank" rel="noopener noreferrer">PR 1</a>, <a href="https://github.com/BerriAI/litellm/pull/10998" target="_blank" rel="noopener noreferrer">PR 2</a>
<ul>
<li><code>gemini-2.5-flash-preview-tts</code></li>
<li><code>gemini-2.0-flash-preview-image-generation</code></li>
<li><code>gemini/gemini-2.5-flash-preview-05-20</code></li>
<li><code>gemini-2.5-flash-preview-05-20</code></li>
</ul>
</li>
</ul>
</li>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li>Claude-4 model family support - <a href="https://github.com/BerriAI/litellm/pull/11060" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>
<ul>
<li>Claude-4 model family support - <a href="https://github.com/BerriAI/litellm/pull/11060" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support for <code>reasoning_effort</code> and <code>thinking</code> parameters for Claude-4 - <a href="https://github.com/BerriAI/litellm/pull/11114" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex">VertexAI</a></strong>
<ul>
<li>Claude-4 model family support - <a href="https://github.com/BerriAI/litellm/pull/11060" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Global endpoints support - <a href="https://github.com/BerriAI/litellm/pull/10658" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>authorized_user credentials type support - <a href="https://github.com/BerriAI/litellm/pull/10899" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/xai">xAI</a></strong>
<ul>
<li><code>xai/grok-3</code> pricing information - <a href="https://github.com/BerriAI/litellm/pull/11028" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/lm_studio">LM Studio</a></strong>
<ul>
<li>Structured JSON schema outputs support - <a href="https://github.com/BerriAI/litellm/pull/10929" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/sambanova">SambaNova</a></strong>
<ul>
<li>Updated models and parameters - <a href="https://github.com/BerriAI/litellm/pull/10900" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/databricks">Databricks</a></strong>
<ul>
<li>Llama 4 Maverick model cost - <a href="https://github.com/BerriAI/litellm/pull/11008" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Claude 3.7 Sonnet output token cost correction - <a href="https://github.com/BerriAI/litellm/pull/11007" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure">Azure</a></strong>
<ul>
<li>Mistral Medium 25.05 support - <a href="https://github.com/BerriAI/litellm/pull/11063" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Certificate-based authentication support - <a href="https://github.com/BerriAI/litellm/pull/11069" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/mistral">Mistral</a></strong>
<ul>
<li>devstral-small-2505 model pricing and context window - <a href="https://github.com/BerriAI/litellm/pull/11103" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/ollama">Ollama</a></strong>
<ul>
<li>Wildcard model support - <a href="https://github.com/BerriAI/litellm/pull/10982" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/custom_llm_server">CustomLLM</a></strong>
<ul>
<li>Embeddings support added - <a href="https://github.com/BerriAI/litellm/pull/10980" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/featherless_ai">Featherless AI</a></strong>
<ul>
<li>Access to 4200+ models - <a href="https://github.com/BerriAI/litellm/pull/10596" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<ul>
<li><strong><a href="/docs/image_generation">Image Edits</a></strong>
<ul>
<li><code>/v1/images/edits</code> - Support for /images/edits endpoint - <a href="https://github.com/BerriAI/litellm/pull/11020" target="_blank" rel="noopener noreferrer">PR</a> <a href="https://github.com/BerriAI/litellm/pull/11123" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Content policy violation error mapping - <a href="https://github.com/BerriAI/litellm/pull/11113" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/response_api">Responses API</a></strong>
<ul>
<li>MCP support for Responses API - <a href="https://github.com/BerriAI/litellm/pull/11029" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/fine_tuning">Files API</a></strong>
<ul>
<li>LiteLLM Managed Files support for finetuning - <a href="https://github.com/BerriAI/litellm/pull/11039" target="_blank" rel="noopener noreferrer">PR</a> <a href="https://github.com/BerriAI/litellm/pull/11040" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Validation for file operations (retrieve/list/delete) - <a href="https://github.com/BerriAI/litellm/pull/11081" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<ul>
<li><strong>Teams</strong>
<ul>
<li>Key and member count display - <a href="https://github.com/BerriAI/litellm/pull/10950" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Spend rounded to 4 decimal points - <a href="https://github.com/BerriAI/litellm/pull/11013" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Organization and team create buttons repositioned - <a href="https://github.com/BerriAI/litellm/pull/10948" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Keys</strong>
<ul>
<li>Key reassignment and &#x27;updated at&#x27; column - <a href="https://github.com/BerriAI/litellm/pull/10960" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Show model access groups during creation - <a href="https://github.com/BerriAI/litellm/pull/10965" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Logs</strong>
<ul>
<li>Model filter on logs - <a href="https://github.com/BerriAI/litellm/pull/11048" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Passthrough endpoint error logs support - <a href="https://github.com/BerriAI/litellm/pull/10990" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Guardrails</strong>
<ul>
<li>Config.yaml guardrails display - <a href="https://github.com/BerriAI/litellm/pull/10959" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Organizations/Users</strong>
<ul>
<li>Spend rounded to 4 decimal points - <a href="https://github.com/BerriAI/litellm/pull/11023" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Show clear error when adding a user to a team - <a href="https://github.com/BerriAI/litellm/pull/10978" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Audit Logs</strong>
<ul>
<li><code>/list</code> and <code>/info</code> endpoints for Audit Logs - <a href="https://github.com/BerriAI/litellm/pull/11102" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--alerting-integrations">Logging / Alerting Integrations<a href="#logging--alerting-integrations" class="hash-link" aria-label="Logging / Alerting Integrations" title="Logging / Alerting Integrations"></a></h2>
<ul>
<li><strong><a href="/docs/proxy/prometheus">Prometheus</a></strong>
<ul>
<li>Track <code>route</code> on proxy_* metrics - <a href="https://github.com/BerriAI/litellm/pull/10992" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#langfuse">Langfuse</a></strong>
<ul>
<li>Support for <code>prompt_label</code> parameter - <a href="https://github.com/BerriAI/litellm/pull/11018" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Consistent modelParams logging - <a href="https://github.com/BerriAI/litellm/pull/11018" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#deepeval">DeepEval/ConfidentAI</a></strong>
<ul>
<li>Logging enabled for proxy and SDK - <a href="https://github.com/BerriAI/litellm/pull/10649" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging">Logfire</a></strong>
<ul>
<li>Fix otel proxy server initialization when using Logfire - <a href="https://github.com/BerriAI/litellm/pull/11091" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="authentication--security">Authentication &amp; Security<a href="#authentication--security" class="hash-link" aria-label="Authentication &amp; Security" title="Authentication &amp; Security"></a></h2>
<ul>
<li><strong><a href="/docs/proxy/token_auth">JWT Authentication</a></strong>
<ul>
<li>Support for applying default internal user parameters when upserting a user via JWT authentication - <a href="https://github.com/BerriAI/litellm/pull/10995" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Map a user to a team when upserting a user via JWT authentication - <a href="https://github.com/BerriAI/litellm/pull/11108" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Custom Auth</strong>
<ul>
<li>Support for switching between custom auth and API key auth - <a href="https://github.com/BerriAI/litellm/pull/11070" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--reliability-improvements">Performance / Reliability Improvements<a href="#performance--reliability-improvements" class="hash-link" aria-label="Performance / Reliability Improvements" title="Performance / Reliability Improvements"></a></h2>
<ul>
<li><strong>aiohttp Transport</strong>
<ul>
<li>97% lower median latency (feature flagged) - <a href="https://github.com/BerriAI/litellm/pull/11097" target="_blank" rel="noopener noreferrer">PR</a> <a href="https://github.com/BerriAI/litellm/pull/11132" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Background Health Checks</strong>
<ul>
<li>Improved reliability - <a href="https://github.com/BerriAI/litellm/pull/10887" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Response Handling</strong>
<ul>
<li>Better streaming status code detection - <a href="https://github.com/BerriAI/litellm/pull/10962" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Response ID propagation improvements - <a href="https://github.com/BerriAI/litellm/pull/11006" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Thread Management</strong>
<ul>
<li>Removed error-creating threads for reliability - <a href="https://github.com/BerriAI/litellm/pull/11066" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<ul>
<li><strong><a href="/docs/proxy/cli">Proxy CLI</a></strong>
<ul>
<li>Skip server startup flag - <a href="https://github.com/BerriAI/litellm/pull/10665" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Avoid DATABASE_URL override when provided - <a href="https://github.com/BerriAI/litellm/pull/11076" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Model Management</strong>
<ul>
<li>Clear cache and reload after model updates - <a href="https://github.com/BerriAI/litellm/pull/10853" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Computer use support tracking - <a href="https://github.com/BerriAI/litellm/pull/10881" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Helm Chart</strong>
<ul>
<li>LoadBalancer class support - <a href="https://github.com/BerriAI/litellm/pull/11064" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes">Bug Fixes<a href="#bug-fixes" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h2>
<p>This release includes numerous bug fixes to improve stability and reliability:</p>
<ul>
<li>
<p><strong>LLM Provider Fixes</strong></p>
<ul>
<li>VertexAI:<!-- -->
<ul>
<li>Fixed quota_project_id parameter issue - <a href="https://github.com/BerriAI/litellm/pull/10915" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed credential refresh exceptions - <a href="https://github.com/BerriAI/litellm/pull/10969" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li>Cohere:
Fixes for adding Cohere models through LiteLLM UI - <a href="https://github.com/BerriAI/litellm/pull/10822" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Anthropic:<!-- -->
<ul>
<li>Fixed streaming dict object handling for /v1/messages - <a href="https://github.com/BerriAI/litellm/pull/11032" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li>OpenRouter:<!-- -->
<ul>
<li>Fixed stream usage ID issues - <a href="https://github.com/BerriAI/litellm/pull/11004" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Authentication &amp; Users</strong></p>
<ul>
<li>Fixed invitation email link generation - <a href="https://github.com/BerriAI/litellm/pull/10958" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed JWT authentication default role - <a href="https://github.com/BerriAI/litellm/pull/10995" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed user budget reset functionality - <a href="https://github.com/BerriAI/litellm/pull/10993" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed SSO user compatibility and email validation - <a href="https://github.com/BerriAI/litellm/pull/11106" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li>
<p><strong>Database &amp; Infrastructure</strong></p>
<ul>
<li>Fixed DB connection parameter handling - <a href="https://github.com/BerriAI/litellm/pull/10842" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed email invitation link  - <a href="https://github.com/BerriAI/litellm/pull/11031" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li>
<p><strong>UI &amp; Display</strong></p>
<ul>
<li>Fixed MCP tool rendering when no arguments required - <a href="https://github.com/BerriAI/litellm/pull/11012" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed team model alias deletion - <a href="https://github.com/BerriAI/litellm/pull/11121" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed team viewer permissions - <a href="https://github.com/BerriAI/litellm/pull/11127" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li>
<p><strong>Model &amp; Routing</strong></p>
<ul>
<li>Fixed team model mapping in route requests - <a href="https://github.com/BerriAI/litellm/pull/11111" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed standard optional parameter passing - <a href="https://github.com/BerriAI/litellm/pull/11124" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li><a href="https://github.com/DarinVerheijke" target="_blank" rel="noopener noreferrer">@DarinVerheijke</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10596" target="_blank" rel="noopener noreferrer">#10596</a></li>
<li><a href="https://github.com/estsauver" target="_blank" rel="noopener noreferrer">@estsauver</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10929" target="_blank" rel="noopener noreferrer">#10929</a></li>
<li><a href="https://github.com/mohittalele" target="_blank" rel="noopener noreferrer">@mohittalele</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10665" target="_blank" rel="noopener noreferrer">#10665</a></li>
<li><a href="https://github.com/pselden" target="_blank" rel="noopener noreferrer">@pselden</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10899" target="_blank" rel="noopener noreferrer">#10899</a></li>
<li><a href="https://github.com/unrealandychan" target="_blank" rel="noopener noreferrer">@unrealandychan</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10842" target="_blank" rel="noopener noreferrer">#10842</a></li>
<li><a href="https://github.com/dastaiger" target="_blank" rel="noopener noreferrer">@dastaiger</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10946" target="_blank" rel="noopener noreferrer">#10946</a></li>
<li><a href="https://github.com/slytechnical" target="_blank" rel="noopener noreferrer">@slytechnical</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10881" target="_blank" rel="noopener noreferrer">#10881</a></li>
<li><a href="https://github.com/daarko10" target="_blank" rel="noopener noreferrer">@daarko10</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/11006" target="_blank" rel="noopener noreferrer">#11006</a></li>
<li><a href="https://github.com/sorenmat" target="_blank" rel="noopener noreferrer">@sorenmat</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10658" target="_blank" rel="noopener noreferrer">#10658</a></li>
<li><a href="https://github.com/matthid" target="_blank" rel="noopener noreferrer">@matthid</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10982" target="_blank" rel="noopener noreferrer">#10982</a></li>
<li><a href="https://github.com/jgowdy-godaddy" target="_blank" rel="noopener noreferrer">@jgowdy-godaddy</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/11032" target="_blank" rel="noopener noreferrer">#11032</a></li>
<li><a href="https://github.com/bepotp" target="_blank" rel="noopener noreferrer">@bepotp</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/11008" target="_blank" rel="noopener noreferrer">#11008</a></li>
<li><a href="https://github.com/jmorenoc-o" target="_blank" rel="noopener noreferrer">@jmorenoc-o</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/11031" target="_blank" rel="noopener noreferrer">#11031</a></li>
<li><a href="https://github.com/martin-liu" target="_blank" rel="noopener noreferrer">@martin-liu</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/11076" target="_blank" rel="noopener noreferrer">#11076</a></li>
<li><a href="https://github.com/gunjan-solanki" target="_blank" rel="noopener noreferrer">@gunjan-solanki</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/11064" target="_blank" rel="noopener noreferrer">#11064</a></li>
<li><a href="https://github.com/tokoko" target="_blank" rel="noopener noreferrer">@tokoko</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10980" target="_blank" rel="noopener noreferrer">#10980</a></li>
<li><a href="https://github.com/spike-spiegel-21" target="_blank" rel="noopener noreferrer">@spike-spiegel-21</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10649" target="_blank" rel="noopener noreferrer">#10649</a></li>
<li><a href="https://github.com/kreatoo" target="_blank" rel="noopener noreferrer">@kreatoo</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10927" target="_blank" rel="noopener noreferrer">#10927</a></li>
<li><a href="https://github.com/baejooc" target="_blank" rel="noopener noreferrer">@baejooc</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10887" target="_blank" rel="noopener noreferrer">#10887</a></li>
<li><a href="https://github.com/keykbd" target="_blank" rel="noopener noreferrer">@keykbd</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/11114" target="_blank" rel="noopener noreferrer">#11114</a></li>
<li><a href="https://github.com/dalssoft" target="_blank" rel="noopener noreferrer">@dalssoft</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/11088" target="_blank" rel="noopener noreferrer">#11088</a></li>
<li><a href="https://github.com/jtong99" target="_blank" rel="noopener noreferrer">@jtong99</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10853" target="_blank" rel="noopener noreferrer">#10853</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="demo-instance">Demo Instance<a href="#demo-instance" class="hash-link" aria-label="Demo Instance" title="Demo Instance"></a></h2>
<p>Here&#x27;s a Demo Instance to test changes:</p>
<ul>
<li>Instance: <a href="https://demo.litellm.ai/" target="_blank" rel="noopener noreferrer">https://demo.litellm.ai/</a></li>
<li>Login Credentials:<!-- -->
<ul>
<li>Username: admin</li>
<li>Password: sk-1234</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="git-diff"><a href="https://github.com/BerriAI/litellm/releases" target="_blank" rel="noopener noreferrer">Git Diff</a><a href="#git-diff" class="hash-link" aria-label="git-diff" title="git-diff"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.70.1-stable">v1.70.1-stable - Gemini Realtime API Support</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-05-17T10:00:00.000Z">2025517</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:main-v1.70.1-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.70.1</span><br></span></code></pre></div></div></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<p>LiteLLM v1.70.1-stable is live now. Here are the key highlights of this release:</p>
<ul>
<li><strong>Gemini Realtime API</strong>: You can now call Gemini&#x27;s Live API via the OpenAI /v1/realtime API</li>
<li><strong>Spend Logs Retention Period</strong>: Enable deleting spend logs older than a certain period.</li>
<li><strong>PII Masking 2.0</strong>: Easily configure masking or blocking specific PII/PHI entities on the UI</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="gemini-realtime-api">Gemini Realtime API<a href="#gemini-realtime-api" class="hash-link" aria-label="Gemini Realtime API" title="Gemini Realtime API"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAfElEQVR4nE3OSw6DMBRD0ex/hzBjwiwPqJTkOR9uFaqqtXSGlh1iNGKMmBnXdVGrkBypUutP2LaNdV1ZloV930kpc9hBOk9qKTR3mkTovTONMRij4y48iy7RW4P7fgRpTn3MiZwzLzPOaKSUuPkk/P+Y3B0vTinlKX0/vgExUsHeR4lbvAAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/gemini_realtime.21d4b5f.640.png" srcset="/assets/ideal-img/gemini_realtime.21d4b5f.640.png 640w,/assets/ideal-img/gemini_realtime.78eed5c.1920.png 1920w" width="640" height="334"></noscript></div>
<p>This release brings support for calling Gemini&#x27;s realtime models (e.g. gemini-2.0-flash-live) via OpenAI&#x27;s/v1/realtimeAPI. This is great for developers as it lets them easily switch from OpenAI to Gemini by just changing the model name.</p>
<p>Key Highlights:</p>
<ul>
<li>Support for text + audio input/output</li>
<li>Support for setting session configurations (modality, instructions, activity detection) in the OpenAI format</li>
<li>Support for logging + usage tracking for realtime sessions</li>
</ul>
<p>This is currently supported via Google AI Studio. We plan to release VertexAI support over the coming week.</p>
<p><a href="/docs/providers/google_ai_studio/realtime"><strong>Read more</strong></a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-logs-retention-period">Spend Logs Retention Period<a href="#spend-logs-retention-period" class="hash-link" aria-label="Spend Logs Retention Period" title="Spend Logs Retention Period"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAEAAoDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAMI/8QAGxAAAgMAAwAAAAAAAAAAAAAAAQIAESEDUbH/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEQMRAD8A06nEqUFLAXesT7LZ0IiB/9k=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="248"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/delete_spend_logs.e9a24e6.640.jpg" srcset="/assets/ideal-img/delete_spend_logs.e9a24e6.640.jpg 640w,/assets/ideal-img/delete_spend_logs.8deb096.1920.jpg 1920w" width="640" height="248"></noscript></div>
<p>This release enables deleting LiteLLM Spend Logs older than a certain period. Since we now enable storing the raw request/response in the logs, deleting old logs ensures the database remains performant in production.</p>
<p><a href="/docs/proxy/spend_logs_deletion"><strong>Read more</strong></a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="pii-masking-20">PII Masking 2.0<a href="#pii-masking-20" class="hash-link" aria-label="PII Masking 2.0" title="PII Masking 2.0"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAGCAYAAAD68A/GAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAiElEQVR4nEWOuQ7EMAhE/f+fmCpFyhQr38PhWeGVtUhPIJgBUq2VYwyqKgFQRH4gMnYvSM/z8Lou3vfNnDPnBGttrLOzdKWI0cyYYvi+7xaFc8xJNyMU7BCeSHEmHHE66tKFuSkbnLb8Lzw/nP9K7fyUxiFKVaO5b/bGAyBsfdCXEyKcEPpamy/77un/yA3YUQAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="384"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/pii_masking_v2.42e1f89.640.png" srcset="/assets/ideal-img/pii_masking_v2.42e1f89.640.png 640w,/assets/ideal-img/pii_masking_v2.cb620f7.1920.png 1920w" width="640" height="384"></noscript></div>
<p>This release brings improvements to our Presidio PII Integration. As a Proxy Admin, you now have the ability to:</p>
<ul>
<li>Mask or block specific entities (e.g., block medical licenses while masking other entities like emails).</li>
<li>Monitor guardrails in production. LiteLLM Logs will now show you the guardrail run, the entities it detected, and its confidence score for each entity.</li>
</ul>
<p><a href="/docs/proxy/guardrails/pii_masking_v2"><strong>Read more</strong></a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<ul>
<li><strong>Gemini (<a href="https://docs.litellm.ai/docs/providers/vertex#usage-with-litellm-proxy-server" target="_blank" rel="noopener noreferrer">VertexAI</a> + <a href="https://docs.litellm.ai/docs/providers/gemini" target="_blank" rel="noopener noreferrer">Google AI Studio</a>)</strong>
<ul>
<li><code>/chat/completion</code>
<ul>
<li>Handle audio input - <a href="https://github.com/BerriAI/litellm/pull/10739" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixes maximum recursion depth issue when using deeply nested response schemas with Vertex AI by Increasing DEFAULT_MAX_RECURSE_DEPTH from 10 to 100 in constants. <a href="https://github.com/BerriAI/litellm/pull/10798" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Capture reasoning tokens in streaming mode - <a href="https://github.com/BerriAI/litellm/pull/10789" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
</li>
<li><strong><a href="/docs/providers/google_ai_studio/realtime">Google AI Studio</a></strong>
<ul>
<li><code>/realtime</code>
<ul>
<li>Gemini Multimodal Live API support</li>
<li>Audio input/output support, optional param mapping, accurate usage calculation - <a href="https://github.com/BerriAI/litellm/pull/10909" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex#metallama-api">VertexAI</a></strong>
<ul>
<li><code>/chat/completion</code>
<ul>
<li>Fix llama streaming error - where model response was nested in returned streaming chunk - <a href="https://github.com/BerriAI/litellm/pull/10878" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
</li>
<li><strong><a href="/docs/providers/ollama">Ollama</a></strong>
<ul>
<li><code>/chat/completion</code>
<ul>
<li>structure responses fix - <a href="https://github.com/BerriAI/litellm/pull/10617" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock#litellm-proxy-usage">Bedrock</a></strong>
<ul>
<li><a href="/docs/providers/bedrock#litellm-proxy-usage"><code>/chat/completion</code></a>
<ul>
<li>Handle thinking_blocks when assistant.content is None - <a href="https://github.com/BerriAI/litellm/pull/10688" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixes to only allow accepted fields for tool json schema - <a href="https://github.com/BerriAI/litellm/pull/10062" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add bedrock sonnet prompt caching cost information</li>
<li>Mistral Pixtral support - <a href="https://github.com/BerriAI/litellm/pull/10439" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Tool caching support - <a href="https://github.com/BerriAI/litellm/pull/10897" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><a href="/docs/anthropic_unified"><code>/messages</code></a>
<ul>
<li>allow using dynamic AWS Params - <a href="https://github.com/BerriAI/litellm/pull/10769" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
</li>
<li><strong><a href="/docs/providers/nvidia_nim">Nvidia NIM</a></strong>
<ul>
<li><a href="/docs/providers/nvidia_nim#usage---litellm-proxy-server"><code>/chat/completion</code></a>
<ul>
<li>Add tools, tool_choice, parallel_tool_calls support - <a href="https://github.com/BerriAI/litellm/pull/10763" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
</li>
<li><strong><a href="/docs/providers/novita">Novita AI</a></strong>
<ul>
<li>New Provider added for <code>/chat/completion</code> routes - <a href="https://github.com/BerriAI/litellm/pull/9527" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure">Azure</a></strong>
<ul>
<li><a href="/docs/providers/azure#image-generation"><code>/image/generation</code></a>
<ul>
<li>Fix azure dall e 3 call with custom model name - <a href="https://github.com/BerriAI/litellm/pull/10776" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
</li>
<li><strong><a href="/docs/providers/cohere">Cohere</a></strong>
<ul>
<li><a href="/docs/providers/cohere#embedding"><code>/embeddings</code></a>
<ul>
<li>Migrate embedding to use <code>/v2/embed</code> - adds support for output_dimensions param - <a href="https://github.com/BerriAI/litellm/pull/10809" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
</li>
<li><strong><a href="/docs/providers/anthropic">Anthropic</a></strong>
<ul>
<li><a href="/docs/providers/anthropic#usage-with-litellm-proxy"><code>/chat/completion</code></a>
<ul>
<li>Web search tool support - native + openai format - <a href="/docs/providers/anthropic#anthropic-hosted-tools-computer-text-editor-web-search">Get Started</a></li>
</ul>
</li>
</ul>
</li>
<li><strong><a href="/docs/providers/vllm">VLLM</a></strong>
<ul>
<li><a href="/docs/providers/vllm#embeddings"><code>/embeddings</code></a>
<ul>
<li>Support embedding input as list of integers</li>
</ul>
</li>
</ul>
</li>
<li><strong><a href="/docs/providers/openai">OpenAI</a></strong>
<ul>
<li><a href="/docs/providers/openai#usage---litellm-proxy-server"><code>/chat/completion</code></a>
<ul>
<li>Fix - b64 file data input handling - <a href="/docs/providers/openai#pdf-file-parsing">Get Started</a></li>
<li>Add supports_pdf_input to all vision models - <a href="https://github.com/BerriAI/litellm/pull/10897" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<ul>
<li><a href="/docs/response_api"><strong>Responses API</strong></a>
<ul>
<li>Fix delete API support - <a href="https://github.com/BerriAI/litellm/pull/10845" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><a href="/docs/rerank"><strong>Rerank API</strong></a>
<ul>
<li><code>/v2/rerank</code> now registered as llm_api_route - enabling non-admins to call it - <a href="https://github.com/BerriAI/litellm/pull/10861" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-improvements">Spend Tracking Improvements<a href="#spend-tracking-improvements" class="hash-link" aria-label="Spend Tracking Improvements" title="Spend Tracking Improvements"></a></h2>
<ul>
<li><strong><code>/chat/completion</code>, <code>/messages</code></strong>
<ul>
<li>Anthropic - web search tool cost tracking - <a href="https://github.com/BerriAI/litellm/pull/10846" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Groq - update model max tokens + cost information - <a href="https://github.com/BerriAI/litellm/pull/10077" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><code>/audio/transcription</code></strong>
<ul>
<li>Azure - Add gpt-4o-mini-tts pricing - <a href="https://github.com/BerriAI/litellm/pull/10807" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Proxy - Fix tracking spend by tag - <a href="https://github.com/BerriAI/litellm/pull/10832" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><code>/embeddings</code></strong>
<ul>
<li>Azure AI - Add cohere embed v4 pricing - <a href="https://github.com/BerriAI/litellm/pull/10806" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<ul>
<li><strong>Models</strong>
<ul>
<li>Ollama - adds api base param to UI</li>
</ul>
</li>
<li><strong>Logs</strong>
<ul>
<li>Add team id, key alias, key hash filter on logs - <a href="https://github.com/BerriAI/litellm/pull/10831" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/10831</a></li>
<li>Guardrail tracing now in Logs UI - <a href="https://github.com/BerriAI/litellm/pull/10893" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/10893</a></li>
</ul>
</li>
<li><strong>Teams</strong>
<ul>
<li>Patch for updating team info when team in org and members not in org - <a href="https://github.com/BerriAI/litellm/pull/10835" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/10835</a></li>
</ul>
</li>
<li><strong>Guardrails</strong>
<ul>
<li>Add Bedrock, Presidio, Lakers guardrails on UI - <a href="https://github.com/BerriAI/litellm/pull/10874" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/10874</a></li>
<li>See guardrail info page - <a href="https://github.com/BerriAI/litellm/pull/10904" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/10904</a></li>
<li>Allow editing guardrails on UI - <a href="https://github.com/BerriAI/litellm/pull/10907" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/10907</a></li>
</ul>
</li>
<li><strong>Test Key</strong>
<ul>
<li>select guardrails to test on UI</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--alerting-integrations">Logging / Alerting Integrations<a href="#logging--alerting-integrations" class="hash-link" aria-label="Logging / Alerting Integrations" title="Logging / Alerting Integrations"></a></h2>
<ul>
<li><strong><a href="/docs/proxy/logging_spec">StandardLoggingPayload</a></strong>
<ul>
<li>Log any <code>x-</code> headers in requester metadata - <a href="/docs/proxy/logging_spec#standardloggingmetadata">Get Started</a></li>
<li>Guardrail tracing now in standard logging payload - <a href="/docs/proxy/logging_spec#standardloggingguardrailinformation">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/logging#custom-callback-apis-async">Generic API Logger</a></strong>
<ul>
<li>Support passing application/json header</li>
</ul>
</li>
<li><strong><a href="/docs/observability/phoenix_integration">Arize Phoenix</a></strong>
<ul>
<li>fix: URL encode OTEL_EXPORTER_OTLP_TRACES_HEADERS for Phoenix Integration - <a href="https://github.com/BerriAI/litellm/pull/10654" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>add guardrail tracing to OTEL, Arize phoenix - <a href="https://github.com/BerriAI/litellm/pull/10896" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/pagerduty">PagerDuty</a></strong>
<ul>
<li>Pagerduty is now a free feature - <a href="https://github.com/BerriAI/litellm/pull/10857" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/alerting">Alerting</a></strong>
<ul>
<li>Sending slack alerts on virtual key/user/team updates is now free - <a href="https://github.com/BerriAI/litellm/pull/10863" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="guardrails">Guardrails<a href="#guardrails" class="hash-link" aria-label="Guardrails" title="Guardrails"></a></h2>
<ul>
<li><strong>Guardrails</strong>
<ul>
<li>New <code>/apply_guardrail</code> endpoint for directly testing a guardrail - <a href="https://github.com/BerriAI/litellm/pull/10867" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/lakera_ai">Lakera</a></strong>
<ul>
<li><code>/v2</code> endpoints support - <a href="https://github.com/BerriAI/litellm/pull/10880" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/pii_masking_v2">Presidio</a></strong>
<ul>
<li>Fixes handling of message content on presidio guardrail integration - <a href="https://github.com/BerriAI/litellm/pull/10197" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Allow specifying PII Entities Config - <a href="https://github.com/BerriAI/litellm/pull/10810" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/guardrails/aim_security">Aim Security</a></strong>
<ul>
<li>Support for anonymization in AIM Guardrails - <a href="https://github.com/BerriAI/litellm/pull/10757" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<ul>
<li><strong>Allow overriding all constants using a .env variable</strong> - <a href="https://github.com/BerriAI/litellm/pull/10803" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong><a href="/docs/proxy/spend_logs_deletion">Maximum retention period for spend logs</a></strong>
<ul>
<li>Add retention flag to config - <a href="https://github.com/BerriAI/litellm/pull/10815" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support for cleaning up logs based on configured time period - <a href="https://github.com/BerriAI/litellm/pull/10872" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<ul>
<li><strong>Authentication</strong>
<ul>
<li>HandleBearer $LITELLM_API_KEYinx-litellm-api-keycustom header <a href="https://github.com/BerriAI/litellm/pull/10776" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>New Enterprise pip package</strong> - <code>litellm-enterprise</code> - fixes issue where <code>enterprise</code> folder was not found when using pip package</li>
<li><strong><a href="/docs/proxy/management_cli">Proxy CLI</a></strong>
<ul>
<li>Add <code>models import</code> command - <a href="https://github.com/BerriAI/litellm/pull/10581" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/tutorials/openweb_ui#per-user-tracking">OpenWebUI</a></strong>
<ul>
<li>Configure LiteLLM to Parse User Headers from Open Web UI</li>
</ul>
</li>
<li><strong><a href="/docs/providers/litellm_proxy#send-all-sdk-requests-to-litellm-proxy">LiteLLM Proxy w/ LiteLLM SDK</a></strong>
<ul>
<li>Option to force/always use the litellm proxy when calling via LiteLLM SDK</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li><a href="https://github.com/imdigitalashish" target="_blank" rel="noopener noreferrer">@imdigitalashish</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10617" target="_blank" rel="noopener noreferrer">#10617</a></li>
<li><a href="https://github.com/LouisShark" target="_blank" rel="noopener noreferrer">@LouisShark</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10688" target="_blank" rel="noopener noreferrer">#10688</a></li>
<li><a href="https://github.com/OscarSavNS" target="_blank" rel="noopener noreferrer">@OscarSavNS</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10764" target="_blank" rel="noopener noreferrer">#10764</a></li>
<li><a href="https://github.com/arizedatngo" target="_blank" rel="noopener noreferrer">@arizedatngo</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10654" target="_blank" rel="noopener noreferrer">#10654</a></li>
<li><a href="https://github.com/jugaldb" target="_blank" rel="noopener noreferrer">@jugaldb</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10805" target="_blank" rel="noopener noreferrer">#10805</a></li>
<li><a href="https://github.com/daikeren" target="_blank" rel="noopener noreferrer">@daikeren</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10781" target="_blank" rel="noopener noreferrer">#10781</a></li>
<li><a href="https://github.com/naliotopier" target="_blank" rel="noopener noreferrer">@naliotopier</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10077" target="_blank" rel="noopener noreferrer">#10077</a></li>
<li><a href="https://github.com/damienpontifex" target="_blank" rel="noopener noreferrer">@damienpontifex</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10813" target="_blank" rel="noopener noreferrer">#10813</a></li>
<li><a href="https://github.com/Dima-Mediator" target="_blank" rel="noopener noreferrer">@Dima-Mediator</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10789" target="_blank" rel="noopener noreferrer">#10789</a></li>
<li><a href="https://github.com/igtm" target="_blank" rel="noopener noreferrer">@igtm</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10814" target="_blank" rel="noopener noreferrer">#10814</a></li>
<li><a href="https://github.com/shibaboy" target="_blank" rel="noopener noreferrer">@shibaboy</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10752" target="_blank" rel="noopener noreferrer">#10752</a></li>
<li><a href="https://github.com/camfarineau" target="_blank" rel="noopener noreferrer">@camfarineau</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10629" target="_blank" rel="noopener noreferrer">#10629</a></li>
<li><a href="https://github.com/ajac-zero" target="_blank" rel="noopener noreferrer">@ajac-zero</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10439" target="_blank" rel="noopener noreferrer">#10439</a></li>
<li><a href="https://github.com/damgem" target="_blank" rel="noopener noreferrer">@damgem</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/9802" target="_blank" rel="noopener noreferrer">#9802</a></li>
<li><a href="https://github.com/hxdror" target="_blank" rel="noopener noreferrer">@hxdror</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10757" target="_blank" rel="noopener noreferrer">#10757</a></li>
<li><a href="https://github.com/wwwillchen" target="_blank" rel="noopener noreferrer">@wwwillchen</a> made their first contribution in PR <a href="https://github.com/BerriAI/litellm/pull/10894" target="_blank" rel="noopener noreferrer">#10894</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="demo-instance">Demo Instance<a href="#demo-instance" class="hash-link" aria-label="Demo Instance" title="Demo Instance"></a></h2>
<p>Here&#x27;s a Demo Instance to test changes:</p>
<ul>
<li>Instance: <a href="https://demo.litellm.ai/" target="_blank" rel="noopener noreferrer">https://demo.litellm.ai/</a></li>
<li>Login Credentials:<!-- -->
<ul>
<li>Username: admin</li>
<li>Password: sk-1234</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="git-diff"><a href="https://github.com/BerriAI/litellm/releases" target="_blank" rel="noopener noreferrer">Git Diff</a><a href="#git-diff" class="hash-link" aria-label="git-diff" title="git-diff"></a></h2></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.69.0-stable">v1.69.0-stable - Loadbalance Batch API Models</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-05-10T10:00:00.000Z">2025510</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:main-v1.69.0-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.69.0.post1</span><br></span></code></pre></div></div></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<p>LiteLLM v1.69.0-stable brings the following key improvements:</p>
<ul>
<li><strong>Loadbalance Batch API Models</strong>: Easily loadbalance across multiple azure batch deployments using LiteLLM Managed Files</li>
<li><strong>Email Invites 2.0</strong>: Send new users onboarded to LiteLLM an email invite.</li>
<li><strong>Nscale</strong>: LLM API for compliance with European regulations.</li>
<li><strong>Bedrock /v1/messages</strong>: Use Bedrock Anthropic models with Anthropic&#x27;s /v1/messages.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="batch-api-load-balancing">Batch API Load Balancing<a href="#batch-api-load-balancing" class="hash-link" aria-label="Batch API Load Balancing" title="Batch API Load Balancing"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAGCAYAAAD68A/GAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAe0lEQVR4nGWOzQ7CMAyD9/4vChNVq/XHaVoRo3Q7MDhYPsT57C3GyBACSykcY/xIzexNVc2biLDWSgC3kOpgrs3mnOy95+2fMvxAQPh4RXMQgJPo9Smlmx9H5jMk8ycROYOttUVyd6kqIXIFL+J3pe9xrXpV7iEZ0BbxA9FO6IBqnorWAAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="363"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/lb_batch.55c1da1.640.png" srcset="/assets/ideal-img/lb_batch.55c1da1.640.png 640w,/assets/ideal-img/lb_batch.d18a26b.1920.png 1920w" width="640" height="363"></noscript></div>
<p>This release brings LiteLLM Managed File support to Batches. This is great for:</p>
<ul>
<li>Proxy Admins: You can now control which Batch models users can call.</li>
<li>Developers: You no longer need to know the Azure deployment name when creating your batch .jsonl files - just specify the model your LiteLLM key has access to.</li>
</ul>
<p>Over time, we expect LiteLLM Managed Files to be the way most teams use Files across <code>/chat/completions</code>, <code>/batch</code>, <code>/fine_tuning</code> endpoints.</p>
<p><a href="https://docs.litellm.ai/docs/proxy/managed_batches" target="_blank" rel="noopener noreferrer">Read more here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="email-invites">Email Invites<a href="#email-invites" class="hash-link" aria-label="Email Invites" title="Email Invites"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAGCAYAAAD68A/GAAAACXBIWXMAACxLAAAsSwGlPZapAAAAi0lEQVR4nFWOQQrDMAwE/f+n5QG9hVxC8aElbhzcWrI1RQkuVLAIltVoQ84ZV631lIj8aXhhXVfmeWZZFlJKSGuUd6G1Ru+G2aXgtG3bOI4DVaVW4R4flJIx6/Te8QmO9cDYTtr3nWmK3G5PzBQzCKPb6OJvRIUYCynpSfsRR+mLrKTXgagffk7fg1+9U+koBm7QPwAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="384"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/email_2_0.0820e1b.640.png" srcset="/assets/ideal-img/email_2_0.0820e1b.640.png 640w,/assets/ideal-img/email_2_0.de676c8.1920.png 1920w" width="640" height="384"></noscript></div>
<p>This release brings the following improvements to our email invite integration:</p>
<ul>
<li>New templates for user invited and key created events.</li>
<li>Fixes for using SMTP email providers.</li>
<li>Native support for Resend API.</li>
<li>Ability for Proxy Admins to control email events.</li>
</ul>
<p>For LiteLLM Cloud Users, please reach out to us if you want this enabled for your instance.</p>
<p><a href="https://docs.litellm.ai/docs/proxy/email" target="_blank" rel="noopener noreferrer">Read more here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<ul>
<li><strong>Gemini (<a href="https://docs.litellm.ai/docs/providers/vertex#usage-with-litellm-proxy-server" target="_blank" rel="noopener noreferrer">VertexAI</a> + <a href="https://docs.litellm.ai/docs/providers/gemini" target="_blank" rel="noopener noreferrer">Google AI Studio</a>)</strong>
<ul>
<li>Added <code>gemini-2.5-pro-preview-05-06</code> models with pricing and context window info - <a href="https://github.com/BerriAI/litellm/pull/10597" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Set correct context window length for all Gemini 2.5 variants - <a href="https://github.com/BerriAI/litellm/pull/10690" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/perplexity">Perplexity</a></strong>:<!-- -->
<ul>
<li>Added new Perplexity models - <a href="https://github.com/BerriAI/litellm/pull/10652" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added sonar-deep-research model pricing - <a href="https://github.com/BerriAI/litellm/pull/10537" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure">Azure OpenAI</a></strong>:<!-- -->
<ul>
<li>Fixed passing through of azure_ad_token_provider parameter - <a href="https://github.com/BerriAI/litellm/pull/10694" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openai">OpenAI</a></strong>:<!-- -->
<ul>
<li>Added support for pdf url&#x27;s in &#x27;file&#x27; parameter - <a href="https://github.com/BerriAI/litellm/pull/10640" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/aws_sagemaker">Sagemaker</a></strong>:<!-- -->
<ul>
<li>Fix content length for <code>sagemaker_chat</code> provider - <a href="https://github.com/BerriAI/litellm/pull/10607" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure_ai">Azure AI Foundry</a></strong>:<!-- -->
<ul>
<li>Added cost tracking for the following models <a href="https://github.com/BerriAI/litellm/pull/9956" target="_blank" rel="noopener noreferrer">PR</a>
<ul>
<li>DeepSeek V3 0324</li>
<li>Llama 4 Scout</li>
<li>Llama 4 Maverick</li>
</ul>
</li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock">Bedrock</a></strong>:<!-- -->
<ul>
<li>Added cost tracking for Bedrock Llama 4 models - <a href="https://github.com/BerriAI/litellm/pull/10582" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed template conversion for Llama 4 models in Bedrock - <a href="https://github.com/BerriAI/litellm/pull/10582" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added support for using Bedrock Anthropic models with /v1/messages format - <a href="https://github.com/BerriAI/litellm/pull/10681" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added streaming support for Bedrock Anthropic models with /v1/messages format - <a href="https://github.com/BerriAI/litellm/pull/10710" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openai">OpenAI</a></strong>: Added <code>reasoning_effort</code> support for <code>o3</code> models - <a href="https://github.com/BerriAI/litellm/pull/10591" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong><a href="/docs/providers/databricks">Databricks</a></strong>:<!-- -->
<ul>
<li>Fixed issue when Databricks uses external model and delta could be empty - <a href="https://github.com/BerriAI/litellm/pull/10540" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/cerebras">Cerebras</a></strong>: Fixed Llama-3.1-70b model pricing and context window - <a href="https://github.com/BerriAI/litellm/pull/10648" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong><a href="/docs/providers/ollama">Ollama</a></strong>:<!-- -->
<ul>
<li>Fixed custom price cost tracking and added &#x27;max_completion_token&#x27; support - <a href="https://github.com/BerriAI/litellm/pull/10636" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed KeyError when using JSON response format - <a href="https://github.com/BerriAI/litellm/pull/10611" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li> <strong><a href="/docs/providers/nscale">Nscale</a></strong>:<!-- -->
<ul>
<li>Added support for chat, image generation endpoints - <a href="https://github.com/BerriAI/litellm/pull/10638" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<ul>
<li><strong><a href="/docs/anthropic_unified">Messages API</a></strong>:<!-- -->
<ul>
<li> Added support for using Bedrock Anthropic models with /v1/messages format - <a href="https://github.com/BerriAI/litellm/pull/10681" target="_blank" rel="noopener noreferrer">PR</a> and streaming support - <a href="https://github.com/BerriAI/litellm/pull/10710" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/moderations">Moderations API</a></strong>:<!-- -->
<ul>
<li>Fixed bug to allow using LiteLLM UI credentials for /moderations API - <a href="https://github.com/BerriAI/litellm/pull/10723" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/realtime">Realtime API</a></strong>:<!-- -->
<ul>
<li>Fixed setting &#x27;headers&#x27; in scope for websocket auth requests and infinite loop issues - <a href="https://github.com/BerriAI/litellm/pull/10679" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/litellm_managed_files">Files API</a></strong>:<!-- -->
<ul>
<li>Unified File ID output support - <a href="https://github.com/BerriAI/litellm/pull/10713" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support for writing files to all deployments - <a href="https://github.com/BerriAI/litellm/pull/10708" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added target model name validation - <a href="https://github.com/BerriAI/litellm/pull/10722" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/batches">Batches API</a></strong>:<!-- -->
<ul>
<li>Complete unified batch ID support - replacing model in jsonl to be deployment model name - <a href="https://github.com/BerriAI/litellm/pull/10719" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Beta support for unified file ID (managed files) for batches - <a href="https://github.com/BerriAI/litellm/pull/10650" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking--budget-improvements">Spend Tracking / Budget Improvements<a href="#spend-tracking--budget-improvements" class="hash-link" aria-label="Spend Tracking / Budget Improvements" title="Spend Tracking / Budget Improvements"></a></h2>
<ul>
<li>Bug Fix - PostgreSQL Integer Overflow Error in DB Spend Tracking - <a href="https://github.com/BerriAI/litellm/pull/10697" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<ul>
<li><strong>Models</strong>
<ul>
<li>Fixed model info overwriting when editing a model on UI - <a href="https://github.com/BerriAI/litellm/pull/10726" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed team admin model updates and organization creation with specific models - <a href="https://github.com/BerriAI/litellm/pull/10539" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Logs</strong>:<!-- -->
<ul>
<li>Bug Fix -  copying Request/Response on Logs Page - <a href="https://github.com/BerriAI/litellm/pull/10720" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Bug Fix -  log did not remain in focus on QA Logs page + text overflow on error logs - <a href="https://github.com/BerriAI/litellm/pull/10725" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added index for session_id on LiteLLM_SpendLogs for better query performance - <a href="https://github.com/BerriAI/litellm/pull/10727" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>User Management</strong>:<!-- -->
<ul>
<li>Added user management functionality to Python client library &amp; CLI - <a href="https://github.com/BerriAI/litellm/pull/10627" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Bug Fix - Fixed SCIM token creation on Admin UI - <a href="https://github.com/BerriAI/litellm/pull/10628" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Bug Fix - Added 404 response when trying to delete verification tokens that don&#x27;t exist - <a href="https://github.com/BerriAI/litellm/pull/10605" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<ul>
<li><strong>Custom Logger API</strong>: v2 Custom Callback API (send llm logs to custom api) - <a href="https://github.com/BerriAI/litellm/pull/10575" target="_blank" rel="noopener noreferrer">PR</a>, <a href="https://docs.litellm.ai/docs/proxy/logging#custom-callback-apis-async" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li><strong>OpenTelemetry</strong>:<!-- -->
<ul>
<li>Fixed OpenTelemetry to follow genai semantic conventions + support for &#x27;instructions&#x27; param for TTS - <a href="https://github.com/BerriAI/litellm/pull/10608" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li>** Bedrock PII**:<!-- -->
<ul>
<li>Add support for PII Masking with bedrock guardrails - <a href="https://docs.litellm.ai/docs/proxy/guardrails/bedrock#pii-masking-with-bedrock-guardrails" target="_blank" rel="noopener noreferrer">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10608" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Documentation</strong>:<!-- -->
<ul>
<li>Added documentation for StandardLoggingVectorStoreRequest - <a href="https://github.com/BerriAI/litellm/pull/10535" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--reliability-improvements">Performance / Reliability Improvements<a href="#performance--reliability-improvements" class="hash-link" aria-label="Performance / Reliability Improvements" title="Performance / Reliability Improvements"></a></h2>
<ul>
<li><strong>Python Compatibility</strong>:<!-- -->
<ul>
<li>Added support for Python 3.11- (fixed datetime UTC handling) - <a href="https://github.com/BerriAI/litellm/pull/10701" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed UnicodeDecodeError: &#x27;charmap&#x27; on Windows during litellm import - <a href="https://github.com/BerriAI/litellm/pull/10542" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Caching</strong>:<!-- -->
<ul>
<li>Fixed embedding string caching result - <a href="https://github.com/BerriAI/litellm/pull/10700" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed cache miss for Gemini models with response_format - <a href="https://github.com/BerriAI/litellm/pull/10635" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<ul>
<li><strong>Proxy CLI</strong>:<!-- -->
<ul>
<li>Added <code>--version</code> flag to <code>litellm-proxy</code> CLI - <a href="https://github.com/BerriAI/litellm/pull/10704" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added dedicated <code>litellm-proxy</code> CLI - <a href="https://github.com/BerriAI/litellm/pull/10578" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Alerting</strong>:<!-- -->
<ul>
<li>Fixed Slack alerting not working when using a DB - <a href="https://github.com/BerriAI/litellm/pull/10370" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Email Invites</strong>:<!-- -->
<ul>
<li>Added V2 Emails with fixes for sending emails when creating keys + Resend API support - <a href="https://github.com/BerriAI/litellm/pull/10602" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added user invitation emails - <a href="https://github.com/BerriAI/litellm/pull/10615" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added endpoints to manage email settings - <a href="https://github.com/BerriAI/litellm/pull/10646" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>General</strong>:<!-- -->
<ul>
<li>Fixed bug where duplicate JSON logs were getting emitted - <a href="https://github.com/BerriAI/litellm/pull/10580" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-contributors">New Contributors<a href="#new-contributors" class="hash-link" aria-label="New Contributors" title="New Contributors"></a></h2>
<ul>
<li><a href="https://github.com/zoltan-ongithub" target="_blank" rel="noopener noreferrer">@zoltan-ongithub</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10568" target="_blank" rel="noopener noreferrer">PR #10568</a></li>
<li><a href="https://github.com/mkavinkumar1" target="_blank" rel="noopener noreferrer">@mkavinkumar1</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10548" target="_blank" rel="noopener noreferrer">PR #10548</a></li>
<li><a href="https://github.com/thomelane" target="_blank" rel="noopener noreferrer">@thomelane</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10549" target="_blank" rel="noopener noreferrer">PR #10549</a></li>
<li><a href="https://github.com/frankzye" target="_blank" rel="noopener noreferrer">@frankzye</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10540" target="_blank" rel="noopener noreferrer">PR #10540</a></li>
<li><a href="https://github.com/aholmberg" target="_blank" rel="noopener noreferrer">@aholmberg</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10591" target="_blank" rel="noopener noreferrer">PR #10591</a></li>
<li><a href="https://github.com/aravindkarnam" target="_blank" rel="noopener noreferrer">@aravindkarnam</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10611" target="_blank" rel="noopener noreferrer">PR #10611</a></li>
<li><a href="https://github.com/xsg22" target="_blank" rel="noopener noreferrer">@xsg22</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10648" target="_blank" rel="noopener noreferrer">PR #10648</a></li>
<li><a href="https://github.com/casparhsws" target="_blank" rel="noopener noreferrer">@casparhsws</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10635" target="_blank" rel="noopener noreferrer">PR #10635</a></li>
<li><a href="https://github.com/hypermoose" target="_blank" rel="noopener noreferrer">@hypermoose</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10370" target="_blank" rel="noopener noreferrer">PR #10370</a></li>
<li><a href="https://github.com/tomukmatthews" target="_blank" rel="noopener noreferrer">@tomukmatthews</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10638" target="_blank" rel="noopener noreferrer">PR #10638</a></li>
<li><a href="https://github.com/keyute" target="_blank" rel="noopener noreferrer">@keyute</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10652" target="_blank" rel="noopener noreferrer">PR #10652</a></li>
<li><a href="https://github.com/GPTLocalhost" target="_blank" rel="noopener noreferrer">@GPTLocalhost</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10687" target="_blank" rel="noopener noreferrer">PR #10687</a></li>
<li><a href="https://github.com/husnain7766" target="_blank" rel="noopener noreferrer">@husnain7766</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10697" target="_blank" rel="noopener noreferrer">PR #10697</a></li>
<li><a href="https://github.com/claralp" target="_blank" rel="noopener noreferrer">@claralp</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10694" target="_blank" rel="noopener noreferrer">PR #10694</a></li>
<li><a href="https://github.com/mollux" target="_blank" rel="noopener noreferrer">@mollux</a> made their first contribution in <a href="https://github.com/BerriAI/litellm/pull/10690" target="_blank" rel="noopener noreferrer">PR #10690</a></li>
</ul></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.68.0-stable">v1.68.0-stable</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-05-03T10:00:00.000Z">202553</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:main-v1.68.0-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.68.0.post1</span><br></span></code></pre></div></div></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<p>LiteLLM v1.68.0-stable will be live soon. Here are the key highlights of this release:</p>
<ul>
<li><strong>Bedrock Knowledge Base</strong>: You can now call query your Bedrock Knowledge Base with all LiteLLM models via <code>/chat/completion</code> or <code>/responses</code> API.</li>
<li><strong>Rate Limits</strong>: This release brings accurate rate limiting across multiple instances, reducing spillover to at most 10 additional requests in high traffic.</li>
<li><strong>Meta Llama API</strong>: Added support for Meta Llama API <a href="https://docs.litellm.ai/docs/providers/meta_llama" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li><strong>LlamaFile</strong>: Added support for LlamaFile <a href="https://docs.litellm.ai/docs/providers/llamafile" target="_blank" rel="noopener noreferrer">Get Started</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bedrock-knowledge-base-vector-store">Bedrock Knowledge Base (Vector Store)<a href="#bedrock-knowledge-base-vector-store" class="hash-link" aria-label="Bedrock Knowledge Base (Vector Store)" title="Bedrock Knowledge Base (Vector Store)"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAgklEQVR4nEWOuRKEIBAF+f9vNLE0BEsOxRkYeos9aoPO3tHuOA5CCMQYERG0NVT1S6O1ieK2bWNZFtZ1fQfLdWHD0G6c5eG8hCodNxu9d8wMUaVcN7kUvA/ElHhEGICbKz/mbbfBGSP7vuO9574rY4D7++jHp3dqraSUKCWTs+C98QJuIsG3UDdKPQAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/bedrock_kb.09fd982.640.png" srcset="/assets/ideal-img/bedrock_kb.09fd982.640.png 640w,/assets/ideal-img/bedrock_kb.ac190fc.1920.png 1920w" width="640" height="334"></noscript></div>
<br>
<p>This release adds support for Bedrock vector stores (knowledge bases) in LiteLLM. With this update, you can:</p>
<ul>
<li>Use Bedrock vector stores in the OpenAI /chat/completions spec with all LiteLLM supported models.</li>
<li>View all available vector stores through the LiteLLM UI or API.</li>
<li>Configure vector stores to be always active for specific models.</li>
<li>Track vector store usage in LiteLLM Logs.</li>
</ul>
<p>For the next release we plan on allowing you to set key, user, team, org permissions for vector stores.</p>
<p><a href="https://docs.litellm.ai/docs/completion/knowledgebase" target="_blank" rel="noopener noreferrer">Read more here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="rate-limiting">Rate Limiting<a href="#rate-limiting" class="hash-link" aria-label="Rate Limiting" title="Rate Limiting"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAABCcAAAQnAEmzTo0AAAAgklEQVR4nD2OzQ6EIAyEff93dFVuHkCQVkQ036Zs1sOknWZ+OvgQCCEQY2TbtneqKq01ruvqGJxzjOPIZ5pYlgXj0zyzriu1VkopP+F9352c5USPg1oKTYQswr7vPdkMg6ljSqSUEFU0RrJzZFVEhMPMJrQ//rD0vj8POWe8973Nbl9skcAOHTyQ7AAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="335"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/multi_instance_rate_limiting.d7cae8c.640.png" srcset="/assets/ideal-img/multi_instance_rate_limiting.d7cae8c.640.png 640w,/assets/ideal-img/multi_instance_rate_limiting.06ee750.1800.png 1800w" width="640" height="335"></noscript></div>
<br>
<p>This release brings accurate multi-instance rate limiting across keys/users/teams. Outlining key engineering changes below:</p>
<ul>
<li><strong>Change</strong>: Instances now increment cache value instead of setting it. To avoid calling Redis on each request, this is synced every 0.01s.</li>
<li><strong>Accuracy</strong>: In testing, we saw a maximum spill over from expected of 10 requests, in high traffic (100 RPS, 3 instances), vs. current 189 request spillover</li>
<li><strong>Performance</strong>: Our load tests show this to reduce median response time by 100ms in high traffic</li>
</ul>
<p>This is currently behind a feature flag, and we plan to have this be the default by next week. To enable this today, just add this environment variable:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">export LITELLM_RATE_LIMIT_ACCURACY=true</span><br></span></code></pre></div></div>
<p><a href="/docs/proxy/users#beta-multi-instance-rate-limiting">Read more here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<ul>
<li><strong>Gemini (<a href="https://docs.litellm.ai/docs/providers/vertex#usage-with-litellm-proxy-server" target="_blank" rel="noopener noreferrer">VertexAI</a> + <a href="https://docs.litellm.ai/docs/providers/gemini" target="_blank" rel="noopener noreferrer">Google AI Studio</a>)</strong>
<ul>
<li>Handle more json schema - openapi schema conversion edge cases <a href="https://github.com/BerriAI/litellm/pull/10351" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Tool calls - return finish_reason=tool_calls on gemini tool calling response <a href="https://github.com/BerriAI/litellm/pull/10485" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/vertex#metallama-api">VertexAI</a></strong>
<ul>
<li>Meta/llama-4 model support <a href="https://github.com/BerriAI/litellm/pull/10492" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Meta/llama3 - handle tool call result in content <a href="https://github.com/BerriAI/litellm/pull/10492" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Meta/* - return finish_reason=tool_calls on tool calling response <a href="https://github.com/BerriAI/litellm/pull/10492" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/bedrock#litellm-proxy-usage">Bedrock</a></strong>
<ul>
<li><a href="/docs/providers/bedrock#image-generation">Image Generation</a> - Support new stable-image-core models - <a href="https://github.com/BerriAI/litellm/pull/10351" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><a href="/docs/completion/knowledgebase">Knowledge Bases</a> - support using Bedrock knowledge bases with <code>/chat/completions</code> <a href="https://github.com/BerriAI/litellm/pull/10413" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><a href="/docs/providers/bedrock#litellm-proxy-usage">Anthropic</a> - add supports_pdf_input for claude-3.7-bedrock models <a href="https://github.com/BerriAI/litellm/pull/9917" target="_blank" rel="noopener noreferrer">PR</a>, <a href="/docs/completion/document_understanding#checking-if-a-model-supports-pdf-input">Get Started</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/openai">OpenAI</a></strong>
<ul>
<li>Support OPENAI_BASE_URL in addition to OPENAI_API_BASE <a href="https://github.com/BerriAI/litellm/pull/10423" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Correctly re-raise 504 timeout errors <a href="https://github.com/BerriAI/litellm/pull/10462" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Native Gpt-4o-mini-tts support <a href="https://github.com/BerriAI/litellm/pull/10462" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li> <strong><a href="/docs/providers/meta_llama">Meta Llama API</a></strong> provider <a href="https://github.com/BerriAI/litellm/pull/10451" target="_blank" rel="noopener noreferrer">PR</a></li>
<li> <strong><a href="/docs/providers/llamafile">LlamaFile</a></strong> provider <a href="https://github.com/BerriAI/litellm/pull/10482" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-api-endpoints">LLM API Endpoints<a href="#llm-api-endpoints" class="hash-link" aria-label="LLM API Endpoints" title="LLM API Endpoints"></a></h2>
<ul>
<li><strong><a href="/docs/response_api">Response API</a></strong>
<ul>
<li>Fix for handling multi turn sessions <a href="https://github.com/BerriAI/litellm/pull/10415" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/embedding/supported_embedding">Embeddings</a></strong>
<ul>
<li>Caching fixes - <a href="https://github.com/BerriAI/litellm/pull/10424" target="_blank" rel="noopener noreferrer">PR</a>
<ul>
<li>handle str -&gt; list cache</li>
<li>Return usage tokens for cache hit</li>
<li>Combine usage tokens on partial cache hits</li>
</ul>
</li>
</ul>
</li>
<li> <strong><a href="/docs/completion/knowledgebase">Vector Stores</a></strong>
<ul>
<li>Allow defining Vector Store Configs - <a href="https://github.com/BerriAI/litellm/pull/10448" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>New StandardLoggingPayload field for requests made when a vector store is used - <a href="https://github.com/BerriAI/litellm/pull/10509" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Show Vector Store / KB Request on LiteLLM Logs Page  - <a href="https://github.com/BerriAI/litellm/pull/10514" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Allow using vector store in OpenAI API spec with tools - <a href="https://github.com/BerriAI/litellm/pull/10516" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/mcp">MCP</a></strong>
<ul>
<li>
<p>Ensure Non-Admin virtual keys can access /mcp routes - <a href="https://github.com/BerriAI/litellm/pull/10473" target="_blank" rel="noopener noreferrer">PR</a></p>
<p><strong>Note:</strong> Currently, all Virtual Keys are able to access the MCP endpoints. We are working on a feature to allow restricting MCP access by keys/teams/users/orgs. Follow <a href="https://github.com/BerriAI/litellm/discussions/9891" target="_blank" rel="noopener noreferrer">here</a> for updates.</p>
</li>
</ul>
</li>
<li><strong>Moderations</strong>
<ul>
<li>Add logging callback support for <code>/moderations</code> API - <a href="https://github.com/BerriAI/litellm/pull/10390" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking--budget-improvements">Spend Tracking / Budget Improvements<a href="#spend-tracking--budget-improvements" class="hash-link" aria-label="Spend Tracking / Budget Improvements" title="Spend Tracking / Budget Improvements"></a></h2>
<ul>
<li><strong><a href="/docs/providers/openai">OpenAI</a></strong>
<ul>
<li><a href="/docs/providers/openai/responses_api#computer-use">computer-use-preview</a> cost tracking / pricing <a href="https://github.com/BerriAI/litellm/pull/10422" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><a href="/docs/providers/openai/text_to_speech">gpt-4o-mini-tts</a> input cost tracking - <a href="https://github.com/BerriAI/litellm/pull/10462" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/fireworks_ai">Fireworks AI</a></strong> - pricing updates - new <code>0-4b</code> model pricing tier + llama4 model pricing</li>
<li><strong><a href="/docs/proxy/users#set-budgets">Budgets</a></strong>
<ul>
<li><a href="/docs/proxy/users#reset-budgets">Budget resets</a> now happen as start of day/week/month - <a href="https://github.com/BerriAI/litellm/pull/10333" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Trigger <a href="/docs/proxy/alerting#soft-budget-alerts-for-virtual-keys">Soft Budget Alerts</a> When Key Crosses Threshold - <a href="https://github.com/BerriAI/litellm/pull/10491" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/completion/token_usage#3-token_counter">Token Counting</a></strong>
<ul>
<li>Rewrite of token_counter() function to handle to prevent undercounting tokens - <a href="https://github.com/BerriAI/litellm/pull/10409" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<ul>
<li><strong>Virtual Keys</strong>
<ul>
<li>Fix filtering on key alias - <a href="https://github.com/BerriAI/litellm/pull/10455" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support global filtering on keys - <a href="https://github.com/BerriAI/litellm/pull/10455" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Pagination - fix clicking on next/back buttons on table - <a href="https://github.com/BerriAI/litellm/pull/10528" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Models</strong>
<ul>
<li>Triton - Support adding model/provider on UI - <a href="https://github.com/BerriAI/litellm/pull/10456" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>VertexAI - Fix adding vertex models with reusable credentials - <a href="https://github.com/BerriAI/litellm/pull/10528" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>LLM Credentials - show existing credentials for easy editing - <a href="https://github.com/BerriAI/litellm/pull/10519" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Teams</strong>
<ul>
<li>Allow reassigning team to other org - <a href="https://github.com/BerriAI/litellm/pull/10527" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Organizations</strong>
<ul>
<li>Fix showing org budget on table - <a href="https://github.com/BerriAI/litellm/pull/10528" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<ul>
<li><strong><a href="/docs/observability/langsmith_integration">Langsmith</a></strong>
<ul>
<li>Respect <a href="/docs/observability/langsmith_integration#local-testing---control-batch-size">langsmith_batch_size</a> param - <a href="https://github.com/BerriAI/litellm/pull/10411" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<ul>
<li><strong><a href="/docs/proxy/caching">Redis</a></strong>
<ul>
<li>Ensure all redis queues are periodically flushed, this fixes an issue where redis queue size was growing indefinitely when request tags were used - <a href="https://github.com/BerriAI/litellm/pull/10393" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/proxy/users#set-rate-limit">Rate Limits</a></strong>
<ul>
<li><a href="/docs/proxy/users#beta-multi-instance-rate-limiting">Multi-instance rate limiting</a> support across keys/teams/users/customers - <a href="https://github.com/BerriAI/litellm/pull/10458" target="_blank" rel="noopener noreferrer">PR</a>, <a href="https://github.com/BerriAI/litellm/pull/10497" target="_blank" rel="noopener noreferrer">PR</a>, <a href="https://github.com/BerriAI/litellm/pull/10500" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong><a href="/docs/providers/azure#entra-id---use-azure_ad_token">Azure OpenAI OIDC</a></strong>
<ul>
<li>allow using litellm defined params for <a href="/docs/providers/azure#entra-id---use-azure_ad_token">OIDC Auth</a> - <a href="https://github.com/BerriAI/litellm/pull/10394" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<ul>
<li><strong>Security</strong>
<ul>
<li>Allow <a href="/docs/proxy/enterprise#blocking-web-crawlers">blocking web crawlers</a> - <a href="https://github.com/BerriAI/litellm/pull/10420" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Auth</strong>
<ul>
<li>Support <a href="/docs/pass_through/vertex_ai#use-with-virtual-keys"><code>x-litellm-api-key</code> header param by default</a>, this fixes an issue from the prior release where <code>x-litellm-api-key</code> was not being used on vertex ai passthrough requests - <a href="https://github.com/BerriAI/litellm/pull/10392" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Allow key at max budget to call non-llm api endpoints - <a href="https://github.com/BerriAI/litellm/pull/10392" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li> <strong><a href="/docs/proxy/management_cli">Python Client Library</a> for LiteLLM Proxy management endpoints</strong>
<ul>
<li>Initial PR - <a href="https://github.com/BerriAI/litellm/pull/10445" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support for doing HTTP requests - <a href="https://github.com/BerriAI/litellm/pull/10452" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li><strong>Dependencies</strong>
<ul>
<li>Dont require uvloop for windows - <a href="https://github.com/BerriAI/litellm/pull/10483" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.67.4-stable">v1.67.4-stable - Improved User Management</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-04-26T10:00:00.000Z">2025426</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:main-v1.67.4-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.67.4.post1</span><br></span></code></pre></div></div></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>Improved User Management</strong>: This release enables search and filtering across users, keys, teams, and models.</li>
<li><strong>Responses API Load Balancing</strong>: Route requests across provider regions and ensure session continuity.</li>
<li><strong>UI Session Logs</strong>: Group several requests to LiteLLM into a session.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="improved-user-management">Improved User Management<a href="#improved-user-management" class="hash-link" aria-label="Improved User Management" title="Improved User Management"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAcElEQVR4nD2Oyw7DIAwE+f+v5JI7bUPwI3gqgxJLo5VtaXeLiGBmuPtSVX1ZuxmmSjmOg1orrTXGEO77JmISEcw5X0rvnfM8ERmIGkMUc0fVCFjklIx8YpN0dXM+39++5c+d8vRK3bHboV8X2f+p8Ac2EcPInd9ALgAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/ui_search_users.67b9368.640.png" srcset="/assets/ideal-img/ui_search_users.67b9368.640.png 640w,/assets/ideal-img/ui_search_users.a5d625c.1920.png 1920w" width="640" height="334"></noscript></div>
<br>
<p>This release makes it easier to manage users and keys on LiteLLM. You can now search and filter across users, keys, teams, and models, and control user settings more easily.</p>
<p>New features include:</p>
<ul>
<li>Search for users by email, ID, role, or team.</li>
<li>See all of a user&#x27;s models, teams, and keys in one place.</li>
<li>Change user roles and model access right from the Users Tab.</li>
</ul>
<p>These changes help you spend less time on user setup and management on LiteLLM.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="responses-api-load-balancing">Responses API Load Balancing<a href="#responses-api-load-balancing" class="hash-link" aria-label="Responses API Load Balancing" title="Responses API Load Balancing"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAvElEQVR4nF3NTwvBcBzH8d+TIAe04kSp+TMb4ULISg48ITV/SinK89rFSFyMNbtv+22/70eKi/fldXyzMAwRxzGICEII/EVfPWbbNkzThHU8wrJOWG4OMFY7GOv9R1qs95gvth5zXZeutxs5zpNO5wvlCm1KSTVK5zXK5FWRlKpIZOUX++x+S/vhQOtM0OrO0O5OobbGVFKHKJZ7HguCgPu+z4WI+f3+4EpTj5TGKKo39aiiDbhc70NW+s4bQgGQOTs756oAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="338"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/ui_responses_lb.1e5e3f1.640.png" srcset="/assets/ideal-img/ui_responses_lb.1e5e3f1.640.png 640w,/assets/ideal-img/ui_responses_lb.1e64cec.1204.png 1204w" width="640" height="338"></noscript></div>
<br>
<p>This release introduces load balancing for the Responses API, allowing you to route requests across provider regions and ensure session continuity. It works as follows:</p>
<ul>
<li>If a <code>previous_response_id</code> is provided, LiteLLM will route the request to the original deployment that generated the prior response  ensuring session continuity.</li>
<li>If no <code>previous_response_id</code> is provided, LiteLLM will load-balance requests across your available deployments.</li>
</ul>
<p><a href="https://docs.litellm.ai/docs/response_api#load-balancing-with-session-continuity" target="_blank" rel="noopener noreferrer">Read more</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ui-session-logs">UI Session Logs<a href="#ui-session-logs" class="hash-link" aria-label="UI Session Logs" title="UI Session Logs"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAfUlEQVR4nDWNWQrDMAxEff8T9ivkAKmJqRctdl6RIYJhJGZR+n4zOWfKfSMimBnTfbOqLiD2TzrPk+M4CO69c+VCKZX1POF550nuzpyTtdZuqV1pw6ltEJqabWdS1XixEUZR3cFhwlBH1fadQnwRDWZOrY0sP646aX0wRPgDkXrCdgbcun4AAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="330"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/ui_session_logs.24ce5ec.640.png" srcset="/assets/ideal-img/ui_session_logs.24ce5ec.640.png 640w,/assets/ideal-img/ui_session_logs.e8f3010.1920.png 1920w" width="640" height="330"></noscript></div>
<br>
<p>This release allow you to group requests to LiteLLM proxy into a session. If you specify a litellm_session_id in your request LiteLLM will automatically group all logs in the same session. This allows you to easily track usage and request content per session.</p>
<p><a href="https://docs.litellm.ai/docs/proxy/ui_logs_sessions" target="_blank" rel="noopener noreferrer">Read more</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<ul>
<li><strong>OpenAI</strong>
<ol>
<li>Added <code>gpt-image-1</code> cost tracking <a href="https://docs.litellm.ai/docs/image_generation" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>Bug fix: added cost tracking for gpt-image-1 when quality is unspecified <a href="https://github.com/BerriAI/litellm/pull/10247" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>Azure</strong>
<ol>
<li>Fixed timestamp granularities passing to whisper in Azure <a href="https://docs.litellm.ai/docs/audio_transcription" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>Added azure/gpt-image-1 pricing <a href="https://docs.litellm.ai/docs/image_generation" target="_blank" rel="noopener noreferrer">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10327" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added cost tracking for <code>azure/computer-use-preview</code>, <code>azure/gpt-4o-audio-preview-2024-12-17</code>, <code>azure/gpt-4o-mini-audio-preview-2024-12-17</code> <a href="https://github.com/BerriAI/litellm/pull/10178" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>Bedrock</strong>
<ol>
<li>Added support for all compatible Bedrock parameters when model=&quot;arn:..&quot; (Bedrock application inference profile models) <a href="https://docs.litellm.ai/docs/providers/bedrock#bedrock-application-inference-profile" target="_blank" rel="noopener noreferrer">Get started</a>, <a href="https://github.com/BerriAI/litellm/pull/10256" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed wrong system prompt transformation <a href="https://github.com/BerriAI/litellm/pull/10120" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>VertexAI / Google AI Studio</strong>
<ol>
<li>Allow setting <code>budget_tokens=0</code> for <code>gemini-2.5-flash</code> <a href="https://docs.litellm.ai/docs/providers/gemini#usage---thinking--reasoning_content" target="_blank" rel="noopener noreferrer">Get Started</a>,<a href="https://github.com/BerriAI/litellm/pull/10198" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Ensure returned <code>usage</code> includes thinking token usage <a href="https://github.com/BerriAI/litellm/pull/10198" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added cost tracking for <code>gemini-2.5-pro-preview-03-25</code> <a href="https://github.com/BerriAI/litellm/pull/10178" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>Cohere</strong>
<ol>
<li>Added support for cohere command-a-03-2025 <a href="https://docs.litellm.ai/docs/providers/cohere" target="_blank" rel="noopener noreferrer">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10295" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>SageMaker</strong>
<ol>
<li>Added support for max_completion_tokens parameter <a href="https://docs.litellm.ai/docs/providers/sagemaker" target="_blank" rel="noopener noreferrer">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10300" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>Responses API</strong>
<ol>
<li>Added support for GET and DELETE operations - <code>/v1/responses/{response_id}</code> <a href="/docs/response_api">Get Started</a></li>
<li>Added session management support for all supported models <a href="https://github.com/BerriAI/litellm/pull/10321" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added routing affinity to maintain model consistency within sessions <a href="https://docs.litellm.ai/docs/response_api#load-balancing-with-routing-affinity" target="_blank" rel="noopener noreferrer">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10193" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-improvements">Spend Tracking Improvements<a href="#spend-tracking-improvements" class="hash-link" aria-label="Spend Tracking Improvements" title="Spend Tracking Improvements"></a></h2>
<ul>
<li><strong>Bug Fix</strong>: Fixed spend tracking bug, ensuring default litellm params aren&#x27;t modified in memory <a href="https://github.com/BerriAI/litellm/pull/10167" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong>Deprecation Dates</strong>: Added deprecation dates for Azure, VertexAI models <a href="https://github.com/BerriAI/litellm/pull/10308" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="users">Users<a href="#users" class="hash-link" aria-label="Users" title="Users"></a></h4>
<ul>
<li>
<p><strong>Filtering and Searching</strong>:</p>
<ul>
<li>Filter users by user_id, role, team, sso_id</li>
<li>Search users by email</li>
</ul>
<br>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAADCAYAAACqPZ51AAAACXBIWXMAABYlAAAWJQFJUiTwAAAAVUlEQVR4nF3FSwrDQBAD0bn/LQPZTMAQDO6fpG4TL1PwqOWe3wIKQEr6F3uz98Z7RaaDGpIjabpnKI26f+950md54qgqL8Ai0khaFQygnaddHgl3vm4psHUHArIcpAAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="204"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/user_filters.4612f45.640.png" srcset="/assets/ideal-img/user_filters.4612f45.640.png 640w,/assets/ideal-img/user_filters.f6de86b.1920.png 1920w" width="640" height="204"></noscript></div>
</li>
<li>
<p><strong>User Info Panel</strong>: Added a new user information pane <a href="https://github.com/BerriAI/litellm/pull/10213" target="_blank" rel="noopener noreferrer">PR</a></p>
<ul>
<li>View teams, keys, models associated with User</li>
<li>Edit user role, model permissions</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="teams">Teams<a href="#teams" class="hash-link" aria-label="Teams" title="Teams"></a></h4>
<ul>
<li>
<p><strong>Filtering and Searching</strong>:</p>
<ul>
<li>Filter teams by Organization, Team ID <a href="https://github.com/BerriAI/litellm/pull/10324" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Search teams by Team Name <a href="https://github.com/BerriAI/litellm/pull/10324" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<br>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAiklEQVR4nE2MQQoCMQxFe/8LeQE3juDCG7jRERkYTdskTdIvLQgGHgnh/5eolFtr7RmBNSImACYR8XD3zcyvSUQEAMw6VA2tGXrv44UIxDyANe3EpaqHqHrOxWtlp5zdzHz0R8rd76mZT+Nvhm0Q0YF/I9H7yMxnVV1EZGGWuT9Ul9dGJ2a5NNXDF+sawSEVLmt/AAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="297"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/team_filters.55369b2.640.png" srcset="/assets/ideal-img/team_filters.55369b2.640.png 640w,/assets/ideal-img/team_filters.4312efd.1920.png 1920w" width="640" height="297"></noscript></div>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="keys">Keys<a href="#keys" class="hash-link" aria-label="Keys" title="Keys"></a></h4>
<ul>
<li><strong>Key Management</strong>:<!-- -->
<ul>
<li>Support for cross-filtering and filtering by key hash <a href="https://github.com/BerriAI/litellm/pull/10322" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed key alias reset when resetting filters <a href="https://github.com/BerriAI/litellm/pull/10099" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed table rendering on key creation <a href="https://github.com/BerriAI/litellm/pull/10224" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="ui-logs-page">UI Logs Page<a href="#ui-logs-page" class="hash-link" aria-label="UI Logs Page" title="UI Logs Page"></a></h4>
<ul>
<li><strong>Session Logs</strong>: Added UI Session Logs <a href="https://docs.litellm.ai/docs/proxy/ui_logs_sessions" target="_blank" rel="noopener noreferrer">Get Started</a></li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="ui-authentication--security">UI Authentication &amp; Security<a href="#ui-authentication--security" class="hash-link" aria-label="UI Authentication &amp; Security" title="UI Authentication &amp; Security"></a></h4>
<ul>
<li><strong>Required Authentication</strong>: Authentication now required for all dashboard pages <a href="https://github.com/BerriAI/litellm/pull/10229" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong>SSO Fixes</strong>: Fixed SSO user login invalid token error <a href="https://github.com/BerriAI/litellm/pull/10298" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>[BETA] <strong>Encrypted Tokens</strong>: Moved UI to encrypted token usage <a href="https://github.com/BerriAI/litellm/pull/10302" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong>Token Expiry</strong>: Support token refresh by re-routing to login page (fixes issue where expired token would show a blank page) <a href="https://github.com/BerriAI/litellm/pull/10250" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="ui-general-fixes">UI General fixes<a href="#ui-general-fixes" class="hash-link" aria-label="UI General fixes" title="UI General fixes"></a></h4>
<ul>
<li><strong>Fixed UI Flicker</strong>: Addressed UI flickering issues in Dashboard <a href="https://github.com/BerriAI/litellm/pull/10261" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong>Improved Terminology</strong>: Better loading and no-data states on Keys and Tools pages <a href="https://github.com/BerriAI/litellm/pull/10253" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong>Azure Model Support</strong>: Fixed editing Azure public model names and changing model names after creation <a href="https://github.com/BerriAI/litellm/pull/10249" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong>Team Model Selector</strong>: Bug fix for team model selection <a href="https://github.com/BerriAI/litellm/pull/10171" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<ul>
<li><strong>Datadog</strong>:<!-- -->
<ol>
<li>Fixed Datadog LLM observability logging <a href="https://docs.litellm.ai/docs/proxy/logging#datadog" target="_blank" rel="noopener noreferrer">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10206" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>Prometheus / Grafana</strong>:<!-- -->
<ol>
<li>Enable datasource selection on LiteLLM Grafana Template <a href="https://docs.litellm.ai/docs/proxy/prometheus#-litellm-maintained-grafana-dashboards-" target="_blank" rel="noopener noreferrer">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10257" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>AgentOps</strong>:<!-- -->
<ol>
<li>Added AgentOps Integration <a href="https://docs.litellm.ai/docs/observability/agentops_integration" target="_blank" rel="noopener noreferrer">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/9685" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>Arize</strong>:<!-- -->
<ol>
<li>Added missing attributes for Arize &amp; Phoenix Integration <a href="https://docs.litellm.ai/docs/observability/arize_integration" target="_blank" rel="noopener noreferrer">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10215" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<ul>
<li><strong>Caching</strong>: Fixed caching to account for <code>thinking</code> or <code>reasoning_effort</code> when calculating cache key <a href="https://github.com/BerriAI/litellm/pull/10140" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong>Model Groups</strong>: Fixed handling for cases where user sets model_group inside model_info <a href="https://github.com/BerriAI/litellm/pull/10191" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong>Passthrough Endpoints</strong>: Ensured <code>PassthroughStandardLoggingPayload</code> is logged with method, URL, request/response body <a href="https://github.com/BerriAI/litellm/pull/10194" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong>Fix SQL Injection</strong>: Fixed potential SQL injection vulnerability in spend_management_endpoints.py <a href="https://github.com/BerriAI/litellm/pull/9878" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="helm">Helm<a href="#helm" class="hash-link" aria-label="Helm" title="Helm"></a></h2>
<ul>
<li>Fixed serviceAccountName on migration job <a href="https://github.com/BerriAI/litellm/pull/10258" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="full-changelog">Full Changelog<a href="#full-changelog" class="hash-link" aria-label="Full Changelog" title="Full Changelog"></a></h2>
<p>The complete list of changes can be found in the <a href="https://github.com/BerriAI/litellm/compare/v1.67.0-stable...v1.67.4-stable" target="_blank" rel="noopener noreferrer">GitHub release notes</a>.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/responses-api">responses_api</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/ui-improvements">ui_improvements</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/security">security</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/session-management">session_management</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.67.0-stable">v1.67.0-stable - SCIM Integration</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-04-19T10:00:00.000Z">2025419</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>SCIM Integration</strong>: Enables identity providers (Okta, Azure AD, OneLogin, etc.) to automate user and team (group) provisioning, updates, and deprovisioning</li>
<li><strong>Team and Tag based usage tracking</strong>: You can now see usage and spend by team and tag at 1M+ spend logs.</li>
<li><strong>Unified Responses API</strong>: Support for calling Anthropic, Gemini, Groq, etc. via OpenAI&#x27;s new Responses API.</li>
</ul>
<p>Let&#x27;s dive in.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="scim-integration">SCIM Integration<a href="#scim-integration" class="hash-link" aria-label="SCIM Integration" title="SCIM Integration"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAaElEQVR4nF2NWQrAIAxEe/+r2TP4U0FRBFOqoilMidDNwCMLM5ml946b1tozzyzzQcRCKQW1VjDzKzyZQbRjMwbWWuScodQKrTWkxDiE4jqODO89QgiIMcI5Nwy/j7JIDBENUkqjf6Mvu/zBbKCirTEAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/scim_integration.2e95ea4.640.png" srcset="/assets/ideal-img/scim_integration.2e95ea4.640.png 640w,/assets/ideal-img/scim_integration.01959e2.1200.png 1200w" width="640" height="334"></noscript></div>
<p>This release adds SCIM support to LiteLLM. This allows your SSO provider (Okta, Azure AD, etc) to automatically create/delete users, teams, and memberships on LiteLLM. This means that when you remove a team on your SSO provider, your SSO provider will automatically delete the corresponding team on LiteLLM.</p>
<p><a href="/docs/tutorials/scim_litellm">Read more</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="team-and-tag-based-usage-tracking">Team and Tag based usage tracking<a href="#team-and-tag-based-usage-tracking" class="hash-link" aria-label="Team and Tag based usage tracking" title="Team and Tag based usage tracking"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAFAAoDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAII/8QAGhABAAMBAQEAAAAAAAAAAAAAAQACAxESIf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIRAxEAPwDUGWTWoGl3n3tnqynQFPJEQP/Z&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/new_team_usage_highlight.1d6c101.640.jpg" srcset="/assets/ideal-img/new_team_usage_highlight.1d6c101.640.jpg 640w,/assets/ideal-img/new_team_usage_highlight.9a0a815.1920.jpg 1920w" width="640" height="334"></noscript></div>
<p>This release improves team and tag based usage tracking at 1m+ spend logs, making it easy to monitor your LLM API Spend in production. This covers:</p>
<ul>
<li>View <strong>daily spend</strong> by teams + tags</li>
<li>View <strong>usage / spend by key</strong>, within teams</li>
<li>View <strong>spend by multiple tags</strong></li>
<li>Allow <strong>internal users</strong> to view spend of teams they&#x27;re a member of</li>
</ul>
<p><a href="#management-endpoints--ui">Read more</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="unified-responses-api">Unified Responses API<a href="#unified-responses-api" class="hash-link" aria-label="Unified Responses API" title="Unified Responses API"></a></h2>
<p>This release allows you to call Azure OpenAI, Anthropic, AWS Bedrock, and Google Vertex AI models via the POST /v1/responses endpoint on LiteLLM. This means you can now use popular tools like <a href="https://docs.litellm.ai/docs/tutorials/openai_codex" target="_blank" rel="noopener noreferrer">OpenAI Codex</a> with your own models.</p>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAACxLAAAsSwGlPZapAAAAdElEQVR4nF2NQQ7DIAwE+f8P+UEOJQUSAtiEqZy0VVVLe5qdtcs5E2Ok1oqI3FFF3xEdiApuWRa894Swsu+FLsI4T3QM1lx45IPSB86ArZj9jInaGiEEUsps2y3OOXGfd70L5WjXkoH/+xZt0Ypd9AJW/hVev6TDLgPpucgAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/unified_responses_api_rn.6446ade.640.png" srcset="/assets/ideal-img/unified_responses_api_rn.6446ade.640.png 640w,/assets/ideal-img/unified_responses_api_rn.c4598c6.1920.png 1920w" width="640" height="334"></noscript></div>
<p><a href="https://docs.litellm.ai/docs/response_api" target="_blank" rel="noopener noreferrer">Read more</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<ul>
<li><strong>OpenAI</strong>
<ol>
<li>gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, o3, o3-mini, o4-mini pricing - <a href="/docs/providers/openai#usage">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/9990" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>o4 - correctly map o4 to openai o_series model</li>
</ol>
</li>
<li><strong>Azure AI</strong>
<ol>
<li>Phi-4 output cost per token fix - <a href="https://github.com/BerriAI/litellm/pull/9880" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Responses API support <a href="/docs/providers/azure#azure-responses-api">Get Started</a>,<a href="https://github.com/BerriAI/litellm/pull/10116" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>Anthropic</strong>
<ol>
<li>redacted message thinking support - <a href="/docs/providers/anthropic#usage---thinking--reasoning_content">Get Started</a>,<a href="https://github.com/BerriAI/litellm/pull/10129" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>Cohere</strong>
<ol>
<li><code>/v2/chat</code> Passthrough endpoint support w/ cost tracking - <a href="/docs/pass_through/cohere">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/9997" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>Azure</strong>
<ol>
<li>Support azure tenant_id/client_id env vars - <a href="/docs/providers/azure#entra-id---use-tenant_id-client_id-client_secret">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/9993" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix response_format check for 2025+ api versions - <a href="https://github.com/BerriAI/litellm/pull/9993" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, o3, o3-mini, o4-mini pricing</li>
</ol>
</li>
<li><strong>VLLM</strong>
<ol>
<li>Files - Support &#x27;file&#x27; message type for VLLM video url&#x27;s - <a href="/docs/providers/vllm#send-video-url-to-vllm">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10129" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Passthrough - new <code>/vllm/</code> passthrough endpoint support <a href="/docs/pass_through/vllm">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10002" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>Mistral</strong>
<ol>
<li>new <code>/mistral</code> passthrough endpoint support <a href="/docs/pass_through/mistral">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10002" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>AWS</strong>
<ol>
<li>New mapped bedrock regions - <a href="https://github.com/BerriAI/litellm/pull/9430" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>VertexAI / Google AI Studio</strong>
<ol>
<li>Gemini - Response format - Retain schema field ordering for google gemini and vertex by specifyingpropertyOrdering - <a href="/docs/providers/vertex#json-schema">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/9828" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Gemini-2.5-flash - return reasoning content <a href="/docs/providers/gemini#usage---thinking--reasoning_content">Google AI Studio</a>, <a href="/docs/providers/vertex#thinking--reasoning_content">Vertex AI</a></li>
<li>Gemini-2.5-flash - pricing + model information <a href="https://github.com/BerriAI/litellm/pull/10125" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Passthrough - new <code>/vertex_ai/discovery</code> route - enables calling AgentBuilder API routes <a href="/docs/pass_through/vertex_ai#supported-api-endpoints">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10084" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>Fireworks AI</strong>
<ol>
<li>return tool calling responses in <code>tool_calls</code> field (fireworks incorrectly returns this as a json str in content) <a href="https://github.com/BerriAI/litellm/pull/10130" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>Triton</strong>
<ol>
<li>Remove fixed remove bad_words / stop wordsfrom <code>/generate</code> call - <a href="/docs/providers/triton-inference-server#triton-generate---chat-completion">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10163" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>Other</strong>
<ol>
<li>Support for all litellm providers on Responses API (works with Codex) - <a href="/docs/tutorials/openai_codex">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10132" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix combining multiple tool calls in streaming response - <a href="/docs/completion/stream#helper-function">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10040" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-improvements">Spend Tracking Improvements<a href="#spend-tracking-improvements" class="hash-link" aria-label="Spend Tracking Improvements" title="Spend Tracking Improvements"></a></h2>
<ul>
<li><strong>Cost Control</strong> - inject cache control points in prompt for cost reduction <a href="/docs/tutorials/prompt_caching">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10000" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong>Spend Tags</strong> - spend tags in headers - support x-litellm-tags even if tag based routing not enabled <a href="/docs/proxy/request_headers#litellm-headers">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10000" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong>Gemini-2.5-flash</strong> - support cost calculation for reasoning tokens <a href="https://github.com/BerriAI/litellm/pull/10141" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<ul>
<li>
<p><strong>Users</strong></p>
<ol>
<li>Show created_at and updated_at on users page - <a href="https://github.com/BerriAI/litellm/pull/10033" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li>
<p><strong>Virtual Keys</strong></p>
<ol>
<li>Filter by key alias - <a href="https://github.com/BerriAI/litellm/pull/10085" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/10085</a></li>
</ol>
</li>
<li>
<p><strong>Usage Tab</strong></p>
<ol>
<li>
<p>Team based usage</p>
<ul>
<li>
<p>New <code>LiteLLM_DailyTeamSpend</code> Table for aggregate team based usage logging - <a href="https://github.com/BerriAI/litellm/pull/10039" target="_blank" rel="noopener noreferrer">PR</a></p>
</li>
<li>
<p>New Team based usage dashboard + new <code>/team/daily/activity</code> API - <a href="https://github.com/BerriAI/litellm/pull/10081" target="_blank" rel="noopener noreferrer">PR</a></p>
</li>
<li>
<p>Return team alias on/team/daily/activityAPI - <a href="https://github.com/BerriAI/litellm/pull/10157" target="_blank" rel="noopener noreferrer">PR</a></p>
</li>
<li>
<p>allow internal user view spend for teams they belong to - <a href="https://github.com/BerriAI/litellm/pull/10157" target="_blank" rel="noopener noreferrer">PR</a></p>
</li>
<li>
<p>allow viewing top keys by team - <a href="https://github.com/BerriAI/litellm/pull/10157" target="_blank" rel="noopener noreferrer">PR</a></p>
</li>
</ul>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAKCAYAAACNMs+9AAAACXBIWXMAABYlAAAWJQFJUiTwAAAA80lEQVR4nGWQS07EQAxEc3i2HGGQENdhjfisR0QwnUw6SfvvQs0MAglLtjelV2UP27bfEuk9sx1Y+S4i/vYhIh5E5GYQ1RGXSvyv6ENVX4Za1+N8riniau4WgHmEoW93uQqfBmYeO8rdk0UQmRBRRASI+ZcoIuPeCOvW0syRmWiNMJ8XqHYDQES6UMeI7IQ0TTAn3A2mDhYOCrkI9XpM48jXo+J97vECzIzPfY4VerE2s/dIYGkRc0OyoeNTzNIj/ZrxeWCRUc1ARNkpRIRtb9/H/LyHSPp76lspUytlWk5lqqVM9eNUaq1rZaJzXVddan38AjXQgQLzqepqAAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="647"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/new_team_usage.2c2fd65.640.png" srcset="/assets/ideal-img/new_team_usage.2c2fd65.640.png 640w,/assets/ideal-img/new_team_usage.026e90c.1754.png 1754w" width="640" height="647"></noscript></div>
</li>
<li>
<p>Tag Based Usage</p>
<ul>
<li>New <code>LiteLLM_DailyTagSpend</code> Table for aggregate tag based usage logging - <a href="https://github.com/BerriAI/litellm/pull/10071" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Restrict to only Proxy Admins - <a href="https://github.com/BerriAI/litellm/pull/10157" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>allow viewing top keys by tag</li>
<li>Return tags passed in request (i.e. dynamic tags) on <code>/tag/list</code> API - <a href="https://github.com/BerriAI/litellm/pull/10157" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAJCAYAAAALpr0TAAAACXBIWXMAABYlAAAWJQFJUiTwAAAA+UlEQVR4nGWPQU7DQAxF53qINbepWKEeAkJzAdhwCBaILaoUFlCpgTRSO5mm48zY4/moSQQLvvS9sK33bRNCuIgiSwq8YM7XOf8ZwALAUkQujfe+wCTBf8m5hBBK45y71ZQBjsM8EJ0N4NyDJ7o3RDQSH7YHKTYHPDUOkhQpJYjwSPREpQGHwgG4ev2U1WaPx9oisoCZkTRNi55KE2bi1g8CZOSsEEnImn9vpDOR+v6OAbz3FN/2vX64qMxRhxD0ZX+KXyPRrwwCFR2AZ59kk4GdTu9GVTRxih6Jdf19451rUu/W3B8rOnbVwdqqs7Y6WbvurG3atl3+AB51T7XiEHjtAAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="561"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/new_tag_usage.da53265.640.png" srcset="/assets/ideal-img/new_tag_usage.da53265.640.png 640w,/assets/ideal-img/new_tag_usage.bccd8dd.1863.png 1863w" width="640" height="561"></noscript></div>
</li>
<li>
<p>Track prompt caching metrics in daily user, team, tag tables - <a href="https://github.com/BerriAI/litellm/pull/10029" target="_blank" rel="noopener noreferrer">PR</a></p>
</li>
<li>
<p>Show usage by key (on all up, team, and tag usage dashboards) - <a href="https://github.com/BerriAI/litellm/pull/10157" target="_blank" rel="noopener noreferrer">PR</a></p>
</li>
<li>
<p>swap old usage with new usage tab</p>
</li>
</ol>
</li>
<li>
<p><strong>Models</strong></p>
<ol>
<li>Make columns resizable/hideable - <a href="https://github.com/BerriAI/litellm/pull/10119" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li>
<p><strong>API Playground</strong></p>
<ol>
<li>Allow internal user to call api playground - <a href="https://github.com/BerriAI/litellm/pull/10157" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li>
<p><strong>SCIM</strong></p>
<ol>
<li>Add LiteLLM SCIM Integration for Team and User management - <a href="/docs/tutorials/scim_litellm">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10072" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<ul>
<li><strong>GCS</strong>
<ol>
<li>Fix gcs pub sub logging with env var GCS_PROJECT_ID - <a href="/docs/observability/gcs_bucket_integration#usage">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10042" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>AIM</strong>
<ol>
<li>Add litellm call id passing to Aim guardrails on pre and post-hooks calls - <a href="/docs/proxy/guardrails/aim_security">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10021" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>Azure blob storage</strong>
<ol>
<li>Ensure logging works in high throughput scenarios - <a href="/docs/proxy/logging#azure-blob-storage">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/9962" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<ul>
<li><strong>Support setting <code>litellm.modify_params</code> via env var</strong> <a href="https://github.com/BerriAI/litellm/pull/9964" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong>Model Discovery</strong> - Check providers <code>/models</code> endpoints when calling proxys <code>/v1/models</code> endpoint - <a href="/docs/proxy/model_discovery">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/9958" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong><code>/utils/token_counter</code></strong> - fix retrieving custom tokenizer for db models - <a href="/docs/proxy/configs#set-custom-tokenizer">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/10047" target="_blank" rel="noopener noreferrer">PR</a></li>
<li><strong>Prisma migrate</strong> - handle existing columns in db table - <a href="https://github.com/BerriAI/litellm/pull/10138" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/sso">sso</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/unified-file-id">unified_file_id</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/cost-tracking">cost_tracking</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/security">security</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.66.0-stable">v1.66.0-stable - Realtime API Cost Tracking</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-04-12T10:00:00.000Z">2025412</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:main-v1.66.0-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.66.0.post1</span><br></span></code></pre></div></div></div></div></div>
<p>v1.66.0-stable is live now, here are the key highlights of this release</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>Realtime API Cost Tracking</strong>: Track cost of realtime API calls</li>
<li><strong>Microsoft SSO Auto-sync</strong>: Auto-sync groups and group members from Azure Entra ID to LiteLLM</li>
<li><strong>xAI grok-3</strong>: Added support for <code>xai/grok-3</code> models</li>
<li><strong>Security Fixes</strong>: Fixed <a href="https://www.cve.org/CVERecord?id=CVE-2025-0330" target="_blank" rel="noopener noreferrer">CVE-2025-0330</a> and <a href="https://www.cve.org/CVERecord?id=CVE-2024-6825" target="_blank" rel="noopener noreferrer">CVE-2024-6825</a> vulnerabilities</li>
</ul>
<p>Let&#x27;s dive in.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="realtime-api-cost-tracking">Realtime API Cost Tracking<a href="#realtime-api-cost-tracking" class="hash-link" aria-label="Realtime API Cost Tracking" title="Realtime API Cost Tracking"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAa0lEQVR4nGWOUQoDIQxEc//r+adXWMvGVjcxwpQRtrT040HIvJCRWiuIqsLMMMbAnPMPKaUgpYScM3rvUG2IWHCnEB8kIkDWYui4zGGUuF9rw1kY3vA1xeNxQttrH/iMjXz3oDwuQ3v+ShTfWUrCEcYQ58oAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/realtime_api.b74d2a9.640.png" srcset="/assets/ideal-img/realtime_api.b74d2a9.640.png 640w,/assets/ideal-img/realtime_api.462b74b.1920.png 1920w" width="640" height="334"></noscript></div>
<p>This release adds Realtime API logging + cost tracking.</p>
<ul>
<li><strong>Logging</strong>: LiteLLM now logs the complete response from realtime calls to all logging integrations (DB, S3, Langfuse, etc.)</li>
<li><strong>Cost Tracking</strong>: You can now set &#x27;base_model&#x27; and custom pricing for realtime models. <a href="/docs/proxy/custom_pricing">Custom Pricing</a></li>
<li><strong>Budgets</strong>: Your key/user/team budgets now work for realtime models as well.</li>
</ul>
<p>Start <a href="https://docs.litellm.ai/docs/realtime" target="_blank" rel="noopener noreferrer">here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="microsoft-sso-auto-sync">Microsoft SSO Auto-sync<a href="#microsoft-sso-auto-sync" class="hash-link" aria-label="Microsoft SSO Auto-sync" title="Microsoft SSO Auto-sync"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAECAYAAAC3OK7NAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAb0lEQVR4nE2NQQpCMQxEe/87eQK3gls3rnRhf/OTJml50oLgwINhYGaKqHGaoz3o7mQmEUk9Gu7OGGNn5fqoXO4vbs/Gu560o6JqNOm7sDTnpHwOQ9SJHKgZIoKZkTnwiL24KJnBJoL443e5/Lr+ApbIm4FSZPtNAAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="284"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/sso_sync.271a408.640.png" srcset="/assets/ideal-img/sso_sync.271a408.640.png 640w,/assets/ideal-img/sso_sync.2f79062.1414.png 1414w" width="640" height="284"></noscript></div>
<p style="text-align:left;color:#666"></p><p>Auto-sync groups and members from Azure Entra ID to LiteLLM</p><p></p>
<p>This release adds support for auto-syncing groups and members on Microsoft Entra ID with LiteLLM. This means that LiteLLM proxy administrators can spend less time managing teams and members and LiteLLM handles the following:</p>
<ul>
<li>Auto-create teams that exist on Microsoft Entra ID</li>
<li>Sync team members on Microsoft Entra ID with LiteLLM teams</li>
</ul>
<p>Get started with this <a href="https://docs.litellm.ai/docs/tutorials/msft_sso" target="_blank" rel="noopener noreferrer">here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<ul>
<li>
<p><strong>xAI</strong></p>
<ol>
<li>Added reasoning_effort support for <code>xai/grok-3-mini-beta</code> <a href="https://docs.litellm.ai/docs/providers/xai#reasoning-usage" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>Added cost tracking for <code>xai/grok-3</code> models <a href="https://github.com/BerriAI/litellm/pull/9920" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li>
<p><strong>Hugging Face</strong></p>
<ol>
<li>Added inference providers support <a href="https://docs.litellm.ai/docs/providers/huggingface#serverless-inference-providers" target="_blank" rel="noopener noreferrer">Get Started</a></li>
</ol>
</li>
<li>
<p><strong>Azure</strong></p>
<ol>
<li>Added azure/gpt-4o-realtime-audio cost tracking <a href="https://github.com/BerriAI/litellm/pull/9893" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li>
<p><strong>VertexAI</strong></p>
<ol>
<li>Added enterpriseWebSearch tool support <a href="https://docs.litellm.ai/docs/providers/vertex#grounding---web-search" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>Moved to only passing keys accepted by the Vertex AI response schema <a href="https://github.com/BerriAI/litellm/pull/8992" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li>
<p><strong>Google AI Studio</strong></p>
<ol>
<li>Added cost tracking for <code>gemini-2.5-pro</code> <a href="https://github.com/BerriAI/litellm/pull/9837" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed pricing for &#x27;gemini/gemini-2.5-pro-preview-03-25&#x27; <a href="https://github.com/BerriAI/litellm/pull/9896" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed handling file_data being passed in <a href="https://github.com/BerriAI/litellm/pull/9786" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li>
<p><strong>Azure</strong></p>
<ol>
<li>Updated Azure Phi-4 pricing <a href="https://github.com/BerriAI/litellm/pull/9862" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added azure/gpt-4o-realtime-audio cost tracking <a href="https://github.com/BerriAI/litellm/pull/9893" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li>
<p><strong>Databricks</strong></p>
<ol>
<li>Removed reasoning_effort from parameters <a href="https://github.com/BerriAI/litellm/pull/9811" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed custom endpoint check for Databricks <a href="https://github.com/BerriAI/litellm/pull/9925" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li>
<p><strong>General</strong></p>
<ol>
<li>Added litellm.supports_reasoning() util to track if an llm supports reasoning <a href="https://docs.litellm.ai/docs/providers/anthropic#reasoning" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>Function Calling - Handle pydantic base model in message tool calls, handle tools = [], and support fake streaming on tool calls for meta.llama3-3-70b-instruct-v1:0 <a href="https://github.com/BerriAI/litellm/pull/9774" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>LiteLLM Proxy - Allow passing <code>thinking</code> param to litellm proxy via client sdk <a href="https://github.com/BerriAI/litellm/pull/9386" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed correctly translating &#x27;thinking&#x27; param for litellm <a href="https://github.com/BerriAI/litellm/pull/9904" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-improvements">Spend Tracking Improvements<a href="#spend-tracking-improvements" class="hash-link" aria-label="Spend Tracking Improvements" title="Spend Tracking Improvements"></a></h2>
<ul>
<li><strong>OpenAI, Azure</strong>
<ol>
<li>Realtime API Cost tracking with token usage metrics in spend logs <a href="https://docs.litellm.ai/docs/realtime" target="_blank" rel="noopener noreferrer">Get Started</a></li>
</ol>
</li>
<li><strong>Anthropic</strong>
<ol>
<li>Fixed Claude Haiku cache read pricing per token <a href="https://github.com/BerriAI/litellm/pull/9834" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added cost tracking for Claude responses with base_model <a href="https://github.com/BerriAI/litellm/pull/9897" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed Anthropic prompt caching cost calculation and trimmed logged message in db <a href="https://github.com/BerriAI/litellm/pull/9838" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li><strong>General</strong>
<ol>
<li>Added token tracking and log usage object in spend logs <a href="https://github.com/BerriAI/litellm/pull/9843" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Handle custom pricing at deployment level <a href="https://github.com/BerriAI/litellm/pull/9855" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<ul>
<li>
<p><strong>Test Key Tab</strong></p>
<ol>
<li>Added rendering of Reasoning content, ttft, usage metrics on test key page <a href="https://github.com/BerriAI/litellm/pull/9931" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAiElEQVR4nE2M0QqDUAxD/f+f3NNQHOpVa+ttm2bMpwVCOBDOILe/1fpRxa13b5loJBrwdItI8YjXcKiLRtEzq+0nUcW/VJA0z3Uws7MAdg8s61aqWp95rr3tdanieQPLICKiqsxEyaXs3Wn246S7P3pUPcbF3T0BO+W6I/ImeWfitzaOiWnK8QvnB8J2yo0rGAAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="335"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/chat_metrics.c9c2c38.640.png" srcset="/assets/ideal-img/chat_metrics.c9c2c38.640.png 640w,/assets/ideal-img/chat_metrics.6787433.1920.png 1920w" width="640" height="335"></noscript></div>
<p style="text-align:left;color:#666"></p><p>View input, output, reasoning tokens, ttft metrics.</p><p></p>
</li>
<li>
<p><strong>Tag / Policy Management</strong></p>
<ol>
<li>Added Tag/Policy Management. Create routing rules based on request metadata. This allows you to enforce that requests with <code>tags=&quot;private&quot;</code> only go to specific models. <a href="https://docs.litellm.ai/docs/tutorials/tag_management" target="_blank" rel="noopener noreferrer">Get Started</a></li>
</ol>
<br>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAcklEQVR4nF2MzQrDMAyD8/7vN9hlu++wH5qmTlLL30gXGFQgGQlZqfX22Eq3nOsKFEll3MkcES7pknpvOZfg9fGRIQXWnF0BoCHAPZnVZRq5e0gKFyERx+cPt1Q2G/PDHHLCf/G9rE9rfQcq0E60Wbx+AX/hxGW962K6AAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="343"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/tag_management.f6c88bf.640.png" srcset="/assets/ideal-img/tag_management.f6c88bf.640.png 640w,/assets/ideal-img/tag_management.ad220e9.1920.png 1920w" width="640" height="343"></noscript></div>
<p style="text-align:left;color:#666"></p><p>Create and manage tags.</p><p></p>
</li>
<li>
<p><strong>Redesigned Login Screen</strong></p>
<ol>
<li>Polished login screen <a href="https://github.com/BerriAI/litellm/pull/9778" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li>
<p><strong>Microsoft SSO Auto-Sync</strong></p>
<ol>
<li>Added debug route to allow admins to debug SSO JWT fields <a href="https://github.com/BerriAI/litellm/pull/9835" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added ability to use MSFT Graph API to assign users to teams <a href="https://github.com/BerriAI/litellm/pull/9865" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Connected litellm to Azure Entra ID Enterprise Application <a href="https://github.com/BerriAI/litellm/pull/9872" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added ability for admins to set <code>default_team_params</code> for when litellm SSO creates default teams <a href="https://github.com/BerriAI/litellm/pull/9895" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed MSFT SSO to use correct field for user email <a href="https://github.com/BerriAI/litellm/pull/9886" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added UI support for setting Default Team setting when litellm SSO auto creates teams <a href="https://github.com/BerriAI/litellm/pull/9918" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
<li>
<p><strong>UI Bug Fixes</strong></p>
<ol>
<li>Prevented team, key, org, model numerical values changing on scrolling <a href="https://github.com/BerriAI/litellm/pull/9776" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Instantly reflect key and team updates in UI <a href="https://github.com/BerriAI/litellm/pull/9825" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-improvements">Logging / Guardrail Improvements<a href="#logging--guardrail-improvements" class="hash-link" aria-label="Logging / Guardrail Improvements" title="Logging / Guardrail Improvements"></a></h2>
<ul>
<li><strong>Prometheus</strong>
<ol>
<li>Emit Key and Team Budget metrics on a cron job schedule <a href="https://docs.litellm.ai/docs/proxy/prometheus#initialize-budget-metrics-on-startup" target="_blank" rel="noopener noreferrer">Get Started</a></li>
</ol>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="security-fixes">Security Fixes<a href="#security-fixes" class="hash-link" aria-label="Security Fixes" title="Security Fixes"></a></h2>
<ul>
<li>Fixed <a href="https://www.cve.org/CVERecord?id=CVE-2025-0330" target="_blank" rel="noopener noreferrer">CVE-2025-0330</a> - Leakage of Langfuse API keys in team exception handling <a href="https://github.com/BerriAI/litellm/pull/9830" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed <a href="https://www.cve.org/CVERecord?id=CVE-2024-6825" target="_blank" rel="noopener noreferrer">CVE-2024-6825</a> - Remote code execution in post call rules <a href="https://github.com/BerriAI/litellm/pull/9826" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="helm">Helm<a href="#helm" class="hash-link" aria-label="Helm" title="Helm"></a></h2>
<ul>
<li>Added service annotations to litellm-helm chart <a href="https://github.com/BerriAI/litellm/pull/9840" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added extraEnvVars to the helm deployment <a href="https://github.com/BerriAI/litellm/pull/9292" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="demo">Demo<a href="#demo" class="hash-link" aria-label="Demo" title="Demo"></a></h2>
<p>Try this on the demo instance <a href="https://docs.litellm.ai/docs/proxy/demo" target="_blank" rel="noopener noreferrer">today</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="complete-git-diff">Complete Git Diff<a href="#complete-git-diff" class="hash-link" aria-label="Complete Git Diff" title="Complete Git Diff"></a></h2>
<p>See the complete git diff since v1.65.4-stable, <a href="https://github.com/BerriAI/litellm/releases/tag/v1.66.0-stable" target="_blank" rel="noopener noreferrer">here</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/sso">sso</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/unified-file-id">unified_file_id</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/cost-tracking">cost_tracking</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/security">security</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.65.4-stable">v1.65.4-stable</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-04-05T10:00:00.000Z">202545</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-this-version">Deploy this version<a href="#deploy-this-version" class="hash-link" aria-label="Deploy this version" title="Deploy this version"></a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Docker</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pip</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">docker run litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:main-v1.65.4-stable</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-showLineNumbers language-showlinenumbers codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">pip install litellm</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-showlinenumbers codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install litellm==1.65.4.post1</span><br></span></code></pre></div></div></div></div></div>
<p>v1.65.4-stable is live. Here are the improvements since v1.65.0-stable.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-highlights">Key Highlights<a href="#key-highlights" class="hash-link" aria-label="Key Highlights" title="Key Highlights"></a></h2>
<ul>
<li><strong>Preventing DB Deadlocks</strong>: Fixes a high-traffic issue when multiple instances were writing to the DB at the same time.</li>
<li><strong>New Usage Tab</strong>: Enables viewing spend by model and customizing date range</li>
</ul>
<p>Let&#x27;s dive in.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="preventing-db-deadlocks">Preventing DB Deadlocks<a href="#preventing-db-deadlocks" class="hash-link" aria-label="Preventing DB Deadlocks" title="Preventing DB Deadlocks"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAFAAoDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAYH/8QAIhAAAgEFAAAHAAAAAAAAAAAAAQIDAAQFBhESExQhMUFR/8QAFAEBAAAAAAAAAAAAAAAAAAAAAf/EABURAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIRAxEAPwCjfZd0jsNhePZUWSG7RLc+gjIiUyMCOd9+gAdP5Wn4DIZWfA42W7vlluHto2lk8lV8bFQSeD46fqlKaI//2Q==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/prevent_deadlocks.9cf4d6a.640.jpg" srcset="/assets/ideal-img/prevent_deadlocks.9cf4d6a.640.jpg 640w,/assets/ideal-img/prevent_deadlocks.b3f9e4e.1920.jpg 1920w" width="640" height="334"></noscript></div>
<p>This release fixes the DB deadlocking issue that users faced in high traffic (10K+ RPS). This is great because it enables user/key/team spend tracking works at that scale.</p>
<p>Read more about the new architecture <a href="https://docs.litellm.ai/docs/proxy/db_deadlocks" target="_blank" rel="noopener noreferrer">here</a></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="new-usage-tab">New Usage Tab<a href="#new-usage-tab" class="hash-link" aria-label="New Usage Tab" title="New Usage Tab"></a></h3>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAFAAoDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAMI/8QAHBAAAgMBAAMAAAAAAAAAAAAAAQIAAxEEFCFB/8QAFQEBAQAAAAAAAAAAAAAAAAAAAQL/xAAYEQACAwAAAAAAAAAAAAAAAAAAAQIRIf/aAAwDAQACEQMRAD8A0upKdA462dQ1RYWbrA7m+5cU3AAeQTn0oumIgpMhbZ//2Q==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/spend_by_model.8158e34.640.jpg" srcset="/assets/ideal-img/spend_by_model.8158e34.640.jpg 640w,/assets/ideal-img/spend_by_model.411d1b0.1920.jpg 1920w" width="640" height="334"></noscript></div>
<p>The new Usage tab now brings the ability to track daily spend by model. This makes it easier to catch any spend tracking or token counting errors, when combined with the ability to view successful requests, and token usage.</p>
<p>To test this out, just go to Experimental &gt; New Usage &gt; Activity.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<ol>
<li>Databricks - claude-3-7-sonnet cost tracking <a href="https://github.com/BerriAI/litellm/blob/52b35cd8093b9ad833987b24f494586a1e923209/model_prices_and_context_window.json#L10350" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>VertexAI - <code>gemini-2.5-pro-exp-03-25</code> cost tracking <a href="https://github.com/BerriAI/litellm/blob/52b35cd8093b9ad833987b24f494586a1e923209/model_prices_and_context_window.json#L4492" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>VertexAI - <code>gemini-2.0-flash</code> cost tracking <a href="https://github.com/BerriAI/litellm/blob/52b35cd8093b9ad833987b24f494586a1e923209/model_prices_and_context_window.json#L4689" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Groq - add whisper ASR models to model cost map <a href="https://github.com/BerriAI/litellm/blob/52b35cd8093b9ad833987b24f494586a1e923209/model_prices_and_context_window.json#L3324" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>IBM - Add watsonx/ibm/granite-3-8b-instruct to model cost map <a href="https://github.com/BerriAI/litellm/blob/52b35cd8093b9ad833987b24f494586a1e923209/model_prices_and_context_window.json#L91" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Google AI Studio - add gemini/gemini-2.5-pro-preview-03-25 to model cost map <a href="https://github.com/BerriAI/litellm/blob/52b35cd8093b9ad833987b24f494586a1e923209/model_prices_and_context_window.json#L4850" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-translation">LLM Translation<a href="#llm-translation" class="hash-link" aria-label="LLM Translation" title="LLM Translation"></a></h2>
<ol>
<li>Vertex AI - Support anyOf param for OpenAI json schema translation <a href="https://docs.litellm.ai/docs/providers/vertex#json-schema" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>Anthropic- response_format + thinking param support  (works across Anthropic API, Bedrock, Vertex) <a href="https://docs.litellm.ai/docs/reasoning_content" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>Anthropic - if thinking token is specified and max tokens is not - ensure max token to anthropic is higher than thinking tokens (works across Anthropic API, Bedrock, Vertex) <a href="https://github.com/BerriAI/litellm/pull/9594" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Bedrock - latency optimized inference support <a href="https://docs.litellm.ai/docs/providers/bedrock#usage---latency-optimized-inference" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>Sagemaker - handle special tokens + multibyte character code in response <a href="https://docs.litellm.ai/docs/providers/aws_sagemaker" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>MCP - add support for using SSE MCP servers <a href="https://docs.litellm.ai/docs/mcp#usage" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>Anthropic - new <code>litellm.messages.create</code> interface for calling Anthropic <code>/v1/messages</code> via passthrough <a href="https://docs.litellm.ai/docs/anthropic_unified#usage" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>Anthropic - support file content type in message param (works across Anthropic API, Bedrock, Vertex) <a href="https://docs.litellm.ai/docs/providers/anthropic#usage---pdf" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>Anthropic - map openai &#x27;reasoning_effort&#x27; to anthropic &#x27;thinking&#x27; param (works across Anthropic API, Bedrock, Vertex) <a href="https://docs.litellm.ai/docs/providers/anthropic#usage---thinking--reasoning_content" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>Google AI Studio (Gemini) - [BETA] <code>/v1/files</code> upload support <a href="/docs/providers/google_ai_studio/files">Get Started</a></li>
<li>Azure - fix o-series tool calling <a href="/docs/providers/azure#tool-calling--function-calling">Get Started</a></li>
<li>Unified file id - [ALPHA] allow calling multiple providers with same file id <a href="https://github.com/BerriAI/litellm/pull/9718" target="_blank" rel="noopener noreferrer">PR</a>
<ul>
<li>This is experimental, and not recommended for production use.</li>
<li>We plan to have a production-ready implementation by next week.</li>
</ul>
</li>
<li>Google AI Studio (Gemini) - return logprobs <a href="https://github.com/BerriAI/litellm/pull/9713" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Anthropic - Support prompt caching for Anthropic tool calls <a href="https://docs.litellm.ai/docs/completion/prompt_caching" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>OpenRouter - unwrap extra body on open router calls <a href="https://github.com/BerriAI/litellm/pull/9747" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>VertexAI - fix credential caching issue <a href="https://github.com/BerriAI/litellm/pull/9756" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>XAI - filter out &#x27;name&#x27; param for XAI <a href="https://github.com/BerriAI/litellm/pull/9761" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Gemini - image generation output support <a href="/docs/providers/gemini#image-generation">Get Started</a></li>
<li>Databricks - support claude-3-7-sonnet w/ thinking + response_format <a href="/docs/providers/databricks#usage---thinking--reasoning_content">Get Started</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-improvements">Spend Tracking Improvements<a href="#spend-tracking-improvements" class="hash-link" aria-label="Spend Tracking Improvements" title="Spend Tracking Improvements"></a></h2>
<ol>
<li>Reliability fix  - Check sent and received model for cost calculation <a href="https://github.com/BerriAI/litellm/pull/9669" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Vertex AI - Multimodal embedding cost tracking <a href="https://docs.litellm.ai/docs/providers/vertex#multi-modal-embeddings" target="_blank" rel="noopener noreferrer">Get Started</a>, <a href="https://github.com/BerriAI/litellm/pull/9623" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAfUlEQVR4nB2N4QrDMBCC8/4POtgK2bqlaePpOa4/5ANFbWMcT0kkuUjCEoCAJERwZdoMPtp5rW7bkpJKI9IRLOv2imtha2Ois1pUjlN+DzpFJ2lFJel5zq19v78OhCNC10ICkUDxlmpxzvlqn33vdSUxJZqkWZSqfF+P49j+drvD2Hey2vYAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="344"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/new_activity_tab.345f59a.640.png" srcset="/assets/ideal-img/new_activity_tab.345f59a.640.png 640w,/assets/ideal-img/new_activity_tab.d65eafe.1920.png 1920w" width="640" height="344"></noscript></div>
<ol>
<li>New Usage Tab<!-- -->
<ul>
<li>Report &#x27;total_tokens&#x27; + report success/failure calls</li>
<li>Remove double bars on scroll</li>
<li>Ensure daily spend chart ordered from earliest to latest date</li>
<li>showing spend per model per day</li>
<li>show key alias on usage tab</li>
<li>Allow non-admins to view their activity</li>
<li>Add date picker to new usage tab</li>
</ul>
</li>
<li>Virtual Keys Tab<!-- -->
<ul>
<li>remove &#x27;default key&#x27; on user signup</li>
<li>fix showing user models available for personal key creation</li>
</ul>
</li>
<li>Test Key Tab<!-- -->
<ul>
<li>Allow testing image generation models</li>
</ul>
</li>
<li>Models Tab<!-- -->
<ul>
<li>Fix bulk adding models</li>
<li>support reusable credentials for passthrough endpoints</li>
<li>Allow team members to see team models</li>
</ul>
</li>
<li>Teams Tab<!-- -->
<ul>
<li>Fix json serialization error on update team metadata</li>
</ul>
</li>
<li>Request Logs Tab<!-- -->
<ul>
<li>Add reasoning_content token tracking across all providers on streaming</li>
</ul>
</li>
<li>API<!-- -->
<ul>
<li>return key alias on /user/daily/activity <a href="/docs/proxy/cost_tracking#daily-spend-breakdown-api">Get Started</a></li>
</ul>
</li>
<li>SSO<!-- -->
<ul>
<li>Allow assigning SSO users to teams on MSFT SSO <a href="https://github.com/BerriAI/litellm/pull/9745" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<ol>
<li>Console Logs - Add json formatting for uncaught exceptions <a href="https://github.com/BerriAI/litellm/pull/9619" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Guardrails - AIM Guardrails support for virtual key based policies <a href="/docs/proxy/guardrails/aim_security">Get Started</a></li>
<li>Logging - fix completion start time tracking <a href="https://github.com/BerriAI/litellm/pull/9688" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Prometheus<!-- -->
<ul>
<li>Allow adding authentication on Prometheus /metrics endpoints <a href="https://github.com/BerriAI/litellm/pull/9766" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Distinguish LLM Provider Exception vs. LiteLLM Exception in metric naming <a href="https://github.com/BerriAI/litellm/pull/9760" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Emit operational metrics for new DB Transaction architecture <a href="https://github.com/BerriAI/litellm/pull/9719" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<ol>
<li>
<p>Preventing Deadlocks</p>
<ul>
<li>Reduce DB Deadlocks by storing spend updates in Redis and then committing to DB <a href="https://github.com/BerriAI/litellm/pull/9608" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Ensure no deadlocks occur when updatingDailyUserSpendTransaction <a href="https://github.com/BerriAI/litellm/pull/9690" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>High Traffic fix - ensure new DB + Redis architecture accurately tracks spend <a href="https://github.com/BerriAI/litellm/pull/9673" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Use Redis for PodLock Manager instead of PG (ensures no deadlocks occur) <a href="https://github.com/BerriAI/litellm/pull/9715" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>v2 DB Deadlock Reduction Architecture  Add Max Size for In-Memory Queue + Backpressure Mechanism <a href="https://github.com/BerriAI/litellm/pull/9759" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li>
<p>Prisma Migrations <a href="/docs/proxy/prod#9-use-prisma-migrate-deploy">Get Started</a></p>
<ul>
<li>connects litellm proxy to litellm&#x27;s prisma migration files</li>
<li>Handle db schema updates from new <code>litellm-proxy-extras</code> sdk</li>
</ul>
</li>
<li>
<p>Redis - support password for sync sentinel clients <a href="https://github.com/BerriAI/litellm/pull/9622" target="_blank" rel="noopener noreferrer">PR</a></p>
</li>
<li>
<p>Fix &quot;Circular reference detected&quot; error when max_parallel_requests = 0 <a href="https://github.com/BerriAI/litellm/pull/9671" target="_blank" rel="noopener noreferrer">PR</a></p>
</li>
<li>
<p>Code QA - Ban hardcoded numbers <a href="https://github.com/BerriAI/litellm/pull/9709" target="_blank" rel="noopener noreferrer">PR</a></p>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="helm">Helm<a href="#helm" class="hash-link" aria-label="Helm" title="Helm"></a></h2>
<ol>
<li>fix: wrong indentation of ttlSecondsAfterFinished in chart <a href="https://github.com/BerriAI/litellm/pull/9611" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<ol>
<li>Fix - only apply service_account_settings.enforced_params on service accounts <a href="https://github.com/BerriAI/litellm/pull/9683" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix - handle metadata null on <code>/chat/completion</code> <a href="https://github.com/BerriAI/litellm/issues/9717" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix - Move daily user transaction logging outside of &#x27;disable_spend_logs&#x27; flag, as theyre unrelated <a href="https://github.com/BerriAI/litellm/pull/9772" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="demo">Demo<a href="#demo" class="hash-link" aria-label="Demo" title="Demo"></a></h2>
<p>Try this on the demo instance <a href="https://docs.litellm.ai/docs/proxy/demo" target="_blank" rel="noopener noreferrer">today</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="complete-git-diff">Complete Git Diff<a href="#complete-git-diff" class="hash-link" aria-label="Complete Git Diff" title="Complete Git Diff"></a></h2>
<p>See the complete git diff since v1.65.0-stable, <a href="https://github.com/BerriAI/litellm/releases/tag/v1.65.4-stable" target="_blank" rel="noopener noreferrer">here</a></p></div></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.65.0-stable">v1.65.0-stable - Model Context Protocol</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-03-30T10:00:00.000Z">2025330</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>v1.65.0-stable is live now. Here are the key highlights of this release:</p>
<ul>
<li><strong>MCP Support</strong>: Support for adding and using MCP servers on the LiteLLM proxy.</li>
<li><strong>UI view total usage after 1M+ logs</strong>: You can now view usage analytics after crossing 1M+ logs in DB.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="model-context-protocol-mcp">Model Context Protocol (MCP)<a href="#model-context-protocol-mcp" class="hash-link" aria-label="Model Context Protocol (MCP)" title="Model Context Protocol (MCP)"></a></h2>
<p>This release introduces support for centrally adding MCP servers on LiteLLM. This allows you to add MCP server endpoints and your developers can <code>list</code> and <code>call</code> MCP tools through LiteLLM.</p>
<p>Read more about MCP <a href="https://docs.litellm.ai/docs/mcp" target="_blank" rel="noopener noreferrer">here</a>.</p>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAGCAYAAAD68A/GAAAACXBIWXMAAAsTAAALEwEAmpwYAAAArElEQVR4nEWOMW4CQQxF92ZcIvdIm+twiTQUlFEKekS2owAF7bJLdsZjm5kXOQil+LItv//tbhyvTNOEqmJmuBnNDVWLuQFRc5dzJlRUqW7MxTkm/+ullHYdBpYl5S5SAoxEUePt88TL+xe7S8YlteNhzzw8QRFMC4vded32rNYfbE8JVNplnLjd5gcYip+ozuF7ZtOfcdO40mJXRP7BhxyTjKefp7nVWsOQfwHfIeSIkE4rfwAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="363"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/mcp_ui.cdfd72e.640.png" srcset="/assets/ideal-img/mcp_ui.cdfd72e.640.png 640w,/assets/ideal-img/mcp_ui.2857485.1920.png 1920w" width="640" height="363"></noscript></div>
<p style="text-align:left;color:#666"></p><p>Expose and use MCP servers through LiteLLM</p><p></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ui-view-total-usage-after-1m-logs">UI view total usage after 1M+ logs<a href="#ui-view-total-usage-after-1m-logs" class="hash-link" aria-label="UI view total usage after 1M+ logs" title="UI view total usage after 1M+ logs"></a></h2>
<p>This release brings the ability to view total usage analytics even after exceeding 1M+ logs in your database. We&#x27;ve implemented a scalable architecture that stores only aggregate usage data, resulting in significantly more efficient queries and reduced database CPU utilization.</p>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAgklEQVR4nE2OQQ6DMBAD8/+H9E2FG6f0koIKJJvNLlOFtqKWRj5Ysh3SM5FSYp5n1nVFtVGronphrRGmaWIYBsZxJMZILoV9z7gfHMeHrEZorWFmuDu1VkTKGXb512/3SOjhjyJCLkIRZRFlb8by2ngsG+H/S//nDmaOmp+NIn2l8gZKO8HzLK4OjwAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/ui_usage.c3911d7.640.png" srcset="/assets/ideal-img/ui_usage.c3911d7.640.png 640w,/assets/ideal-img/ui_usage.3ffdba3.1200.png 1200w" width="640" height="334"></noscript></div>
<p style="text-align:left;color:#666"></p><p>View total usage after 1M+ logs</p><p></p>
<ul>
<li>
<p>How this works:</p>
<ul>
<li>We now aggregate usage data into a dedicated DailyUserSpend table, significantly reducing query load and CPU usage even beyond 1M+ logs.</li>
</ul>
</li>
<li>
<p>Daily Spend Breakdown API:</p>
<ul>
<li>Retrieve granular daily usage data (by model, provider, and API key) with a single endpoint.
Example Request:</li>
</ul>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">Daily Spend Breakdown API</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv codeBlockLinesWithNumbering_o6Pm" style="counter-reset:line-count 0"><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">curl -L -X GET &#x27;http://localhost:4000/user/daily/activity?start_date=2025-03-20&amp;end_date=2025-03-27&#x27; \</span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">-H &#x27;Authorization: Bearer sk-...&#x27;</span></span><br></span></code></pre></div></div>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_OeMC">Daily Spend Breakdown API Response</div><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv codeBlockLinesWithNumbering_o6Pm" style="counter-reset:line-count 0"><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    </span><span class="token property" style="color:#36acaa">&quot;results&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">            </span><span class="token property" style="color:#36acaa">&quot;date&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;2025-03-27&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">            </span><span class="token property" style="color:#36acaa">&quot;metrics&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">                </span><span class="token property" style="color:#36acaa">&quot;spend&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.0177072</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">                </span><span class="token property" style="color:#36acaa">&quot;prompt_tokens&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">111</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">                </span><span class="token property" style="color:#36acaa">&quot;completion_tokens&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1711</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">                </span><span class="token property" style="color:#36acaa">&quot;total_tokens&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1822</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">                </span><span class="token property" style="color:#36acaa">&quot;api_requests&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">11</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">            </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">            </span><span class="token property" style="color:#36acaa">&quot;breakdown&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">                </span><span class="token property" style="color:#36acaa">&quot;models&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">                    </span><span class="token property" style="color:#36acaa">&quot;gpt-4o-mini&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">                        </span><span class="token property" style="color:#36acaa">&quot;spend&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1.095e-05</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">                        </span><span class="token property" style="color:#36acaa">&quot;prompt_tokens&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">37</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">                        </span><span class="token property" style="color:#36acaa">&quot;completion_tokens&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">9</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">                        </span><span class="token property" style="color:#36acaa">&quot;total_tokens&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">46</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">                        </span><span class="token property" style="color:#36acaa">&quot;api_requests&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">                </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">                </span><span class="token property" style="color:#36acaa">&quot;providers&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;openai&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"> ... </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;azure_ai&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"> ... </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">                </span><span class="token property" style="color:#36acaa">&quot;api_keys&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;3126b6eaf1...&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"> ... </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">            </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    </span><span class="token property" style="color:#36acaa">&quot;metadata&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;total_spend&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.7274667</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;total_prompt_tokens&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">280990</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;total_completion_tokens&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">376674</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;total_api_requests&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">14</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span></span><br></span><span class="token-line codeLine_lJS_" style="color:#393A34"><span class="codeLineNumber_Tfdd"></span><span class="codeLineContent_feaV"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span></span><br></span></code></pre></div></div>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<ul>
<li>Support for Vertex AI gemini-2.0-flash-lite &amp; Google AI Studio gemini-2.0-flash-lite <a href="https://github.com/BerriAI/litellm/pull/9523" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support for Vertex AI Fine-Tuned LLMs <a href="https://github.com/BerriAI/litellm/pull/9542" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Nova Canvas image generation support <a href="https://github.com/BerriAI/litellm/pull/9525" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>OpenAI gpt-4o-transcribe support <a href="https://github.com/BerriAI/litellm/pull/9517" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added new Vertex AI text embedding model <a href="https://github.com/BerriAI/litellm/pull/9476" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-translation">LLM Translation<a href="#llm-translation" class="hash-link" aria-label="LLM Translation" title="LLM Translation"></a></h2>
<ul>
<li>OpenAI Web Search Tool Call Support <a href="https://github.com/BerriAI/litellm/pull/9465" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Vertex AI topLogprobs support <a href="https://github.com/BerriAI/litellm/pull/9518" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support for sending images and video to Vertex AI multimodal embedding <a href="https://docs.litellm.ai/docs/providers/vertex#multi-modal-embeddings" target="_blank" rel="noopener noreferrer">Doc</a></li>
<li>Support litellm.api_base for Vertex AI + Gemini across completion, embedding, image_generation <a href="https://github.com/BerriAI/litellm/pull/9516" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Bug fix for returning <code>response_cost</code> when using litellm python SDK with LiteLLM Proxy <a href="https://github.com/BerriAI/litellm/commit/6fd18651d129d606182ff4b980e95768fc43ca3d" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support for <code>max_completion_tokens</code> on Mistral API <a href="https://github.com/BerriAI/litellm/pull/9606" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Refactored Vertex AI passthrough routes - fixes unpredictable behaviour with auto-setting default_vertex_region on router model add <a href="https://github.com/BerriAI/litellm/pull/9467" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-improvements">Spend Tracking Improvements<a href="#spend-tracking-improvements" class="hash-link" aria-label="Spend Tracking Improvements" title="Spend Tracking Improvements"></a></h2>
<ul>
<li>Log &#x27;api_base&#x27; on spend logs <a href="https://github.com/BerriAI/litellm/pull/9509" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support for Gemini audio token cost tracking <a href="https://github.com/BerriAI/litellm/pull/9535" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed OpenAI audio input token cost tracking <a href="https://github.com/BerriAI/litellm/pull/9535" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ui">UI<a href="#ui" class="hash-link" aria-label="UI" title="UI"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="model-management">Model Management<a href="#model-management" class="hash-link" aria-label="Model Management" title="Model Management"></a></h3>
<ul>
<li>Allowed team admins to add/update/delete models on UI <a href="https://github.com/BerriAI/litellm/pull/9572" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added render supports_web_search on model hub <a href="https://github.com/BerriAI/litellm/pull/9469" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="request-logs">Request Logs<a href="#request-logs" class="hash-link" aria-label="Request Logs" title="Request Logs"></a></h3>
<ul>
<li>Show API base and model ID on request logs <a href="https://github.com/BerriAI/litellm/pull/9572" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Allow viewing keyinfo on request logs <a href="https://github.com/BerriAI/litellm/pull/9568" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="usage-tab">Usage Tab<a href="#usage-tab" class="hash-link" aria-label="Usage Tab" title="Usage Tab"></a></h3>
<ul>
<li>Added Daily User Spend Aggregate view - allows UI Usage tab to work &gt; 1m rows <a href="https://github.com/BerriAI/litellm/pull/9538" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Connected UI to &quot;LiteLLM_DailyUserSpend&quot; spend table <a href="https://github.com/BerriAI/litellm/pull/9603" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging-integrations">Logging Integrations<a href="#logging-integrations" class="hash-link" aria-label="Logging Integrations" title="Logging Integrations"></a></h2>
<ul>
<li>Fixed StandardLoggingPayload for GCS Pub Sub Logging Integration <a href="https://github.com/BerriAI/litellm/pull/9508" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Track <code>litellm_model_name</code> on <code>StandardLoggingPayload</code> <a href="https://docs.litellm.ai/docs/proxy/logging_spec#standardlogginghiddenparams" target="_blank" rel="noopener noreferrer">Docs</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--reliability-improvements">Performance / Reliability Improvements<a href="#performance--reliability-improvements" class="hash-link" aria-label="Performance / Reliability Improvements" title="Performance / Reliability Improvements"></a></h2>
<ul>
<li>LiteLLM Redis semantic caching implementation <a href="https://github.com/BerriAI/litellm/pull/9356" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Gracefully handle exceptions when DB is having an outage <a href="https://github.com/BerriAI/litellm/pull/9533" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Allow Pods to startup + passing /health/readiness when allow_requests_on_db_unavailable: True and DB is down <a href="https://github.com/BerriAI/litellm/pull/9569" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-improvements">General Improvements<a href="#general-improvements" class="hash-link" aria-label="General Improvements" title="General Improvements"></a></h2>
<ul>
<li>Support for exposing MCP tools on litellm proxy <a href="https://github.com/BerriAI/litellm/pull/9426" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Support discovering Gemini, Anthropic, xAI models by calling their /v1/model endpoint <a href="https://github.com/BerriAI/litellm/pull/9530" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fixed route check for non-proxy admins on JWT auth <a href="https://github.com/BerriAI/litellm/pull/9454" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Added baseline Prisma database migrations <a href="https://github.com/BerriAI/litellm/pull/9565" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>View all wildcard models on /model/info <a href="https://github.com/BerriAI/litellm/pull/9572" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="security">Security<a href="#security" class="hash-link" aria-label="Security" title="Security"></a></h2>
<ul>
<li>Bumped next from 14.2.21 to 14.2.25 in UI dashboard <a href="https://github.com/BerriAI/litellm/pull/9458" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="complete-git-diff">Complete Git Diff<a href="#complete-git-diff" class="hash-link" aria-label="Complete Git Diff" title="Complete Git Diff"></a></h2>
<p><a href="https://github.com/BerriAI/litellm/compare/v1.63.14-stable.patch1...v1.65.0-stable" target="_blank" rel="noopener noreferrer">Here&#x27;s the complete git diff</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/mcp">mcp</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/custom-prompt-management">custom_prompt_management</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.65.0">v1.65.0 - Team Model Add - update</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-03-28T10:00:00.000Z">2025328</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>v1.65.0 updates the <code>/model/new</code> endpoint to prevent non-team admins from creating team models.</p>
<p>This means that only proxy admins or team admins can create team models.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="additional-changes">Additional Changes<a href="#additional-changes" class="hash-link" aria-label="Additional Changes" title="Additional Changes"></a></h2>
<ul>
<li>Allows team admins to call <code>/model/update</code> to update team models.</li>
<li>Allows team admins to call <code>/model/delete</code> to delete team models.</li>
<li>Introduces new <code>user_models_only</code> param to <code>/v2/model/info</code> - only return models added by this user.</li>
</ul>
<p>These changes enable team admins to add and manage models for their team on the LiteLLM UI + API.</p>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAECAYAAAC3OK7NAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAcklEQVR4nGWKOw7CQAwF9/53oeUKtAihFNDkBou/a5M1D5EqEiNNM5qW+T4BeFTVraruRwH82jMzz03Elm0CkVn4Z2+ZuTYSWXL7wMzLfGDOiZqFMQLusY+qtrb+4gtphpl3dydVI2YlEaUxopt5MPP1Cyf8mW69FKQ8AAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="274"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/team_model_add.140e27a.640.png" srcset="/assets/ideal-img/team_model_add.140e27a.640.png 640w,/assets/ideal-img/team_model_add.1ddd404.1251.png 1251w" width="640" height="274"></noscript></div></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/management-endpoints">management endpoints</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/team-models">team models</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/ui">ui</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.63.14-stable">v1.63.14-stable</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-03-22T10:00:00.000Z">2025322</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>These are the changes since <code>v1.63.11-stable</code>.</p>
<p>This release brings:</p>
<ul>
<li>LLM Translation Improvements (MCP Support and Bedrock Application Profiles)</li>
<li>Perf improvements for Usage-based Routing</li>
<li>Streaming guardrail support via websockets</li>
<li>Azure OpenAI client perf fix (from previous release)</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="docker-run-litellm-proxy">Docker Run LiteLLM Proxy<a href="#docker-run-litellm-proxy" class="hash-link" aria-label="Docker Run LiteLLM Proxy" title="Docker Run LiteLLM Proxy"></a></h2>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:main-v1.63.14-stable.patch1</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="demo-instance">Demo Instance<a href="#demo-instance" class="hash-link" aria-label="Demo Instance" title="Demo Instance"></a></h2>
<p>Here&#x27;s a Demo Instance to test changes:</p>
<ul>
<li>Instance: <a href="https://demo.litellm.ai/" target="_blank" rel="noopener noreferrer">https://demo.litellm.ai/</a></li>
<li>Login Credentials:<!-- -->
<ul>
<li>Username: admin</li>
<li>Password: sk-1234</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<ul>
<li>Azure gpt-4o - fixed pricing to latest global pricing - <a href="https://github.com/BerriAI/litellm/pull/9361" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>O1-Pro - add pricing + model information - <a href="https://github.com/BerriAI/litellm/pull/9397" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Azure AI - mistral 3.1 small pricing added - <a href="https://github.com/BerriAI/litellm/pull/9453" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Azure - gpt-4.5-preview pricing added - <a href="https://github.com/BerriAI/litellm/pull/9453" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-translation">LLM Translation<a href="#llm-translation" class="hash-link" aria-label="LLM Translation" title="LLM Translation"></a></h2>
<ol>
<li><strong>New LLM Features</strong></li>
</ol>
<ul>
<li>Bedrock: Support bedrock application inference profiles <a href="https://docs.litellm.ai/docs/providers/bedrock#bedrock-application-inference-profile" target="_blank" rel="noopener noreferrer">Docs</a>
<ul>
<li>Infer aws region from bedrock application profile id - (<code>arn:aws:bedrock:us-east-1:...</code>)</li>
</ul>
</li>
<li>Ollama - support calling via <code>/v1/completions</code> <a href="/docs/providers/ollama#using-ollama-fim-on-v1completions">Get Started</a></li>
<li>Bedrock - support <code>us.deepseek.r1-v1:0</code> model name <a href="/docs/providers/bedrock#supported-aws-bedrock-models">Docs</a></li>
<li>OpenRouter - <code>OPENROUTER_API_BASE</code> env var support <a href="/docs/providers/openrouter.md">Docs</a></li>
<li>Azure - add audio model parameter support - <a href="/docs/providers/azure#azure-audio-model">Docs</a></li>
<li>OpenAI - PDF File support <a href="/docs/completion/document_understanding#openai-file-message-type">Docs</a></li>
<li>OpenAI - o1-pro Responses API streaming support <a href="/docs/response_api.md#streaming">Docs</a></li>
<li>[BETA] MCP - Use MCP Tools with LiteLLM SDK <a href="/docs/mcp">Docs</a></li>
</ul>
<ol start="2">
<li><strong>Bug Fixes</strong></li>
</ol>
<ul>
<li>Voyage: prompt token on embedding tracking fix - <a href="https://github.com/BerriAI/litellm/commit/56d3e75b330c3c3862dc6e1c51c1210e48f1068e" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Sagemaker - Fix Too little data for declared Content-Length error - <a href="https://github.com/BerriAI/litellm/pull/9326" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>OpenAI-compatible models - fix issue when calling openai-compatible models w/ custom_llm_provider set - <a href="https://github.com/BerriAI/litellm/pull/9355" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>VertexAI - Embedding outputDimensionality support - <a href="https://github.com/BerriAI/litellm/commit/437dbe724620675295f298164a076cbd8019d304" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Anthropic - return consistent json response format on streaming/non-streaming - <a href="https://github.com/BerriAI/litellm/pull/9437" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-improvements">Spend Tracking Improvements<a href="#spend-tracking-improvements" class="hash-link" aria-label="Spend Tracking Improvements" title="Spend Tracking Improvements"></a></h2>
<ul>
<li><code>litellm_proxy/</code> - support reading litellm response cost header from proxy, when using client sdk</li>
<li>Reset Budget Job - fix budget reset error on keys/teams/users <a href="https://github.com/BerriAI/litellm/pull/9329" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Streaming - Prevents final chunk w/ usage from being ignored (impacted bedrock streaming + cost tracking) <a href="https://github.com/BerriAI/litellm/pull/9314" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ui">UI<a href="#ui" class="hash-link" aria-label="UI" title="UI"></a></h2>
<ol>
<li>Users Page<!-- -->
<ul>
<li>Feature: Control default internal user settings <a href="https://github.com/BerriAI/litellm/pull/9328" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li>Icons:<!-- -->
<ul>
<li>Feature: Replace external &quot;artificialanalysis.ai&quot; icons by local svg <a href="https://github.com/BerriAI/litellm/pull/9374" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
<li>Sign In/Sign Out<!-- -->
<ul>
<li>Fix: Default login when <code>default_user_id</code> user does not exist in DB <a href="https://github.com/BerriAI/litellm/pull/9395" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging-integrations">Logging Integrations<a href="#logging-integrations" class="hash-link" aria-label="Logging Integrations" title="Logging Integrations"></a></h2>
<ul>
<li>Support post-call guardrails for streaming responses <a href="/docs/proxy/guardrails/custom_guardrail#1-write-a-customguardrail-class">Get Started</a></li>
<li>Arize <a href="/docs/observability/arize_integration">Get Started</a>
<ul>
<li>fix invalid package import <a href="https://github.com/BerriAI/litellm/pull/9338" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>migrate to using standardloggingpayload for metadata, ensures spans land successfully <a href="https://github.com/BerriAI/litellm/pull/9338" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>fix logging to just log the LLM I/O <a href="https://github.com/BerriAI/litellm/pull/9353" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Dynamic API Key/Space param support <a href="/docs/observability/arize_integration#pass-arize-spacekey-per-request">Get Started</a></li>
</ul>
</li>
<li>StandardLoggingPayload - Log litellm_model_name in payload. Allows knowing what the model sent to API provider was <a href="/docs/proxy/logging_spec#standardlogginghiddenparams">Get Started</a></li>
<li>Prompt Management - Allow building custom prompt management integration <a href="/docs/proxy/custom_prompt_management.md">Get Started</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--reliability-improvements">Performance / Reliability improvements<a href="#performance--reliability-improvements" class="hash-link" aria-label="Performance / Reliability improvements" title="Performance / Reliability improvements"></a></h2>
<ul>
<li>Redis Caching - add 5s default timeout, prevents hanging redis connection from impacting llm calls <a href="https://github.com/BerriAI/litellm/commit/db92956ae33ed4c4e3233d7e1b0c7229817159bf" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Allow disabling all spend updates / writes to DB - patch to allow disabling all spend updates to DB with a flag <a href="https://github.com/BerriAI/litellm/pull/9331" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Azure OpenAI - correctly re-use azure openai client, fixes perf issue from previous Stable release <a href="https://github.com/BerriAI/litellm/commit/f2026ef907c06d94440930917add71314b901413" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Azure OpenAI - uses litellm.ssl_verify on Azure/OpenAI clients <a href="https://github.com/BerriAI/litellm/commit/f2026ef907c06d94440930917add71314b901413" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Usage-based routing - Wildcard model support <a href="/docs/proxy/usage_based_routing#wildcard-model-support">Get Started</a></li>
<li>Usage-based routing - Support batch writing increments to redis - reduces latency to same as simple-shuffle <a href="https://github.com/BerriAI/litellm/pull/9357" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Router - show reason for model cooldown on no healthy deployments available error <a href="https://github.com/BerriAI/litellm/pull/9438" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Caching - add max value limit to an item in in-memory cache (1MB) - prevents OOM errors on large image urls being sent through proxy <a href="https://github.com/BerriAI/litellm/pull/9448" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-improvements">General Improvements<a href="#general-improvements" class="hash-link" aria-label="General Improvements" title="General Improvements"></a></h2>
<ul>
<li>Passthrough Endpoints - support returning api-base on pass-through endpoints Response Headers <a href="/docs/proxy/response_headers#litellm-specific-headers">Docs</a></li>
<li>SSL - support reading ssl security level from env var - Allows user to specify lower security settings <a href="/docs/guides/security_settings">Get Started</a></li>
<li>Credentials - only poll Credentials table when <code>STORE_MODEL_IN_DB</code> is True <a href="https://github.com/BerriAI/litellm/pull/9376" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Image URL Handling - new architecture doc on image url handling <a href="/docs/proxy/image_handling">Docs</a></li>
<li>OpenAI - bump to pip install &quot;openai==1.68.2&quot; <a href="https://github.com/BerriAI/litellm/commit/e85e3bc52a9de86ad85c3dbb12d87664ee567a5a" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Gunicorn - security fix - bump gunicorn==23.0.0 <a href="https://github.com/BerriAI/litellm/commit/7e9fc92f5c7fea1e7294171cd3859d55384166eb" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="complete-git-diff">Complete Git Diff<a href="#complete-git-diff" class="hash-link" aria-label="Complete Git Diff" title="Complete Git Diff"></a></h2>
<p><a href="https://github.com/BerriAI/litellm/compare/v1.63.11-stable...v1.63.14.rc" target="_blank" rel="noopener noreferrer">Here&#x27;s the complete git diff</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/credential-management">credential management</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/thinking-content">thinking content</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/responses-api">responses api</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/snowflake">snowflake</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.63.11-stable">v1.63.11-stable</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-03-15T10:00:00.000Z">2025315</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://pbs.twimg.com/profile_images/1613813310264340481/lz54oEiB_400x400.jpg" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>These are the changes since <code>v1.63.2-stable</code>.</p>
<p>This release is primarily focused on:</p>
<ul>
<li>[Beta] Responses API Support</li>
<li>Snowflake Cortex Support, Amazon Nova Image Generation</li>
<li>UI - Credential Management, re-use credentials when adding new models</li>
<li>UI - Test Connection to LLM Provider before adding a model</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="known-issues">Known Issues<a href="#known-issues" class="hash-link" aria-label="Known Issues" title="Known Issues"></a></h2>
<ul>
<li> Known issue on Azure OpenAI - We don&#x27;t recommend upgrading if you use Azure OpenAI. This version failed our Azure OpenAI load test</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="docker-run-litellm-proxy">Docker Run LiteLLM Proxy<a href="#docker-run-litellm-proxy" class="hash-link" aria-label="Docker Run LiteLLM Proxy" title="Docker Run LiteLLM Proxy"></a></h2>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:main-v1.63.11-stable</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="demo-instance">Demo Instance<a href="#demo-instance" class="hash-link" aria-label="Demo Instance" title="Demo Instance"></a></h2>
<p>Here&#x27;s a Demo Instance to test changes:</p>
<ul>
<li>Instance: <a href="https://demo.litellm.ai/" target="_blank" rel="noopener noreferrer">https://demo.litellm.ai/</a></li>
<li>Login Credentials:<!-- -->
<ul>
<li>Username: admin</li>
<li>Password: sk-1234</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<ul>
<li>Image Generation support for Amazon Nova Canvas <a href="https://docs.litellm.ai/docs/providers/bedrock#image-generation" target="_blank" rel="noopener noreferrer">Getting Started</a></li>
<li>Add pricing for Jamba new models <a href="https://github.com/BerriAI/litellm/pull/9032/files" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add pricing for Amazon EU models <a href="https://github.com/BerriAI/litellm/pull/9056/files" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add Bedrock Deepseek R1 model pricing <a href="https://github.com/BerriAI/litellm/pull/9108/files" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Update Gemini pricing: Gemma 3, Flash 2 thinking update, LearnLM <a href="https://github.com/BerriAI/litellm/pull/9190/files" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Mark Cohere Embedding 3 models as Multimodal <a href="https://github.com/BerriAI/litellm/pull/9176/commits/c9a576ce4221fc6e50dc47cdf64ab62736c9da41" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add Azure Data Zone pricing <a href="https://github.com/BerriAI/litellm/pull/9185/files#diff-19ad91c53996e178c1921cbacadf6f3bae20cfe062bd03ee6bfffb72f847ee37" target="_blank" rel="noopener noreferrer">PR</a>
<ul>
<li>LiteLLM Tracks cost for <code>azure/eu</code> and <code>azure/us</code> models</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-translation">LLM Translation<a href="#llm-translation" class="hash-link" aria-label="LLM Translation" title="LLM Translation"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAn0lEQVR4nHWMvQqCUABG79toDSpCPxY0ZCiECtIQDQ1BzxkNZhE6O/oGOoioKJzw1toHh7N8HFGWJUVRSLqu499EmqZkWUae59zuEfEr4fFMvo7fRD/EWGyaRtb24QXVdNGsAH3po5oOiulIi77vGYaBqqoIDldW9hHLPTHfhOjrAM3y0BYeom1bxnNd17j+GUW3mc5cJsYO1bBR9K3kA0wFkAA8BpoGAAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/responses_api.8f881fa.640.png" srcset="/assets/ideal-img/responses_api.8f881fa.640.png 640w,/assets/ideal-img/responses_api.8fec0a9.1200.png 1200w" width="640" height="334"></noscript></div>
<ol>
<li><strong>New Endpoints</strong></li>
</ol>
<ul>
<li>[Beta] POST <code>/responses</code> API. <a href="https://docs.litellm.ai/docs/response_api" target="_blank" rel="noopener noreferrer">Getting Started</a></li>
</ul>
<ol start="2">
<li><strong>New LLM Providers</strong></li>
</ol>
<ul>
<li>Snowflake Cortex <a href="https://docs.litellm.ai/docs/providers/snowflake" target="_blank" rel="noopener noreferrer">Getting Started</a></li>
</ul>
<ol start="3">
<li><strong>New LLM Features</strong></li>
</ol>
<ul>
<li>Support OpenRouter <code>reasoning_content</code> on streaming <a href="https://docs.litellm.ai/docs/reasoning_content" target="_blank" rel="noopener noreferrer">Getting Started</a></li>
</ul>
<ol start="4">
<li><strong>Bug Fixes</strong></li>
</ol>
<ul>
<li>OpenAI: Return <code>code</code>, <code>param</code> and <code>type</code> on bad request error <a href="https://docs.litellm.ai/docs/exception_mapping" target="_blank" rel="noopener noreferrer">More information on litellm exceptions</a></li>
<li>Bedrock: Fix converse chunk parsing to only return empty dict on tool use <a href="https://github.com/BerriAI/litellm/pull/9166" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Bedrock: Support extra_headers <a href="https://github.com/BerriAI/litellm/pull/9113" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Azure: Fix Function Calling Bug &amp; Update Default API Version to <code>2025-02-01-preview</code> <a href="https://github.com/BerriAI/litellm/pull/9191" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Azure: Fix AI services URL <a href="https://github.com/BerriAI/litellm/pull/9185" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Vertex AI: Handle HTTP 201 status code in response <a href="https://github.com/BerriAI/litellm/pull/9193" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Perplexity: Fix incorrect streaming response <a href="https://github.com/BerriAI/litellm/pull/9081" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Triton: Fix streaming completions bug <a href="https://github.com/BerriAI/litellm/pull/8386" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Deepgram: Support bytes.IO when handling audio files for transcription <a href="https://github.com/BerriAI/litellm/pull/9071" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Ollama: Fix &quot;system&quot; role has become unacceptable <a href="https://github.com/BerriAI/litellm/pull/9261" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>All Providers (Streaming): Fix String <code>data:</code> stripped from entire content in streamed responses <a href="https://github.com/BerriAI/litellm/pull/9070" target="_blank" rel="noopener noreferrer">PR</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-improvements">Spend Tracking Improvements<a href="#spend-tracking-improvements" class="hash-link" aria-label="Spend Tracking Improvements" title="Spend Tracking Improvements"></a></h2>
<ol>
<li>Support Bedrock converse cache token tracking <a href="https://docs.litellm.ai/docs/completion/prompt_caching" target="_blank" rel="noopener noreferrer">Getting Started</a></li>
<li>Cost Tracking for Responses API <a href="https://docs.litellm.ai/docs/response_api" target="_blank" rel="noopener noreferrer">Getting Started</a></li>
<li>Fix Azure Whisper cost tracking <a href="https://docs.litellm.ai/docs/audio_transcription" target="_blank" rel="noopener noreferrer">Getting Started</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ui">UI<a href="#ui" class="hash-link" aria-label="UI" title="UI"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="re-use-credentials-on-ui">Re-Use Credentials on UI<a href="#re-use-credentials-on-ui" class="hash-link" aria-label="Re-Use Credentials on UI" title="Re-Use Credentials on UI"></a></h3>
<p>You can now onboard LLM provider credentials on LiteLLM UI. Once these credentials are added you can re-use them when adding new models <a href="https://docs.litellm.ai/docs/proxy/ui_credentials" target="_blank" rel="noopener noreferrer">Getting Started</a></p>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAFAAoDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAII/8QAGhAAAwADAQAAAAAAAAAAAAAAAAECAxETYf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIRAxEAPwDUGGeaUuqv2ntlNgAf/9k=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/credentials.eaf34a1.640.jpg" srcset="/assets/ideal-img/credentials.eaf34a1.640.jpg 640w,/assets/ideal-img/credentials.bfa783a.1920.jpg 1920w" width="640" height="334"></noscript></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="test-connections-before-adding-models">Test Connections before adding models<a href="#test-connections-before-adding-models" class="hash-link" aria-label="Test Connections before adding models" title="Test Connections before adding models"></a></h3>
<p>Before adding a model you can test the connection to the LLM provider to verify you have setup your API Base + API Key correctly</p>
<img src="/assets/images/litellm_test_connection-029765a2de4dcabccfe3be9a8d33dbdd.gif">
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="general-ui-improvements">General UI Improvements<a href="#general-ui-improvements" class="hash-link" aria-label="General UI Improvements" title="General UI Improvements"></a></h3>
<ol>
<li>Add Models Page<!-- -->
<ul>
<li>Allow adding Cerebras, Sambanova, Perplexity, Fireworks, Openrouter, TogetherAI Models, Text-Completion OpenAI on Admin UI</li>
<li>Allow adding EU OpenAI models</li>
<li>Fix: Instantly show edit + deletes to models</li>
</ul>
</li>
<li>Keys Page<!-- -->
<ul>
<li>Fix: Instantly show newly created keys on Admin UI (don&#x27;t require refresh)</li>
<li>Fix: Allow clicking into Top Keys when showing users Top API Key</li>
<li>Fix: Allow Filter Keys by Team Alias, Key Alias and Org</li>
<li>UI Improvements: Show 100 Keys Per Page, Use full height, increase width of key alias</li>
</ul>
</li>
<li>Users Page<!-- -->
<ul>
<li>Fix: Show correct count of internal user keys on Users Page</li>
<li>Fix: Metadata not updating in Team UI</li>
</ul>
</li>
<li>Logs Page<!-- -->
<ul>
<li>UI Improvements: Keep expanded log in focus on LiteLLM UI</li>
<li>UI Improvements: Minor improvements to logs page</li>
<li>Fix: Allow internal user to query their own logs</li>
<li>Allow switching off storing Error Logs in DB <a href="https://docs.litellm.ai/docs/proxy/ui_logs" target="_blank" rel="noopener noreferrer">Getting Started</a></li>
</ul>
</li>
<li>Sign In/Sign Out<!-- -->
<ul>
<li>Fix: Correctly use <code>PROXY_LOGOUT_URL</code> when set <a href="https://docs.litellm.ai/docs/proxy/self_serve#setting-custom-logout-urls" target="_blank" rel="noopener noreferrer">Getting Started</a></li>
</ul>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="security">Security<a href="#security" class="hash-link" aria-label="Security" title="Security"></a></h2>
<ol>
<li>Support for Rotating Master Keys <a href="https://docs.litellm.ai/docs/proxy/master_key_rotations" target="_blank" rel="noopener noreferrer">Getting Started</a></li>
<li>Fix: Internal User Viewer Permissions, don&#x27;t allow <code>internal_user_viewer</code> role to see <code>Test Key Page</code> or <code>Create Key Button</code> <a href="https://docs.litellm.ai/docs/proxy/access_control" target="_blank" rel="noopener noreferrer">More information on role based access controls</a></li>
<li>Emit audit logs on All user + model Create/Update/Delete endpoints <a href="https://docs.litellm.ai/docs/proxy/multiple_admins" target="_blank" rel="noopener noreferrer">Getting Started</a></li>
<li>JWT<!-- -->
<ul>
<li>Support multiple JWT OIDC providers <a href="https://docs.litellm.ai/docs/proxy/token_auth" target="_blank" rel="noopener noreferrer">Getting Started</a></li>
<li>Fix JWT access with Groups not working when team is assigned All Proxy Models access</li>
</ul>
</li>
<li>Using K/V pairs in 1 AWS Secret <a href="https://docs.litellm.ai/docs/secret#using-kv-pairs-in-1-aws-secret" target="_blank" rel="noopener noreferrer">Getting Started</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging-integrations">Logging Integrations<a href="#logging-integrations" class="hash-link" aria-label="Logging Integrations" title="Logging Integrations"></a></h2>
<ol>
<li>Prometheus: Track Azure LLM API latency metric <a href="https://docs.litellm.ai/docs/proxy/prometheus#request-latency-metrics" target="_blank" rel="noopener noreferrer">Getting Started</a></li>
<li>Athina: Added tags, user_feedback and model_options to additional_keys which can be sent to Athina <a href="https://docs.litellm.ai/docs/observability/athina_integration" target="_blank" rel="noopener noreferrer">Getting Started</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--reliability-improvements">Performance / Reliability improvements<a href="#performance--reliability-improvements" class="hash-link" aria-label="Performance / Reliability improvements" title="Performance / Reliability improvements"></a></h2>
<ol>
<li>Redis + litellm router - Fix Redis cluster mode for litellm router <a href="https://github.com/BerriAI/litellm/pull/9010" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-improvements">General Improvements<a href="#general-improvements" class="hash-link" aria-label="General Improvements" title="General Improvements"></a></h2>
<ol>
<li>OpenWebUI Integration - display <code>thinking</code> tokens</li>
</ol>
<ul>
<li>Guide on getting started with LiteLLM x OpenWebUI. <a href="https://docs.litellm.ai/docs/tutorials/openweb_ui" target="_blank" rel="noopener noreferrer">Getting Started</a></li>
<li>Display <code>thinking</code> tokens on OpenWebUI (Bedrock, Anthropic, Deepseek) <a href="https://docs.litellm.ai/docs/tutorials/openweb_ui#render-thinking-content-on-openweb-ui" target="_blank" rel="noopener noreferrer">Getting Started</a></li>
</ul>
<img src="/assets/images/litellm_thinking_openweb-5ec7dddb7e7b6a10252694c27cfc177d.gif">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="complete-git-diff">Complete Git Diff<a href="#complete-git-diff" class="hash-link" aria-label="Complete Git Diff" title="Complete Git Diff"></a></h2>
<p><a href="https://github.com/BerriAI/litellm/compare/v1.63.2-stable...v1.63.11-stable" target="_blank" rel="noopener noreferrer">Here&#x27;s the complete git diff</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/credential-management">credential management</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/thinking-content">thinking content</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/responses-api">responses api</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/snowflake">snowflake</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.63.2-stable">v1.63.2-stable</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-03-08T10:00:00.000Z">202538</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&amp;v=beta&amp;t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>These are the changes since <code>v1.61.20-stable</code>.</p>
<p>This release is primarily focused on:</p>
<ul>
<li>LLM Translation improvements (more <code>thinking</code> content improvements)</li>
<li>UI improvements (Error logs now shown on UI)</li>
</ul>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span></div><div class="admonitionContent_BuS1"><p>This release will be live on 03/09/2025</p></div></div>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAFAAoDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAMI/8QAGRAAAwEBAQAAAAAAAAAAAAAAAAECMQMR/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/EABQRAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhEDEQA/ANPXNPrDVtStnzSoAH//2Q==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/v1632_release.c99eac9.640.jpg" srcset="/assets/ideal-img/v1632_release.c99eac9.640.jpg 640w,/assets/ideal-img/v1632_release.215df28.1920.jpg 1920w" width="640" height="334"></noscript></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="demo-instance">Demo Instance<a href="#demo-instance" class="hash-link" aria-label="Demo Instance" title="Demo Instance"></a></h2>
<p>Here&#x27;s a Demo Instance to test changes:</p>
<ul>
<li>Instance: <a href="https://demo.litellm.ai/" target="_blank" rel="noopener noreferrer">https://demo.litellm.ai/</a></li>
<li>Login Credentials:<!-- -->
<ul>
<li>Username: admin</li>
<li>Password: sk-1234</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<ol>
<li>Add<code>supports_pdf_input</code>for specific Bedrock Claude models<a href="https://github.com/BerriAI/litellm/commit/f63cf0030679fe1a43d03fb196e815a0f28dae92" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add pricing for amazon <code>eu</code> models <a href="https://github.com/BerriAI/litellm/commits/main/model_prices_and_context_window.json" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Fix Azure O1 mini pricing <a href="https://github.com/BerriAI/litellm/commit/52de1949ef2f76b8572df751f9c868a016d4832c" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-translation">LLM Translation<a href="#llm-translation" class="hash-link" aria-label="LLM Translation" title="LLM Translation"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAFAAoDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAQI/8QAGxABAAICAwAAAAAAAAAAAAAAAQACESEDIrH/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/EABQRAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhEDEQA/ANOUocAVoqbeyvspwO8REK//2Q==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/anthropic_thinking.5fdc16d.640.jpg" srcset="/assets/ideal-img/anthropic_thinking.5fdc16d.640.jpg 640w,/assets/ideal-img/anthropic_thinking.faa0fac.1920.jpg 1920w" width="640" height="334"></noscript></div>
<ol>
<li>Support <code>/openai/</code> passthrough for Assistant endpoints. <a href="https://docs.litellm.ai/docs/pass_through/openai_passthrough" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>Bedrock Claude - fix tool calling transformation on invoke route. <a href="/docs/providers/bedrock#usage---function-calling--tool-calling">Get Started</a></li>
<li>Bedrock Claude - response_format support for claude on invoke route. <a href="/docs/providers/bedrock#usage---structured-output--json-mode">Get Started</a></li>
<li>Bedrock - pass <code>description</code> if set in response_format. <a href="/docs/providers/bedrock#usage---structured-output--json-mode">Get Started</a></li>
<li>Bedrock - Fix passingresponse_format: <code>{&quot;type&quot;: &quot;text&quot;}</code>. <a href="https://github.com/BerriAI/litellm/commit/c84b489d5897755139aa7d4e9e54727ebe0fa540" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>OpenAI - Handle sendingimage_urlas str to openai. <a href="https://docs.litellm.ai/docs/completion/vision" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>Deepseek - return &#x27;reasoning_content&#x27; missing on streaming. <a href="https://docs.litellm.ai/docs/reasoning_content" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>Caching - Support caching on reasoning content. <a href="https://docs.litellm.ai/docs/proxy/caching" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>Bedrock - handle thinking blocks in assistant message. <a href="https://docs.litellm.ai/docs/providers/bedrock#usage---thinking--reasoning-content" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>Anthropic - Return <code>signature</code> on streaming. <a href="https://docs.litellm.ai/docs/providers/bedrock#usage---thinking--reasoning-content" target="_blank" rel="noopener noreferrer">Get Started</a></li>
</ol>
<ul>
<li>Note: We&#x27;ve also migrated from <code>signature_delta</code> to <code>signature</code>. <a href="https://docs.litellm.ai/release_notes/v1.63.0" target="_blank" rel="noopener noreferrer">Read more</a></li>
</ul>
<ol start="11">
<li>Supportformatparam for specifying image type. <a href="/docs/completion/vision.md#explicitly-specify-image-type">Get Started</a></li>
<li>Anthropic - <code>/v1/messages</code> endpoint - <code>thinking</code> param support. <a href="/docs/anthropic_unified.md">Get Started</a></li>
</ol>
<ul>
<li>Note: this refactors the [BETA] unified <code>/v1/messages</code> endpoint, to just work for the Anthropic API.</li>
</ul>
<ol start="13">
<li>Vertex AI - handle $id in response schema when calling vertex ai. <a href="https://docs.litellm.ai/docs/providers/vertex#json-schema" target="_blank" rel="noopener noreferrer">Get Started</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-improvements">Spend Tracking Improvements<a href="#spend-tracking-improvements" class="hash-link" aria-label="Spend Tracking Improvements" title="Spend Tracking Improvements"></a></h2>
<ol>
<li>Batches API - Fix cost calculation to run on retrieve_batch. <a href="https://docs.litellm.ai/docs/batches" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>Batches API - Log batch models in spend logs / standard logging payload. <a href="/docs/proxy/logging_spec.md#standardlogginghiddenparams">Get Started</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAFAAoDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAII/8QAHRAAAgEEAwAAAAAAAAAAAAAAABEBAgMTISJhof/EABQBAQAAAAAAAAAAAAAAAAAAAAL/xAAWEQEBAQAAAAAAAAAAAAAAAAABAAL/2gAMAwEAAhEDEQA/ANPSrNPGNNJlZOvQB5BIrf/Z&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="334"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/error_logs.b0c8af2.640.jpg" srcset="/assets/ideal-img/error_logs.b0c8af2.640.jpg 640w,/assets/ideal-img/error_logs.a6cad71.1920.jpg 1920w" width="640" height="334"></noscript></div>
<ol>
<li>Virtual Keys Page<!-- -->
<ul>
<li>Allow team/org filters to be searchable on the Create Key Page</li>
<li>Addcreated_byandupdated_byfields to Keys table</li>
<li>Show &#x27;user_email&#x27; on key table</li>
<li>Show 100 Keys Per Page, Use full height, increase width of key alias</li>
</ul>
</li>
<li>Logs Page<!-- -->
<ul>
<li>Show Error Logs on LiteLLM UI</li>
<li>Allow Internal Users to View their own logs</li>
</ul>
</li>
<li>Internal Users Page<!-- -->
<ul>
<li>Allow admin to control default model access for internal users</li>
</ul>
</li>
<li>Fix session handling with cookies</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<ol>
<li>Fix prometheus metrics w/ custom metrics, when keys containing team_id make requests. <a href="https://github.com/BerriAI/litellm/pull/8935" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<ol>
<li>Cooldowns - Support cooldowns on models called with client side credentials. <a href="https://docs.litellm.ai/docs/proxy/clientside_auth#pass-user-llm-api-keys--api-base" target="_blank" rel="noopener noreferrer">Get Started</a></li>
<li>Tag-based Routing - ensures tag-based routing across all endpoints (<code>/embeddings</code>, <code>/image_generation</code>, etc.). <a href="https://docs.litellm.ai/docs/proxy/tag_routing" target="_blank" rel="noopener noreferrer">Get Started</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<ol>
<li>RaiseBadRequestErrorwhen unknown model passed in request</li>
<li>Enforce model access restrictions on Azure OpenAI proxy route</li>
<li>Reliability fix - Handle emojis in text - fix orjson error</li>
<li>Model Access Patch - don&#x27;t overwrite litellm.anthropic_models when running auth checks</li>
<li>Enable setting timezone information in docker image</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="complete-git-diff">Complete Git Diff<a href="#complete-git-diff" class="hash-link" aria-label="Complete Git Diff" title="Complete Git Diff"></a></h2>
<p><a href="https://github.com/BerriAI/litellm/compare/v1.61.20-stable...v1.63.2-stable" target="_blank" rel="noopener noreferrer">Here&#x27;s the complete git diff</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/llm-translation">llm translation</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/thinking">thinking</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/reasoning-content">reasoning_content</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/claude-3-7-sonnet">claude-3-7-sonnet</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.63.0">v1.63.0 - Anthropic &#x27;thinking&#x27; response update</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-03-05T10:00:00.000Z">202535</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&amp;v=beta&amp;t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>v1.63.0 fixes Anthropic &#x27;thinking&#x27; response on streaming to return the <code>signature</code> block. <a href="https://github.com/BerriAI/litellm/issues/8964" target="_blank" rel="noopener noreferrer">Github Issue</a></p>
<p>It also moves the response structure from <code>signature_delta</code> to <code>signature</code> to be the same as Anthropic. <a href="https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#implementing-extended-thinking" target="_blank" rel="noopener noreferrer">Anthropic Docs</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="diff">Diff<a href="#diff" class="hash-link" aria-label="Diff" title="Diff"></a></h2>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">&quot;message&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;reasoning_content&quot;: &quot;The capital of France is Paris.&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;thinking_blocks&quot;: [</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &quot;type&quot;: &quot;thinking&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &quot;thinking&quot;: &quot;The capital of France is Paris.&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-            &quot;signature_delta&quot;: &quot;EqoBCkgIARABGAIiQL2UoU0b1OHYi+...&quot; #  OLD FORMAT</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+            &quot;signature&quot;: &quot;EqoBCkgIARABGAIiQL2UoU0b1OHYi+...&quot; #  KEY CHANGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre></div></div></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/llm-translation">llm translation</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/thinking">thinking</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/reasoning-content">reasoning_content</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/claude-3-7-sonnet">claude-3-7-sonnet</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.61.20-stable">v1.61.20-stable</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-03-01T10:00:00.000Z">202531</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&amp;v=beta&amp;t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>These are the changes since <code>v1.61.13-stable</code>.</p>
<p>This release is primarily focused on:</p>
<ul>
<li>LLM Translation improvements (claude-3-7-sonnet + &#x27;thinking&#x27;/&#x27;reasoning_content&#x27; support)</li>
<li>UI improvements (add model flow, user management, etc)</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="demo-instance">Demo Instance<a href="#demo-instance" class="hash-link" aria-label="Demo Instance" title="Demo Instance"></a></h2>
<p>Here&#x27;s a Demo Instance to test changes:</p>
<ul>
<li>Instance: <a href="https://demo.litellm.ai/" target="_blank" rel="noopener noreferrer">https://demo.litellm.ai/</a></li>
<li>Login Credentials:<!-- -->
<ul>
<li>Username: admin</li>
<li>Password: sk-1234</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<ol>
<li>Anthropic 3-7 sonnet support + cost tracking (Anthropic API + Bedrock + Vertex AI + OpenRouter)<!-- -->
<ol>
<li>Anthropic API <a href="https://docs.litellm.ai/docs/providers/anthropic#usage---thinking--reasoning_content" target="_blank" rel="noopener noreferrer">Start here</a></li>
<li>Bedrock API <a href="https://docs.litellm.ai/docs/providers/bedrock#usage---thinking--reasoning-content" target="_blank" rel="noopener noreferrer">Start here</a></li>
<li>Vertex AI API <a href="/docs/providers/vertex#usage---thinking--reasoning_content">See here</a></li>
<li>OpenRouter <a href="https://github.com/BerriAI/litellm/blob/ba5bdce50a0b9bc822de58c03940354f19a733ed/model_prices_and_context_window.json#L5626" target="_blank" rel="noopener noreferrer">See here</a></li>
</ol>
</li>
<li>Gpt-4.5-preview support + cost tracking <a href="https://github.com/BerriAI/litellm/blob/ba5bdce50a0b9bc822de58c03940354f19a733ed/model_prices_and_context_window.json#L79" target="_blank" rel="noopener noreferrer">See here</a></li>
<li>Azure AI - Phi-4 cost tracking <a href="https://github.com/BerriAI/litellm/blob/ba5bdce50a0b9bc822de58c03940354f19a733ed/model_prices_and_context_window.json#L1773" target="_blank" rel="noopener noreferrer">See here</a></li>
<li>Claude-3.5-sonnet - vision support updated on Anthropic API <a href="https://github.com/BerriAI/litellm/blob/ba5bdce50a0b9bc822de58c03940354f19a733ed/model_prices_and_context_window.json#L2888" target="_blank" rel="noopener noreferrer">See here</a></li>
<li>Bedrock llama vision support <a href="https://github.com/BerriAI/litellm/blob/ba5bdce50a0b9bc822de58c03940354f19a733ed/model_prices_and_context_window.json#L7714" target="_blank" rel="noopener noreferrer">See here</a></li>
<li>Cerebras llama3.3-70b pricing <a href="https://github.com/BerriAI/litellm/blob/ba5bdce50a0b9bc822de58c03940354f19a733ed/model_prices_and_context_window.json#L2697" target="_blank" rel="noopener noreferrer">See here</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-translation">LLM Translation<a href="#llm-translation" class="hash-link" aria-label="LLM Translation" title="LLM Translation"></a></h2>
<ol>
<li>Infinity Rerank - support returning documents when return_documents=True <a href="/docs/providers/infinity#usage---returning-documents">Start here</a></li>
<li>Amazon Deepseek - <code>&lt;think&gt;</code> param extraction into reasoning_content <a href="https://docs.litellm.ai/docs/providers/bedrock#bedrock-imported-models-deepseek-deepseek-r1" target="_blank" rel="noopener noreferrer">Start here</a></li>
<li>Amazon Titan Embeddings - filter out aws_ params from request body <a href="https://docs.litellm.ai/docs/providers/bedrock#bedrock-embedding" target="_blank" rel="noopener noreferrer">Start here</a></li>
<li>Anthropic thinking + reasoning_content translation support (Anthropic API, Bedrock, Vertex AI)  <a href="https://docs.litellm.ai/docs/reasoning_content" target="_blank" rel="noopener noreferrer">Start here</a></li>
<li>VLLM - support video_url <a href="/docs/providers/vllm#send-video-url-to-vllm">Start here</a></li>
<li>Call proxy via litellm SDK: Support <code>litellm_proxy/</code> for embedding, image_generation, transcription, speech, rerank <a href="https://docs.litellm.ai/docs/providers/litellm_proxy" target="_blank" rel="noopener noreferrer">Start here</a></li>
<li>OpenAI Pass-through - allow using Assistants GET, DELETE on /openai pass through routes <a href="https://docs.litellm.ai/docs/pass_through/openai_passthrough" target="_blank" rel="noopener noreferrer">Start here</a></li>
<li>Message Translation - fix openai message for assistant msg if role is missing - openai allows this</li>
<li>O1/O3 - support drop_params for o3-mini and o1 parallel_tool_calls param (not supported currently) <a href="https://docs.litellm.ai/docs/completion/drop_params" target="_blank" rel="noopener noreferrer">See here</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-improvements">Spend Tracking Improvements<a href="#spend-tracking-improvements" class="hash-link" aria-label="Spend Tracking Improvements" title="Spend Tracking Improvements"></a></h2>
<ol>
<li>Cost tracking for rerank via Bedrock <a href="https://github.com/BerriAI/litellm/commit/b682dc4ec8fd07acf2f4c981d2721e36ae2a49c5" target="_blank" rel="noopener noreferrer">See PR</a></li>
<li>Anthropic pass-through - fix race condition causing cost to not be tracked <a href="https://github.com/BerriAI/litellm/pull/8874" target="_blank" rel="noopener noreferrer">See PR</a></li>
<li>Anthropic pass-through: Ensure accurate token counting <a href="https://github.com/BerriAI/litellm/pull/8880" target="_blank" rel="noopener noreferrer">See PR</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<ol>
<li>Models Page - Allow sorting models by created at</li>
<li>Models Page - Edit Model Flow Improvements</li>
<li>Models Page - Fix Adding Azure, Azure AI Studio models on UI</li>
<li>Internal Users Page - Allow Bulk Adding Internal Users on UI</li>
<li>Internal Users Page - Allow sorting users by created at</li>
<li>Virtual Keys Page - Allow searching for UserIDs on the dropdown when assigning a user to a team <a href="https://github.com/BerriAI/litellm/pull/8844" target="_blank" rel="noopener noreferrer">See PR</a></li>
<li>Virtual Keys Page - allow creating a user when assigning keys to users <a href="https://github.com/BerriAI/litellm/pull/8844" target="_blank" rel="noopener noreferrer">See PR</a></li>
<li>Model Hub Page  - fix text overflow issue <a href="https://github.com/BerriAI/litellm/pull/8749" target="_blank" rel="noopener noreferrer">See PR</a></li>
<li>Admin Settings Page - Allow adding MSFT SSO on UI</li>
<li>Backend - don&#x27;t allow creating duplicate internal users in DB</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="helm">Helm<a href="#helm" class="hash-link" aria-label="Helm" title="Helm"></a></h2>
<ol>
<li>support ttlSecondsAfterFinished on the migration job - <a href="https://github.com/BerriAI/litellm/pull/8593" target="_blank" rel="noopener noreferrer">See PR</a></li>
<li>enhance migrations job with additional configurable properties - <a href="https://github.com/BerriAI/litellm/pull/8636" target="_blank" rel="noopener noreferrer">See PR</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<ol>
<li>Arize Phoenix support</li>
<li>No-log - fix no-log param support on embedding calls</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--loadbalancing--reliability-improvements">Performance / Loadbalancing / Reliability improvements<a href="#performance--loadbalancing--reliability-improvements" class="hash-link" aria-label="Performance / Loadbalancing / Reliability improvements" title="Performance / Loadbalancing / Reliability improvements"></a></h2>
<ol>
<li>Single Deployment Cooldown logic - Use allowed_fails or allowed_fail_policy if set <a href="https://docs.litellm.ai/docs/routing#advanced-custom-retries-cooldowns-based-on-error-type" target="_blank" rel="noopener noreferrer">Start here</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<ol>
<li>Hypercorn - fix reading / parsing request body</li>
<li>Windows - fix running proxy in windows</li>
<li>DD-Trace - fix dd-trace enablement on proxy</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="complete-git-diff">Complete Git Diff<a href="#complete-git-diff" class="hash-link" aria-label="Complete Git Diff" title="Complete Git Diff"></a></h2>
<p>View the complete git diff <a href="https://github.com/BerriAI/litellm/compare/v1.61.13-stable...v1.61.20-stable" target="_blank" rel="noopener noreferrer">here</a>.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/llm-translation">llm translation</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/rerank">rerank</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/ui">ui</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/thinking">thinking</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/reasoning-content">reasoning_content</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/claude-3-7-sonnet">claude-3-7-sonnet</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.59.8-stable">v1.59.8-stable</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-01-31T10:00:00.000Z">2025131</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&amp;v=beta&amp;t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span></div><div class="admonitionContent_BuS1"><p>Get a 7 day free trial for LiteLLM Enterprise <a href="https://litellm.ai/#trial" target="_blank" rel="noopener noreferrer">here</a>.</p><p><strong>no call needed</strong></p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models--updated-models">New Models / Updated Models<a href="#new-models--updated-models" class="hash-link" aria-label="New Models / Updated Models" title="New Models / Updated Models"></a></h2>
<ol>
<li>New OpenAI <code>/image/variations</code> endpoint BETA support <a href="/docs/image_variations">Docs</a></li>
<li>Topaz API support on OpenAI <code>/image/variations</code> BETA endpoint <a href="/docs/providers/topaz">Docs</a></li>
<li>Deepseek - r1 support w/ reasoning_content (<a href="/docs/providers/deepseek#reasoning-models">Deepseek API</a>, <a href="/docs/providers/vertex#model-garden">Vertex AI</a>, <a href="/docs/providers/bedrock#deepseek">Bedrock</a>)</li>
<li>Azure - Add azure o1 pricing <a href="https://github.com/BerriAI/litellm/blob/b8b927f23bc336862dacb89f59c784a8d62aaa15/model_prices_and_context_window.json#L952" target="_blank" rel="noopener noreferrer">See Here</a></li>
<li>Anthropic - handle <code>-latest</code> tag in model for cost calculation</li>
<li>Gemini-2.0-flash-thinking - add model pricing (its 0.0) <a href="https://github.com/BerriAI/litellm/blob/b8b927f23bc336862dacb89f59c784a8d62aaa15/model_prices_and_context_window.json#L3393" target="_blank" rel="noopener noreferrer">See Here</a></li>
<li>Bedrock - add stability sd3 model pricing <a href="https://github.com/BerriAI/litellm/blob/b8b927f23bc336862dacb89f59c784a8d62aaa15/model_prices_and_context_window.json#L6814" target="_blank" rel="noopener noreferrer">See Here</a>  (s/o <a href="https://github.com/marty-sullivan" target="_blank" rel="noopener noreferrer">Marty Sullivan</a>)</li>
<li>Bedrock - add us.amazon.nova-lite-v1:0 to model cost map <a href="https://github.com/BerriAI/litellm/blob/b8b927f23bc336862dacb89f59c784a8d62aaa15/model_prices_and_context_window.json#L5619" target="_blank" rel="noopener noreferrer">See Here</a></li>
<li>TogetherAI - add new together_ai llama3.3 models <a href="https://github.com/BerriAI/litellm/blob/b8b927f23bc336862dacb89f59c784a8d62aaa15/model_prices_and_context_window.json#L6985" target="_blank" rel="noopener noreferrer">See Here</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-translation">LLM Translation<a href="#llm-translation" class="hash-link" aria-label="LLM Translation" title="LLM Translation"></a></h2>
<ol>
<li>LM Studio -&gt; fix async embedding call</li>
<li>Gpt 4o models - fix response_format translation</li>
<li>Bedrock nova - expand supported document types to include .md, .csv, etc. <a href="/docs/providers/bedrock#usage---pdf--document-understanding">Start Here</a></li>
<li>Bedrock - docs on IAM role based access for bedrock - <a href="https://docs.litellm.ai/docs/providers/bedrock#sts-role-based-auth" target="_blank" rel="noopener noreferrer">Start Here</a></li>
<li>Bedrock - cache IAM role credentials when used</li>
<li>Google AI Studio (<code>gemini/</code>) - support gemini &#x27;frequency_penalty&#x27; and &#x27;presence_penalty&#x27;</li>
<li>Azure O1 - fix model name check</li>
<li>WatsonX - ZenAPIKey support for WatsonX <a href="/docs/providers/watsonx">Docs</a></li>
<li>Ollama Chat - support json schema response format <a href="/docs/providers/ollama#json-schema-support">Start Here</a></li>
<li>Bedrock - return correct bedrock status code and error message if error during streaming</li>
<li>Anthropic - Supported nested json schema on anthropic calls</li>
<li>OpenAI - <code>metadata</code> param preview support<!-- -->
<ol>
<li>SDK - enable via <code>litellm.enable_preview_features = True</code></li>
<li>PROXY - enable via <code>litellm_settings::enable_preview_features: true</code></li>
</ol>
</li>
<li>Replicate - retry completion response on status=processing</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spend-tracking-improvements">Spend Tracking Improvements<a href="#spend-tracking-improvements" class="hash-link" aria-label="Spend Tracking Improvements" title="Spend Tracking Improvements"></a></h2>
<ol>
<li>Bedrock - QA asserts all bedrock regional models have same <code>supported_</code> as base model</li>
<li>Bedrock - fix bedrock converse cost tracking w/ region name specified</li>
<li>Spend Logs reliability fix - when <code>user</code> passed in request body is int instead of string</li>
<li>Ensure base_model cost tracking works across all endpoints</li>
<li>Fixes for Image generation cost tracking</li>
<li>Anthropic - fix anthropic end user cost tracking</li>
<li>JWT / OIDC Auth - add end user id tracking from jwt auth</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui">Management Endpoints / UI<a href="#management-endpoints--ui" class="hash-link" aria-label="Management Endpoints / UI" title="Management Endpoints / UI"></a></h2>
<ol>
<li>allows team member to become admin post-add (ui + endpoints)</li>
<li>New edit/delete button for updating team membership on UI</li>
<li>If team admin - show all team keys</li>
<li>Model Hub - clarify cost of models is per 1m tokens</li>
<li>Invitation Links - fix invalid url generated</li>
<li>New - SpendLogs Table Viewer - allows proxy admin to view spend logs on UI<!-- -->
<ol>
<li>New spend logs - allow proxy admin to opt in to logging request/response in spend logs table - enables easier abuse detection</li>
<li>Show country of origin in spend logs</li>
<li>Add pagination + filtering by key name/team name</li>
</ol>
</li>
<li><code>/key/delete</code> - allow team admin to delete team keys</li>
<li>Internal User view - fix spend calculation when team selected</li>
<li>Model Analytics is now on Free</li>
<li>Usage page - shows days when spend = 0, and round spend on charts to 2 sig figs</li>
<li>Public Teams - allow admins to expose teams for new users to join on UI - <a href="https://docs.litellm.ai/docs/proxy/public_teams" target="_blank" rel="noopener noreferrer">Start Here</a></li>
<li>Guardrails<!-- -->
<ol>
<li>set/edit guardrails on a virtual key</li>
<li>Allow setting guardrails on a team</li>
<li>Set guardrails on team create + edit page</li>
</ol>
</li>
<li>Support temporary budget increases on <code>/key/update</code> - new <code>temp_budget_increase</code> and <code>temp_budget_expiry</code> fields - <a href="/docs/proxy/virtual_keys#temporary-budget-increase">Start Here</a></li>
<li>Support writing new key alias to AWS Secret Manager - on key rotation <a href="/docs/secret#aws-secret-manager">Start Here</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="helm">Helm<a href="#helm" class="hash-link" aria-label="Helm" title="Helm"></a></h2>
<ol>
<li>add securityContext and pull policy values to migration job (s/o <a href="https://github.com/Hexoplon" target="_blank" rel="noopener noreferrer">https://github.com/Hexoplon</a>)</li>
<li>allow specifying envVars on values.yaml</li>
<li>new helm lint test</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging--guardrail-integrations">Logging / Guardrail Integrations<a href="#logging--guardrail-integrations" class="hash-link" aria-label="Logging / Guardrail Integrations" title="Logging / Guardrail Integrations"></a></h2>
<ol>
<li>Log the used prompt when prompt management used. <a href="/docs/proxy/prompt_management">Start Here</a></li>
<li>Support s3 logging with team alias prefixes - <a href="https://docs.litellm.ai/docs/proxy/logging#team-alias-prefix-in-object-key" target="_blank" rel="noopener noreferrer">Start Here</a></li>
<li>Prometheus <a href="/docs/proxy/prometheus">Start Here</a>
<ol>
<li>fix litellm_llm_api_time_to_first_token_metric not populating for bedrock models</li>
<li>emit remaining team budget metric on regular basis (even when call isnt made) - allows for more stable metrics on Grafana/etc.</li>
<li>add key and team level budget metrics</li>
<li>emit <code>litellm_overhead_latency_metric</code></li>
<li>Emit <code>litellm_team_budget_reset_at_metric</code> and <code>litellm_api_key_budget_remaining_hours_metric</code></li>
</ol>
</li>
<li>Datadog - support logging spend tags to Datadog. <a href="/docs/proxy/enterprise#tracking-spend-for-custom-tags">Start Here</a></li>
<li>Langfuse - fix logging request tags, read from standard logging payload</li>
<li>GCS - dont truncate payload on logging</li>
<li>New GCS Pub/Sub logging support <a href="https://docs.litellm.ai/docs/proxy/logging#google-cloud-storage---pubsub-topic" target="_blank" rel="noopener noreferrer">Start Here</a></li>
<li>Add AIM Guardrails support <a href="/docs/proxy/guardrails/aim_security">Start Here</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="security">Security<a href="#security" class="hash-link" aria-label="Security" title="Security"></a></h2>
<ol>
<li>New Enterprise SLA for patching security vulnerabilities. <a href="/docs/enterprise#slas--professional-support">See Here</a></li>
<li>Hashicorp - support using vault namespace for TLS auth. <a href="/docs/secret#hashicorp-vault">Start Here</a></li>
<li>Azure - DefaultAzureCredential support</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="health-checks">Health Checks<a href="#health-checks" class="hash-link" aria-label="Health Checks" title="Health Checks"></a></h2>
<ol>
<li>Cleanup pricing-only model names from wildcard route list - prevent bad health checks</li>
<li>Allow specifying a health check model for wildcard routes - <a href="https://docs.litellm.ai/docs/proxy/health#wildcard-routes" target="_blank" rel="noopener noreferrer">https://docs.litellm.ai/docs/proxy/health#wildcard-routes</a></li>
<li>New health_check_timeout  param with default 1min upperbound to prevent bad model from health check to hang and cause pod restarts. <a href="/docs/proxy/health#health-check-timeout">Start Here</a></li>
<li>Datadog - add data dog service health check + expose new <code>/health/services</code> endpoint. <a href="/docs/proxy/health#healthservices">Start Here</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance--reliability-improvements">Performance / Reliability improvements<a href="#performance--reliability-improvements" class="hash-link" aria-label="Performance / Reliability improvements" title="Performance / Reliability improvements"></a></h2>
<ol>
<li>3x increase in RPS - moving to orjson for reading request body</li>
<li>LLM Routing speedup - using cached get model group info</li>
<li>SDK speedup - using cached get model info helper - reduces CPU work to get model info</li>
<li>Proxy speedup - only read request body 1 time per request</li>
<li>Infinite loop detection scripts added to codebase</li>
<li>Bedrock - pure async image transformation requests</li>
<li>Cooldowns - single deployment model group if 100% calls fail in high traffic - prevents an o1 outage from impacting other calls</li>
<li>Response Headers - return<!-- -->
<ol>
<li><code>x-litellm-timeout</code></li>
<li><code>x-litellm-attempted-retries</code></li>
<li><code>x-litellm-overhead-duration-ms</code></li>
<li><code>x-litellm-response-duration-ms</code></li>
</ol>
</li>
<li>ensure duplicate callbacks are not added to proxy</li>
<li>Requirements.txt - bump certifi version</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<ol>
<li>JWT / OIDC Auth - new <code>enforce_rbac</code> param,allows proxy admin to prevent any unmapped yet authenticated jwt tokens from calling proxy. <a href="/docs/proxy/token_auth#enforce-role-based-access-control-rbac">Start Here</a></li>
<li>fix custom openapi schema generation for customized swaggers</li>
<li>Request Headers - support reading <code>x-litellm-timeout</code> param from request headers. Enables model timeout control when using Vercels AI SDK + LiteLLM Proxy. <a href="/docs/proxy/request_headers#litellm-headers">Start Here</a></li>
<li>JWT / OIDC Auth - new <code>role</code> based permissions for model authentication. <a href="https://docs.litellm.ai/docs/proxy/jwt_auth_arch" target="_blank" rel="noopener noreferrer">See Here</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="complete-git-diff">Complete Git Diff<a href="#complete-git-diff" class="hash-link" aria-label="Complete Git Diff" title="Complete Git Diff"></a></h2>
<p>This is the diff between v1.57.8-stable and v1.59.8-stable.</p>
<p>Use this to see the changes in the codebase.</p>
<p><a href="https://github.com/BerriAI/litellm/compare/v1.57.8-stable...v1.59.8-stable" target="_blank" rel="noopener noreferrer"><strong>Git Diff</strong></a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/admin-ui">admin ui</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/logging">logging</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/db-schema">db schema</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.59.0">v1.59.0</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-01-17T10:00:00.000Z">2025117</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&amp;v=beta&amp;t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span></div><div class="admonitionContent_BuS1"><p>Get a 7 day free trial for LiteLLM Enterprise <a href="https://litellm.ai/#trial" target="_blank" rel="noopener noreferrer">here</a>.</p><p><strong>no call needed</strong></p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ui-improvements">UI Improvements<a href="#ui-improvements" class="hash-link" aria-label="UI Improvements" title="UI Improvements"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="opt-in-admin-ui---view-messages--responses">[Opt In] Admin UI - view messages / responses<a href="#opt-in-admin-ui---view-messages--responses" class="hash-link" aria-label="[Opt In] Admin UI - view messages / responses" title="[Opt In] Admin UI - view messages / responses"></a></h3>
<p>You can now view messages and response logs on Admin UI.</p>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAGCAYAAAD68A/GAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAt0lEQVR4nDWN22rCQBRF57v7BUUkgYJfIFKLKV5ISpO2iKI++CG++BDjZZI5mTOrNNoN62HDZi9jX3s0PwluOUd2X8imQDbfyKpAlgUuz7AvMcauJwjgvEdUkRBQuKOKB27pFHNdj6k9eOdQ9YSgBH3QtgTAZgnmuh1jFVzTdA//+Rvw6PYzwdT7N1qgLCuk9XgNHZ1W5K4uJpjb4olTHlN+9CmziFMaU80jDrMBx3zEefXOZfjMLwxA2zip5XrxAAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="380"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/ui_logs.ef71671.640.png" srcset="/assets/ideal-img/ui_logs.ef71671.640.png 640w,/assets/ideal-img/ui_logs.a3e4f1c.1497.png 1497w" width="640" height="380"></noscript></div>
<p>How to enable it - add <code>store_prompts_in_spend_logs: true</code> to your <code>proxy_config.yaml</code></p>
<p>Once this flag is enabled, your <code>messages</code> and <code>responses</code> will be stored in the <code>LiteLLM_Spend_Logs</code> table.</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">general_settings</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">store_prompts_in_spend_logs</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token boolean important" style="color:#36acaa">true</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="db-schema-change">DB Schema Change<a href="#db-schema-change" class="hash-link" aria-label="DB Schema Change" title="DB Schema Change"></a></h2>
<p>Added <code>messages</code> and <code>responses</code> to the <code>LiteLLM_Spend_Logs</code> table.</p>
<p><strong>By default this is not logged.</strong> If you want <code>messages</code> and <code>responses</code> to be logged, you need to opt in with this setting</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">general_settings</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">store_prompts_in_spend_logs</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token boolean important" style="color:#36acaa">true</span><br></span></code></pre></div></div></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/admin-ui">admin ui</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/logging">logging</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/db-schema">db schema</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.57.8-stable">v1.57.8-stable</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-01-11T10:00:00.000Z">2025111</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&amp;v=beta&amp;t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p><code>alerting</code>, <code>prometheus</code>, <code>secret management</code>, <code>management endpoints</code>, <code>ui</code>, <code>prompt management</code>, <code>finetuning</code>, <code>batch</code></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new--updated-models">New / Updated Models<a href="#new--updated-models" class="hash-link" aria-label="New / Updated Models" title="New / Updated Models"></a></h2>
<ol>
<li>Mistral large pricing - <a href="https://github.com/BerriAI/litellm/pull/7452" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/7452</a></li>
<li>Cohere command-r7b-12-2024 pricing - <a href="https://github.com/BerriAI/litellm/pull/7553/files" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/7553/files</a></li>
<li>Voyage - new models, prices and context window information - <a href="https://github.com/BerriAI/litellm/pull/7472" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/7472</a></li>
<li>Anthropic - bump Bedrock claude-3-5-haiku max_output_tokens to 8192</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-proxy-improvements">General Proxy Improvements<a href="#general-proxy-improvements" class="hash-link" aria-label="General Proxy Improvements" title="General Proxy Improvements"></a></h2>
<ol>
<li>Health check support for realtime models</li>
<li>Support calling Azure realtime routes via virtual keys</li>
<li>Support custom tokenizer on <code>/utils/token_counter</code> - useful when checking token count for self-hosted models</li>
<li>Request Prioritization - support on <code>/v1/completion</code> endpoint as well</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llm-translation-improvements">LLM Translation Improvements<a href="#llm-translation-improvements" class="hash-link" aria-label="LLM Translation Improvements" title="LLM Translation Improvements"></a></h2>
<ol>
<li>Deepgram STT support. <a href="https://docs.litellm.ai/docs/providers/deepgram" target="_blank" rel="noopener noreferrer">Start Here</a></li>
<li>OpenAI Moderations - <code>omni-moderation-latest</code> support. <a href="https://docs.litellm.ai/docs/moderation" target="_blank" rel="noopener noreferrer">Start Here</a></li>
<li>Azure O1 - fake streaming support. This ensures if a <code>stream=true</code> is passed, the response is streamed. <a href="https://docs.litellm.ai/docs/providers/azure" target="_blank" rel="noopener noreferrer">Start Here</a></li>
<li>Anthropic - non-whitespace char stop sequence handling - <a href="https://github.com/BerriAI/litellm/pull/7484" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Azure OpenAI - support Entra ID username + password based auth. <a href="https://docs.litellm.ai/docs/providers/azure#entra-id---use-tenant_id-client_id-client_secret" target="_blank" rel="noopener noreferrer">Start Here</a></li>
<li>LM Studio - embedding route support. <a href="https://docs.litellm.ai/docs/providers/lm-studio" target="_blank" rel="noopener noreferrer">Start Here</a></li>
<li>WatsonX - ZenAPIKeyAuth support. <a href="https://docs.litellm.ai/docs/providers/watsonx" target="_blank" rel="noopener noreferrer">Start Here</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="prompt-management-improvements">Prompt Management Improvements<a href="#prompt-management-improvements" class="hash-link" aria-label="Prompt Management Improvements" title="Prompt Management Improvements"></a></h2>
<ol>
<li>Langfuse integration</li>
<li>HumanLoop integration</li>
<li>Support for using load balanced models</li>
<li>Support for loading optional params from prompt manager</li>
</ol>
<p><a href="https://docs.litellm.ai/docs/proxy/prompt_management" target="_blank" rel="noopener noreferrer">Start Here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="finetuning--batch-apis-improvements">Finetuning + Batch APIs Improvements<a href="#finetuning--batch-apis-improvements" class="hash-link" aria-label="Finetuning + Batch APIs Improvements" title="Finetuning + Batch APIs Improvements"></a></h2>
<ol>
<li>Improved unified endpoint support for Vertex AI finetuning - <a href="https://github.com/BerriAI/litellm/pull/7487" target="_blank" rel="noopener noreferrer">PR</a></li>
<li>Add support for retrieving vertex api batch jobs - <a href="https://github.com/BerriAI/litellm/commit/13f364682d28a5beb1eb1b57f07d83d5ef50cbdc" target="_blank" rel="noopener noreferrer">PR</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-alerting-integration"><em>NEW</em> Alerting Integration<a href="#new-alerting-integration" class="hash-link" aria-label="new-alerting-integration" title="new-alerting-integration"></a></h2>
<p>PagerDuty Alerting Integration.</p>
<p>Handles two types of alerts:</p>
<ul>
<li>High LLM API Failure Rate. Configure X fails in Y seconds to trigger an alert.</li>
<li>High Number of Hanging LLM Requests. Configure X hangs in Y seconds to trigger an alert.</li>
</ul>
<p><a href="https://docs.litellm.ai/docs/proxy/pagerduty" target="_blank" rel="noopener noreferrer">Start Here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="prometheus-improvements">Prometheus Improvements<a href="#prometheus-improvements" class="hash-link" aria-label="Prometheus Improvements" title="Prometheus Improvements"></a></h2>
<p>Added support for tracking latency/spend/tokens based on custom metrics. <a href="https://docs.litellm.ai/docs/proxy/prometheus#beta-custom-metrics" target="_blank" rel="noopener noreferrer">Start Here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-hashicorp-secret-manager-support"><em>NEW</em> Hashicorp Secret Manager Support<a href="#new-hashicorp-secret-manager-support" class="hash-link" aria-label="new-hashicorp-secret-manager-support" title="new-hashicorp-secret-manager-support"></a></h2>
<p>Support for reading credentials + writing LLM API keys. <a href="https://docs.litellm.ai/docs/secret#hashicorp-vault" target="_blank" rel="noopener noreferrer">Start Here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="management-endpoints--ui-improvements">Management Endpoints / UI Improvements<a href="#management-endpoints--ui-improvements" class="hash-link" aria-label="Management Endpoints / UI Improvements" title="Management Endpoints / UI Improvements"></a></h2>
<ol>
<li>Create and view organizations + assign org admins on the Proxy UI</li>
<li>Support deleting keys by key_alias</li>
<li>Allow assigning teams to org on UI</li>
<li>Disable using ui session token for &#x27;test key&#x27; pane</li>
<li>Show model used in &#x27;test key&#x27; pane</li>
<li>Support markdown output in &#x27;test key&#x27; pane</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="helm-improvements">Helm Improvements<a href="#helm-improvements" class="hash-link" aria-label="Helm Improvements" title="Helm Improvements"></a></h2>
<ol>
<li>Prevent istio injection for db migrations cron job</li>
<li>allow using migrationJob.enabled variable within job</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging-improvements">Logging Improvements<a href="#logging-improvements" class="hash-link" aria-label="Logging Improvements" title="Logging Improvements"></a></h2>
<ol>
<li>braintrust logging: respect project_id, add more metrics  - <a href="https://github.com/BerriAI/litellm/pull/7613" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/7613</a></li>
<li>Athina - support base url - <code>ATHINA_BASE_URL</code></li>
<li>Lunary - Allow passing custom parent run id to LLM Calls</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="git-diff">Git Diff<a href="#git-diff" class="hash-link" aria-label="Git Diff" title="Git Diff"></a></h2>
<p>This is the diff between v1.56.3-stable and v1.57.8-stable.</p>
<p>Use this to see the changes in the codebase.</p>
<p><a href="https://github.com/BerriAI/litellm/compare/v1.56.3-stable...189b67760011ea313ca58b1f8bd43aa74fbd7f55" target="_blank" rel="noopener noreferrer">Git Diff</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/langfuse">langfuse</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/humanloop">humanloop</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/alerting">alerting</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/prometheus">prometheus</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/secret-management">secret management</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/management-endpoints">management endpoints</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/ui">ui</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/prompt-management">prompt management</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/finetuning">finetuning</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/batch">batch</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.57.7">v1.57.7</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-01-10T10:00:00.000Z">2025110</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&amp;v=beta&amp;t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p><code>langfuse</code>, <code>management endpoints</code>, <code>ui</code>, <code>prometheus</code>, <code>secret management</code></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="langfuse-prompt-management">Langfuse Prompt Management<a href="#langfuse-prompt-management" class="hash-link" aria-label="Langfuse Prompt Management" title="Langfuse Prompt Management"></a></h2>
<p>Langfuse Prompt Management is being labelled as BETA. This allows us to iterate quickly on the feedback we&#x27;re receiving, and making the status clearer to users. We expect to make this feature to be stable by next month (February 2025).</p>
<p>Changes:</p>
<ul>
<li>Include the client message in the LLM API Request. (Previously only the prompt template was sent, and the client message was ignored).</li>
<li>Log the prompt template in the logged request (e.g. to s3/langfuse).</li>
<li>Log the &#x27;prompt_id&#x27; and &#x27;prompt_variables&#x27; in the logged request (e.g. to s3/langfuse).</li>
</ul>
<p><a href="https://docs.litellm.ai/docs/proxy/prompt_management" target="_blank" rel="noopener noreferrer">Start Here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="teamorganization-management--ui-improvements">Team/Organization Management + UI Improvements<a href="#teamorganization-management--ui-improvements" class="hash-link" aria-label="Team/Organization Management + UI Improvements" title="Team/Organization Management + UI Improvements"></a></h2>
<p>Managing teams and organizations on the UI is now easier.</p>
<p>Changes:</p>
<ul>
<li>Support for editing user role within team on UI.</li>
<li>Support updating team member role to admin via api - <code>/team/member_update</code></li>
<li>Show team admins all keys for their team.</li>
<li>Add organizations with budgets</li>
<li>Assign teams to orgs on the UI</li>
<li>Auto-assign SSO users to teams</li>
</ul>
<p><a href="https://docs.litellm.ai/docs/proxy/self_serve" target="_blank" rel="noopener noreferrer">Start Here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="hashicorp-vault-support">Hashicorp Vault Support<a href="#hashicorp-vault-support" class="hash-link" aria-label="Hashicorp Vault Support" title="Hashicorp Vault Support"></a></h2>
<p>We now support writing LiteLLM Virtual API keys to Hashicorp Vault.</p>
<p><a href="https://docs.litellm.ai/docs/proxy/vault" target="_blank" rel="noopener noreferrer">Start Here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="custom-prometheus-metrics">Custom Prometheus Metrics<a href="#custom-prometheus-metrics" class="hash-link" aria-label="Custom Prometheus Metrics" title="Custom Prometheus Metrics"></a></h2>
<p>Define custom prometheus metrics, and track usage/latency/no. of requests against them</p>
<p>This allows for more fine-grained tracking - e.g. on prompt template passed in request metadata</p>
<p><a href="https://docs.litellm.ai/docs/proxy/prometheus#beta-custom-metrics" target="_blank" rel="noopener noreferrer">Start Here</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/langfuse">langfuse</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/management-endpoints">management endpoints</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/ui">ui</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/prometheus">prometheus</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/secret-management">secret management</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.57.3">v1.57.3 - New Base Docker Image</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-01-08T10:00:00.000Z">202518</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&amp;v=beta&amp;t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p><code>docker image</code>, <code>security</code>, <code>vulnerability</code></p>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAkUlEQVR4nE2NQQuCQBhE/f/3fk7XCEzpkBehQAp3rSVdNl0/dvWFRtTchnnMSwDGAG0/8xJoreNxV2itqOsaEVkQkhWUQOuE7iV0tudYGlTzxBjD4P0H9KNwKgry7ECepTSmY7MNnK+W/S6lvFTEGEnmeSKEsJZFMwye6rY8jcQYiNP0U39jrUVrjVY1zrn/iTczXL7nlnaTRgAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="320"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/security.41d05b0.640.png" srcset="/assets/ideal-img/security.41d05b0.640.png 640w,/assets/ideal-img/security.8eb0218.1200.png 1200w" width="640" height="320"></noscript></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-changed">What changed?<a href="#what-changed" class="hash-link" aria-label="What changed?" title="What changed?"></a></h2>
<ul>
<li>LiteLLMBase image now uses <code>cgr.dev/chainguard/python:latest-dev</code></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-the-change">Why the change?<a href="#why-the-change" class="hash-link" aria-label="Why the change?" title="Why the change?"></a></h2>
<p>To ensure there are 0 critical/high vulnerabilities on LiteLLM Docker Image</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="migration-guide">Migration Guide<a href="#migration-guide" class="hash-link" aria-label="Migration Guide" title="Migration Guide"></a></h2>
<ul>
<li>If you use a custom dockerfile with litellm as a base image + <code>apt-get</code></li>
</ul>
<p>Instead of <code>apt-get</code> use <code>apk</code>, the base litellm image will no longer have <code>apt-get</code> installed.</p>
<p><strong>You are only impacted if you use <code>apt-get</code> in your Dockerfile</strong></p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># Use the provided base image</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">FROM docker.litellm.ai/berriai/litellm:main-latest</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Set the working directory</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">WORKDIR /app</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Install dependencies - CHANGE THIS to `apk`</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">RUN apt-get update &amp;&amp; apt-get install -y dumb-init </span><br></span></code></pre></div></div>
<p>Before Change</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">RUN apt-get update &amp;&amp; apt-get install -y dumb-init</span><br></span></code></pre></div></div>
<p>After Change</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">RUN apk update &amp;&amp; apk add --no-cache dumb-init</span><br></span></code></pre></div></div></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/docker-image">docker image</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/security">security</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/vulnerability">vulnerability</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.56.4">v1.56.4</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-12-29T10:00:00.000Z">20241229</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&amp;v=beta&amp;t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p><code>deepgram</code>, <code>fireworks ai</code>, <code>vision</code>, <code>admin ui</code>, <code>dependency upgrades</code></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models">New Models<a href="#new-models" class="hash-link" aria-label="New Models" title="New Models"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="deepgram-speech-to-text"><strong>Deepgram Speech to Text</strong><a href="#deepgram-speech-to-text" class="hash-link" aria-label="deepgram-speech-to-text" title="deepgram-speech-to-text"></a></h3>
<p>New Speech to Text support for Deepgram models. <a href="https://docs.litellm.ai/docs/providers/deepgram" target="_blank" rel="noopener noreferrer"><strong>Start Here</strong></a></p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> litellm </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> transcription</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> os </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># set api keys </span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">os</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">environ</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&quot;DEEPGRAM_API_KEY&quot;</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">audio_file </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token builtin">open</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;/path/to/audio.mp3&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;rb&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">response </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> transcription</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;deepgram/nova-2&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token builtin">file</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">audio_file</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string-interpolation string" style="color:#e3116c">f&quot;response: </span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">response</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">&quot;</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="fireworks-ai---vision-support-for-all-models"><strong>Fireworks AI - Vision</strong> support for all models<a href="#fireworks-ai---vision-support-for-all-models" class="hash-link" aria-label="fireworks-ai---vision-support-for-all-models" title="fireworks-ai---vision-support-for-all-models"></a></h3>
<p>LiteLLM supports document inlining for Fireworks AI models. This is useful for models that are not vision models, but still need to parse documents/images/etc.
LiteLLM will add <code>#transform=inline</code> to the url of the image_url, if the model is not a vision model <a href="https://github.com/BerriAI/litellm/blob/1ae9d45798bdaf8450f2dfdec703369f3d2212b7/litellm/llms/fireworks_ai/chat/transformation.py#L114" target="_blank" rel="noopener noreferrer">See Code</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="proxy-admin-ui">Proxy Admin UI<a href="#proxy-admin-ui" class="hash-link" aria-label="Proxy Admin UI" title="Proxy Admin UI"></a></h2>
<ul>
<li><code>Test Key</code> Tab displays <code>model</code> used in response</li>
</ul>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAn0lEQVR4nF2LwQqCYBCE/yeIjvUKduotkroX+p5eVAR7BqOEoEsR2UHY3/1Xtv03lAhq4JthYMao6kyV10QcElHIxCHzlxUzb6y1c9M0EInoIBER9d6P5ZOvwQAgNu25jvR+U71exHcwjtT7nyEixqY9VltbH9Seqg6fD4eOHCI6InJ933fDAQB2JkmSyT7PF1mWBf+UZRmkabosimL6Bg2jp6dF/x1GAAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="311"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/ui_model.b7f05bf.640.png" srcset="/assets/ideal-img/ui_model.b7f05bf.640.png 640w,/assets/ideal-img/ui_model.ff3e711.1920.png 1920w" width="640" height="311"></noscript></div>
<ul>
<li><code>Test Key</code> Tab renders content in <code>.md</code>, <code>.py</code> (any code/markdown format)</li>
</ul>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAGCAYAAAD68A/GAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAkklEQVR4nG2N4QqCQBCE7zWi/z6PBEoPYuBbnhgePYZ257nujbvhVVDRwDfLDgNjpmkqiLlVRcPMlx0AbxoALREVJkZfq6rGqLLfH+XMe1+bcRwrEdUQYiJaQesKZgZzQgLSq1iZ3fIzh22hRQDItj0REXwW8/Q9zMLI+f9p59zBOXeyti+H4Vb2/Tdddz1ba48PkLTUml8MaHoAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="403"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/ui_format.1954430.640.png" srcset="/assets/ideal-img/ui_format.1954430.640.png 640w,/assets/ideal-img/ui_format.213bf0f.1920.png 1920w" width="640" height="403"></noscript></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="dependency-upgrades">Dependency Upgrades<a href="#dependency-upgrades" class="hash-link" aria-label="Dependency Upgrades" title="Dependency Upgrades"></a></h2>
<ul>
<li>(Security fix) Upgrade to <code>fastapi==0.115.5</code> <a href="https://github.com/BerriAI/litellm/pull/7447" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/7447</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fixes">Bug Fixes<a href="#bug-fixes" class="hash-link" aria-label="Bug Fixes" title="Bug Fixes"></a></h2>
<ul>
<li>Add health check support for realtime models <a href="https://docs.litellm.ai/docs/proxy/health#realtime-models" target="_blank" rel="noopener noreferrer">Here</a></li>
<li>Health check error with audio_transcription model <a href="https://github.com/BerriAI/litellm/issues/5999" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/issues/5999</a></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/deepgram">deepgram</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/fireworks-ai">fireworks ai</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/vision">vision</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/admin-ui">admin ui</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/dependency-upgrades">dependency upgrades</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.56.3">v1.56.3</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-12-28T10:00:00.000Z">20241228</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&amp;v=beta&amp;t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p><code>guardrails</code>, <code>logging</code>, <code>virtual key management</code>, <code>new models</code></p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span></div><div class="admonitionContent_BuS1"><p>Get a 7 day free trial for LiteLLM Enterprise <a href="https://litellm.ai/#trial" target="_blank" rel="noopener noreferrer">here</a>.</p><p><strong>no call needed</strong></p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-features">New Features<a href="#new-features" class="hash-link" aria-label="New Features" title="New Features"></a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="-log-guardrail-traces"> Log Guardrail Traces<a href="#-log-guardrail-traces" class="hash-link" aria-label=" Log Guardrail Traces" title=" Log Guardrail Traces"></a></h3>
<p>Track guardrail failure rate and if a guardrail is going rogue and failing requests. <a href="https://docs.litellm.ai/docs/proxy/guardrails/quick_start" target="_blank" rel="noopener noreferrer">Start here</a></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="traced-guardrail-success">Traced Guardrail Success<a href="#traced-guardrail-success" class="hash-link" aria-label="Traced Guardrail Success" title="Traced Guardrail Success"></a></h4>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAICAYAAADA+m62AAAACXBIWXMAABYlAAAWJQFJUiTwAAAAx0lEQVR4nI2QTQrCMBCFewURXVXv7a7iyk0v0aXdSIRC6crGGKFVY2PT/ExGmoprH7yZgfl4AxNF/4oxthi0Pggpi2EwpFfqBAAnYwxRShda20NVVcuoLMvYGNODx1HeTz3MYwGAd57n6wAqreWztygH4ywihDWAG0HnXEcIWUXHso7F/dEJxlFwZm3LnZ8g+018TWDTxPre9CieCLLz2HJEG5jf6QBmWTa7Xa+b85nuLnWdXCjdcs5HJ4yxPaV0l6bp/O/vfAANBdR566xfGwAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="487"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/gd_success.2858f5c.640.png" srcset="/assets/ideal-img/gd_success.2858f5c.640.png 640w,/assets/ideal-img/gd_success.02a2daf.1862.png 1862w" width="640" height="487"></noscript></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="traced-guardrail-failure">Traced Guardrail Failure<a href="#traced-guardrail-failure" class="hash-link" aria-label="Traced Guardrail Failure" title="Traced Guardrail Failure"></a></h4>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAJCAYAAAALpr0TAAAACXBIWXMAABYlAAAWJQFJUiTwAAABTElEQVR4nC3KsWrCQByA8aN27FYsGqpTZ2kcO3fwBXwM925OFXwRFxFSh0yCS7s1IDSakiAqQQ1GczHJXS6Xy/2L0uE3fR8CgNJkMmmMxuPmYDhQNU1TdV1XNV1XR+NRc/gxbEyn01u0WCyqnOckSQnEhEiSpkAZA5ZxmfIMKEsj27YfkWEYCs8ymhAGCWEyzwVc8FxIAADOeWyaZv06ZllGmZAQUSa54JcOAMV1FIWIHMepoU/DUHZ+SBPXhdB1JfNckL4Hhe9J8PcgTofY3AZ1ZFgbhZ7jRHg7YDwX2D8WZ4yLEIeCno4g9tvzD6U1ZG02iqCEAj5CASCTlAFhGcSMS8FSEN42NoOgjr5ms4cYBxa25u7RsVcn21pj21qHS2cV2fNdtLR/vw+HKkII3bz1evedVqvcqdyV35+f/lXK/dcXpd/tKu12u/QHtJEbwYpEFhUAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="593"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/gd_fail.97b85e3.640.png" srcset="/assets/ideal-img/gd_fail.97b85e3.640.png 640w,/assets/ideal-img/gd_fail.ab3b6b8.1848.png 1848w" width="640" height="593"></noscript></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="guardrailslist"><code>/guardrails/list</code><a href="#guardrailslist" class="hash-link" aria-label="guardrailslist" title="guardrailslist"></a></h3>
<p><code>/guardrails/list</code> allows clients to view available guardrails + supported guardrail params</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">curl -X GET &#x27;http://0.0.0.0:4000/guardrails/list&#x27;</span><br></span></code></pre></div></div>
<p>Expected response</p>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">&quot;guardrails&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;guardrail_name&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;aporia-post-guard&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;guardrail_info&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token property" style="color:#36acaa">&quot;params&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token property" style="color:#36acaa">&quot;name&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;toxicity_score&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token property" style="color:#36acaa">&quot;type&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;float&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token property" style="color:#36acaa">&quot;description&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Score between 0-1 indicating content toxicity level&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token property" style="color:#36acaa">&quot;name&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;pii_detection&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token property" style="color:#36acaa">&quot;type&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;boolean&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="-guardrails-with-mock-llm"> Guardrails with Mock LLM<a href="#-guardrails-with-mock-llm" class="hash-link" aria-label=" Guardrails with Mock LLM" title=" Guardrails with Mock LLM"></a></h3>
<p>Send <code>mock_response</code> to test guardrails without making an LLM call. More info on <code>mock_response</code> <a href="https://docs.litellm.ai/docs/proxy/guardrails/quick_start" target="_blank" rel="noopener noreferrer">here</a></p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">curl -i http://localhost:4000/v1/chat/completions \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -H &quot;Content-Type: application/json&quot; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -H &quot;Authorization: Bearer sk-npnwjPQciVRok5yNZgKmFQ&quot; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -d &#x27;{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;messages&quot;: [</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;hi my email is ishaan@berri.ai&quot;}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;mock_response&quot;: &quot;This is a mock response&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;guardrails&quot;: [&quot;aporia-pre-guard&quot;, &quot;aporia-post-guard&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }&#x27;</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="assign-keys-to-users">Assign Keys to Users<a href="#assign-keys-to-users" class="hash-link" aria-label="Assign Keys to Users" title="Assign Keys to Users"></a></h3>
<p>You can now assign keys to users via Proxy UI</p>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAGCAYAAAD68A/GAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAyklEQVR4nDWNQYvCMBCF84f1JL2sxWSHvbjQezGWehFUFksbuyh6L5Ie+jcEjwVDM5llujjw3rvMe59QSkVpmpqmaXZt2x6stYe7tWN2XbfL89wAwExIKU1d10REno0vsAI7+ev1RgBfhVhIWRZFQYjoQkDf9/2o18tx0e33hibTjx9erIypuD3w0DD4gEgBEXlyKMtfiqL4KObzT3M61eS998gf/0gmjOjL5UwA6ijiWG2Xy2+3WumH1vqp1+tnlmVvPZIkcQCw+QMUDL6dseXY9AAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="380"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/ui_key.1a10687.640.png" srcset="/assets/ideal-img/ui_key.1a10687.640.png 640w,/assets/ideal-img/ui_key.9642332.1212.png 1212w" width="640" height="380"></noscript></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-models">New Models<a href="#new-models" class="hash-link" aria-label="New Models" title="New Models"></a></h2>
<ul>
<li><code>openrouter/openai/o1</code></li>
<li><code>vertex_ai/mistral-large@2411</code></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="fixes">Fixes<a href="#fixes" class="hash-link" aria-label="Fixes" title="Fixes"></a></h2>
<ul>
<li>Fix <code>vertex_ai/</code> mistral model pricing: <a href="https://github.com/BerriAI/litellm/pull/7345" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/7345</a></li>
<li>Missing model_group field in logs for aspeech call types <a href="https://github.com/BerriAI/litellm/pull/7392" target="_blank" rel="noopener noreferrer">https://github.com/BerriAI/litellm/pull/7392</a></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/guardrails">guardrails</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/logging">logging</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/virtual-key-management">virtual key management</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/new-models">new models</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.56.1">v1.56.1</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-12-27T10:00:00.000Z">20241227</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&amp;v=beta&amp;t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p><code>key management</code>, <code>budgets/rate limits</code>, <code>logging</code>, <code>guardrails</code></p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span></div><div class="admonitionContent_BuS1"><p>Get a 7 day free trial for LiteLLM Enterprise <a href="https://litellm.ai/#trial" target="_blank" rel="noopener noreferrer">here</a>.</p><p><strong>no call needed</strong></p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-budget--rate-limit-tiers"> Budget / Rate Limit Tiers<a href="#-budget--rate-limit-tiers" class="hash-link" aria-label=" Budget / Rate Limit Tiers" title=" Budget / Rate Limit Tiers"></a></h2>
<p>Define tiers with rate limits. Assign them to keys.</p>
<p>Use this to control access and budgets across a lot of keys.</p>
<p><strong><a href="https://docs.litellm.ai/docs/proxy/rate_limit_tiers" target="_blank" rel="noopener noreferrer">Start here</a></strong></p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">curl -L -X POST &#x27;http://0.0.0.0:4000/budget/new&#x27; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-H &#x27;Authorization: Bearer sk-1234&#x27; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-H &#x27;Content-Type: application/json&#x27; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-d &#x27;{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;budget_id&quot;: &quot;high-usage-tier&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;model_max_budget&quot;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        &quot;gpt-4o&quot;: {&quot;rpm_limit&quot;: 1000000}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}&#x27;</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="otel-bug-fix">OTEL Bug Fix<a href="#otel-bug-fix" class="hash-link" aria-label="OTEL Bug Fix" title="OTEL Bug Fix"></a></h2>
<p>LiteLLM was double logging litellm_request span. This is now fixed.</p>
<p><a href="https://github.com/BerriAI/litellm/pull/7435" target="_blank" rel="noopener noreferrer">Relevant PR</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging-for-finetuning-endpoints">Logging for Finetuning Endpoints<a href="#logging-for-finetuning-endpoints" class="hash-link" aria-label="Logging for Finetuning Endpoints" title="Logging for Finetuning Endpoints"></a></h2>
<p>Logs for finetuning requests are now available on all logging providers (e.g. Datadog).</p>
<p>What&#x27;s logged per request:</p>
<ul>
<li>file_id</li>
<li>finetuning_job_id</li>
<li>any key/team metadata</li>
</ul>
<p><strong>Start Here:</strong></p>
<ul>
<li><a href="https://docs.litellm.ai/docs/fine_tuning" target="_blank" rel="noopener noreferrer">Setup Finetuning</a></li>
<li><a href="https://docs.litellm.ai/docs/proxy/logging#datadog" target="_blank" rel="noopener noreferrer">Setup Logging</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="dynamic-params-for-guardrails">Dynamic Params for Guardrails<a href="#dynamic-params-for-guardrails" class="hash-link" aria-label="Dynamic Params for Guardrails" title="Dynamic Params for Guardrails"></a></h2>
<p>You can now set custom parameters (like success threshold) for your guardrails in each request.</p>
<p><a href="https://docs.litellm.ai/docs/proxy/guardrails/custom_guardrail#-pass-additional-parameters-to-guardrail" target="_blank" rel="noopener noreferrer">See guardrails spec for more details</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/key-management">key management</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/budgets-rate-limits">budgets/rate limits</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/logging">logging</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/guardrails">guardrails</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.55.10">v1.55.10</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-12-24T10:00:00.000Z">20241224</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&amp;v=beta&amp;t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p><code>batches</code>, <code>guardrails</code>, <code>team management</code>, <code>custom auth</code></p>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAGCAYAAAD68A/GAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAmElEQVR4nI2OSwrCMBRFk6YvpqnW1JDo1E4DBvwMMhQEQUFckVtyFa7rSqLFqYMzeJzD5THGGDJaayy9R2d6NJ1BayxINcUVOOcgohL2xqCdzlBPVIm35zvmbvUJhRBIKSHGiBACvPdFqM7g8Xxhc7yMqww5ds7B2gWqShQxDGucrjfs9gdw/g0z+YXMeFNNkESQUv4W/+ENHBNF9u/fUmoAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="360"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/batches_cost_tracking.d076e78.640.png" srcset="/assets/ideal-img/batches_cost_tracking.d076e78.640.png 640w,/assets/ideal-img/batches_cost_tracking.8fc9663.1208.png 1208w" width="640" height="360"></noscript></div>
<br>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span></div><div class="admonitionContent_BuS1"><p>Get a free 7-day LiteLLM Enterprise trial here. <a href="https://www.litellm.ai/enterprise#trial" target="_blank" rel="noopener noreferrer">Start here</a></p><p><strong>No call needed</strong></p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-cost-tracking-logging-for-batches-api-batches"> Cost Tracking, Logging for Batches API (<code>/batches</code>)<a href="#-cost-tracking-logging-for-batches-api-batches" class="hash-link" aria-label="-cost-tracking-logging-for-batches-api-batches" title="-cost-tracking-logging-for-batches-api-batches"></a></h2>
<p>Track cost, usage for Batch Creation Jobs. <a href="https://docs.litellm.ai/docs/batches" target="_blank" rel="noopener noreferrer">Start here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-guardrailslist-endpoint"> <code>/guardrails/list</code> endpoint<a href="#-guardrailslist-endpoint" class="hash-link" aria-label="-guardrailslist-endpoint" title="-guardrailslist-endpoint"></a></h2>
<p>Show available guardrails to users. <a href="https://litellm-api.up.railway.app/#/Guardrails" target="_blank" rel="noopener noreferrer">Start here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-allow-teams-to-add-models"> Allow teams to add models<a href="#-allow-teams-to-add-models" class="hash-link" aria-label=" Allow teams to add models" title=" Allow teams to add models"></a></h2>
<p>This enables team admins to call their own finetuned models via litellm proxy. <a href="https://docs.litellm.ai/docs/proxy/team_model_add" target="_blank" rel="noopener noreferrer">Start here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-common-checks-for-custom-auth"> Common checks for custom auth<a href="#-common-checks-for-custom-auth" class="hash-link" aria-label=" Common checks for custom auth" title=" Common checks for custom auth"></a></h2>
<p>Calling the internal common_checks function in custom auth is now enforced as an enterprise feature. This allows admins to use litellm&#x27;s default budget/auth checks within their custom auth implementation. <a href="https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth" target="_blank" rel="noopener noreferrer">Start here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-assigning-team-admins"> Assigning team admins<a href="#-assigning-team-admins" class="hash-link" aria-label=" Assigning team admins" title=" Assigning team admins"></a></h2>
<p>Team admins is graduating from beta and moving to our enterprise tier. This allows proxy admins to allow others to manage keys/models for their own teams (useful for projects in production). <a href="https://docs.litellm.ai/docs/proxy/virtual_keys#restricting-key-generation" target="_blank" rel="noopener noreferrer">Start here</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/batches">batches</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/guardrails">guardrails</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/team-management">team management</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/custom-auth">custom auth</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/release_notes/v1.55.8-stable">v1.55.8-stable</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-12-22T10:00:00.000Z">20241222</time></div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGrlsJ3aqpHmQ/profile-displayphoto-shrink_400_400/B4DZSAzgP7HYAg-/0/1737327772964?e=1749686400&amp;v=beta&amp;t=Hkl3U8Ps0VtvNxX0BNNq24b4dtX5wQaPFp6oiKCIHD8" alt="Krrish Dholakia"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/krish-d/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Krrish Dholakia</span></a></div><small class="authorTitle_nd0D" title="CEO, LiteLLM">CEO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://media.licdn.com/dms/image/v2/D4D03AQGiM7ZrUwqu_Q/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1675971026692?e=1741824000&amp;v=beta&amp;t=eQnRdXPJo4eiINWTZARoYTfqh064pgZ-E21pQTSy8jc" alt="Ishaan Jaffer"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://www.linkedin.com/in/reffajnaahsi/" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Ishaan Jaffer</span></a></div><small class="authorTitle_nd0D" title="CTO, LiteLLM">CTO, LiteLLM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div class="markdown"><p>A new LiteLLM Stable release <a href="https://github.com/BerriAI/litellm/releases/tag/v1.55.8-stable" target="_blank" rel="noopener noreferrer">just went out</a>. Here are 5 updates since v1.52.2-stable.</p>
<p><code>langfuse</code>, <code>fallbacks</code>, <code>new models</code>, <code>azure_storage</code></p>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAGCAYAAAD68A/GAAAACXBIWXMAACxLAAAsSwGlPZapAAAAeUlEQVR4nH2OwQrDIBBEd9WKQg+abk45pCgI+v+/l0zJNodQSA+PYYeZZYiIdiLCFT5ghjFGlZk3+g3dYq3FSwQpJYQQ8LAOy3NCKQW9d9Ra0VoDee+xrm89RETbc5o0MMbQQs758L97ruqc0+8xRtXTp/0c/G/j9gH/VkGXlU4ZhAAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="360"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/langfuse_prmpt_mgmt.643b38a.640.png" srcset="/assets/ideal-img/langfuse_prmpt_mgmt.643b38a.640.png 640w,/assets/ideal-img/langfuse_prmpt_mgmt.cd0c83c.1920.png 1920w" width="640" height="360"></noscript></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="langfuse-prompt-management">Langfuse Prompt Management<a href="#langfuse-prompt-management" class="hash-link" aria-label="Langfuse Prompt Management" title="Langfuse Prompt Management"></a></h2>
<p>This makes it easy to run experiments or change the specific models <code>gpt-4o</code> to <code>gpt-4o-mini</code> on Langfuse, instead of making changes in your applications. <a href="https://docs.litellm.ai/docs/proxy/prompt_management" target="_blank" rel="noopener noreferrer">Start here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="control-fallback-prompts-client-side">Control fallback prompts client-side<a href="#control-fallback-prompts-client-side" class="hash-link" aria-label="Control fallback prompts client-side" title="Control fallback prompts client-side"></a></h2>
<blockquote>
<p>Claude prompts are different than OpenAI</p>
</blockquote>
<p>Pass in prompts specific to model when doing fallbacks. <a href="https://docs.litellm.ai/docs/proxy/reliability#control-fallback-prompts" target="_blank" rel="noopener noreferrer">Start here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-providers--models">New Providers / Models<a href="#new-providers--models" class="hash-link" aria-label="New Providers / Models" title="New Providers / Models"></a></h2>
<ul>
<li><a href="https://developer.nvidia.com/triton-inference-server" target="_blank" rel="noopener noreferrer">NVIDIA Triton</a> <code>/infer</code> endpoint. <a href="https://docs.litellm.ai/docs/providers/triton-inference-server" target="_blank" rel="noopener noreferrer">Start here</a></li>
<li><a href="https://github.com/michaelfeil/infinity" target="_blank" rel="noopener noreferrer">Infinity</a> Rerank Models <a href="https://docs.litellm.ai/docs/providers/infinity" target="_blank" rel="noopener noreferrer">Start here</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-azure-data-lake-storage-support"> Azure Data Lake Storage Support<a href="#-azure-data-lake-storage-support" class="hash-link" aria-label=" Azure Data Lake Storage Support" title=" Azure Data Lake Storage Support"></a></h2>
<p>Send LLM usage (spend, tokens) data to <a href="https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction" target="_blank" rel="noopener noreferrer">Azure Data Lake</a>. This makes it easy to consume usage data on other services (eg. Databricks)
<a href="https://docs.litellm.ai/docs/proxy/logging#azure-blob-storage" target="_blank" rel="noopener noreferrer">Start here</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="docker-run-litellm">Docker Run LiteLLM<a href="#docker-run-litellm" class="hash-link" aria-label="Docker Run LiteLLM" title="Docker Run LiteLLM"></a></h2>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-e STORE_MODEL_IN_DB=True \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-p 4000:4000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker.litellm.ai/berriai/litellm:litellm_stable_release_branch-v1.55.8-stable</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="get-daily-updates">Get Daily Updates<a href="#get-daily-updates" class="hash-link" aria-label="Get Daily Updates" title="Get Daily Updates"></a></h2>
<p>LiteLLM ships new releases every day. <a href="https://www.linkedin.com/company/berri-ai/" target="_blank" rel="noopener noreferrer">Follow us on LinkedIn</a> to get daily updates.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b></b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/langfuse">langfuse</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/fallbacks">fallbacks</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/new-models">new models</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/release_notes/tags/azure-storage">azure_storage</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label=""></nav></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://docs.litellm.ai/docs/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Getting Started</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://discord.com/invite/wuPM9dRgDw" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://twitter.com/LiteLLM" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/BerriAI/litellm/" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright  2026 liteLLM</div></div></div></footer><div id="inkeep-shadowradix-_R_p_" style="display:contents"></div><span></span></div>
</body>
</html>